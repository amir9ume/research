sid,title,abstract
2,High-Dimensional Feature Selection by Kernel-Based Feature-Wise Non-Linear Lasso,"The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features."
3,Context-Dependent Topic Modeling Using Multinomial Probit Random Effect Regression (MPR),"This paper presents the multinomial probit random effect regression (MPR) topic model that can incorporate document meta-data for topic inference. Document meta-data enter the topic model through regression covariates of a multinomial-probit-like setting that influence the prior of topic distribution. Both discrete and continuous variables can be included. The MPR contains document-specific random effects to allow flexible inference. We developed Gibbs sampling for the non-Dirichlet-conjugate setting. We demonstrate the utility of MPR by incorporating intra-day, weekly, and monthly cyclical patterns into topic models. Experimental results show that MPR can capture interesting short-term cyclical patterns in investor forum postings, newswires, and newspaper articles. Likelihood evaluation shows that MPR outperforms LDA in the three testbeds considered in this study."
4,Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information,"The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high non-linearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets."
5,A Covariate dependent HMM for learning hybrid dynamical models,"Our main contribution is a learning system that acquires an hybrid representation of a dynamical system. For this we extend hidden markov models by including a covariate in the transition model. We propose an efficient Gibbs sampler and show that we can reliably estimate such models compute the full posterior model thus having access also to the confidence on our model. In the work of \cite{ghahramani2000variational} we can see a general discussion of many different graphical models with different structures. Our model adds some extra complexity due to the dependency of the switch state transitions on the continuous variables.  This model does not only provide better prediction quality but it is easier to interpret and is suitable for control, filtering and planning that is grounded on the physical system dynamics and on the task goal. We will show a simple example of how planning can be achieved with such model."
6,Can We Recognize Tiger by Bus Images? ?Robust and Discriminative Self-Taught Image Categorization,"The lack of training data is a common challenge in many real-world image categorization problems, which is often tackled by semi-supervised learning or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught image categorization approach to utilize any unlabeled images (e.g., those randomly downloaded from Internet) without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled images. Because our new objective involves non-smooth terms in both the loss function and the regularization, it is difficult to solve in general. Thus, we derive an efficient iterative algorithm to solve the optimization problem, and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.  "
8,"Robust Linear Discriminant Analysis Using Ratio Minimization of $l_{1,2}$-Norms","Traditional Linear Discriminant Analysis (LDA) minimizes the ratio of squared $l_2$-norms, which is sensitive to outliers. In recent research, many $l_1$-norm based robust learning models were proposed. However, so far there is no existing work to utilize $l_1$-norm based objective for LDA, due to the difficulty of $l_1$-norm ratio optimization. Meanwhile, trivially replacing $l_2$-norms by $l_1$-norms in LDA objective introduces the $l_1$-norm maximization problem and doesn't provide the robustness. In this paper, we propose a novel robust LDA formulation based on the $l_{1,2}$-norm ratio minimization. Minimizing the $l_{1,2}$-norm ratio is a much more challenging problem than the traditional methods, and existing optimization algorithms cannot solve such a non-smooth terms ratio problem. We derive a new efficient algorithm to solve this challenging problem, and provide the theoretical analysis on the algorithm convergence. Our algorithm is easy to be implemented, and converges fast in practice with the same computational complexity as the trace ratio LDA. Extensive experiments on both synthetic data and nine real benchmark data sets show the effectiveness of the proposed robust LDA method."
11,A Near Neighbor Scoring Function Improves Error Bounds Based on Worst Likely Assignments,"Error bounds based on worst likely assignments operate in a transductive classification setting, where there are training examples with known class labels and the goal is to classify a set of working examples with known inputs and unknown class labels. These bounds have proven effective even for small data sets. This note describes a method to improve these bounds. The method evaluates potential assignments of class labels to working examples based on whether the assigned labels agree with labels on nearby training examples. The method shows the greatest improvement for validating accurate classifiers."
12,Efficient Local Image Description and Matching Based on Permutation Distances,"Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a randomsampling of pairwise comparisons of pixel intensities in an image patch.  Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a descriptor based on permutation distances between the ordering of intensities or RGB values between two patches. LUCID is computable in linear time of patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF yet up to five times more accuate, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster."
13,Clustering the Stochastic Block Model via Positions in the Network,"We consider the stochastic block model and analyze methods based on the positions of the nodes in the network.  This perspective was introduced by Burt (1976) and algorithmically amounts to embedding the graph by mapping the vertices to the corresponding rows of the adjacency matrix.  Once this is done, off-the-shelf methods for clustering points in Euclidean (or Hamming) space can be applied.  We study some popular dissimilarities in this context and provide theoretical guarantees for them that are sufficient for hierarchical clustering to succeed in correctly clustering the nodes.  The analysis is relatively simple in the context of a stochastic block model, yielding competitive performance bounds, particularly when the number of communities in the network grows with the number of nodes.  We evaluate some of these methods in some simulations, comparing them with spectral clustering.  "
15,Learning from Distributions via Support Measure Machines,"This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework."
16,Community Detection in the Labelled Stochastic Block Model,"We consider the problem of community detection from observed interactions between individuals, in the context where multiple types of interaction are possible. We use labelled stochastic block models to represent the observed data, where labels correspond to interaction types. Focusing on a two-community scenario, we formulate a conjecture that the detection task goes from infeasible to feasible as the average degree in the model crosses a particular threshold. To substantiate the conjecture, we prove that the given threshold correctly identifies a transition on the behaviour of belief propagation from insensitive to sensitive. We further prove that the same threshold corresponds to the transition in a related inference problem on a tree model from infeasible to feasible. Finally, numerical results using belief propagation for community detection give further support to the conjecture."
19,Multiscale Hidden Conditional Neural Fields with Adaptive-Rate Latent Variable Grouping,"We hypothesize that considering multiple levels of abstraction with adaptive-rate time quantization improves temporal sequence learning. To prove this empirically, we developed multiscale hidden conditional neural fields with adaptive-rate latent variable grouping. Our model is comprised of multiple layers, where each layer is recursively built from the preceding layer by aggregating observations that are similar in the latent space, representing the sequence at a coarser scale. We extract a nonlinear combination of features from grouped variables using a set of gate functions, learning the optimal abstraction of the sequence at each scale. This allows our model to learn higher level abstractions at ever more coarse-grained time scale as the layer gets higher. Optimization is performed layer-wise, making the complexity grow linearly with the number of layers. We evaluate our approach on three human activity datasets: ArmGesture, NATOPS, and Canal9. Our method achieves a near perfect recognition accuracy on the ArmGesture dataset, and outperforms all baseline models on the NATOPS and Canal9 dataset."
23,Jointly Segmenting Multiple Web Photo Streams,"As online sharing of personal photo streams is becoming popular and many of such photo streams often share overlapping contents, the cosegmentation can potentiate a wide range of intriguing Web applications. However, existing cosegmentation algorithms are still far limited for these new opportunities in that input images must be carefully prepared by human. In this paper, we address the problem of jointly segmenting an arbitrary number of unaligned and uncalibrated Web photo streams from multiple anonymous users. Given that the main difficulty of cosegmenting such photo streams lies in their extreme diversity in visual contents, we propose a multi-round segmentation algorithm that consists of the companion sampling  and cosegmentation  steps. Theoretically, we show that the developed method achieves the sublinear bound of regrets (i.e. the cumulative sum of difference between unknown ideal cosegmentation scores and actual scores). With experiments on more than 16K images of Flickr dataset and LabelMe dataset, wedemonstrate that our algorithm is more successful in both segmentation quality and scalability over other state-of-art methods."
24,Statistical Computations Underlying the Dynamics of Memory Updating,"Psychophysical and neurophysiological studies have suggested that the formation of memory traces is sensitive to the temporal structure of the environment. We present a statistical theory of memory formation in a dynamic environment, based on a nonparametric generalization of the switching Kalman filter. We show that this theory can account for existing data on the dynamics of memory updating, as well as the results of a new behavioral experiment. Our behavioral findings suggest that humans use temporal discontinuities in the structure of the environment to determine when to form new memory traces. The statistical perspective provides a coherent account of the conditions under which old memories are modified and new memories are formed."
25,Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery,"Given pairwise dissimilarities between data points, we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection. We formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming. The solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives. We obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives. When data points are distributed around multiple clusters according to the dissimilarities, we show that the data in each cluster select only representatives from that cluster. Unlike metric-based methods, our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality. We demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text."
26,Feature Clustering for Accelerating Parallel Coordinate Descent,"Large scale $\ell_1$-regularized loss minimization problemsarise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\ell_1$ regularizedproblems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\ell_1$-regularization problems."
28,Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA,"Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules in correspondence to components of a monosynaptic spinal loop. Results of emulated activities are shown. The paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions. "
29,Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition,We present data-driven techniques to augment Bag of Word (BoW) models within a Vector Space Model framework. This approach allows for more robust modeling and recognition of complex activities for instances where the structure and topology of the activities are not known a priori. Our approach addresses the limitation of standard BoW approaches that fail to represent the temporal information inherent in activity streams. We also introduce the use of randomly sampled Regular Expressions to capture the global structure of activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in three complex data-sets. 
30,From Sparsity to Compositionality: Unsupervised Learning of Active Basis Models for Image Representation,"Sparsity and compositionality are two fundamental concepts in image representation and vision. In this paper, we explore the transition between these two concepts. Specifically, we adopt the Olshausen-Field model where images are represented by linear superpositions of small sets of wavelet elements selected from a large dictionary. We seek to learn recurring compositional patterns of the wavelet elements in the sparse representations. We represent each compositional pattern by an active basis model, which is a composition of a small number of Gabor wavelets automatically selected from a dictionary of such wavelets. The selected wavelets are allowed to perturb their locations and orientations so that the linear basis formed by the selected wavelets become active and the active basis forms a deformable template. For a given set of training images, our method learns a vocabulary of such active basis templates, so that each training image can be represented by a small number of templates that are translated, rotated, scaled and deformed versions of the learned templates in the vocabulary. "
31,Stratified Sensor Network Calibration from TDOA Measurements,"This paper presents a study of the sensor network calibrationthat arise in TOA and TDOA measurements.Such calibration arise in several applications such ascalibration of (acoustic) microphone arrays, calibration ofwifi-transmittor arraysand radio antenna networks. There are at present no solution methodsfor the minimal cases. In the paper we improve on the currentstate-of-the-artby solving several new cases that are 'closer' to the minimal ones. We apply a three-step stratification process, (i) using a novel set ofrank constraintsto determine the unknown offsets, (ii) applying factorization techniquesto determinesenders and receivers up to unknown affine transformation and (iii) determiningthe affine stratification using the remaining constraints. For thetime-of-arrivalcase only steps (ii) and (iii) are needed.Experiments are shown both for simulated and real data with promising results.For simulated data we explore how sensitive the estimated parameters are withrespect to different degrees noise in the data and show that the proposed methods are numerically similar to previous methods. For real data, we test the method on several microphone and sound measurements and verify the results using computer vision based methods. "
32,Active Learning of Model Evidence Using Bayesian Quadrature ,"Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy."
33,A Unified 3D Scene Parsing Framework,"We propose a unified framework for parsing an image to predict the scene category, the 3D boundary of the space, camera parameters, and all objects in the scene, represented by their 3D bounding boxes and categories. Using a structural SVM, we build a complete end-to-end system which learns all parameters together in a single step. We encode many novel image features and context rules into the structural SVM feature function and our framework automatically weighs the relative importance of all these rules based on training data. By optimizing a unified objective function, we do not require extra training of other models, nor an additional fusion step. We design an intuitive web-based tool to annotate 3D bounding boxes for objects, build our ?SUN3D? database and demonstrate that our model outperforms the state-of-the-art algorithms on several individual subtasks."
35,Coupling Nonparametric Mixtures via Latent Dirichlet Processes,"Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Specifically, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures, allowing mixture models to be coupled more flexibly. In addition, we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling."
36,Shaping for a Differential Game of Guarding a Territory,This paper applies fuzzy reinforcement learning to a three-player differential game of guarding a territory. A shaping reward function is designed to help the defenders learn their Nash equilibrium strategies. Simulation results illustrate the performance of fuzzy actor-critic learning with and without the shaping reward function in a three-player differential game of guarding a territory.
37,Bayesian Nonparametric Maximum Margin Matrix Factorization for Collaborative Prediction,"We present a probabilistic formulation to max-margin matrix factorization and build accordingly an infinite nonparametric Bayesian model to automatically resolve the unknown number of latent factors. Our work demonstrates a successful example that integrates Bayesian nonparametrics and max-margin learning, which are conventionally two separate paradigms and enjoy complementary advantages. We develop an efficient variational learning algorithm for posterior inference, and our extensive empirical studies on large-scale MovieLens and EachMovie data sets appear to demonstrate the advantages inherited from both max-margin matrix factorization and Bayesian nonparametrics."
39,The Subspace Intersection Problem,"This paper introduces a novel and very general problem termed the subspace intersection problem. The goal is to infer an unknown subspace from a set of given subspaces which are known to each intersect this unknown subspace. The intersections however are unknown, as well. As an example, an intuitive instance of such a subspace intersection problem is given by 4 lines in 3D space where an unknown additional line has to be determined such that the known 4 lines intersect with it. A general algebraic formulation based on the Laplace expansion for determinants is presented enabling the computation of the unknown subspace in closed-form, given sufficiently many constraining known subspaces. Moreover, an efficient numerical scheme based on a partial reduced row-echelon form is introduced. The theory and algorithms for subspace intersection problems are showcased on a structure-from-sound problem. The theory enables the computation of an unknown synchronization of sound sources in closed-form. Furthermore, improving upon previous work, new cases such as missing observations can be handled in closed-form. "
40,A Bayesian Framework for Low-Rank and Sparse Patterns Inference in Multi-Task Learning,"In this paper we study the low-rank modeling for task relatedness in multi-task learning paradigm. We propose a Bayesian framework to infer the underlying low-rank and sparse patterns from multiple tasks. With the assumption of a shared low-rank hypothesis subspace, the framework treats the low-rank and sparse components as hidden variables which are constrained by certain sparsity inducing priors. We present a principled framework to infer the low-rank and sparse parts alternatively. Unlike optimization based methods, our method doesn't suffer from convex constraints and provides a novel perspective for multi-task learning from Bayesian standpoint. Experimental results on real-world data demonstrate the competitive capability of our method in dealing with multi-task learning problem."
42,Visual Topics Without Visual Words,The computer vision community has greatly benefitted from transferring techniques originally developed in the document processing domain to the visual domain by means of discretizing the features space into visual words.This paper reinvestigates the necessity of this artificially discretization of the continuous space of visual features and consequently proposes an alternative formulation of the popular topic models that is based on kernel density estimates.Results indicate the benefits of our model in terms of decreased perplexity as well as improved performance on object discovery and classification tasks.
44,"Greedy Bilateral Sketch, Completion & Smoothing","Recovering a large low-rank matrix from highly corrupted, incomplete or sparse outlier overwhelmed observations is the crux of various intriguing statistical problems. We explore the power of ``greedy bilateral (GreB)'' paradigm in reducing both time and sample complexities for solving these problems. GreB models a low-rank variable as a bilateral factorization, and updates the left and right factors in a mutually adaptive and greedy incremental manner. We detail how to model and solve low-rank approximation, matrix completion and robust PCA in GreB's paradigm. On their MATLAB implementations, approximating a noisy $10^4\times 10^4$ matrix of rank $500$ with SVD accuracy takes $6$s; MovieLens10M matrix can be completed in $10$s from $30\%$ of $10^7$ ratings with RMSE $0.86$ on the rest $70\%$; the low-rank background and sparse moving outliers in a $120\times 160$ video of $500$ frames are accurately separated in $1$s."
48,Bayesian Hierarchical Reinforcement Learning,"We describe an approach to incorporating Bayesian priors in the maxq framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, given sensible priors, (ii) task hierarchies and Bayesian priors can be complementary sources of information, and using both sources is better than either alone, (iii) taking advantage of the structural decomposition induced by the task hierarchy significantly reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to automatic learning of hierarchically optimal rather than recursively optimal policies. This paper is a resubmission of ICML paper number 660, where we have addressed the reviewer requests for experiments with additional baselines."
49,Stereoscopic Tracking with Neural Network Hardware,"A stereoscopic learning and tracking system is built using off-the-shelf parts: a pair of Cognimem V1KU neural network boards with onboard CMOS cameras; four HiTec servo motors; a Phidgets servo controller; and a laptop. A simple acrylic mount was constructed. Object learning and recognition occurs primarily within the Cognimem neural network chips, with tracking and triangulation done in software. A Kalman filter is used for predictive tracking. The resulting prototype can learn a new object quickly, usually within a second. It then can track the object?s motion in 3D space with depth-perception, at fairly fast speeds even in the presence of noisy background, without the use of structured light. The software is designed for extension to a variety of practical applications."
50,Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction,"We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable's marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufficiently certain about any individual decision. Whenever this is the case, we dynamically prune the variable we are confident about from the underlying factor graph. Consequently, at any time only samples of variable whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classification and image inpainting, shows that adaptive sampling can drastically accelerate MMP without sacrificing prediction accuracy."
54,Local Supervised Learning through Space Partitioning,"We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers. We formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error. We train locally linear classifiers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also show improved robustness to label noise."
55,Multi-View Clustering and Feature Learning via Structured Sparsity-Inducing Norms,"Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which make the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods."
56,Localized Gaussian Process Kernel Combining,"This paper investigates learning to combine multiple kernels in the context of Gaussian Process modeling. The fusing of kernels empowers the learner to take advantage of multiple heterogeneous data sources and views but poses the problem of how to best combine them. Unlike many existing algorithms where kernels are linearly combined at the matrix level, we propose an element-wise kernel combining approach with the former being a special case. The lower-level combining scheme is not limited to more flexible data integration, it also motivates new problem settings. We explore one such setting, rejecting noisy instances of MRI data to improve the learning performance. We propose EM and efficient gradient based optimization methods which make it possible to handle large scale problems. The promising experimental results demonstrate the performance of our model, and validate the effectiveness of element-level kernel combining."
57,A Generative Model for Parts-based Object Segmentation,"The Shape Boltzmann Machine (SBM) has recently been introduced as a state-of-the-art model of foreground/background object shape. We extend the SBM to account for the foreground object's parts. Our model, the Multinomial SBM (MSBM), can capture both local and global statistics of part shapes accurately. We combine the MSBM with an appearance model to form a fully generative model of images of objects. Parts-based image segmentations are obtained simply by performing probabilistic inference in the model. We apply the model to two challenging datasets which exhibit significant shape and appearance variability, and find that it obtains results that are comparable to the state-of-the-art."
58,Contextual-?-greedy for Context-aware recommender System,"Most existing approaches in Mobile Context-Aware Recommender Systems focus on recommending relevant items to users taking into account contextual information, such as time, location, or social aspects. However, none of them has considered the problem of user?s content dynamicity. We introduce in this paper an algorithm that tackles this dynamicity. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which user?s situation is most relevant to exploration or exploitation. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms."
60,Super-Bit Locality-Sensitive Hashing,"Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method towards scalable nearest neighbor search in high dimensional data space, which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments."
61,Fisher?s Discriminant with Natual Image Priors,"We suggest a Bayesian framework to combine Fisher's discriminant and natural image statistics. The probability structure of Fisher's discriminant is expressed, and the idea of natural image statistics is utilized as prior probabilities (\emph{Natural Image Priors}). Previous methods which directly employed the spatial smoothness assumption of images can be shown as special cases within this framework, but with potentially improper priors which are different from the natural image prior. We also propose a novel method which is a \emph{maximum a posteriori probability} (MAP) estimate with the natural image prior. Experimental results on the Yale face database and the ETH-80 object categorization dataset show that the proposed method significantly outperforms the state-of-the-art methods for general image data."
62,Accelerated Training of Linear Object Detectors,"We describe a general and exact method to speed up the training of linear object detection systems operating in a sliding, multi-scale window fashion, such as deformable part-based models.Our approach consists of reformulating the computation of the gradient as a convolution, and making use of properties of the Fourier transform to obtain a speedup factor proportional to the linear filters' sizes. This technique does not rely on the sparsity induced by a specific loss, nor on a stochastic sub-sampling of the training examples.Experiments on the PASCAL VOC benchmark show a speedup factor of more than one order of magnitude compared to a standard exact generic method."
63,The Bethe Partition Function of Log-supermodular Graphical Models,"Sudderth, Wainwright, and Willsky conjectured that the Bethe approximation corresponding to any fixed point of the belief propagation algorithm over an attractive, pairwise binary graphical model provides a lower bound on the true partition function. In this work, we resolve this conjecture in the affirmative by demonstrating that, for any graphical model with binary variables whose potential functions (not necessarily pairwise) are all log-supermodular, the Bethe partition function always lower bounds the true partition function.  The proof of this result follows from a new variant of the ?four functions? theorem that may be of independent interest."
64,A Tree Based Classifier for Non-Disjoint Classification,"Traditional approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that best describes it, and that all other labels are equally bad. In this paper we investigate the non-disjoint classification problem. In this scenario, each output label is instead a vector representing a data point's affinity for each of the classes. At test time, the class(es) with the highest affinity can be compared probabilistically. To this end, we propose a new supervised learning classifier underpinned by ensembles of decision trees. We show that by exploiting a multidimensional label vector, we can create less complex classifiers that generalize better at test time, even when a single label is sought. We compare our classifier to ensembles of classification and regression trees on both synthetic and real image data, and show the advantages of our model in terms of probabilistic interpretability and accuracy."
66,Convergence Analysis and Ef?cient Algorithms for Hyper-Graph Matching,"This paper focuses on the theoretical and algorithmic design for hyper graph matching where the affinity is represented by a tensor. We start with the simple Gradient Assignment (HGA) that works in the discrete domain iteratively, and ?nd its m-point loop convergence property under rather weak conditions, where m is the tensor order. Then Hyper Constrained Gradient Assignment (HCGA) is proposed to avoid such unwanted iteration track, and we show this algorithm will ensure to converge to a unique discrete point. Then we further extend HCGA to the continuous domain: HSCGA which plays the role to the classical Graduate Assignment (HGAGM) as HCGA to HGA. Then we explore the underlying connection between HCGA and its continuous counterpart both theoretically and empirically: under weak conditions, we first prove HGAGM will converge to m-discrete point like HGA, then illustrate HSCGA have the same convergence property with HCGA. These findings build the theoretical connection between the proposed two algorithms. Experimental results on both synthetic and real data consent our theoretical analysis: both algorithms perform competitively to state-of-the-arts. While HCGA outstands due to its working in the discrete space, able to handle ill cases when Hungarian method return multiple solutions, and being an ef?cient and anytime algorithm."
69,Learning Compositional Model with Attributes,"We present a framework for unsupervised learning of a compositional model of human figures from raw images and assigning attributes - text labels associated with images - to the parts of the model. The objectives of our approach are 1) to learn a meaningful part dictionary and unknown structure that can explain the images well, and 2) to assign given set of attributes onto the parts (nodes) of themodel based on three criteria: uniqueness, precision and consistency. The learning process is structure learning and associated parameter estimation driven by images and attributes. For evaluation, we propose a new dataset containing 1000 images of upper bodies of people with attribute annotations. On this dataset, we show that our learning algorithm discovers meaningful compositional parts of human bodies and that the final model captures meaningful information about the given attributes."
70,A Comparative Study of Evolutionary Clustering Algorithm for Predicting User Preference ,"Recommender systems are the tools that predict user preferences and thus help a na?ve user in finding useful information on the world wide web. They have become a necessary agent in the information bombardment arena of World Wide Web. A number of algorithms are implemented to predict the preference of user and thereby give them recommendation. Majority of these algorithm use data mining techniques. In this paper, we present a comparative analysis of various classification algorithm and there integration with various clustering algorithm that could effectively and accurately predict temporal changes in user preferences. The paper also presents a newly developed evolutionary clustering approach and its comparative analysis. Several experiments were conducted using these algorithms based upon various parameters using WEKA (Waikato Environment for Knowledge Analysis), a Data Mining tool. The results of the experiment show that integration of clustering and classification gives promising results with higher accuracy rate and lower error rates when compared over temporal parameters."
71,A Constraint Boosting Approach for Matching Problems,"In matching problems, we have two goals: $1$) maximizing the compatibility function, and $2$) satisfying the matching constraints. Since matching constraints, such as one-to-one or many-to-one, fragment the feasible space, the matching problems usually have large numbers of local optima. Existing methods are vulnerable to these local optima and easily get stuck in poor local optima. In this paper, we propose a \textbf{constraint boosting algorithm}, where matching constraints are expressed as a penalty term in the objective function, and the weight of penalty term adaptively increases. When the weight of penalty term is small, the optimization process mainly depends on the compatibility function, and thus approaches regions with large compatible values; when the weight of penalty term is large, the penalty term dominates the optimization process and force it to reach a nearby point satisfying the matching constraints. Empirically, this optimization procedure can escape from poor local optima and finally reach a good optimum. Moreover, we devise dependent optimization processes which utilize the best known optimum to escape from worse optima and reach a better optimum. In this way, an optimal or close-to-optimal solution can be quickly obtained. The experiments on various matching problems clearly demonstrate the superiority of our proposed method."
72,Learning Mixed Graphical Models,We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables. The structure and parameters of this model are learned using the pseudo-likelihood approximation with group-sparsity regularization. The pairwise model is also extended to incorporate features. Two algorithms for solving the resulting optimization problem are presented. The proposed models are compared with competing methods on synthetic data and a survey dataset.
73,Efficient Robust Recovery of Low-rank Tensors,"Robust tensor recovery in the presence of outliers, gross corruptions and missing values plays an instrumental role in robustifying tensor decompositions for multilinear data analysis and has a diverse array of applications.  In this paper, we study the problem of robust low-rank tensor recovery in a convex optimization framework, drawing upon the recent advance in robust PCA and tensor completion.  We propose tailored optimization algorithms with global convergence guarantees for solving both the constrained and the Lagrangian formulations of the problem.  These algorithms are based on the highly efficient alternating direction augmented Lagrangian and accelerated proximal gradient methods.  We investigate the empirical recoverability properties of the convex formulation and compare the recovery and computational performance of the algorithms on both simulated and real data."
74,Feature Selection using Partial Least Squares Regression and Optimal Experiment Design,We introduce a supervised feature selection criterion based on linear Partial Least Squares (PLS) regression. We show that an optimal feature subset can be identified by employing the optimal experiment design criterion on the loadings covariance matrix obtained from PLS. Our feature selection criterion can be derived in two different ways. We first derive it using the properties of maximum relevance and minimum redundancy on PLS models and then obtain the same criterion by applying optimal experiment design to PLS. To overcome the computational challenges in evaluating this criterion we use an approximate criterion and still obtain superior results. In our experiments we use the D-optimality criterion which maximizes the determinant of loadings covariance matrix. Experimental evaluation on four datasets demonstrates a consistent and better performance of our Optimal Loadings criterion when compared to other supervised feature selection techniques.
75,"Towards Bridging the Gaps Between Pattern Recognition, Symbolic Representations, and Online Learning","In neural networks, and more specifically perceptrons that can perform pattern recognition, underlying associations are opaque: the weights are sub-symbolic.  This complicates the ability to use symbolic representations, reuse networks, learn, and learn online.  Methods have been proposed to address these difficulties separately, with various degrees of success.  This work shows that by implementing network dynamics differently, during the testing phase instead of the training phase, connection weights can represent the fixed points or solutions of the network.  This allows the weights to be symbolically relevant: by looking at the weights, fixed-points and symbolic-like components can be inferred.  Moreover, it is easier to learn and modify existing representations, and localizes changes required for online learning.  Although this method is functionally analogous to a single layer linear perceptron and has similar limitations, it is an important step towards realizing neural-symbolic representations and online learning using methods optimized for pattern recognition."
76,Graduated Non-Convexity and Graduated Concavity Procedure (GNCGCP),"In this paper we propose the graduated nonconvexity and graduated concavity procedure (GNCGCP) as a general optimization framework to approximately solve the discrete optimization problems (usually NP-hard) over the set of partial permutation matrices. As implied by its name, the GNCGCP comprises two sub-procedures, the graduated nonconvexity (GNC) which realizes a convex relaxation and graduated concavity which realizes a concave relaxation. It is proved that the GNCGCP is a type of convex-concave relaxation procedure (CCRP), but with a much simpler formulation which does not involve the convex or concave relaxation in an explicit way. Two typical NP-hard problems, (sub)graph matching and quadratic assignment problem (QAP), are employed to demonstrate the simplicity as well as state-of-the-art performance of the GNCGCP."
77,Random Utility Theory for Social Choice: Theory and Algorithms,"Random utility theory models an agent's preferences onalternatives by drawing a real-valued score on each alternative(typically independently) from a parameterized distribution, and thenranking according to scores. A special case that has receivedsignificant attention is the Plackett-Luce model, for which fastinference methods for maximum likelihood estimators areavailable. This paper develops conditions on general,random utility models that enable fast inference withina Bayesian framework through MC-EM, providing unimodal log-likelihoodfunctions. Results on both real-world and simulated data provide supportfor the scalability of the approach, despite its flexibility."
78,Local and Global Manifold Preserving Embedding,"Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional Euclidean space. In this paper, we propose a novel linear subspace learning algorithm called Local and Global Manifold Preserving Embedding (LGMPE). LGMPE can explicitly preserve both the local and global manifold structures which respectively describe local linear reconstruction structure and global geodesic distance structure, and can balance the contributions of the two parts. Therefore, our algorithm has the merit of handling complex data space. Several experiments on synthetic and real face datasets demonstrate the effectiveness of the proposed algorithm."
79,Exponential weight algorithms for the Exploration and Exploitation of Finite Sequences,"The adversarial multi-armed bandit algorithms are efficient, easy to implement and useful to solve online optimization problems. This paper considers a new setting, which we call scratch-game, where the sequences of rewards are finite. Is it possible to adapt standard adversarial bandit algorithms to take advantage of the proposed setting? We propose two new algorithms for finite sequences of rewards. In order to tune the exploration factor ?, we provide a lower bound of expected gain for each algorithm. We have compared favorably these algorithms with Exp3 on synthetic problems, on an ad serving problem and on emailing campaigns."
80,Implicit Collaborative Filtering with Random Graphs,"Implicit collaborative filtering harnesses the co-occurrence of edges between user and item vertices in a graph, to interpolate the presence of other edges. We advocate a distribution over random graphs as a novel foundation to collaborative filtering. By mimicking the power law properties of real world networks in a model, we achieve state of the art results on large scale problems. Inference is performed with a mean field approximation, and we show how a tractable procedure for the inclusion of the graph-based prior can be derived by drawing on Monte Carlo samples and stochastic optimization."
83,Variational Inference in Nonconjugate Models," Mean-field variational inference is a powerful algorithm for approximate posterior inference, but is difficult to derive for nonconjugate probabilistic models. We develop two variational strategies for nonconjugate priors---Laplace variational inference and delta method variational inference---which place minimal conditions on the model. These strategies extend and unify existing methods that were derived for specific models.  We illustrate our approach on the correlated topic models, Bayesian logistic regression, and hierarchical Bayesian logistic regression.  Our experimental results show that our methods work well on real-world datasets."
84,Venue Discovery,Didn't finish writing the abstract yet...
85,ForeCA: Forecastable Component Analysis,"Blind source separation (BSS) techniques are often applied to multivariate time series with the goal to obtain better forecasts. But BSS and the need for better forecasts are often treated separately, in the sense that finding an optimally transformed (sub-)space has nothing to do with the aim to predict well. Here I introduce Forecastable \textbf{C}omponent Analysis (ForeCA), a new BSS technique for temporally dependent signals that uses forecastability as the explicit objective in finding an optimal transformation. It separates the signal into the forecastable, $\mathbf{F}$, and the orthogonal white noise space, $\mathbf{F}^{\bot}$. Simulations and applications to financial data show that ForeCA successfully finds signals that can be used to forecast. ForeCA therefore automatically discovers informative structure in multivariate signals."
86,A Schrodinger formalism for simultaneously computing the Euclidean distance transform and its gradient density,"In this paper, we leverage the well-known Hamilton-Jacobi to Schrodinger connection to present a unified framework for computing both the Euclidean distance function and its gradient density in two dimensions. We introduce a novel Schrodinger wave function for representing the Euclidean distance transform from a discrete set of points. An approximate distance transform is computed from themagnitude of the wave function while the gradient density is estimated from the Fourier transform of the phase of the wave function. In addition to its simplicity and efficient O(N log N)computation, we prove that the wave function-based density estimator increasingly, closely approximatesthe distance transform gradient density (as a free parameter approaches zero) without requiring the true distance function."
87,Stock Clustering through Equity Analyst Hypergraph Partitioning,"Use of industry classifications in the finance community is pervasive. They are critical to deriving a balanced portfolio of stocks and, more broadly, to risk management. Businesses, academics and government agencies have all researched and developed various schemes with mixed success. Recognizing major brokerages and research firms tend to assign their analysts to cover highly similar companies, we propose a scheme that makes use of stock analyst coverage assignments. Although creating coverage groups of highly similar stocks is not the direct goal of research firms, it may be imperative to their success because increasing similarity in coverage helps maximize synergy and derive the most value per analyst. To create our industry scheme, we construct a hypergraph where vertices represent stocks and hyperedges represent analyst coverage, connecting his/her similar companies. Using no additional information, we perform hypergraph partitioning to form clusters of stocks. Our scheme can produce any number of clusters and can dynamically update as research firms change analyst coverage as opposed to  today's leading industry schemes which have only fixed numbers of industries and require periodic expert review. Can our dynamic scheme match the quality of stock groups from the expert-driven schemes? We make head-to-head comparisons to a leading academic and a leading commercial scheme using a methodology from the finance community that measures the coincidence of stock price movements. We also compare our scheme against a clusterer that creates groups based on past return correlations. Our results rival and often exceed all 3 schemes."
88,Learning Multiple Concepts with Incremental Diverse Density,"We present a novel method of learning multiple disjunct concepts with diverse density using an incremental approach.  We demonstrate that by maximizing the diverse density over individual target concept points and minimizing the probability of their intersection, concepts can be learned incrementally.  This method reduces the complexity of the algorithm from factorial, with respect to the number of targets, to exponential order.  We demonstrate that this greedy approach successfully learns disjunctive target concepts with competitive classification accuracy on a benchmark multiple instance learning dataset in comparison to other common diverse density approaches.  We also introduce a novel application of the multiple instance learning framework to an emotion recognition task using prosodic and spectral speech features."
89,Experimental Proposal on Simulating Artificial Neural Netwworks Using Local Area Networks.,"An experiment for the simulation of artificial neural networks using a Local Area Network (LAN) is proposed in order to see if fundamental questions can eventually be answered such as: what is thinking made of (visual initially, but it may be possible to generalize later)? Is it possible to simulate brains using the arrangement here proposed? The paper is intended to propose the experiments in order to gather feedback concerning its nature before it is actually implemented."
90,Locality-Sensitive Hashing With Margin Based Feature Selection,"This paper proposes a method for locality-sensitive hashing with margin based feature selection for large-scale, high dimension data.The basic concept is to generate a hash function and importance for selection that targets a much larger size than the final targeted bits, and to select the importance in descending order until the final targeted bits are reached. The new algorithm was applied to biometric, speech, and image datasets and compared with other methods, and the effects for each data application were verified."
91,A Growing Technique for Construction of Conlitron and Multiconlitron,"Based on the concepts of conlitron and multiconlitron, we propose a growing construction technique for improving the performance of piecewise linear classifiers on two-class problems. This growing technique consists of two basic operations: SQUEEZE and INFLATE, it can make the classification boundary adjusted to improve the generalization ability. Experimental evaluation shows that the growing technique can simplify the structure of a conlitron/multiconlitron effectively by reducing the number of linear functions, largely keeping and even greatly improving the level of classification performances. It would come to play an important role in the subsequent development of piecewise linear learning."
92,Coordinate Descent Optimization for L1 Norm Low Rank Tensor Factorization,"The L1 norm low-rank tensor factorization (LRTF) is recently attracting attention due to its intrinsic robustness to outliers and missing data. However, this problem is difficult to be resolved due to its non-convexity and non-smoothness. In this paper, we propose a coordinate descent (CoD) method to solve this problem. The main idea is to coordinate-wisely optimize each scalar parameter involved in the L1 LRTF model with all the others fixed. Each of these one-scalar subproblems is proved to have a closed-form (global) solution, and thus by recursively solving these small problems, an efficient algorithm can be readily constructed to tackle the original problem. The specific advantage of the proposed CoD strategy is that it provides a unified framework of solving L1 LRTF problems on both non-missing and missing data cases, and also can be easily extended to multiple other important tensor factorization tasks, such as nonnegative tensor factorization and sparse tensor factorization. Based on a series of TensorFace experiments, it is verified that our method performs significantly more robust than previous methods in the presence of outliers and/or missing data."
93,A new metric on the manifold of kernel matrices with application to matrix geometric means,"Symmetric positive definite (spd) matrices are remarkably pervasive in a multitude of scientific disciplines, including machine learning and optimization. We consider the fundamental task of measuring distances between two spd matrices; a task that is often nontrivial whenever an application demands the distance function to respect the non-Euclidean geometry of spd matrices. Unfortunately, typical non-Euclidean distance measures such as the Riemannian metric $\riem(X,Y)=\frob{\log(X\inv{Y})}$, are computationally demanding and also complicated to use. To allay some of these difficulties, we introduce a new metric on spd matrices: this metric not only respects non-Euclidean geometry, it also offers faster computation than $\riem$ while being less complicated to use. We support our claims theoretically via a series of theorems that relate our metric to $\riem(X,Y)$, and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances."
94,Multi-Label Multi-View Laplacian Hashing,"With the advent of the Internet, large scale datasets are available.The data may be high dimensional,  represented by multiple features,and associated with more than one concepts. Hashing is an effectivestrategy for dimensionality reduction and efficient nearest neighborsearch in massive datasets. We propose a novel method to seekcompact hash code that allows efficient retrieval with multi-labelmulti-view data. Based on multi-graph Laplacian, we learn theoptimal combination of heterogeneous features to effectivelydescribe multi-view data, which exploit the feature correlationsbetween different views. We obtain  the hash embedding whichpreserves the neighborhood context in the original spaces, and thesemantic embedding (i.e., multi-label vectors) at the same time.Both labeled and unlabeled data are employed for learning, andinter-label correlations are sufficiently captured to improve theperformance of hash learning with multi-label data. The experimentalevaluation on real-world datasets demonstrates promising resultsthat validate our method."
95,Learning a Discriminative Isotropic Space from Labeled and Unlabeled Data,"Euclidean distance measure is computationally simple and commonlyused in the task of classification. However, it does not capitalizeon any discriminant information from training samples, which iscrucial to classification performance. Moreover, Euclidean distanceis invalid when the input space is not isotropic, which often occursin many practical applications. In this paper, we learn aDiscriminative Isotropic Space (DIS) from both labeled and unlabeldata to improve classification accuracy as well as generalizationability. Intra-class compactness and inter-class separability areachieved simultaneously in the learned space, so it isdiscriminative and benefits the task of classification. The learnedspace looks as the same in every direction as possible, so Euclideandistance is suitable to measure dissimilarities between samples insuch space. Our regularized objective function implicitly minimizesmutual information between input space and transformed space, whichis reasonable from the perspective of information theory. Our methodis scalable and can be applicable to large dataset. There is nolocal optimum problem in our algorithm since the objective functionis convex and its closed form solution can be easily obtained,therefore the proposed method is more effective and more efficientthan the alternative. Experiments on real data sets demonstrate theefficacy of our method."
98,Shortest Path Gaussians For State Action Graphs: An Empirical Study,"We approximate action-value functions, defined on state graphs and state-action graphs derived from the Markov decision problem (MDP) to be solved, by a linear combination of shortest path Gaussian kernels. An empirical comparison on a testbed of 3 MDPs shows that this works better than using other basis functions derived from the state or state-action graph. Examples of such other basis functions are the smoothest eigenfunctions of the combinatorial and normalized Laplacian, eigenfunctions of random walk operator and diffusion wavelet bases."
99,Efficient Regularized Isotonic Regression,"Isotonic regression is a nonparametric approach that fits the model subject to a set of isotonic constraints. In structured variable selection and estimation, isotonic constraints can also be used to capture the hierarchical relationships among variables according to the heredity principle. However, isotonic regression solvers cannot handle regularizers (e.g., sparsity regularizers) and constraints (e.g., non-negative constraints) on the parameters, which on the other hand are often important ingredients in learning with structured sparsity. In this paper, we propose a general optimization formulation that addresses these limitations. Efficient solvers are proposed for regression with both tree-ordered and DAG-ordered isotonic constraints. Experiments on a number of large data sets show that they are fast and accurate. Using together with proximal gradient methods, hierarchy information can now be flexibly incorporated, leading to better structured sparse models."
100,Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification,"In hierarchical classification, the prediction paths may be required to always end at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes. However, while there have been a lot of MLNP methods in hierarchical multiclass classification, performing MLNP in hierarchical multilabel classification is much more difficult. In this paper, we propose a novel MLNP algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and DAGs. We show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm. Moreover, this can be further extended to the minimization of the expected symmetric loss. Experiments are performed on a number of real-world data sets with tree- and DAG-structured label hierarchies. The  proposed method consistently outperforms other hierarchical and flat multilabel classification methods."
101,How informative is your algorithm?,"This paper elaborates a new framework for the analysis of algorithms. The generalization capability of algorithms w.r.t. their task of processing input data to meaningful outputs is introduced. The trade-off between the informativeness of algorithmic procedures and their stability against noise is attacked by generalizing Shannon's channel capacity to an arbitrary learning problem.The concept of a generalization capacity (GC) is proposedto characterize the sensitivity of an algorithm to noisy input data. GC optimally measures the generalization ability of algorithms to extract context-sensitive information w.r.t. a given output space. For the first time, different algorithms for the same data processing task, e.g. the various clustering methods, can now be objectively compared based on the bit rate of their respective capacities.The problem of grouping data is used to demonstrate this validation principle for clustering algorithms, e.g. K-means, pairwise clustering, DBSCAN and dominant set clustering. Our new validation approach selects the most informative clustering algorithm, i.e. the procedure which filters out the maximal number of stable, task-related bits relative to the hypothesis class."
102,Topical Structural Analysis on Social Communications,"The popularity of online social networks has lowered the barrier of online communications, which results in massive number of users using the networks for interaction and friendship making.To characterize the user positions and message content generated by users of different positions, we propose the Dirichlet Allocation Blockmodels (DABM) for topical structural analysis.DABM model allows each pair of users to generate message content following the topic distribution conditioned on the social positions of the two users.Compared with the earlier model, DABM allows users of the same positions to have some variability in their message topic distribution.We evaluate both DABM and the earlier model on tweets generated by a set of Twitter users connected by follow links, and show that DABM achieves better likelihood and perplexity than the earlier model."
103,An Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks,"We present an axiomatic construction of hierarchical clustering in asymmetric networks where the dissimilarity from node $a$ to node $b$ is not necessarily equal to the dissimilarity from node $b$ to node $a$. The theory is built on the axioms of value, influence, and transformation. The Axiom of Value says that in a two-node network the nodes cluster at resolution equal to the maximum dissimilarity between them. The Axiom of Influence says that no clusters are formed at resolutions that do not allow bidirectional paths to be formed. The Axiom of Transformation states that if we consider a network and do not increase any pairwise dissimilarity, the level at which two nodes become part of the same cluster is not larger than the level at which they were clustered together in the original network. Two asymmetric hierarchical clustering methods that abide to these axioms are derived. Reciprocal clustering requires clusters to form through arcs that are similar in both directions. Nonreciprocal clustering allows clusters to form through cycles of small dissimilarity. We further show that any clustering method that satisfies the axioms of value, influence, and transformation lies between reciprocal and nonreciprocal clustering in the sense that all other methods cluster two points together at resolutions larger than nonreciprocal clustering and smaller than reciprocal clustering. To conclude, we apply this theory to the formation of circles of trust in social networks."
104,Adaptive Sparseness for function learning using Parametric and Nonparametric Bayesian LASSO Methodology,"One of the most important problems in supervised learning is that of accurately inferring functional mappings based on (typically)  high dimensional training data.  Learning is accomplished by estimating parameters which weight the features used in the inference.  To achieve good generalization,  it is considered desirable, while retaining accuracy, to obtain sparse solutions to this problem (i.e. solutions which properly select the parameters which need to be estimated). The lasso methodology has been used extensively  to solve this problem;  it controls both complexity and accuracy by adding a regularization term to the least squares fit. EM methodology provides non-adaptive solutions which require manual control of complexity.  Parametric Bayesian approaches to the lasso adopt parametric priors to learn the function parameters;  these serve to adaptively control for both the complexity and accuracy of the learned function.   Typically, the function parameters have complicated relationships with one another and with the data.  Parametric priors frequently fail to accurately learn these relationships.Learning in the aforementioned settings can be handled using prior distributions which stipulate mixture models with a potentially infinite number of components;  these are known as nonparametric Bayesian priors.  Dirichlet process priors are a particular example of a nonparametric Bayesian prior.  We propose a Hierarchical Bayesian function learning algorithm which employs Dirichlet process priors to infer functional mappings. Model comparisons between algorithms employing Dirichlet process priors, models employing parametric priors, and models employing expectation-maximization algorithms demonstrate the superiority of the former.  An example illustrates the advantages and disadvantages which accrue from using these models for function learning. "
105,Learning Separable Dictionaries,"Many techniques in neuroscience, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are better adapted to the considered class of signals. However, high dimensional signals and the numerical costs for applying the learned dictionary in reconstruction tasks pose enormous computational challenges.In this paper, we combine the advantages of fast implementation and of capturing the global structure of a signal. This is achieved by enforcing a separable structure on the dictionary throughout the learning process. Depending on the dimension of the signal, we propose two algorithms based on geometric optimization on the product of spheres. For signals of moderate dimension, we suggest a geometric conjugate gradient method, while for learning large scale dictionaries we use an adaption of stochastic gradient descent to the geometric setting. "
106,State-Based Hierarchical Clustering,"Once data points are grouped together into a cluster, standard complete linkage hierarchical agglomerative clustering does not allow them to migrate into separate clusters at higher strata in the hierarchy.  On the other hand, once data points are assigned to separate clusters, standard complete linkage hierarchical divisive clustering does not allow them to recombine within a single cluster at lower strata in the hierarchy.  Further, alternative dendrograms are used to resolve ties between the inter-cluster distances that determine which two clusters (cluster) will be combined (subdivided) next.  These problems make these methods difficult to use where correctness and relatively precise mathematical models are required.  The notion of finding sets of clusters based solely on the distances between the data points, as opposed to inter-cluster distances, is used to design a basic algorithm that overcomes these problems.  The state of a data set and the degrees of the data points as of a variable threshold distance are used to find the sets of clusters.  The algorithm was successfully tested on numerous test patterns and several real world data sets."
107,Efficient Incremental Feature Learning in Manufacturing Environments,"Most heavy duty manufacturing operations consist of hundreds of steps, where multiple measurements are taken at each step to monitor the qualityof the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate stepscan prove to be extremely useful in enhancing productivity. In this paper, we provide an approach for learning regression models in an environmentwhere features (i.e. measurements) and datapoints (i.e. individual products) are added incrementally. At each step, any finite number of features maybeadded and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares,weighted least squares, generalized least squares and ridge regression, but also for generalized linear models and lasso regression that useiterated re-weighted least squares for maximum likelihood estimation. For arbitrary regression methods, even a relaxation of the approach is noworse than using the model from a previous step or using a model that learns on the additional features and optimizes the residual of the modelat the previous step. We further validate these claims through experiments on a real industrial dataset."
109,High Dimensional Semiparametric Scale-invariant Principal Component Analysis,"We propose a high dimensional semiparametric scale-invariant principal component analysis, named Copula Component Analysis (COCA). The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. The COCA accordingly estimates the leading eigenvector of the correlation matrix of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefficient estimator, Spearman?s rho, is exploited in estimation. We prove that, although the marginal distributions can be arbitrarily continuous, the COCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the simulated data are conducted under both ideal and noisy settings, which suggest that the COCA loses little even when the data are truely Gaussian. The COCA is also implemented on a large-scale genomic data to illustrate itsempirical usefulness."
110,CODA: High Dimensional Copula Discriminant Analysis,"We propose a high dimensional classification method, named Copula Discriminant Analysis(CODA), which generalizes the normal-based linear discriminant analysis to the larger nonparanormal as proposed by Han Liu (2009). To simultaneously achieve estimation efficiency and robustness, the nonparametric rank-based methods including the Spearman?s rho and Kendall?s tau are exploited in estimating the covariance matrix. In high dimensional settings, we prove that the sparsity pattern of the discriminant features can be consistently recovered with the parametric rate, and the expected misclassification error is consistent to the Bayes risk. Our theory is backed up by careful numerical experiments, which show that the extra flexibility gained by the CODA method incurs little efficiency loss even when the data are truly Gaussian. These results suggest that the CODA method can be a safe replacement of the normal-based high dimensional linear discriminant analysis."
111,Continuous-weight neural networks for learning from sparse continuous dictionary representations,"Sparse feature representations from overdetermined dictionaries are commonly seen in digital signal processing fields, including computer vision and audio processing. Examples of such representations are sinusoidal models, wavelet representations, and interest point sets. Due to the high dimension of the dictionaries involved and the variable number of features per representation, current machine learning algorithms are not designed to train well on these feature sets. In this paper, we present continuous-weight neural networks, a novel machine learning paradigm with universal continuous approximation capabilities that can directly train on these sparse dictionary representations. The algorithm thus allows for sparser feature sets in numerous multimedia applications, and thus improved learning quality in many of these cases, as feature sparsity is strongly correlated with learning robustness. This paper additionally presents a class of training algorithms for the paradigm by demonstrating how to efficiently compute the gradient of the parametric model."
112,Optimal Calculation of Tensor Learning Approaches,"Tensors provide a general framework for exploring multiple factors in learning. A tensor representation also helps to reduce the small sample size problem in discriminative subspace selection and the overfitting problem in vector-based learning. Most algorithms have been extended to the tensor space to create algorithm versions with direct tensor inputs. However, very unfortunately basically all objective functions of algorithms in the tensor space are non-convex. However, sub-problems constructed by fixing all the modes but one are often convex and very easy to solve. So most of the algorithms use alternating projection optimization procedure to solve the problem. However, this method may lead to difficulty converging; iterative algorithms sometimes get stuck in a local minimum and have difficulty converging to the global solution. Here, we propose a computational framework for constrained and unconstrained tensor methods. Using our methods, the algorithm convergence situation can be improved to some extent and better solutions obtained. We applied our technique to Uncorrelated Multilinear Principal Component Analysis (UMPCA), Tensor Rank one Discriminant Analysis (TR1DA) and Support Tensor Machines (STM); results showthe effectiveness of our method."
113,Spectral Graph Cut from a Filtering Point of View,"We analyze spectral graph theory based image segmentation algorithms and show there is a natural connection with edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated application of bilateral filtering. Then, using this interpretation we present and implement a fast normalized cut algorithm. Experiments show that our implementation can solve the original optimization problem with a  10x-100x speedup. In addition to these practical advantages, our work shows a deep connection between two currently separate approaches to segmentation, which suggests further directions for improvements."
114,Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing,"Statistical features of neuronal spike trains are known to be non-Poisson. Here, we investigate the extent to which the non-Poissonian feature affects the efficiency of transmitting information on fluctuating firing rates. For this purpose, we introduce the Kullbuck-Leibler (KL) divergence as a measure of the efficiency of information encoding, and assume that spike trains are generated by time-rescaled renewal processes. We show that the KL divergence determines the lower bound of the degree of rate fluctuations below which the temporal variation of the firing rates is undetectable from sparse data. We also show that the KL divergence, as well as the lower bound, depends not only on the variability of spikes in terms of the coefficient of variation, but also significantly on the higher-order moments of interspike interval (ISI) distributions. We examine three specific models that are commonly used for describing the stochastic nature of spikes (the gamma, inverse Gaussian (IG) and lognormal ISI distributions), and find that the time-rescaled renewal process with the IG distribution achieves the largest KL divergence, followed by the lognormal and gamma distributions. "
115,Balanced Relative Margin Machines --- Closing the Gap Between Fisher's Discriminant and SVM Classification,"We approach the class of relative margin classification algorithms from the mathematical programming perspective. In particular, we propose a Balanced Relative Margin Machine and then extend it by a 1-norm regularization. Subsequently, we show the strong relations of the methods to SVMs as well as toregularized discriminant analysis techniques."
116,Application of moving variance calculation to EMG based movement onset detection,"  Adaptation of human-machine interaction devices by means of physiological data requires online analysis.  To save memory and resources for realtime time series data processing  as, e.g. movement detection based on electromyographic (EMG) data,  new update formulas are needed,  when calculating mean and variance of the signal.  Applications were presented, where the length of the relevant time frame is fixed  and moving average and variance have to be calculated in realtime.  This differs from incremental calculations, which have been largely analyzed.  Formulas for an efficient calculation are introduced and applied on  synthetic and EMG data to show the benefits."
117,The representer theorem for Hilbert spaces: a necessary and sufficient condition,"A family of regularization functionals is said to admit a linear representer theorem if every member of the family admits minimizers that lie in a fixed finite dimensional subspace. A recent characterization states that a general class of regularization functionals with differentiable regularizer admits a linear representer theorem if and only if the regularization term is a non-decreasing function of the norm. In this paper, we improve over such result by replacing the differentiability assumption with lower semi-continuity and deriving a proof that is independent of the dimensionality of the space."
118,"On the (Non-)existence of Convex, Calibrated Surrogate  Losses for Ranking","We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pairwise preferences. Our results cast lights on the intrinsic difficulty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk."
120,Hierarchical Clustered Importance Sampling,"We present a Bayesian approach that integrates sequential importance sampling within an unsupervised Dirichlet clustering model.  The sequential importance sampling provides refined estimates to hidden variables through iterative comparison with observed data, while the Dirichlet clustering partitions the space over which importance sampling is implemented.  This allows each importance sampling process to estimate a regional peak.  Without the partitioning of feature space through Dirichlet clustering, the sequential importance sampling would converge to the global peak.  We show the effectiveness of the model through a toy example as well as with high resolution radar data used for target analysis.  Additionally, we provide the inference equations for our proposed model using a variational Bayesian solution. "
121,Acquiring Dynamical Primitives from Unlabeled Demonstrations through Parameter Space Clustering,"In this paper we introduce a method to learn multiple dynamical systems from unlabeled data. This problem has applications in a wide range of domains where labeled data is expensive or even impossible to obtain. Applications include multi-task learning from demonstration in robotics, learning from multiple teachers that may have different strategies,  behavior modeling for surveillance or human motion interpretation. One of the difficulties of this problem is that, due to multiple objectives, trajectories can be very mixed in measurement space. Based on mixture models, we propose to cluster trajectories in a latent representation of potential functions. These potential functions are parameterized as linear combinations of a large number of features. We derive two algorithms based on Expectation Maximization for a known number of clusters and Dirichlet Processes to estimate this number from data. In both cases, we enforce sparsity to perform feature selection while clustering the data. We evaluate the proposed method using 2D synthetic trajectories and real 3D human motion data."
122,Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress,"Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such asRmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which driveexploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions."
123,Supervised Learning with Similarity Functions,"In this paper we address the problem of general supervised learning where data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multi-class classification problems. Inspired by an existing work on classification with similarity functions [Balcan-Blum '06], we propose a general ``goodness'' criterion for similarity functions w.r.t. a given supervised learning task. Our definition is generic enough to handle any supervised learning task and also subsumes the goodness condition of [Balcan-Blum '06]. We then adapt a landmarking technique by [Balcan-Blum '06, Balcan et al '08] to provide efficient algorithms for supervised learning using ``good'' similarity functions. In particular, we consider three important supervised learning problems : a) real-valued regression, b) ordinal regression and c) ranking. For each of these problems we show that our goodness definition satisfies the following two key properties : 1) Utility : given good similarity functions, our algorithms guarantee bounded generalization error with polynomial sample complexities, 2) Admissibility : our goodness definitions are flexible enough to at least admit all good PSD kernels; the goodness of a PSD kernel being defined according to standard definitions in literature. Furthermore, for the case of real-valued regression, we provide a natural goodness definition that when used in conjunction with a recent work on sparse vector recovery [Shalev-Schwartz '10], guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs. "
124,Bilinear Low-Rank Matrix Hashing with Rank-Sensitive Block Permutation,"Conventional locality-sensitive hashing only handles the inputs in the forms of vectors or sets. This paper explores a new topic of matrix hashing. Search of nearest neighbor data in the matrix form can be found in many applications such as detection of image duplicates and geographical distributions. Reducing the task to 1D vector search may incur significant information loss. Our contributions are two-folds: first, under mild assumptions on the matrix, we investigate the relationship between the matrix rank and the difficulty of nearest matrix search. Based on the notation of \emph{random matrix similarity}, we show that low-rank matrices are often more favorable in such a task. Second, we compare different schemes on matrix hashing. Among them, the linear hashing scheme is a natural extension from the conventional vector field to matrices. However, for matrices of size $n \times m$, its complexity is $O(n m)$ for both computation and storage, which is unaffordable for large matrices.To solve this problem, this paper proposes a bilinear matrix hashing scheme which greatly reduces the complexity by exploiting the matrix singular structures. We present very interesting observations on bilinear matrix hashing: although the efficacy of the scheme is mainly determined by the matrix low-rankness, its performance will be enhanced when matrix blocks are permutated to increase the rank. Therefore the final bilinear scheme involves a very interesting and practical tradeoff between block-level high-rankness and inner-block low-rankness. We conduct an in-depth study of this issue based on kurtosis analysis and provide a rank-sensitive algorithm for learning a universal block permutation. Extensive experiments are presented to corroborate the effectiveness of the proposed matrix hashing scheme."
126,What is foreground: From the view of photographers,"In this paper we focus on foreground extraction in digital images. We redefine foreground as an object close to the camera in real distance, and at the same time clear in the image. Based on this observation, we propose a depth of field measure which represents the distance of object from camera, and cooperate with saliency based approach. Specifically, for a given image, we firstly extract a Saliency Map (SM) and a Defocus Map (DM), with their values corresponding to saliency value and depth of field value. Then we segment images and extract segment feature based on these two extracted maps. A classifier is trained to separate foreground from background. On test images, segments are considered to be foreground if they have high enough confidence on the trained classification model. The proposed method is tested on MSRA Salient Object Detection image set and Flickr image set. Experimental result demonstrates that our method can obtain better result on foreground extraction over related methods."
127,Constructing a Design Matrix by Stepping Vertex Features on Multiple Networks,"Suppose there are $n$ vertices which are embedded in $p$ networks and which take values $\mathbf{z}$.  We are interested in the simultaneous interactions between the vertices and the networks in which they are embedded.  In pursuit of this, we propose constructing an $n$ by $p$ design matrix by simply taking a step from $\mathbf{z}$ on each adjacency matrix (premultiplying each adjacency matrix by $\mathbf{z}$).  We then rely on traditional statistical techniques to analyze this constructed design matrix either with exploratory techniques or predictive techniques if the vertices take on a value $\mathbf{y}$ (we also consider the autoregressive case where $\mathbf{y} = \mathbf{z}$).  We compare this design matrix construction approach to methods specialized to handle the network analysis."
128,Cocktail Party Processing via Structured Prediction,"While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random fields (CRFs) to classify speech dominance withineach time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises."
129,Robustness and risk-sensitivity in Markov decision processes,"We uncover relations between robust MDPs and risk-sensitive MDPs.  The objective of a robust MDP is to minimize a function, such as the expectation of cumulative cost, for the worst case when the parameters have uncertainties.  The objective of a risk-sensitive MDP is to minimize a risk measure of the cumulative cost when the parameters are known.  We show that a risk-sensitive MDP of minimizing the expected exponential utility is equivalent to a robust MDP of minimizing the worst-case expectation with a penalty for the deviation of the uncertain parameters from their nominal values, which is measured with the Kullback-Leibler divergence.  We also show that a risk-sensitive MDP of minimizing an iterated risk measure that is composed of certain coherent risk measures is equivalent to a robust MDP of minimizing the worst-case expectation when the possible deviations of uncertain parameters from their nominal values are characterized with a concave function."
130,Group Bridge Regression is Beta Uniformly Stable,"Sparsity and stability are the desired properties of a machine learning algorithm, especially in high-dimensional data problems. The group Lasso is a sparsity promoting method that exploits grouped variables and has been studied intensively in the literature. However, it has been recently discovered that like the Lasso, the group Lasso does not possess the desirable stability property which is used to establish generalization. As sparsity and uniform stability are conflicting goals, an immediate question of the optimal trade-off is raised: What would be a stable algorithm that trades off sparsity well? In the context of regression with grouped variables, we show that the existing bridge regression method already provides a natural trade-off between stability and sparsity. We show that group bridge regression, where in the group sparsity is controlled via its bridge order, is uniformly beta-stable and thus generalizes. Numerical studies on high-dimensional synthetic and splice detection problems demonstrate that group bridge regression is competitive to the group Lasso in  machine learning contexts."
131,Dynamical And-Or Graph Learning for Object Shape Modeling and Detection,"This paper studies a novel discriminative part-based model to represent and recognize object shapes with an ``And-Or graph''. We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP, is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to well handle large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization.  We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches."
133,Bayesian Score for Orders of Variables,"Analysing high-dimensional data is hard. Search spaces of many optimizationproblems grow exponentially with respect to the dimension of data, while at thesame time, a large number of data points is needed in order to havestatistically solid results.  One way to approach the curse of dimensionalityis to impose some structure on the variables. In this paper we study linearorders between variables, a natural structure for many datasets.  The goal ofthis paper is measure the quality of the order for the variables inmulti-dimensional data. Such a score will help us to decide if the order athand is genuinely significant. Our score will be based on the intuition thatthe order should have a good score when variables that depend on each other areclose to each in the given order.More specifically, given a dataset and an order we consider a set of Bayesiannetworks that depends on this order. If the order is good, that is, if thedependent variables are close to each other, then these models will have a goodposterior probability, which we will estimate with Bayesian InformationCriterion (BIC). Since we are not interested in the actual models, wemarginalize them out.  Since there are exponential number of such models forone given order, we will give a non-trivial technique for computing the orderin polynomial time. We also show how to significantly improve the computationaltime if we allow some minimal error in the score."
134,Some Results About the Vapnik-Chervonenkis Entropy and the Rademacher Complexity,"This paper deals with the problem of identifying a connection between the Vapnik-Chervonenkis (VC) Entropy, a notion of complexity introduced by Vapnik in his seminal work, and the Rademacher Complexity, a more powerful notion of complexity, which has been in the limelight of several works in the recent literature. In order to establish this connection, we refine some previously known relationships and derive a new result. Our proposal allows computing an admissible range for the Rademacher Complexity, given a value of the VC-Entropy, and vice versa, therefore opening new appealing research perspectives in the field of assessing the complexity of an hypothesis space. "
135,Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning,"Many problems in machine learning can be (re)formulated as linearly constrained convex programs. When there are only two variables, such problems can be efficiently solved by the alternating direction method (ADM) or its linearized version (LADM). However, more often there are more than two variables, but the corresponding theories on ADM and LADM are rather scarce. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-variable separable convex programs efficiently. LADMPSAP is particularly suitable for sparse representation and low rank recovery problems because its subproblems have closed form solutions and the sparsity and low rankness of the iterates can be preserved during the iteration. It is also highly parallelizable and hence fits for parallel or distributed computing. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the sufficient and necessary conditions} for global convergence. We further propose a simple optimality measure and reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, we devise a practical version of LADMPSAP for faster convergence. Numerical experiments on the latent low rank representation problem testify to the speed and accuracy advantages of LADMPSAP."
136,Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a differentiable function given a finite number of evaluations to the function. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost as accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and we provide a finite-sample analysis."
137,"Managing sparsity, time, and quality of inference in topic models","Inference is an integral part of probabilistic topic models, but is often non-trivial to derive an efficient algorithm for a specific model. It is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents. In this paper we propose a simple framework for inference in probabilistic topic models, denoted by FW. This framework is general and flexible enough to be easily adapted to mixture models. It has a linear convergence rate, offers an easy way to incorporate prior knowledge, and allows us to swiftly recover sparse latent representations of documents and to trade off sparsity against quality and time. We demonstrate the goodness and flexibility of FW over existing inference methods by a number of tasks and experiments."
139,HMM-based Temporal Pattern Modeling of Brain States in Smoke Rehabilitation using fMRI,"Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging method widely used in research on human physio-cognitive architecture. Substantial work has been performed using fMRI to map various cognitive functions to regions of the brain or to functional networks. Also, considerable effort has been made to interpret spatial activation patterns of these functional networks to better explain and understand the underlying cognitive states. Recent approaches to this problem involve data-driven analysis such as Independent Component Analysis (ICA) and Machine Learning (ML). However, these approaches do not fully account for the intrinsic temporal properties of fMRI data and do not always provide the explanatory power necessary to reveal underlying neural processes. To achieve a more thorough representation, we propose a novel technique based upon Hidden Markov Models (HMM). We apply it to classify cognitive states such as craving and resisting using fMRI data collected in a prior study. We address the challenges in modifying Independent Component (IC) based features to fit the proposed model, evaluate its classification accuracy along with its explanatory power, and compare it to other popular ML-based techniques such as Support Vector Machine (SVM) and Neural Network (NN). The results show that HMM-based models achieve an average classification accuracy of 83% which compares favorably to SVM. We also show that although NN achieves similar average accuracy across various numbers of classes, the HMM-based models show superior performance when distinguishing among a larger number of classes. Further, and most interestingly, we find that the optimal number of hidden states in HMM-based models agrees with the optimal number of functional networks for cognitive state classification previously found in the literature, which suggests that in this context HMM-based models possess the explanatory power often lacking in ML-based approaches."
140,Distributed Non-Stochastic Experts,"We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator. At each time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon T, while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal O(\sqrt{log(n)T}) regret bound at the cost of T communication. (ii) No communication: Each site runs an independent copy ? the regret is O(\sqrt{log(n)kT}) and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than \sqrt{kT} and communication better than T. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret O(\sqrt{k^{5(1+\epsilon)/6} T}) and communication O(T/k^\epsilon), for any value of \epsilon in (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off."
141,Ordered Rules for Classification: A Discrete Optimization Approach to Associative Classification,"We aim to design classifiers that have the interpretability of association rules yet have predictive power on par with the top machine learning algorithms for classification. We propose a novel mixed integer optimization (MIO) approach called Ordered Rules for Classification (ORC) for this task. Our method has two parts. The first part mines a particular frontier of solutions in the space of rules, and we show that this frontier contains the best rules according to a variety of interestingness measures. The second part learns an optimal ranking for the rules to build a decision list classifier that is simple and insightful. We report empirical evidence using several different datasets to demonstrate the performance of this method."
143,A Fast Greedy Algorithm for Structure Learning of Markov Random Fields,The problem of automatic structure learning of Markov Random Fields (MRFs) from data is considered. Structure learning is an NP-hard problem whose exact solution is intractable even for a fairly small number of variables. There are several approximate algorithms for solving this problem. Only few of them work with arbitrary size factors (not limited to pairwise factors) and they are still slow. We present a new faster algorithm for structure learning of discrete MRFs with arbitrary size factors. It is based on a greedy approach and uses a heuristics that reduces the search space to two subspaces guaranteed to have features with the highest score at each iteration of the algorithm. We show through experiments on real-world and simulated data sets that the proposed algorithm gives same accuracy and significantly improves learning time compared to the existing commonly used algorithms for structure learning of MRFs with arbitrary size factors.
144,Learning Image Descriptors with the Boosting-Trick,"In this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection. The main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes. This representation can  be improved using machine learning, however, past approaches  have been limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.  While kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive. We propose  to use the {\em  boosting-trick}  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space. The non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance."
145,Fast Resampling Weighted v-Statistics,"In this paper, a novel, computationally fast, and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed. To avoid any real resampling, we have linked this problem with finite group action and converted it into a problem of orbit enumeration. For further computational cost reduction, an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively. The computational complexity analysis shows reduction in the computational cost from n! or nn level to low-order polynomial level."
146,Multi-task Vector Field Learning,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach."
147,Squared-loss Mutual Information Regularization,"The information maximization principle, which prefers classifiers that maximize an information measure between data and labels, is a useful probabilistic alternative to the low-density separation principle. In this paper, we specify the squared-loss mutual information (SMI) as the information measure to be maximized and propose SMI regularization (SMIR) for semi-supervised classification. SMIR offers all of the following four abilities to semi-supervised algorithms: analytical solution, out-of-sample and multi-class classification, and probabilistic output. Furthermore, SMIR results in learning algorithms with data-dependent risk bounds that even incorporate the information of unlabeled data. Experiments demonstrate that SMIR compares favorably with state-of-the-art information-theoretic regularization approaches in terms of both accuracy and computational efficiency. "
149,Stochastic Frank-Wolfe Optimization for Large Margin Structured Prediction,"We consider the use of Frank-Wolfe optimization algorithms on the dualformulation of structural SVMs. These yield simple algorithms which only needaccess to an approximate maximization oracle for the structured predictionproblem and thus have wide applicability. This perspective provides insightson previous popular algorithms as we show that batch subgradient as well asthe cutting plane algorithms are equivalent to versions of Frank-Wolfealgorithms, enabling us to improve on their convergence analysis by harvestingthe Frank-Wolfe literature. Moreover, we propose a new stochastic coordinatedescent version of Frank-Wolfe which yields a provably convergent optimizationalgorithm for structural SVMs with total run-time \note{independent} of the number of training examples, like Pegasos, but with duality gap certificate guarantees and step-size robustness thanks to the use of line-search. Our experiments on sequence prediction indicate that this simple algorithm outperforms all other optimization algorithms which only have access to the maximization oracle."
150,Regularization Cascade for Joint Learning,"We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. We propose a top-down iterative method which starts with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased, until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes where different groupings of tasks seem particularly useful for effective sharing, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods."
151,Positivity and Transportation,"We study in this paper positive definite kernels between discrete probability measures. We prove that the weighted volume -- or generating function -- of the set of integral transportation matrices between two integral histograms $r$ and $c$ of equal sum is a positive definite kernel of $r$ and $c$ when the set of considered weights forms a positive definite matrix. The computation of this quantity, despite being the subject of a significant research effort, remains computationally intractable for histograms of very modest dimensions. We propose an alternative kernel which, rather than considering all matrices of the transportation polytope, only focuses on a sub-sample of its vertices known as Northwestern corner solutions. The resulting kernel is positive definite and can be computed with a linear complexity $O(R^2d)$ in the dimension $d$, where $R^2$ --  the total amount of sampled vertices -- is a parameter that controls the complexity of the kernel."
152,Multidimensional Artificial Field Embedding,"Embedding data nonlinearly has generated a surge on techniques that compute low dimensional representations of high dimensional observations. This paper examines and exploits the force field interpretation from mechanics to devise a general nonlinear embedding framework with properties for developing new dimension reduction models. In its simplified nature, the unifying framework yields formulations of several related existing techniques and yet with a fast optimization strategy. As an example, we propose a new dimension reduction model based on intuitive superposition of pair-dependent local functions of attraction and repulsion fields. Experiments on standard data sets suggest that the proposed approach offers models for better dimension reduction and visualization with strong capabilities to preserve local distances compared to other methods. Additional illustrations and a main theoretical result are provided to present new design insights for developing new nonlinear embedding methods."
154,Modeling the Forgetting Process using Image Regions,"While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten over time has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten over time, using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. "
156,Multi-agent Coordination through Adaptively Regularized Parallel Distributed Optimization,"Many problems require the analysis of large, high-dimensional datasets, which make the problem intractable for most optimization algorithms. In this paper, we propose using a parallel distributed adaptive regularizer (PDAR) to solve both the joint objective problems (JOP) and separated objective problems (SOP). The regularizer is dependent on the stepsize between iterations and the number of iterations executed, and the sub-problems are solved in parallel. We show convergence of our algorithm, and use a multi-agent two-bin resource allocation example to illustrate effectiveness. We also present example applications of our algorithm to biological and demand response problems. The numerical examples indicate that our parallel algorithm converges to the same optimal solution as sequential versionsdo, with significantly reduced computation time."
157,Object Focused Q-learning for Autonomous Agents ,"We present Object Focused Q-learning (OF-Q), a novel reinforcement learning algorithm that can offer exponential speedups over classic Q-learning on domains composed of nearly-independent objects. OF-Q treats the state space as a collection of different objects organized into different object classes. Our key contribution is to estimate the risk of different objects by learning non-optimal Q-functions that we incorporate into a control policy. We compare our algorithm to traditional Q-learning and previous arbitration algorithms in two domains, including a version of Space Invaders."
158,User Distances in Composite Social Networks,"An important challenge in social network analysis is how to measure users' distances or latent similarity as a single measure. It is the basis for link prediction, community detection, social marketing, etc. Due to the sparsity of data, where each user may just have a few connected friends, it is hard to effectively learn distance measures for any given pair of users in a single network. Nowadays however, people engage in multiple social networks, such as Facebook, Twitter, LinkedIn, etc., where these networks form a composite social network. To alleviate the data sparsity problem for a given single network, we propose a transfer metric learning approach to collectively exploit the knowledge from multiple networks, to extract related and richer knowledge through an optimization and boosting-based framework. Then, we use this knowledge for user distance modeling in a target network. It projects and combine social information, behaviors and profile attributes, measured in different magnitudes, in an embedding space that preserves important community information and network structure. We empirically evaluate the effectiveness of the constructed user distance measure on link prediction - an important social modeling task, and state-of-the-art methods are typically based on user distances. Empirical studies demonstrate that the proposed approach significantly improves the link-prediction precision over several state-of-the-art baselines."
159,Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions,"We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to be met in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efficient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains."
160,A Generalized Theory of PAC-Learning,"In this article we prove that probably approximately correct (PAC) learning is guaranteed under a more general assumption that the loss function has finite variance under the sample distribution. We establish our theory based on the existence of a metric between hypothesis, to which the loss function is Lipschitz continuous. These results will give hope for learning using unbouded loss functions and real-valued hypotheses, such as energy-based models, unbouded loss regression and neural networks. More importantly, even with such generalization, asymptotic bounds as good as previous formulations of PAC-learning have been achieved."
161,Kernel Information Bottleneck,"The Information Bottleneck (IB) method was introduced as a principled approach to extracting efficient representations of one set of variables with respect to another from empirical data, thus extending the classical notion of minimal sufficient statistics. The method was proposed as a general computational principle for information processing in the brain and has been used in many machine learning and neuroscience applications. The original algorithm for solving the problem was based on the Arimoto-Blahut alternating projection algorithm, but was not guaranteed to converge to a global optimum, which jeopardized the practicality of the approach. One exception was the multivariate Gaussian case, for which the IB was shown to have an efficient globally converging algorithm (GIB) that extended Canonical Correlation Analysis (CCA). The main advantage over CCA was that it provided a continuous optimal tradeoff between the minimality and sufficiency of the representation (described by the information curve), hence allowing for optimal multi-scale analysis of the data using simple spectral methods. Here we extend the Gaussian solution of the Information Bottleneck (GIB) to a much wider family of distributions using the kernel trick, and make it practical for essentially any empirical data. Our main theoretical result is in proving that for any kernel we can obtain a lower bound on the true information-curve, using information geometry. We illustrate the algorithm on real data and discuss some of its potential new applications. "
162,Compressed learning: learning in the smallest capacity machine,"Supervised  learning focuses on using finite input-output samples topredict the intrinsic relationship  between  input and output.Several methods such as neural networks, kernel method and redundantdictionary learning have been developed to tackle the problem andall of them have been proved to be universal approximants anduniversal consistent learners. However, the good approximation andgeneralization capabilities are established on the basis ofimplementing high computational complexity algorithms in largecapacity machines, which results extremely high computationalburdens in the learning processes. Motivated by the well knownKolmogorov width theory, we deduce optimal linear sparserepresentations for a large number of priors. Taking the set ofsparse representations as the desired machine, we construct smallestcapacity machines which achieves the optimal learning rate. Based onthis, we propose a new learning methodology called the compressedlearning that implements learning in the smallest capacity machinewith simple linear problem algorithms. Our analysis reveals that thecompressed learning is a high quality method that possesses palmaryapproximation capability, prominent generalization capability andlow computational burden."
163,Layered Dirichlet Process for Hierarchical Modeling and Multi-Level Segmentation of Sequential Data,"Hierarchical Bayesian Non-parametric models, such as the Hierarchical Dirichlet Process (HDP) and the HDP-HMM, have been proposed as infinite dimensional mixture models for grouped data problems. Many applications, such as multi-layer segmentation of news transcripts into broad categories and individual stories, require incorporation of prior knowledge at different layers of the model hierarchy. Such prior knowledge may include layer-specific exchangeability assumptions for the data, and group-specific prior distribution on atoms. Elegant incorporation of such knowledge requires a hierarchy of non-parametric processes where atoms at each layer correspond to one layer in the Bayesian parameter hierarchy. We propose the Layered Dirichlet Process (LDP), which consists of layered sets of DPs, where atoms at each layer map to Dirichlet Processes  for the next layer. This can also be interpreted as Layered Chinese Restaurant Process (LCRP), where table assignments for customers at one layer provide restaurant assignments at the next layer. We show how prior information at different layers can be naturally incorporated in our framework. For learning and inference, we propose a block-wise Gibbs sampling algorithm, that samples the layered table assignment of each data item as a block. We demonstrate using experiments that the proposed model outperforms the HDP-HMM and the sticky HDP-HMM in modeling and multi-layer segmentation of news transcripts. "
164,Part-segment Features for Body Pose Estimation,"We propose part-segment (PS) features for estimating an articulated pose in still images. The proposed PS features are developed to evaluate image likelihood of each body part (e.g. head, torso, and arms) robustly to background clutter and nuisance textures on the body and clothing. While general gradient-based features (e.g. HOG) might include many nuisance responses, the PS features represent only the boundaries of the body parts by iterative binary segmentation with updating shape prior on each part. The PS features are fused complementarily with gradient features using discriminative training and adaptive weighting. Comparative experiments with public datasets demonstrate improvement in pose estimation by the PS features compared with conventional features."
166,Mean Shift by Hebbian Learning for the L0-norm based Sparse Coding Problem,"It is well known that the emergence of Gabor-like receptive fields of simple cells in the visual cortex can be predicted by sparse coding, which has been validated to be a general coding strategy for sensory systems. Until now, the neural mechanism of the learning procedure is far to be understood. In this paper, a novel mean shift algorithm using Hebbian learning for the L0-norm based sparse coding problem is proposed. Different from other studies on sparse coding, our work do not consider the coefficients of basis functions but model the selection of basis functions. We perform an analysis on the spatial distribution of input samples and conclude that the basis functions are related to the local maxima of the distribution. Detailed theoretical investigation affirms this conclusion, showing that the sparse coding problem with the L0-norm is essentially one of mode detection and the basis functions are the modes of the kernel density estimate. The mean shift algorithm is presented for mode detection, and its updating rule is proved to be Hebbian. Experimental results demonstrate the robustness of the algorithm in producing basis functions well tuned for orientation as well as spatial frequency."
167,Incremental Subspace Learning for Unsupervised Domain Adaptation Using Manifold Optimization,"Developing recognition methods to handle cases in which the distribution of data changes between the training and testing phases of model building is a common problem in many fields. In computer vision, it is often the case that both well-defined geometric changes can be estimated, such as rotation in images, as well as more abstract transformations, such as changes in camera quality. It has recently been proposed that utilizing an incremental framework for adapting between two different domains is beneficial as it is able to encapsulate much of the change in the distribution between training and testing sets, regardless of the domain shift. In this paper, we adopt a general method for learning intermediate subspaces between training and testing domains on which we build models to perform both regression andclassification. We show that utilizing this method to obtain intermediate subspaces is more beneficial than similar methods both from a practical and a theoretical standpoint, as the proposed method admits an attractive interpretation in terms of posing it as an optimization problem. The method is used to improve performance in object recognition tasks when the shift in domain is not a geometric one and in age estimation tasks both when domain shifts are geometric and nongeometric."
168,On the Distribution of Salient Objects in Photographs,"In recent years it has become apparent that a Gaussian center bias can serve as an important prior for visual saliency detection, which has been demonstrated for predicting human eye fixations and salient object detection. Tseng et al. have shown that the photographer?s tendency to place interesting objects in the center is a likely cause for the center bias of eye fixations. In this contribution, we investigate the potential influence of the photographer?s center bias on salient object detection. Most importantly, we show that the centroid locations of salient objects in photographs correlate strongly with a Gaussian model. This is an important insight, because it provides a theoretical motivation and justification for the integration of such a center bias in salient object detection algorithms and helps to understand why Gaussian models are so effective. To assess the influence of the center bias on salient object detection, we integrate an explicit Gaussian center bias model into two recently proposed salient object detection algorithms. This way, we not just quantify the influence of the Gaussian center bias on pixel- and segment-based salient object detection, but we are also able to improve the state-of-the-art in terms of F1 score, F_\beta score, area under the recall-precision curve, area under the receiver operating characteristic curve, and hit-rate on the well-known data set by Achanta and Liu."
169,An Optimal Policy for Target Localization with Application to Electron Microscopy,"This paper considers the task of finding a target location by making a limited number of sequential observation. Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position. Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked. In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon. Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies."
170,Data split strategies for evolving predictive models,"A conventional textbook prescription for building good predictive models is to split the data into three parts: training set (for model fitting), validation set (for model selection), and test set (for final model assessment).  Predictive models can potentially evolve over time as developers improve their performance either by acquiring new data or improving the existing model. The main contribution of this paper is to discuss problems encountered and propose various workflows to manage the allocation of newly acquired data into different sets in such dynamic model building and updating scenarios. We propose three different workflows (parallel dump, serial waterfall, and hybrid) for allocating new data into the existing training, validation, and test splits. Particular emphasis is laid on avoiding the bias due to the repeated use of the existing validation or the test set. "
171,Multimodal similarity-preserving hashing,"We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable.The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches, our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks."
172,Multimodal diffusion geometry by joint diagonalization of Laplacians,"We construct an extension of diffusion geometry to multiple modalities through joint approximate diagonalization of Laplacian matrices. This naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of manifold learning, retrieval, and clustering demonstrating that the joint diffusion geometry frequently better captures the inherent structure of multi-modal data. We also show that many previous attempts to construct multimodal spectral clustering can be seen as particular cases of joint approximate diagonalization of the Laplacians."
173,Sparse Locality Preserving,"In this paper, we introduce a new subspace learning framework called Sparse Locality Preserving (SLP). Compared with the conventional methods considering global data structure, e.g., PCA, LDA, SLP aims at preserving the local neighborhood structure on data manifold and provides a more accurate data representation via locality sparse coding. In addition, it removes the common concerns of many local structure based subspace learning methods e.g., Local Linear Embedding (LLE), Neighborhood Preserving Embedding (NPE), that how to choose appropriate neighbors. SLP adaptively select neighbors based on their distances and importance, which is less sensitive to outliers than NPE. Moreover, the dual-sparse processes, i.e., the locality sparse coding, and sparse eigen-decomposition in graph embedding yield a noise-tolerant framework. Finally, SLP is learned in an inductive fashion, and therefore easily extended to different tests. We exhibit experimental results on several databases and demonstrate the effectiveness of the proposed method."
174,Active sampling as a curriculum for model selection,Conventionally active learning has been used to query the best example to label in order to reduce the labeling cost. In this paper we show how active learning (uncertainty sampling in particular) can be used as a curriculum strategy for nested model selection problems. We exploit the phase transition like phenomenon observed in active learning for misspecified models to select the number of components in mixture models.
175,Annotation models for crowdsourced ordinal labels,"In supervised learning scenarios when acquiring good quality labels is hard, practitioners often resort to getting the data labeled by multiple noisy annotators. Various methods have been proposed to estimate the consensus labels for binary and categorical labels bycorrecting for the bias of annotators. A commonly used paradigm to annotate instances when the labels are inherently subjective is to use ordinal scales. Theannotator is asked to rate an instance on a certain discrete ordinal scale. In this paper we propose annotator models based on Receiver Operating Characteristic (ROC) curve analysis to consolidate the ordinal annotations from multiple annotators.  The models lead to simple Expectation-Maximization (EM) algorithms that estimate both the consensus labels and annotator performance jointly. Experiments on data from different domains indicate that the proposed algorithm is superior to the commonly used majority voting rule. The ROC based models have an added advantage that the annotators can be ranked using the area under the estimated ROC curve. "
177,MILEAGE: Multiple Instance LEArning with Global Embedding," Multiple Instance Learning (MIL) methods generally represent each example as a collection of  individual instances, whereastraditional learning methods typically extract a global featurevector for the whole content of each example. Substantial priorresearch work has been proposed to solve  MIL problems. However, MILmethods do not always perform better than traditional learningmethods in all the cases. Limited research work has studied thisissue. This paper proposes a novel framework -- \emph{MultipleInstance LEArning with Global Embedding (MILEAGE)}, in which theglobal feature vectors for traditional learning methods areintegrated into the MIL setting. MILEAGE can leverage the benefitsderived from both learning settings. Within the proposed framework,a large margin method is formulated. In particular,  the proposedmethod adaptively tunes the weights on the two different kinds offeature representations (i.e., global and multiple instance) foreach example and trains the classifier simultaneously. Analternative algorithm is proposed to solve the resultingoptimization problem, which extends the bundle method to thenon-convex case. Some important properties of the proposed method,such as the convergence rate and the generalization error rate, areanalyzed. A series of experiments on both the image and textclassification tasks have been conducted to demonstrate theadvantages of the proposed method over several state-of-the-artmultiple instance and traditional learning methods."
178,Automatic Feature Induction for Stagewise Collaborative Filtering,"Recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms."
179,Learning Heteroscedastic Models via SOCP under Group Sparsity,"Sparse estimation methods based on  $\ell_1$  relaxation, such as the Lasso and the Dantzig selector,are powerful tools for estimating high dimensional linear models. However, in order to properly tune these methods, the variance ofthe noise is often required. This constitutes a major obstacle in applying these methods in several frameworks---such astime series, random fields, inverse problems---for which noise is rarely homoscedastic and the noise level is hard to know in advance.In this paper, we propose a new approach to the joint estimation of the conditional mean andthe conditional variance in a high-dimensional (auto-)regression setting. An attractive feature of the proposed estimator isthat it is efficiently computable even for very large scale problems by solving a second-order cone program (SOCP). We present numericalresults assessing the performance of the proposed procedure. We also establish non-asymptotic risk bounds which are nearly asstrong as those for original $\ell_1$-penalized estimators: the Lasso, the Dantzig selector and their grouped counterparts."
180,Selective Labeling via Error Bound Minimization,"In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods."
181,Textual features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks."
182,Volume Regularization for Binary Classification,"We introduce a large-volume box classification for binary  prediction, which maintains a subset of weight vectors, and  specifically axis-aligned boxes. Our learning algorithm seeks for a  box of large volume that contains ``simple'' weight vectors which  most of are accurate on the training set. Two versions of the  learning process are cast as convex optimization problems, and it  is shown how to solve them efficiently.  The formulation yields a  natural PAC-Bayesian performance bound and it is shown to minimize a  quantity directly aligned with it. The algorithm outperforms SVM and  the recently proposed AROW algorithm on a majority of $30$ NLP  datasets and binarized USPS optical character recognition datasets."
183,Towards Massive Multi-Way Classification: Structured Sparse Output Coding,"Multi-way classification with massive classes is a practical and challenging problem. In this paper, we propose structured sparse output coding, a principled way for massive multi-way classification, where a sparse output coding matrix is learned to maximize codeword separation and accuracy of each bit predictor. Moreover, we provide a concave-convex procedure based algorithm for the resultant optimization problem, which solves a series of l1 regularized convex optimization problems under linear constraints, using dual proximal gradient method. Experimental results demonstrate the effectiveness of our proposed approach."
184,Image Denoising and Inpainting with Deep Neural Networks,"We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method achieves state-of-the-art performance in the image denoising task. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning."
185,Low Rank Tensor Completion with Spatio-Temporal Consistency,"Video completion is a computer vision technique to recover the missing values in video sequences by filling the unknown regions with the known information.  In recent research, tensor completion, a generalization of matrix completion for higher order data, emerges as a new solution to estimate the missing information in video with the assumption that the video frames are homogenous and correlated.  However, each video clip often stores the heterogeneous episodes and the correlations among all video frames are not high. Thus, the regular tenor completion methods are not suitable to recover the video missing values in practical applications.  To solve this problem, we propose a novel spatially-temporally consistent tensor completion method for recovering the video missing data. Instead of minimizing the average of the trace norms of all matrices unfolded along each mode in a tensor data, we introduce a new smoothness regularization along video time direction to utilize the temporal information between consecutive video frames. Meanwhile, we also minimize the trace norm of each individual video frame to employ the spatial correlations among pixels. Different to previous tensor completion approaches, our new method can keep the spatio-temporal consistency in video and do not assume the global correlation in video frames. Thus, the proposed method can be applied to the general and practical video completion applications. Our method shows promising results in all evaluations on 3D biomedical image sequence and video benchmark data sets.  "
186,Max-Margin Structured Output Regression for Spatio-Temporal Action Localization,"Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because one needs to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efficient Max-Path search method, thus makes it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method significantly outperforms the state-of-the-art methods."
187,TCA: High Dimensional Principal Component Analysis for non-Gaussian Data,"We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate t and logistic and it is extended to the meta-elliptical by Fang (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s(log d/n)^{1/2} estimation consistency rate in the transelliptical distribution family, even if the distributions are very heavy-tailed, have infinite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is also implemented in both numerical simulations and large-scale stock data to illustrate its empirical performance. Both theories and experiments confirm that TCA can achieve model flexibility, estimation accuracy and robustness at almost no cost."
189,Matching Objects across the Textured-Smooth Continuum,"The problem of 3D object recognition is of immense practical importance and potential, with the last decade witnessing a number of breakthroughs in the state of the art. Most of the past work focused on the matching of textured objects using local appearance descriptors extracted around salient points in an image. The recently proposed bag of boundaries method was the first to address directly the problem of matching smooth objects using image based boundary features. However, no previous work has attempted to achieve a holistic treatment of the problem by jointly using textural and shape features which is what we describe in the present paper. Due to the complementarity of the two modalities (texture and shape), we combine the matching scores of textural and shape based representations by weighted summation. The optimal weighting is learnt in a data specific manner by optimizing discriminative performance on synthetically distorted data. For the textural description of an object we adopt a representation in the form of a histogram of SIFT based visual words. Similarly the apparent shape of an object is represented by a histogram of discretized features capturing local shape. Unlike previous work which uses an implicit, image based shape descriptor, we propose a more compact descriptor based on the local profile of boundary normals' directions. This descriptor is extracted at salient object boundary loci and at the corresponding characteristic scale, both detected automatically. On a large database of a diverse set of objects, the proposed method is shown to significantly outperform both purely textural and purely shape based approaches for matching across viewpoint variation. The advantage was particularly significant in the cases of large viewpoint changes between training and query data, when the correct rank-1 recognition rate was nearly twice that achieved using either of the modalities in isolation."
190,Efficiently Sampling Probabilistic Programs via Symbolic Execution,"Probabilistic programs are intuitive and succinct representations of complex  probability distributions. A natural approach to performing inference over these programs is to execute them, collect samples and compute statistics over the events of  interest. Indeed, this approach has been taken before in a number of probabilistic  programming tools. In this paper,  we address two key challenges of this paradigm: (i) ensuring samples are well distributed in the combinatorial space of the program, and (ii) efficiently  generating samples with minimal rejection. Our technique tackles the  first via systematic exploration across combinatorial choices, even in unbounded recursive programs, using symbolic computation. To solve the latter,  we compute weakest preconditions to hoist conditions to elementary distributions, and sample from the resulting conditional distributions thus avoiding rejection. We have implemented our algorithm in a tool called ESP and have used it on a number of benchmarks that include Probabilistic Context Free Grammars (PCFG) and Latent Dirichlet Allocation (LDA). Our results are encouraging -- we show comparable results with the state-of-the-art on these benchmarks, and significantly outperform other sampling based probabilistic inference approaches."
191,A Data-Dependent Risk Bound for Incremental Learning of Radial Basis Function Networks,"A data-dependent upper bound of an expected risk for radial basisfunction networks (RBFNs) is investigated. Because the risk bound isprovided in a quite practical form, it can be used for modelselection in nonlinear regression problems, especially when the RBFNincrementally recruits its basis functions. Asymptotic properties ofRBFNs are described for consistency of the incremental learningmethods. The risk bound is closely investigated for the leastsquares method and its properties are discussed."
192,Estimating the Maximum Expected Value: An Analysis of (Nested) Cross Validation and the Maximum Sample Average,"We investigate the accuracy of the two most commonestimators for the maximum expected value ofa general set of random variables: a generalizationof the maximum sample average, and cross validation.No unbiased estimator exists and we show that it isnon-trivial to select a good estimator without knowledgeabout the distributions of the random variables.We investigate and bound the bias and variance of theaforementioned estimators and prove consistency.The variance of cross validation can besignificantly reduced, but not without riskinga large bias. The bias and variance ofdifferent variants of cross validation are veryproblem-dependent, and a wrong choice canlead to very inaccurate estimates."
194,Action-Model Based Multi-agent Plan Recognition,"Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. Previous approaches require a library of team activity sequences (team plans) be given as input. However, collecting or maintaining a library of team plans is often difficult or costly. In this paper we relax this constraint, i.e., team plans are not required to be provided beforehand. We assume that a set of action models are available, which are already created to describe domain physics, i.e., preconditions that an activity is applied and effects after the activity is applied. These action models can be used to help identify team plans. We propose a novel approach to recognizing multi-agent plans based on action models rather than libraries of team plans. We encode the MAPR problem as a satisfaction problem and solve the problem using a state-of-the-art weighted MAX-SAT solver. In the experiment, we show that our algorithm is both effective and efficient."
195,On the L1 Distance Between Subspaces,"Subspaces are a popular representation in many computer vision applications.Among others, subspaces are used to represent faces under varying illumination, articulation of 2D shapes, and motion trajectories of rigid as well as non-rigid objects.As such, metrics for measuring the distance between subspaces are required for utilizing the subspace representation, e.g., for classification, matching, and clustering of subspaces.Indeed, in recent years various distance measures have been explored, often with a particular application in mind.Mutual to these measures is their reliance on the L2 metric, that ties the distance between the subspaces with the principal angles.While the L2-based distance measures are intuitive, it is long known that L1 norms are more adequate for optimization due to lower sensitivity to outliers.Therefore, in this paper we explore the L1 distance between subspaces.We analyze it's properties and provide intuition on its nature.We further suggest methods for speeding-up its computation.Finally, we show empirically that it is superior to the L2 distance in several applications."
196,Scalable Manifold Learning,"High computational costs of manifold learning make its application prohibitive for large point sets. A common strategy to overcome this problem is to sample a subset of points, called landmarks, on which the dimensionality reduction is performed and to reconstruct the embedding of all points using the Nystr?m method. In this paper, we address the two main challenges that arise in this setup. First, the selected subset of landmarks in non-Euclidean geometries must result in a low reconstruction error. Second, the nearest neighbor graph construction on sparsely sampled subsets must be robust and approximate the original data well. We propose an extension for sampling from determinantal distributions on non-Euclidean spaces by opearting on the geodesic distance on the manifold. Since current determinantal sampling algorithms have the same complexity as manifold learning, we propose an efficient approximation running in $\mO(ndk)$. We achieve excellent results with the proposed algorithm for manifold sampling by restricting the probability update to local neighborhoods. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Mahalanobis distance increases the robustness of dimensionality reduction on sparsely sampled manifolds. "
197,Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity,"Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superfluous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach."
198,Projection Kurtosis Concentration and Noise Estimation,"Kurtosis of 1D projections provides important characteristics of high dimensional data. In this work, based on the Gaussian scale mixture models of natural sensory signals, we first provide a theoretical underpinning to an empirically observed phenomenon that the kurtosis of band-pass filtered natural sensory signals tend to concentrate around a ``typical'' value.  Based on this result, we further describe a new effective methodology to estimate the variance and covariance matrix of Gaussian noise from a noise corrupted signal using {\em randomly} selected band-pass filters. The noise variance estimation method uses an objective function that has a closed-form solution and is robust to infrequent outlying kurtosis values. The estimation method of covariance matrix also affords efficient coordinate descent optimization. We demonstrate significant performance improvement of our methods on natural image and audio data sets over the current state-of-the-art methods."
199,Non-parametric Approximate Dynamic Programming via the Kernel	Method,"This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful, dimension-independent approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our non-parametric procedure is competitive with parametric ADP approaches."
200,Sampling GMRFs by Subgraph Correction,"The problem of efficiently drawing samples from a Gaussian Markov random field is studied. In this paper, we introduce the subgraph correction sampling algorithm, which makes use of any pre-existing tractable sampling algorithm for a subgraph by perturbing this algorithm so as to yield asymptotically exact samples for the intended distribution. The subgraph can have any structure for which efficient sampling algorithms exist: for example, tree-structured, with low tree-width, or with a small feedback vertex set. Experimental results demonstrate that the subgraph correction algorithm yields accurate samples much faster than many traditional sampling methods---such as Gibbs sampling---for many graph topologies."
201,Multidimensional Membership Mixture Models,"We present the multidimensional membership mixture (M3) models where every dimension of the membership represents an independent mixture model and each data point is generated from the selected mixture components jointly. This is helpful when the data has a certain shared structure. For example, three unique means and three unique variances can effectively form a Gaussian mixture model with nine components, while requiring only six parameters to fully describe it. In this paper, we present three instantiations of M3 models (together with the learning and inference algorithms): infinite, finite, and hybrid, depending on whether the number of mixtures is fixed or not. They are built upon Dirichlet process mixture models, latent Dirichlet allocation, and a combination respectively. We then consider two applications: topic modeling and learning 3D object arrangements. Our experiments show that our M3 models achieve better performance using fewer topics than many classic topic models. We also observe that topics from the different dimensions of M3 models are meaningful and orthogonal to each other. "
202,Multiclass Active Learning with Hierarchical-Structured Embedded Variance,"  We consider the problem of multiclass active learning where the relationship of the labels are represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to exploit the hierarchical structure of the label tree as well as the characteristics of the training data to select the most informative data for human labeling.  This goal can be achieved by a novel embedding-based approach called hierarchical-structured embedded variance, which learns an embedding of the labels that both preserves the structure of the label tree and reflects the characteristics of the training data. We show that the proposed approach is a generalization of entropy-based and cost-based uncertainty measure. We also demonstrate that notable improvement on the performance can be achieved with the proposed approach on synthetic and benchmark datasets."
203,Nearest Neighbor with Part-To-Whole Voting,"Near and nearest neighbor finding in high dimensions is an important building block in several machine learning paradigms, including KNN and RBF based classification and regression. In several applications, like visual object detection, near neighbor finding is the bottleneck for real time performance of such methods. We propose a new method for the problem, based on decomposition of the database vectors into parts, finding near neighbor for the parts, and voting from part to wholes to find the final neighbors. The algorithm is based on pre-computation of part to whole relations, enabling high run-time efficiency and effective trading of computation for memory resources. Analysis reveals that the method complexity is a sum of two terms, both sub-linear in the database size. Comparison with the state of the art methods shows the advantage of the proposed method."
204,Optimal Regularized Dual Averaging Methods for Stochastic Optimization,"This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth.  We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the optimal  rate of $O(\frac{1}{N}+\frac{1}{N^2})$ for $N$  iterations, which improves the best known rate $O(\frac{\log N }{N})$ of previous stochastic dual averaging algorithms. In addition, our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., $\ell_1$-norm), it has the advantage of encouraging sparser solutions. We further develop a multi-stage extension using the proposed algorithm as a subroutine, which achieves the uniformly-optimal rate $O(\frac{1}{N}+\exp\{-N\})$ for strongly convex loss."
205,Learning global properties of scene images from conditional correlational structure,"Scene images with similar spatial layout properties often display characteristic statistical regularities on a global scale. In order to develop an efficient code for these global properties that reflects their inherent regularities, we train a hierarchical probabilistic model to infer conditional correlational information from scene images. Fitting a model to a scene database yields a compact representation of global information that encodes salient visual structures with low dimensional latent variables. Using perceptual ratings and scene similarities based on spatial layouts of scene images, we demonstrate that the model representation is more consistent with perceptual similarities of scene images than the metrics based on the state-of-the-art visual features. "
206,The variational hierarchical EM algorithm for clustering hidden Markov models.,"In this paper, we derive a novel algorithm to cluster  hidden Markov models (HMMs) according to their probability distributions.We propose a variational hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ``cluster center'', i.e., a novel HMM that is representative for the group.We illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging."
207,Measuring Crowd Collectiveness,"Collective motions widely exist in crowd systems and receive many attentions from multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union, is a fundamental measurement for the studies of various crowd systems in nature. In this paper, by quantifying the topological properties of collective manifold of crowd, we propose a measurement of collectiveness for crowd systems as well as their constituent individuals, along with its efficient computation. \emph{Collective Thresholding} is then proposed to detect collective motions from random motions. We validate the proposed collectiveness on the system of self-driven particles, and analyze its effectiveness and robustness to identify collectively moving particles from randomly moving particles.  It is further evaluated through experiments on real bacterial colonies and pedestrian crowds."
208,Truncation-free Online Variational Inference for Bayesian Nonparametric Models," We present a truncation-free online variational inference algorithm for Bayesian nonparametric models. Unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly. Our experiments for Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms."
209,Consistency Analysis of  Empirical MEE Algorithm,"In this paper we study the consistency of the empirical minimum error entropy (MEE) algorithm for regression learning.Two types of consistency are studied. The error entropy consistency,which requires the error entropy of the learned functionapproximates the minimum error entropy, is shown to be always trueif the bandwidth parameter tends to 0 at an appropriate rate. Theregression consistency, which requires the learned functionapproximates the regression function, however, is a complicatedissue. We prove that the error entropy consistency implies theregression consistency for homoskedastic models where the noise isindependent of the input variable. But for heteroskedastic models, acounter-example is used to show the two types of consistency do notcoincide. A surprising result is that the regression consistency isalways true, provided that the bandwidth parameter tends to infinityat certain rates. This result, however, contradicts the motivationof MEE principle because the minimum error entropy is believed to benot approximated well with this choice of bandwidth parameter."
210,Multi-Granularity Image Categorization via Probabilistic Decoding,"Modern image data sets organize classes in a hierarchical taxonomy structure, such as a tree or DAG. Usually formulated as a multi-way classification, classical image categorization approaches can only predict leaf labels in such hierarchy. In this paper, based on the error correcting output coding formulation for multi-way classification, we propose a probabilistic decoding approach for both single-granularity, where only leaf level labels are allowed, and multi-granularity image categorization, which could generate internal labels from the taxonomy hierarchy if it is uncertain on leaf level labels. Experimental results demonstrate the effectiveness of our proposed image categorization approach on both single-granularity scenario and multi-granularity case."
212,3D Gaze Concurrences from Head-mounted Cameras,"A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanating from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the fixed relationship between the primary gaze ray and the head-mounted camera pose. The resulting gaze model enables us to build a social saliency field in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent mode-seeking in the social saliency field. Our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth."
213,Maximize the Ratio of Split to Sum of Diameters with Applications to Image Segmentation,"The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster, and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster. In this paper, we study the following problem: maximize the ratio of the minimum split to the sum of cluster diameters. In general, the problem is NP-hard for k >= 3 (k is the number of clusters). Here, we present an exact bipartition algorithm with the worst-case runtime O(n^4logn), where n is the number of objects. We apply the proposed algorithm to image segmentation to verify the validity of the proposed clustering criterion. Since the proposed algorithm is with high computational complexity, it is impractical to directly apply it to an image. So, we first use the farthest-point clustering algorithm to obtain a given number of superpixels of the original images, and then apply the proposed algorithm to those superpixels. The experimental results on Weizmann image segmentation challenge database demonstrate that the proposed algorithm is promising."
214,ROST: Realtime Online Spatiotemporal Topics for Navigation Summaries and Surprise Detection," We describe a novel online topic modeling framework to compute a low dimension descriptor of visual observations made by a mobile robot, which is sensitive to the structural and thematic changes in the environment. Our approach is designed to run in realtime, and is suitable for long term execution on a robotic platform. Using this image descriptor we build online anytime summaries consisting of surprising observations experienced by a robot thus far. The observations in the summary are chosen such that they cover the set of all observations in topic space, while minimizing the cover radius. Like almost any summarization method, the technique is meant to produce data for human consumption. Thus, we assess our approach on 307 human subjects and compare it to the classic bag-of-words description based summaries, and find it superior."
215,Efficient Sample Reuse in Policy Gradients with Parameter-based Exploration,"The policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control.A common challenge in this scenario is how to stabilizepolicy gradient estimates for reliable policy updates.In this paper, we combine the following three ideas and givea highly stable and practical policy gradient method:(a) the policy gradients with parameter based exploration,which is a recently proposed policy search method with high stability,(b) an importance sampling technique, which allows us to reuse previously gathered data in a consistent way,and (c) an optimal baseline, which minimizes the variance of gradient estimates with their unbiasedness being maintained.For the proposed method, we give theoretical analysis of the variance of gradient estimates and show its usefulness through experiments."
216,Multi-Label Learning With Millions of Categories,"Our objective is to build an algorithm for classifying a data point into a set of labels when the output space contains millions of categories. This is a relatively novel setting in supervised learning and brings forth interesting challenges such as efficient training and prediction, learning from only positively labeled data with missing and incorrect labels and handling label correlations. We propose a random forest based solution for jointly tackling these issues. We develop a novel extension of random forests for multi-label classification which can learn from positive data alone and can scale to large data sets. We generate real valued beliefs indicating the state of labels and adapt our classifier to train on these belief vectors so as to compensate for missing and noisy labels. In addition, we modify the random forest cost function to avoid overfitting in high dimensional feature spaces and learn short, balanced trees. Finally, we write highly efficient  training routines which let us train on problems with forty million training points, over a million dimensional sparse feature vector and over a million categories. Extensive experiments reveal that our proposed solution is not only significantly better than other multi-label classification algorithms but also more than 10% better than the state-of-the-art in our application domain."
218,Non-rigid Segmentation of Deformable Contour Objects in Sparse Low Dimensional Manifolds,"The segmentation of non-rigid visual objects using machine learning techniques usually involves high complexity search and training methodologies. This complexity is a consequence of the large dimensionality of the presentation used for the object contour, where the approach usually taken to circumvent this problem is to subdivide the original problem into a rigid detection followed by a non-rigid segmentation. The rationale behind this sub-division is based on the fact that the rigid detection is run in a lower dimensionality space than the original contour space and its result is then used to initialize and constrain the non-rigid segmentation in a higher dimensionality space. In this paper, we propose a new methodology for segmenting non-rigid visual objects, where the contours are directly represented in sparse low dimensional manifolds without requiring the sub-division of the problem described above. Our proposal shows significant smaller search and training complexities given that the dimensionality of the manifoldis much smaller than the dimensionality of the rigid and non-rigid search spaces aforementioned, and that we no longer require a two-stage segmentation process. We focus on the problem of left ventricle (LV) endocardial segmentation from ultrasound images, and our experiments testify that the use of sparse low dimensional manifolds reduces the search and training complexities of currentsegmentation approaches without a significant impact on thesegmentation precision shown by state-of-the-art approaches."
219,Which Ranking Measure shall We Use --- Some Suggestions from a Theoretical Perspective,"Ranking has been extensively studied in information retrieval, machine learning and statistics. A central problem in ranking is how to design or choose a ranking measure for the evaluation of ranking functions. In this paper we study, from a theoretical perspective, a class of ranking measures including NDCG, NDCG@k and Precision@k. Weanalyze, under some theoretical assumptions, the behavior of these ranking measures as the number of objects to rank getting large. Our theoretical results provide several suggestions for choosing ranking measures when there is a large set of objects to rank: 1) When employing NDCG as the ranking measure, it would be better to choose its cut-offversion NDCG@k and let $k$ grows with the number of objects; 2) If the users prefer a not-too-small $k$, it would be better to use a $r^{-\alpha}$ ($\alpha \in (0,1)$) discount instead of the $\frac{1}{\log(1+r)}$ in NDCG@k. We also conduct experiments on real data and find that our theory works well although the assumptions for the theorems may not hold in the real dataset."
221,Efficient Pool-Based Active Learning of Halfspaces,"We study pool-based active learning of halfspaces, in which a learner receives a pool of unlabeled examples, and iteratively queries a teacher for the labels of examples from the pool, in order to identify all the labels of pool examples. We revisit the idea of greedily selecting examples to label, and use it to derive an efficient algorithm, called ALuMA, that approximates the optimal label complexity for a given pool in $\reals^d$. We show that ALuMA obtains an $O(d^2 \log(d))$ approximation factor if the examples in the pool are numbers with a finite accuracy. We further prove a result for general hypothesis classes, showing that a slight change to the greedy approach leads to an improved target-dependent guarantee on the label complexity. In particular, we conclude a better guarantee for ALuMA if the target hypothesis has a large margin.  We further compare our approach to other common active learning strategies, and provide a theoretical and empirical evaluation of the advantages and disadvantages of the approach. "
222,Context-Sensitive Decision Forests for Object Detection,"In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods."
224,A Topic Model for Continuous Word Embeddedings,"While words in documents are generally treated as discrete entities,they can be embedded in a Euclidean space which reflects an \textit{a priori} notion of similarity between them.In such a case, a text document can be viewed as a bag-of-embedded-words (BoEW):a set of real-valued vectors.We propose a topic model for the BoEW:the generation process of words is modeled with a continuous mixture modelwhere each mixture component can be identified with a different topic.Retrieval and clustering experiments with the proposed BoEW representationshow significant improvements with respect to topic models computed from the traditional bag-of-words."
225,Estimating Nonstationary Inputs from a Single Spike Train Based on a Neuron Model with Adaptation,"Because every spike of a neuron is determined by input signals, a train of spikes may contain information about the dynamics of unobserved neurons. A state-space method based on the leaky integrate-and-fire (LIF) model, describing neuronal transformation from input signals to a spike train has been proposed for tracking input parameters represented by their mean and fluctuation. In the present paper, we propose to make the estimation more realistic by adopting an LIF model augmented with an adaptive moving threshold. Moreover, because the direct state-space method is computationally infeasible for a data set comprising thousands of spikes, we further develop a practical method for transforming instantaneous firing characteristics back to input parameters. The instantaneous firing characteristics, represented by the firing rate and non-Poisson irregularity, can be estimated using a computationally feasible algorithm. We applied our proposed methods to synthetic data and experimental data to clarify that they perform well."
226,Generalized sequential tree-reweighted message passing,"This paper addresses the problem of approximate MAP-MRF inference in general graphical models. Following [23], we consider a family of linear programming relaxations of the problem where each relaxation is specified by a set of nested pairs of factors for which the  marginalization constraint needs to be enforced. We develop a generalization of the TRW-S algorithm [6] for this problem, where we use a decomposition into junction chains, monotonic w.r.t. some ordering on the nodes. This generalizes the monotonic chains in [6] in a natural way. We also show how to deal with nested factors in an efficient way. Experiments show an improvement over min-sum diffusion, MPLP and subgradient ascent algorithms on a number of computer vision and natural language processing problems."
227,Dense Scattering Layer Removal ,"We propose a new model, together with advanced optimization, to separate a thick scattering media layer from a single natural image. It is able to handle challenging underwater scenes and images taken in fog and sandstorm, both of which are with significantly reduced visibility. Our method addresses the critical issue -- this is, originally unnoticeable impurities will be greatly magnified after removing the scattering media layer -- with transmission-aware optimization. We introduce non-local structure-aware regularization to properly constrain transmission estimation without introducing the halo artifacts. A selective-neighbor criterion is presented to convert the unconventional constrained optimization problem to an unconstrained one where the latter can be efficiently solved."
228,Unsupervised Object Matching via Probabilistic Latent Variable Models,"We propose a probabilistic latent variable model for unsupervised object matching, which is the task of finding correspondences between objects in different domains. With existing object matching methods, the numbers of objects in different domains must be the same, and the methods find one-to-one matching in two domains. The proposed model can handle multiple domains with different numbers of objects, and can find many-to-many matching. The proposed model assumes that there is a set of latent vectors that is shared by all domains, and each object is generated using one of the latent vectors and a domain-specific linear projection. By inferring a latent vector to be used for generating each object, we can match objects in an unsupervised manner. We demonstrate the effectiveness of the proposed model with experiments using synthetic, handwritten digit, music, and text data sets."
229,Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button,"A brain-computer interface (BCI) allows users to ?communicate? with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue.This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm UCB-classif based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session.Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%."
230,Online Discrimination of Nonlinear Dynamics with Switching Differential Equations,"How to recognise whether an observed person walks or runs? We consider a dynamic environment where observations (e.g. the posture of a person) are caused by different dynamic processes (walking or running) which are active one at a time and which may transition from one to another at any time. For this setup, switching dynamic models have been suggested previously, mostly, for linear and nonlinear dynamics in discrete time. Motivated by basic principles of computations in the brain (dynamic, internal models) we suggest a model for switching nonlinear differential equations. The switching process in the model is implemented by a Hopfield network and we use parametric dynamic movement primitives to represent arbitrary rhythmic motions. The model generates observed dynamics by linearly interpolating the primitives weighted by the switching variables and it is constructed such that standard filtering algorithms can be applied. In two experiments with synthetic planar motion and a human motion capture data set we show that inference with the unscented Kalman filter can successfully discriminate several dynamic processes online. "
231,Discriminative Low-Rank Representation Graph for Semi-supervised Learning,"The recently proposed low-rank representation (LRR) method is effective in ex-ploring subspace structures. However, LRR graph has no explicit connection tothe classification task. In this paper, we propose a discriminative low-rank rep-resentation (DisLRR) graph for semi-supervised learning. Our DisLRR graphcould not only seek the lowest rank representations among all the elements in thedictionary, but also incorporate discriminative information from labeled and un-labeled samples. The convergence of our algorithm is theoretically proved. Thenwe present a semi-supervised learning method by incorporating DisLRR graphand Gaussian harmonic function (GHF). Experimental results on a toy data set,the PIE, Extended YaleB and ORL databases demonstrate that our DisLRR graphoutperforms other related graphs, especially when the data are heavily corrupted."
232,PAC-Bayesian Structured Output Regression,We provide a theoretical basis for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the structured prediction loss when it is defined in terms of a positive definite kernel in the output space. We provide two PAC-Bayes upper bounds of the structured prediction risk that depend on the empirical quadratic risk of the deterministic predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that has never been proposed so far. Both predictors are compared on practical tasks.
233,Information-Theoretic Limits on Model Selection for Gaussian Markov Random Fields in the High-Dimensional Setting,"This paper focuses on the information-theoretic limitations of model selection for Gaussian Markov random fields in the high-dimensional setting, where the graph size $p$ and the number of edges $k$ are allowed to scale with the sample size $n$. We provide an a rigorous analysis of this problem for generic graphs in an ensemble. Our result establishes a necessary condition on the sample size $n(p,k)$ for any procedure, regardless of its computational complexity, to consistently recover the underlying graph. Moreover, our analysis implies a connection between that graphical model selection limits and eigenvalues of concentration matrices. The key way out of the difficulty is found via investigating the orthogonal systems from concentration matrices, making it possible to calculate the symmetric Kullback-Leibler divergence between generic graphs and obtain the final simple result. Our method of analyzing generic graphs using orthogonal systems would be of use to other model selection problems."
235,Dynamic Classification of Ballistic Missiles Using Neural Network and HMM,"This paper addresses dynamic classification of different Ballistic Missiles based on kinematic attributes acquired by radars for taking appropriate measures to tackle them. Real Time Neural Network and Hidden Markov Model is applied for dynamic classification of the target trajectory and results compared for performance and time taken in each case. There are many applications which require capturing scenario dynamically and predicting its class. For air defense application, it is a major challenge to recognize the kind of incoming threats and counteract with corresponding counter-measures. A model is developed and experiment conducted with 6DOF simulated data for evaluating the model. "
236,Learning Useful Abstractions from the Web: Case Study on Patient Medications and Outcomes,"The successful application of machine learning to electronic medical records typically turns on the construction of an appropriate feature vector.  That often depends upon the ability to find an appropriate way to abstract the large number of variables found in such records.  In this paper, we explore the use of topic modeling to design feature vectors in an automated manner by harnessing expertise available on the Web. We test the proposed methods on the task of inferring useful abstractions from a list of thousands of medications. Using Latent Dirichlet Allocation we learn a topic model based on Web entries corresponding to each drug in the list. Using only knowledge from Wikipedia pages, we were able to learn a model that is similar to the curated drug classification scheme that serves as an industry standard. We further demonstrate the utility of these learned abstractions through the construction of a kernel based on the earth mover's distance and derived from the learned topic model. Applied to a corpus of 25,000 patient admissions, we use this kernel to predict three different adverse outcomes (death, an abnormally long stay, or admission through the emergency room) for the next hospital admission.  Somewhat surprisingly,  the classifiers built using the learned abstractions outperform classifiers learned from the curated drug classification scheme."
237,Learning Model-Based Sparsity via Projected Gradient Descent,"Several convex formulation methods have been proposed previously for statistical estimation with structured sparsity as the prior. These methods often require a carefully tuned regularization parameter, often a cumbersome or heuristic exercise. Furthermore, the estimate that these methods produce might not belong to the desired sparsity model, albeit accurately approximating the true parameter. Therefore, greedy-type algorithms could often be more desirable in estimating structured-sparse parameters. So far, these greedy methods have mostly focused on linear statistical models. In this paper we study the projected gradient descent with non-convex structured-sparse parameter model as the constraint set. Should the cost function have a Stable Model-Restricted Hessian the algorithm converges to the desired minimizer up to an approximation error. As an example we elaborate on application of the main results to estimation in Generalized Linear Model."
238,Fast Probabilistic Optimization from Noisy Gradients,"Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, we construct a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from *noisy* evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions."
239,Hierarchical Visual Feature Learning for Computer Aided Diagnosis Using 3D Medical Images on Large-Scale Evaluation,"Computer aided diagnosis (CAD) of cancerous anatomical structures via 3D medical images has emerged as an intensively studied research area. In this paper, we present a principled three-tiered image feature learning approach, to capture task specific and data-driven class discriminative statistics from annotated image database, apart from often hand-crafted, heuristic approaches. It integrates voxel-level, instance-level and database-level feature learning, aggregation and parsing. We demonstrate its effectiveness in unified lung nodule, lung vascular structure and colon polyp feature computation and detection by classification in CAD applications. The other advantage includes that it only requires fast segmentation-less (e.g., simple thresholding on voxel-level labeling probabilities) image processing in training and runtime, which also alleviates the classification bias on certain (over or under) segmentation algorithms. After instance-level aggregation, features can also be flexibly tuned for classifying positive and negative cancers/tumors, or discriminating different subcategories of nodule/polyp, e.g., according to the clinically-relevant size or shape morphologies, within our designed gating classifier. Our hierarchical feature learning framework enables to achieve significantly superior performances than previous state-of-the-art CAD systems, extensively validated on highly representative multi-site clinical datasets using $879$ and $770$ CT volumes, for lung and colon CAD tasks respectively. "
240,Discriminatively Activated Sparselets,"As the number of object classes becomes large, redundancy amonglearned object models increases substantially and thus naturallymotivates the idea of compact intermediate representations that canbe shared across classes for efficient multiclass inference. Recently,a new universal intermediate representation for multiclass objectdetection, sparselets, was introduced yieldingone to two orders of magnitude reduction in inference time andenabling real-time multiclass object detection. However, as computationalefficiency is gained by making the sparselet activations increasingly sparse, the task performance of reconstructive sparseletmodels degrades unfavorably.  This paper provides a generalformalism where sparselet activations are learned discriminativelyin a structured output prediction framework. Our experimental resultson multiclass object detection and multiclass image classificationdemonstrate that the proposed discriminative sparselet activationsmaintain high task performance while achieving greater sparsity,which in turn significantly improves inference efficiency."
241,On Consistent Classification with Imbalanced Classes,"We consider the problem of imbalanced classes in binary classification, where one class is rare compared to the other. This problem arises frequently in practice and has been widely studied. However very little is understood in terms of the theoretical properties of the problem or of the algorithms proposed: what performance measures are appropriate, how these affect the learning process, and whether the algorithms are statistically consistent with respect to the desired performance measures. In this paper, we initiate a formal study of these issues, focusing on the balanced 0-1 error that evaluates errors on the majority and minority classes separately and effectively balances the two. The underlying balanced 0-1 loss bears similarity to cost-sensitive losses; however a critical difference between the two is that the balanced loss depends on the underlying distribution, while cost-sensitive losses are defined independent of the distribution. We establish statistical consistency of two types of algorithms with respect to the balanced 0-1 error: plug-in rules that use an empirically determined threshold, and certain types of empirically balanced risk minimization algorithms. Our experiments support our theoretical results, showing that both these approaches perform as well as (or better than) under-/over-sampling methods that are currently viewed as the state of the art."
242,Passivity-based Monitoring of POMDPs,"Maintaining exact belief states in POMDPs can be a difficult task since the size of the belief state grows exponentially with the number of state variables. Boyen and Koller described an approximation method which exploits locality in the process by maintaining smaller belief states over clusters of correlated variables. While this is a useful method, it does not fully account for the causal relations between the variables. We study a particular type of causal relation, called passivity, which captures the notion that a variable changes only if any of the variables that directly influence it change or if it is the target of an action. We show that passivity can be exploited in conjunction with locality to accelerate the monitoring task. The idea is to maintain separate beliefs over subsets of correlated variables, and to update only those beliefs whose variables we suspect to have changed. We present an algorithm, called Passivity-based Parallel Monitoring (PPM), that implements this idea. We show empirically that PPM outperforms two state-of-the-art solutions, while maintaining competitive accuracy. Our experiments indicate that the relative computational gains grow significantly with the size of the process."
243,Multiplicative Forests for Continuous-Time Processes,"Learning temporal dependencies between variables over continuous time is an important and challenging task. Continuous-time Bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices, which grows exponentially in the number of parents per variable. We develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits. Using a multiplicative assumption we show how to update the forest likelihood in closed form, producing efficient model updates. Our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability."
245,Group-wise FMRI Activation Detection Based on DICCCOL ,"Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and its less sensitivity to variabilities in individual subject?s brains. However, current group-wise fMRI activation detection methods rely on the co-registration of individual brain?s fMRI images into the same atlas space, which has difficulty in dealing with the remarkable anatomical variation of different brains. As a consequence, the resulted misalignments between different brains could significantly degrade the required inter-subject correspondences, thus reducing the sensitivity and specificity of group-wise fMRI activation detection. This paper presents a novel approach to detect group-wise fMRI activation on the recently developed and validated Dense Individualized and Common Connectivity-based Cortical Landmarks (DICCCOL), which is a dense map of cortical landmarks that possess intrinsic structural and anatomical correspondences across individuals and populations. The basic idea in this paper is that fMRI activation detection is first performed on each corresponding DICCCOL landmark in the individual brain?s own space using the general linear model (GLM), and then the activation significances of the same landmark from a group of subjects are statistically integrated and assessed at the group level. Finally, the consistently activated landmarks are determined and declared as the detected brain locations in response to external stimuli. Our experimental results demonstrated that the proposed approach can locate activation substantially more precisely than the traditional group-wise activation detection methods. "
246,An Analytic and Empirical Evaluation of Return-on-Investment-Based Active Learning,"Return-on-Investment (ROI) is a cost-conscious approach to active learning (AL)that considers both estimates of cost and of benefit in active sample selection.In this paper, we investigate the conditions for successful cost-conscious ALusing ROI by proving the conditions under which ROI would be optimal. We thenempirically measure the degree to which optimality is jeopardized in practicewhen the conditions are violated. We find that the more linearly related abenefit estimator is to true benefit, the better it performs when paired with animperfect cost estimate in ROI.  Lastly, we use our analysis to explain themixed results of previous work. Our results show that ROI can indeedsuccessfully reduce total annotation costs."
247,Patient Risk Stratification for Hospital-Associated C. Diff as a Time-Series Classification Task,"A patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state. We explore representing patient risk as a time series. In doing so, patient risk stratification becomes a time-series classification task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted. Thus, we begin by defining and extracting approximate \textit{risk processes}, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns. We apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \textit{Clostridium Difficile}. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05). "
249,A Stochastic Spiking Network Model of Sensorimotor Control,"Despite decades of research on sensorimotor control, there are no models thatprovide a link between the stochastic activity of neuron populations and humanbehavior when faced with uncertain, redundant, or novel environments. Here wepropose a new computational model that provides a direct link between neuronalactivity and behavior. Our model is based on a recent mathematical frameworkthat works with state probability densities rather than explicit state variables. Itcomprises a spiking neural network including sensory receptors, sensory cortex,control operators, and motoneurons. Sensory cortex computes internal state probabilityestimates by Bayesian filtering of measurements from sensory receptorsand efference information from the motor neurons. Motoneuron commands arecalculated by optimizing a cost/value function, using internal estimates and controloperators stored as synaptic weights. We simulated the model in Matlab andimplemented it on a Phantom robot arm. Simulations and robotic demonstrationspredicted a wide variety of behavior, such as reaching and tracking, reflexes intask-relevant directions, and reward/penalty trade-off responses. Our new computationalmodel can provide a neurophysiological explanation of specific humansensorimotor functions under uncertainty."
250,Multivariate Convex Regression with Adaptive Partitioning,"We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function.  Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is computationally feasible for more than a few hundred observations.  We introduce Convex Adaptive Partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is computationally efficient, in stark contrast to current methods. We show that CAP has a computational complexity of O(n log(n)^2) and also give consistency results. Empirically, CAP shows dramatic improvement over existing methods in terms of runtime and predictive error."
251,Multiclass Learning Approaches: A Theoretical Comparison with Implications,"We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error."
252,Human Activity Learning using Object Affordances from RGB-D Videos,"Human activities comprise several sub-activities performed in a sequence and involve interactions with various objects. This makes reasoning about the object affordances a central task for activity recognition. In this work, we consider the problem of jointly labeling the object affordances and human activities from RGB-D videos. We frame the problem as a Markov Random Field where the nodes represent  objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural SVM approach, where labeling over various alternate temporal segmentations are considered as latent variables. We tested our method on a dataset comprising 120 activity videos collected from four subjects, and obtained an end-to-end precision of 81.8% and recall of 80.0% for labeling the activities."
253,"Top-k Feature Selection via ?2,0-Norm Constraint","Real-world applications, such as bioinformatics, often need select top-$k$ features in the classification tasks. Previous sparse learning based feature selection methods impose the sparsity regularization to learn the features weights and rank them to select the top-$k$ features. Because the ranking weights were learned not for the exact top-$k$ features, such feature selection methods may not get the optimal results. We propose a novel and robust exact top-$k$ feature selection approach using an explicit $\ell_{2,0}$-norm constraint without any extra parameter. An efficient algorithm based on augmented Lagrangian method is derived to solve the $\ell_{2,0}$-norm constrained objective to find out the stable local solution. Extensive experiments on four biological datasets show that although our proposed model is not a convex problem, it outperforms the approximate convex counterparts and state-of-the-art feature selection methods in terms of classification accuracy on two popular classifiers. Because the regularization parameter of our method has explicit meaning (in bioinformatics applications $k$ is often fixed), \emph{i.e.} the number of selected feature, it avoids the burden of tuning the parameter and is a pragmatic feature selection method."
254,Stochastic Gradient Descent with Only One Projection,"Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $O(1/\sqrt{T})$ convergence rate for general convex optimization, and an $O(\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function."
255,Learning for Structured Prediction Using Stochastic Descent with Working Sets,"We propose an approximate subgradient descent algorithm using working sets to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM.  We focus on the settings of general graphical models, such as MRF and CRF with loops, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM as well as existing subgradient based methods. We show that under mild conditions the proposed method approximates subgradient descent arbitrarily closely by finding a true subgradient with probability tending to one as the working set grows.  This is desirable in our settings, since the intractability of exact inference implies that true subgradients cannot be reliably obtained.  Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, dramatically reducing learning time while maintaining the same level of performance.  We demonstrate the strength of our method empirically in the context of image segmentation."
256,Affect Sensitive Music Recommendation using Brain Computer Interfaces,"This paper aims to explore the problem of building an affect sensitive music recommendation system. Specifically, the proposed system harnesses electroencephalograph (EEG) signals for the purpose of assessing the listener's affective state. The core idea lies on the hypothesis that cortical signals captured by off-the-shelf electrodes carry enough information about mental state of a listener and can be used to build preference models over musical taste for each individual user. We present a reinforcement learning algorithm that aims to build such models over a period of time and then use it effectively to provide recommendations such that enable the listener to achieve the target mental state. Our experiments on real users indicate that the recommendation policy learnt via the brain-computer interface provides better recommendations than commercial services such as Pandora, that do not incorporate affect."
257,Balancing a Pole with a Cerebellum,"Much is yet to be learned about the detailed biological functioning of the human brain. Of all the parts of the brain, one of the best understood at the level of synaptic organization is the cerebellum. This article describes a biologically constrained cellular-level model of the mammalian cerebellum which is capable of learning to balance an inverted pendulum. Results indicate that the sample complexity of this novel learning agent surpasses baseline reinforcement learning and control agents. Experiments with this model predict that regular error signals play an essential part in avoiding extinction of learned responses. These predictions indicate a direction for future studies involving the acquisition of long-lasting behaviors in the absence of repeated error signals."
258,On the Reconstruction of Piecewise Constant Dependences,"An approach to the restoration of dependences (regressions) is proposed that is based on solving problems of supervised classification. The main task is finding the optimal partitioning of the range of values of dependent variable on a finite number of intervals. It is necessary to find optimal number of change-points and their positions. This task is formulated as search and application of  piece-wise constant function. When restoring piecewise constant functions, the problem of local discrete optimization using a model of logic supervised classification in leave ?one-out mode is solved. The value of the dependent value is calculated in two steps. At first, the problem of classification of feature vector is solved. Further, the dependent variables is calculated as half of the sum of change-points values of the corresponding class"
259,Fast Max-kernel Search,"The wide applicability of kernels make the problem of max-kernel searchubiquitous and more general than the usual similarity search. We focus onsolving this problem efficiently. We begin by characterizing the inherenthardness of the max-kernel search problem with a novel notion of {\emdirectional concentration}. Following that, we present a method to use an $O(n\log n)$ scheme to index any set of objects (points in $\Real^\dims$ or abstractobjects) \textit{directly in the kernel space} without any explicitrepresentation of the points in this kernel space. A provably $O(\log n)$branch-and-bound algorithm is presented using this index for exact max-kernelsearch. Empirical results for a variety of data sets as well as abstract objectsdemonstrate up to 4 orders of magnitude speedup in some cases. Extensions forapproximate max-kernel search are also presented."
260,Sparse Approximation via Penalty Decomposition Methods,"In this paper we consider general sparse approximation problems, that is, general $l_0$ minimization problems with the $l_0$-``norm'' of a vector being a part of constraints or objective function. We assume that the $l_0$ part is the only nonconvex part in these problems. We then propose penalty decomposition (PD) methods for solving them in which a sequence of penalty subproblems are solved by a block coordinate descent (BCD) method. Under some suitable assumptions, we show that any accumulation point of the sequence generated by the PD method is a local minimizer of the problems. Moreover, we establish that any accumulation point of the sequence generated by the BCD method is a local minimizer of the penalty subproblem. Finally, we test the performance of our PD methods by applying them to sparse logistic regression and compressed sensing problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality.    "
261,Sparse Principal Components Analysis via Decomposition Methods,"In this paper we first propose new formulations for sequentially finding sparse PCs by using $l_0$-``norm'' of the loading vector as part of the constraints. By using these formulations, a sparse PC can be obtained with the desired cardinality of the loading vectors. Then we propose formulations that also consider the orthogonality of the loading vectors. We further generalize the proposed formulations to simultaneously find the first $r$ sparse PCs. To solve the proposed formulations, we apply decomposition methods. Under some suitable assumptions,  we show that any accumulation point of the sequence generated by the proposed decomposition methods satisfies the first-order optimality conditions of the corresponding problems. Finally, we compare the sparse PCA approaches proposed in this paper with several existing methods using gene expression data. The computational results demonstrate that the sparse PCs obtained by our approaches substantially outperform those obtained by the other methods in terms of solution quality. "
262,"Neuronal spike generation mechanism as an oversampling, noise-shaping A-to-D converter","We explore the hypothesis that the neuronal spike generation mechanism is an analog-to-digital converter, which rectifies low-pass filtered summed synaptic currents and encodes them into spike trains linearly decodable in post-synaptic neurons. To digitally encode an analog current waveform, the sampling rate of the spike generation mechanism must exceed its Nyquist rate. Such oversampling is consistent with the experimental observation that the precision of the spike-generation mechanism is an order of magnitude greater than the cut-off frequency of dendritic low-pass filtering. To achieve additional reduction in the error of analog-to-digital conversion, electrical engineers rely on noise-shaping. If noise-shaping were used in neurons, it would introduce correlations in spike timing to reduce low-frequency (up to Nyquist) transmission error at the cost of high-frequency one (from Nyquist to sampling rate). Using experimental data from three different classes of neurons, we demonstrate that biological neurons utilize noise-shaping. We also argue that rectification by the spike-generation mechanism may improve energy efficiency and carry out de-noising. Finally, the zoo of ion channels in neurons may be viewed as a set of predictors, various subsets of which are activated depending on the statistics of the input current. "
264,Assessing Blinding in Clinical Trials,"The interaction between the patient's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants' feedback."
265,Supervised $N$-gram Topic Model,"We propose a Bayesian nonparametric topic model that detects both label and the corresponding word/phrase, simultaneously, from labeled data. Unlike existing supervised topic models, which are based on the bag-of-words assumption, our proposal, labeled $N$-gram topic model (LNT) focuses on word order, and the interdependency between a given label and word/phrase in a text to detect objective information. Toward this direction, this model takes a hierarchical Bayesian nonparametric approach where each hidden variable is distributed  according to a Pitman-Yor process.Theoretically, this model overcomes several inherent limitations and weaknesses of conventional topic models: the decision of selecting appropriate number of topics and the lack of power-law distribution in the word frequencies.  Experiments on review articles show that LNT is useful as a generative model to discover objective phrase, and provides a complement of a human expert and domain specific knowledge. "
266,Loss-Regularized CRF for Preference Aggregation,"We develop a flexible Conditional Random Field framework for supervised preference aggregation, which combines preferences from multiple experts over items to form a distribution over rankings. We explore methods of optimizing the parameters of this distribution given the expert preferences, some of which incorporate the loss metric used at test time, and propose a new loss-based training objective. Unlike existing aggregation methods the resulting model can incorporate most existing test metrics, and permits efficient optimization. Experiments on benchmark tasks demonstrates significant gains over existing rank aggregation methods."
267,Tree-structured Kernel Dimension Reduction,"Tree-structured approaches are to iteratively split data until some termination criterion is satisfied.  Most of the approaches can deal with either supervised learning, or unsupervised learning, but not both.  In this paper, we propose (residual) tree-structured Kernel Dimension Reduction (rtKDR/tKDR) approaches to address the issue. Specifically, we alternatively use kernel dimension reduction (KDR) to estimate a linear projection direction of (residual) response variables in each non-terminal node for maximizing conditional independence between explanatory variables and (residual) response variables under Reproducing Kernel Hilbert Spaces, and split the (residual) data based on the median of projection indices that the (residual) data project into the direction. When the explanatory variables are continuous, discrete and response ones, rtKDR/tKDR can deal with not only supervised learning but also unsupervised learning.  Furthermore, rtKDR has the potential of discovering the intrinsic dimensions from high-dimensional nonlinear data sets. Experiments in several benchmark datasets indicate that rtKDR/tKDR attain better space partition and prediction performances compared with several recently published space partition algorithms. "
268,Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification,"In graph-based semi-supervised learning approaches, the classification rateis highly dependent on the size of the available labeled data, as well as the accuracy of the similaritymeasures (or equivalently the distance metric).Here, we propose a semi-supervised multi-class/multi-label classification scheme,dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process.Existing classification methods often have difficulty in dealing withmulti-class/multi-label problems due to the use of fixed similarity measures; our algorithm insteademphasizes metric fusion in a dynamic way. Significant improvement over thestate-of-the-art methods is observed onbenchmark datasets for hand-written digits and object recognition."
269,A Recalibration Procedure which maximizes the AUC: A Use-Case for Bi-Normal Assumptions,"Area under the receiver operating characteristic curve (AUC) is a popular measure for evaluating the quality of binary classification rules. Commonly used score-based classifiers label an outcome as a positive if the score is greater than a certain threshold. We show that this may not be optimal in terms of maximizing the AUC. Under certain assumptions the optimal thresholding rule is derived using the Neyman-Pearson lemma. Specifically, we show that a thresholding rule that isquadratic in the score dominates the commonly used linear thresholding rule. Our work provides a real data use-case where the recalibration significantly improvesthe AUC."
270,Human Boosting,"Humans are exceptional learners, but like machine learning algorithms, they have inductive biases and on some learning tasks they perform poorly. In this paper, we try to work around these biases using boosting. Boosting allows us to combine several weak learners and construct a strong learner. We consider classification (category learning) tasks and adapt the FilterBoost framework for use with human learners. Boosting is sequential in nature and cannot be easily verified in a laboratory setting. We use Mechanical Turk to design and conduct experiments on synthetic and real world datasets which test certain properties of human learners and discuss potential applications for this framework."
271,Scalable nonconvex inexact proximal splitting,"We study large-scale, nonsmooth, nonconconvex optimization problems. In particular, we focus on nonconvex problems with \emph{composite} objectives. This class of problems includes the extensively studied convex, composite objective problems as a special case. To tackle composite nonconvex problems, we introduce a powerful new framework based on asymptotically \emph{nonvanishing} errors, avoiding the common convenient assumption of eventually vanishing errors. Within our framework we derive both batch and incremental nonconvex proximal splitting algorithms. To our knowledge, our framework is first to develop and analyze incremental \emph{nonconvex} proximal-splitting algorithms, even if we disregard the ability to handle nonvanishing errors. We illustrate our theoretical framework by showing how it applies to difficult large-scale, nonsmooth, and nonconvex problems."
272,Learning to Discover Social Circles in Ego Networks,"Our personal social networks are big and cluttered, and currently there is no good way to organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g. `circles' on Google+, and `lists' on Facebook and Twitter), however they are laborious to construct and must be updated whenever a user's network grows. We define a novel machine learning task of identifying users' social circles. We pose the problem as a node clustering problem on a user's ego-network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user profile information. For each circle we learn its members and the circle-specific user profile similarity metric. Modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter for all of which we obtain hand-labeled ground-truth data."
273,Statistical modeling of indoor scenes and objects,"We develop a Bayesian generative model for understanding indoor scenes.  While state-of-the-art methods approximate objects in these environments with gross 3D geometry (e.g., bounding boxes), we propose using geometric representations with a finer granularity. For example, we model a table as a set of four legs and a top.  Such models enhance recognition and reconstruction, and enable more refined use of appearance for scene understanding. In particular, we introduce a new likelihood function that rewards 3D object hypotheses whose 2D projection are more uniform in color distribution. Such a model would be confused if used on a bounding box for a concave object like a table. We present results showing the positive effect of each of these innovations.  The performance of our method is comparable to, and often exceeds, that of state-of-the-art methods on the scene surface orientation task, as well as object recognition, as evaluated on the two bench mark data sets used in this domain. "
274,Understanding Trees via Margins,"Building off recent work such as [Fruend, et.al.,2007], we seek to further understanding of  the behavior of multidimensional trees in high dimensional data. These trees are used in nearest-neighbor search, vector quantization, classification and other tasks. Usual analysis of trees investigate the capability of trees to adapt to some notion of intrinsic dimensions. In this paper, we provide an alternate avenue to mitigate high dimension effects -- margins. To this end, (1) we quantify the contribution of margins to the performance of tree, and (2) we formalize an intuitive notion of robustness and present its dependence on the margins.  We also provide empirical evidence showing that large margin splits can result in good quality indexing in high dimensions while being fairly robust to perturbations. "
275,Probabilistic Topic Coding for Superset Label Learning,"In the superset label learning problem, each training instance provides a set of candidate labels of which one is the true label of the instance.  Most approaches learn a discriminative classifier that tries to minimize an upper bound of the unobserved 0/1 loss. In this work, we propose a probabilistic model, Probabilistic Topic Coding (PTC), for the superset label learning problem. The PTC model is derived from logistic stick breaking process. It first maps the data to ``topics'', and then assigns to each topic a label drawn from a multinomial distribution.  The layer of topics can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art.  The discovered underlying structures also provide improved explanations of the classification predictions."
276,On Learning Camera Motion for Cinematographic Shot Classification,"In this paper, we propose a discriminative representation of a video shot based on its camera motion and demonstrate how the representation can be used for high level tasks like event recognition. In our technique, we assume that a homography exists between subsequent pairs of frames in a given video shot. Using purely image-based methods, we compute homography parameters that serve as coarseindicators of the camera motion. Next, using Lie algebra, we map the homography matrices to an intermediate vector space that preserves the intrinsic geometric structure of the transformation. Multiple time series are then constructed from these mappings. Features computed on these time series are used for discriminative classification of video shots. Our empirical evaluations on eight cinematographicshot classes show that our technique performs better than approaches that are based on image-based estimation of camera trajectories. Finally we show an application of our shot representation for detection of complex events in consumer videos."
277,Weighted Online Learning,"We consider an unconventional online learning problem which we  call Weighted Online Learning (WOL), where each training example has associated with it a (non-uniform) non-negative weight. WOL problems occur in many real life applications including banking business, medical diagnosis and visual tracking, where different samples are of differing value to the learning process. We propose several algorithms for WOL and show these algorithms have similar regret bounds and convergence rates to Pegasos. Applications in bank credit estimation, medical diagnosis and visual tracking show a significant improvement over state-of-the-art methods using traditional online learning."
279,Majorization for CRFs and Latent Likelihoods,"The partition function plays a key role in probabilistic modeling including conditional random fields, graphical models, and maximum likelihood estimation. To optimize partition functions, this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Suchbounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems, low-rank versions of the bound are provided and outperformLBFGS as well as first-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods."
280,Ensemble weighted kernel estimators for multivariate entropy estimation,"The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, kernel plug-in estimators suffer from the curse of dimensionality, wherein the MSE rate of convergence is glacially slow - of order  $O(T^{-{\gamma}/{d}})$, where $T$ is the number of samples, and $\gamma>0$ is a rate parameter. In this paper, it is shown that for sufficiently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order $O(T^{-1})$. Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed offline. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates."
281,Solving L1 Norm Matrix Factorization from the Elementary Unit by Coordinate Descent,"The L1 norm low-rank matrix factorization (LRMF) has been attracting much attention due to its intrinsic robustness to outliers and missing data. However, L1 norm LRMF is difficult to be resolved due to its non-convexity and non-smoothness. In this paper, we propose a novel solution, which is essentially a coordinate descent approach, to L1 norm LRMF. The main idea is to break the original difficult problem into its elementary sub-problems, each involving only one single scalar parameter, and recursively solve them. Each of these one-scalar sub-problems is convex and has a closed-form solution. The proposed method thus involves only simple computations and avoids any time-consuming inner loop numerical optimization. One important advantage of our method is that it provides a unified framework of solving L1 norm LRMF problems with or without missing data, and can be naturally extended to other matrix factorization tasks, such as nonnegative matrix factorization and sparse PCA. The extensive experimental results validate that our method outperforms state-of-the-arts in term of both computational time and accuracy, especially on large-scale applications such as face recognition and structure from motion."
282,Efficient deep learning through novel sparsity constraints and heterogeneous maps,"Deep convolutional neural networks (CNNs) have been successfully applied to a range of tasks spanning object recognition to sparse basis transformation. However, these very high dimensional networks are computationally expensive and have not realized their potential for real time applications. How can we minimize run time and yet retain state of the art results? To answer this question, we introduce two key ideas not yet fully explored for CNNs: (i) True Sparsity and (ii) Heterogeneous Feature Mappings. True sparsity comes to us from biological networks where neurons have a binary activation scheme. Although in machine learning a penalty norm is usually used to impose network sparsity, we achieve sparsity by modifying the activation function for a discrete response and introduce a 'burn-in' optimization scheme. Heterogeneous feature mappings are inspired by mixed selectivity -- a recent neuroscience term for diverse neuronal behavior. These two ideas lead to deep sparse networks that are more computationally feasible for high dimensional classification and extend naturally to unsupervised basis extraction. We achieve very competitive results on benchmark datasets with a speed-up factor over conventional methods. "
283,QuickBoost - Quickly Training Boosted Decision Trees,"Boosting is one of the most popular and effective learning techniques in use today. While exhibiting fast classification speed at test time, the training of an ensemble or cascade of boosted weak classifiers is slow, making it impractical for applications with real-time training requirements. In this paper, we propose a principled approach to overcome this drawback. We compute and prove a bound on the error of a weak classifier given its error on a subset of the training data;the bound may be used to prune unpromising weak classifiers early on. We propose a fast training algorithm that exploits this bound, yielding up to a 10-fold speedup at no cost in the final performance of the classifier."
284,Inverse of Lorentzian Mixture for Supervised Learning of Prototypes and Weights,"This paper presents a novel distance-based classifier based on the multiplicative inverse of Lorentzian mixture, which can be regarded as a natural extension of the conventional nearest neighbor rule. We show that prototypes and weights can be trained simultaneously by General Loss Minimization, which is a generalized version of supervised learning framework used in Generalized Learning Vector Quantization. Experimental results for UCI machine learning repository reveal that the proposed method achieves higher classification accuracy than Support Vector Machine with a much fewer prototypes than support vectors."
285,Efficient high dimensional maximum entropy modeling via symmetric partition functions,"  The application of the maximum entropy principle to sequence  modeling has been popularized by methods such as Conditional Random  Fields (CRFs).  However, these approaches are generally limited to  modeling paths in discrete spaces of low dimensionality.  We  consider the problem of modeling distributions over paths in  continuous spaces of high dimensionality---a problem for which  inference is generally intractable.  Our main contribution is to  show that maximum entropy modeling of high-dimensional, continuous  paths is tractable as long as the constrained features   possess a certain kind of low dimensional structure.  In this case, we show that the associated {\em partition function} is  symmetric and that this symmetry can be exploited to compute the  partition function efficiently in a compressed form.  Empirical  results are given showing an application of our method to maximum  entropy modeling of high dimensional human motion capture data."
286,Contour Detection using Sparse Code Gradients,"Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most existing approaches including the state-of-the-art Global Pb (gPb) operator.  In this work, we show that contour detection accuracy can be vastly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding.  We use K-SVD and Orthogonal Matching Pursuit for efficient dictionary learning and encoding, and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear SVM. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and find contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours).  Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth images and surface normals lead to promising contour detection using depth and depth+color, as verified on the NYU Depth Dataset.  Our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation."
287,Analyzing 3D Objects in Cluttered Images,"We present an approach to detecting and analyzing the 3D configuration of objects in real-world images with heavy occlusion and clutter. We focus on the application of finding and analyzing cars. We do so with a two-stage model; the first stage reasons about 2D shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then refined by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset."
288,"Nonconvex Penalization, Levy Processes and Concave Conjugates","In this paper we study sparsity-inducing nonconvex penalty functions using Levy processes. We define such a  penalty as the Laplace exponent of a subordinator. Accordingly, we propose a novel approach for the construction of sparsity-inducing nonconvex penalties. Particularly,  we show that the nonconvex logarithmic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionaly, we explore the concave conjugate of nonconvex penalties. We find that the LOG and EXP penalties are the concave conjugates of the negatives of Kullback-Leiber (KL) distance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance."
289,3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model,"This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects  in 3D by enclosing them with tight  oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model[Felz.] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are  continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2D[Felz09] and 3D object detection[Hedau12]. "
290,Computing the M Most Probable Modes of a Graphical Model,"We introduce the M-modes problem for graphical models: predicting the M label configurations of highest probability that are at the same time local maxima of the probability landscape. M-modes have multiple possible applications: because they are intrinsically diverse, they provide a principled alternative to non-maximum suppression techniques for structured prediction, they can act as codebook vectors for quantizing the configuration space, or they can form component centers for mixture model approximation.We present two complementary algorithms for solving the M-modes problem on junction chains. When the underlying graphical model is a simple chain, their complexity is polynomial in all relevant quantities. On general junction chains algorithms do not come with worst-case guarantees, but show promising performance in our experiments. Besides the M-modes problem, our techniques are also applicable to a more general class of optimization problems."
291,Structured  Sparse Learning of Multiple Gaussian Graphical Models,"We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data."
292,Active MAP Inference in CRFs for Efficient Semantic Segmentation,"Most MAP inference algorithms for CRFs optimize an energy function knowing all the parameters of the potentials. In this note, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to \emph{on-the-fly} select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 and MSRC-21, show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains. "
293,Fast Multiple Kernel Learning With Multiplicative Weight Updates,"In this work, we propose a fast algorithm for multiple kernel learning (MKL). Our proposed approach builds on the original QCQP formulation of Lanckriet et al. It uses a multiplicative weight update based approximation for the underlying SDP, exploiting a careful reformulation of the MKL problem as well as a novel fast matrix exponentiation routine for QCQP constraints that might be of independent interest. Our method avoids the use of commercial nonlinear solvers, and scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with an uniform unweighted combination of kernels."
294,Local Importance Weight for Transductive Classification,"This paper proposes a weighting scheme of training data named local importance weighting for transductive classification when training and test distributions are different. Most of the previous efforts to resolve the distribution difference rarely focus on matching joint distributions of input and output, but we show that it is important at least in classification tasks to adjust the difference of joint distributions. To match up the joint distribution of training data with that of test data. the proposed scheme estimates the joint distributions of both training and test data. Since the class labels of test data are unknown, an EM algorithm is used to estimate them and the parameters of the transductive classifier simultaneously. Through a series of experiments, we show that the transductive classifier with the proposed scheme outperforms that with the existing weighting method. "
295,Multi-Attribute Sparse Representation With Group Constraints,"A novel multi-attribute sparse representation enforced with group constraints is proposed in this paper. Data with multiple attributes can be represented by individual binary matrices to indicate the group properties for each data sample. Then, these attribute matrices are incorporated into the formulation of $l_1$-minimization. The solution is obtained by jointly considering the data reconstruction error, the sparsity property as well as the group constraints, thus making the basis selection in sparse coding more efficient in term of accuracy. The proposed optimization formulation with group constraints is simple yet very efficient for classification problems with multiple attributes. In addition, it can be derived into a modified sparse coding form so that any $l_1$-minimization solver can be employed in the corresponding optimization problem. We demonstrate the performance of the proposed multi-attribute sparse representation algorithm through experiments on face recognition with different kinds of variations. Experimental results show that the proposed method is very competitive compared to the state-of-the-art methods."
296,A Polylog Pivot Steps Simplex Algorithm for Classification,"We present a simplex algorithm for linear programming in a linear classification formulation. The paramount complexity parameter in linear classification problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear. This is in contrast to general linear programming, for which no sub-polynomial pivot rule is known. "
297,Order Preserving Hashing for Approximate Nearest Neighbor Search,"In this paper, we study hash-based indexing techniques for approximate nearest neighbor (ANN) search. The principle of searching ANNs is that the points corresponding to the hash codes near that of the query are more similar to the query than those corresponding to the farther hash codes. Motivated by this point, we propose a novel hashing approach, called order preserving hashing, which learns the hash functions by maximizing the alignment between the similarity orders computed from the original space and the hamming space. We also impose the constraint that the points are as uniformly as possible distributed in the buckets. To this end, we formulate the problem of mapping the NN points for a query point into different hash codes as a classifier learning problem in which a nearest neighbor classifier is used based on the hamming distance over the hash codes, and find the hash functions from the aggregated classifier pooled over all the training points. To make the optimization feasible, we develop several techniques, including Sigmoid relaxation, stochastic gradient decent, and active set, to efficiently learn hash functions. Experimental results demonstrate the superiority of our approach over existing state-of-the-art techniques."
298,Shifting Weights: Adapting Object Detectors from Image to Video,"Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video."
299,Nonparametric Bayesian Microphone Array Processing,"Sound source localization and separation from a mixture of sounds are essential functions for computational auditory scene analysis.The main challenges are designing a unified framework for joint optimization and estimating the sound sources under auditory uncertainties such as reverberation or unknown number of sounds.Since sound source localization and separation are mutually dependent, their simultaneous estimation is required for better and more robust performance. A unified model is presented for sound source localization and separation that is based on a nonparametric Bayesian model.Experiments using simulated and recorded audio mixtures show that a method based on this model achieves state-of-the-art sound source separation quality and has more robust performance on the source number estimation under reverberant environments."
300,Nonconvex Relaxation Approaches to Robust Matrix Recovery,"Motivated by the recent developments of nonconvex penalties in sparsity modeling, we propose a nonconvex optimization model for handing the low-rank matrix recovery problem.Different from the famous robust principal component analysis (RPCA), we suggest recovering low-rank and sparse matrices via a nonconvex loss function and a nonconvex penalty instead of the convex ones.The advantages of the nonconvex approach lie in its stronger robustness.To solve the model, we devise a majorization-minimization augmented Lagrange multiplier (MM-ALM) algorithm which finds the local optimal solutions of the proposed nonconvex model.We also provide two efficient strategies to speedup MM-ALM, which make the running time comparable with the state-of-the-art algorithms solving RPCA.Finally, the experimental results demonstrate the superiority of our nonconvex approach over RPCA in terms of matrix recovery accuracy."
302,Fast multiple-part based object detection using KD-Ferns,"Part-based models are currently considered state-of-the-art for object detection due to their ability to represent large appearance variations. Furthermore, using a large number of parts with diverse appearances can improve classification accuracy. The computational cost of the detection stage in existing methods has been a limitation for adopting such models in real-time applications. In addition, since the computation time grows linearly with the number of parts, most methods are limited to using several parts only. Our first contribution is the ``KD-Ferns'' algorithm for approximate nearest neighbor search which allows to compare each image location to only a subset of the model parts. This allows scaling the number of parts in the model. Our second contribution is a new algorithm for object detection which is an efficient variant of the ``Feature Synthesis'' (FS) method[1]. The FS uses multiple object parts for detection and is among state-of-the-art methods on human detection benchmarks but suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses learned classifiers with hundreds of parts in a coarse-to-fine strategy, achieving significantly faster object detection compared to existing part based methods. The AFS uses the ``KD-Ferns'' algorithm, and spatially sparse part locations, to reduce the required computation. We evaluate the AFS on the INRIA and Caltech pedestrian detection benchmarks. In this evaluation, the AFS is among the state-of-the-art methods in detection accuracy and is also the fastest method for close range pedestrians, reaching nearly 10 frames per-second on $640\times 480$ images. The AFS is to our best knowledge the first part-based object detection method capable of running in real-time."
304,Recursive Deep Learning on 3D Point Clouds,"Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a novel model based on sparse and recursive autoencoders (RAE) for learning both features and object categories from raw 3D point clouds as well as standard images. The model differs from previous RAE models in that it fixes the tree structures and includes short-circuit connections from all tree nodes to the final classifier. This allows the model to take into consideration both low-level features as well as global features of the object. Using our fully learned architecture, we achieve state of the art performance on a standard RGB-D object recognition dataset, rivaling random forest classifiers on hand-designed features such as SIFT and spin images. Our method is very fast and can classify 71 images in 1 second on a standard desktop machine in Matlab. This is possible because the method only requires 16 matrix multiplications to classify each image into one of 51 household objects."
305,Matching Human Actions in Videos,"Matching human motion in images and videos is one of the most fundamental problems in computer vision. In this paper, we attempt to extend the traditional appearance-similarity-based feature matching to action-similarity-based matching. The matching method we proposed is independent of appearances, camera views, and robust to the pace and speed of the specific human action. Our action similarity measure is based on a novel covariance descriptor for encoding the point motion trajectories of people. We encode each point trajectory into an optical flow covariance matrix, which is symmetry and positive definite (SPD). Using the metric of SPD matrix space, we can identify the similarity correspondence of any pairs of trajectories. We also introduce a subspace alignment algorithm, which allows us to perform dense matching between two trajectory set in a low-dimensional subspace. The resulting matching provides us a set of dense point-point correspondences between two people in videos, who are conducting the same action in the sampled video clips. The action-similarity-based human matching can provide a new view/appearance independent approach for video registration, human tracking, and action recognition."
306,Weighted regret-based likelihood: a new approach to describing uncertainty,"A new representation of likelihood is proposed, based on the notion ofregret, and is completely characterized."
307,Semi-Supervised Domain Adaptation with Non-parametric Copulas,"A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model across different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques."
308,Linked Tensor/Tucker Decomposition and its Application for Multi-block Group Tensor Analysis,"In this paper we propose a new algorithm for flexible group tensor analysis model called the linked tensor/Tucker decomposition (LTD).The LTD can decompose given multiple tensors into common factor matrices, individual factor matrices, and individual core tensors, simultaneously.The LTD provides a novel application called the linked multiway principal component analysis (LMPCA) by addition of orthogonal constraints.This method is derived by the alternating least squares (ALS) algorithm.Furthermore, we conducted experiments of this model for face reconstruction and denoising and EEG classification to demonstrate its advantages."
309,Gated Local Metric Nearest Neighbor Classi?cation,The large margin nearest neighbor (LMNN) algorithm learns a global metric to improve the accuracy of the k-nearest neighbor classi?er. We propose a novel variant with a gating function that divides up the input space into regions and learns a localized distance metric in each using a low-rank approximation. The proposed method is experimented over real data sets and compared with LMNN with good results in terms of higher accuracy and better visualization.
310,On Detecting Multiple Simultaneous Change-points in High Dimensional Time Series,"This paper studies the detection of multiple simultaneous (systematic) change points for high-dimensional nonstantionary time series data. The analytic framework used is based on the standard and adaptive fused group lasso method, where the mixed $L_{2,1}$ norms in the penalty are either uniform or re-weighted by data-dependent weights. This paper shows that, under appropriate conditions, this approach is $L_2$ consistent and, by adopting the data-dependent weights, could correctly select the change points with probability approaching unity. It quantifies the conditions on the interplay among the averaged (over different dimensions) minimum magnitude of structural changes, the number of change points and the number of observations for consistently discovering the change points. The performance of this approach is illustrated via an analysis of a large panel of U.S. economic and financial time series data over the past $50$ years. "
311,k-Nearest Neighbor Transform,"In this paper, we proposes the exact Euclidean k-nearest neighbor transform (kNNT) for binary images.Similar to Distance Transform (DT) finding the closest feature point for each point of an image, kNNT assigns each point in the binary image with its k-nearest feature points.Therefore, kNNT extends the space of DT which is just the extreme case of kNNT when k=1.We propose a linear time algorithm to compute kNNT in O(Nk^2) time with O(Nk) space where N is the size of the image.The query method to answer the kNN query is to find the result in a lookup table in time O(k).The KNNT is suitable for an extremely large number of queries, and thus an efficient method for many applications in which the distance transform is applied."
312,Gossip-based distributed stochastic bandit algorithms,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called \emph{exploitation-exploration dilemma} in various bandit setups. But significantly less effort has been spent on adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines or peer-to-peer (P2P) environment, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms are available in each peer. In every iteration each peer can pull one arm independently, and then some limited communication is possible with a few random other peers.  The algorithm we chose to adapt is the \Algo{$\epsilon$-GREEDY} policy that was shown by Auer et. al.~\cite{AuCeFi02} to play a suboptimal arm with probability $\bigO(1/t)$ in iteration $t$, and thereby to achieve a regret of $\bigO(\log t)$. This is optimal up to some constant factors~\cite{LaRo85}. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability that a suboptimal arm is played at a peer in iteration $t = \Omega( \log N )$ is $\bigO(1/(Nt))$, where $N$ denotes the number of peers. This result, as mentioned above, means that we achieve the best speedup that is theoretically possible~\cite{LaRo85}. The restriction $t = \Omega(\log N)$ is unavoidable in adapting such algorithms to P2P networks.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
313,Semantic GIST: Probabilistic Modelling of Scenes using Scenelet,"In this paper, we propose a probabilistic modeling framework for scenes to encode semantic information of images into a compact Semantic Gist representation. The representation is based on a key concept called {\em scenelets}, which serves as building blocks for scenes. We learn these scenelets using a topic model to group correlated objects such that the learned set of scenelets maximally retain the semantic saliency of images in terms of KL-divergence. Our model also integrates information from individual discriminative object detectors and global image features by coding them as priors. Empirical results demonstrate the power of our model. We first show that using a small set of scenelet classifiers, we can predict the existence of a large set of objects without running individual object detectors.Furthermore, we can even predict the presence of objects without running large sets of object detectors by MAP estimation using our model.We also show that the framework can improve the performance of individual detectors by incorporating the contextual object and scenelet information. Experiments on challenging datasets including PASCAL and SUN09  demonstrate that our model outperforms other state-of-the-art ones."
314,Learning Modewise Independent Components from Tensor Data via Multilinear Mixing Model,"Independent component analysis (ICA) is a popular unsupervised learning method. This paper extends it to multilinear modewise ICA (MMICA) for tensors and explores two architectures in learning and recognition. MMICA models tensor data as mixtures generated from modewise source matrices that encode statistically independent information. It operates on much lower dimension than ICA and its compact representations require much fewer parameters to estimate. We embed ICA into the multilinear principal component analysis framework to solve for each source matrix alternatively with a few iterations. Then we obtain mixing tensors through regularized inverses of the source matrices. Simulations on synthetic data demonstrate that MMICA can estimate hidden sources from structured tensor data. Moreover, in face recognition experiments, it outperforms competing solutions, while being particularly effective with Architecture II due to sparser and more structured bases."
315,Identification of Recurrent Patterns in the Activation of Brain Networks,"Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks."
317,Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning,"Learning from experience how to manipulate an environment in a goal-directedmanner is one of the central challenges in research on autonomous agents. Inthe case of object manipulation, efficient learning and planning should exploit theunderlying relational structure of manipulation problems and combine geometricstate descriptions with abstract symbolic representations. When appropriate sym-bols are not predefined they need to be learned from geometric data. In this paperwe present an approach for learning symbolic relational abstractions of geometricfeatures such that these symbols enable to learn abstract transition models and touse them for goal-directed planning of motor primitive sequences. This is framedas an optimization problem, where a loss function evaluates how predictive thelearned symbols are for the effects of motor primitives as well as for reward. Theapproach is embedded in a full-fledged symbolic relational model-based reinforce-ment learning setting, where both the symbols as well as the abstract transitionand reward models are learned from experience. We quantitatively compare theapproach to simpler baselines in an object manipulation task and demonstrate iton a real-world robot."
318,Variational EM for Sound-Source Separation and Localization with Probabilistic Piecewise Affine Mapping,"We address the problem of sound-source separation and localization in real-world conditions with just two microphones. Separation and localization tasks are solved within a unified formulation that uses a probabilistic piecewise affine mapping. While the parameters of the direct mapping are learned during a training stage that uses sources emitting white noise, the inverse mapping is estimated using a variational EM formulation. The proposed algorithm can deal with reverberations and natural sound sources such as speech data which are known to yield sparse spectrograms, and is able to locate multiple sources both in azimuth and in elevation. Extensive experiments with real data show that the method compares positively with several other recently proposed methods."
320,Spike-Timing-Dependent Plasticity and Mutual Information Maximization for a Hetero-Associative Memory Model,"We explored optimal neural implementations for executing a computation in the hippocampal CA1 network and interpreted the computational roles of spike-timing-dependent plasticity (STDP). We propose an approach consisting of bottom-up and top-down steps in order to clarify physical limits to computations in the neural implementation. In the bottom-up step, hypothesizing that the hippocampal CA1 network computationally works as the hetero-associative memory, we formulate a minimum model of temporal memory functions as a phase reduction model composed of a STDP window function and a phase response curve (PRC). Next, we analytically derive a macroscopic reliability index of memory functions defined as the mutual information between a stored phase memory pattern and a network output. In the top-down step, by maximizing the mutual information, we derive pairs of STDP window functions and PRCs optimally functioning as a hetero-associative memory. We use PRCs of the hippocampal CA1 pyramidal neurons recorded in vitro, and under the constraint of measured PRCs, we search for a set of optimal STDP window functions. The theoretically derived set of these window functions qualitatively conforms to the various STDPs reported previously."
321,Learning Hamming Clique Potentials for Higher Order CRFs,"This paper investigates Conditional Random Fields (CRF) clique potentials that are invariant to permutations of label variables within the clique. We propose a general formulation for potential functions of this kind in terms of the Hamming distance from an absolute labeling ? an assignment of all clique variables to the same class. We present an analytical treatment as to which constituent functions of the proposed clique cost facilitate learning them from data through Likelihood Maximization and describe how to learn a non-parametric form of it. Experimental results demonstrate i) how some of the existing higher order clique costs can be either rewritten or approximated by the proposed formulation, and, ii) the advantage of learning the potentials, both for pairwise and higher order cliques, fortwo different applications."
322,Imbalanced Random Subspace Ensemble Methods for computer-aided nodule detection,"Many lung computer-aided detection (CAD) methods have been proposed for pulmonary nodules. Because high sensitivity is essential in the candidate identification stage, there are countless false positives produced by the initial suspect nodule generation process, giving more work to radiologists. Moreover, there are more false positives than real nodules detected. In order to eliminate or reduce the false positives, we address this issue as a binary classification problem. However, given the imbalance between false positives and true positives and the different misclassification costs, we propose two new strategies of random subspace ensemble methods. Experimental results show the effectiveness of the proposed methods in terms of sensitivity and specificity."
323,Tangent Bundle Manifold Learning via Grassmannian eigenmaps,"The goal of Manifold Learning (ML) is, given a set of data points sampled from an unknown nonlinear low-dimensional Data manifold embedded in a high-dimensional input space, to construct an Empirical manifold approximating the Data manifold. We propose an extension of ML, called Tangent Bundle ML (TBML), in which proximity not only between the Data and Empirical manifolds but also between their tangent spaces is required. We present a new algorithm, Grassmannian Eigenmaps, that solves TBML problems and also gives new solution for the ML."
324,Density-Difference Estimation,"We address the problem of estimating the difference betweentwo probability densities.A naive approach is a two-step procedure of first estimating two densities separatelyand then computing their difference.However, such a two-step procedure does not necessarily work wellbecause the first step is performed without regard to the second stepand thus a small error incurred in the first stage can cause a big error in the second stage.In this paper, we propose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities.We derive a non-parametric finite-sample error boundfor the proposed single-shot density-difference estimatorand show that it achieves the optimal convergence rate.We experimentally demonstrate the usefulness of the proposed methodin class-prior estimation and change-point detection."
325,Sharp analysis of low-rank kernel matrix approximations,"We consider supervised learning problems within the positive-definite kernel framework, such as kernel ridge regression, kernel logistic regression or the support vector machine. With  kernels leading to infinite-dimensional feature spaces, a common practical limiting difficulty is the necessity of computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic in the number of observations n, i.e., O(n^2). Low-rank approximations of the kernel matrix are often considered as they allow the reduction of  running time complexities  to O(p^2 n), where p is the rank of the approximation. The practicality of such methods thus depends on the required rank p. In this paper, we show that for approximations based on a random subset of columns of the original kernel matrix, the rank p may be chosen to be linear in the degrees of freedom associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have sub-quadratic running time complexity, but provably exhibit the same predictive performance than existing algorithms."
326,Online Learning via Optimizing the Variational Inequalities,"A wide variety of learning problems can be posed in the framework of convex optimization. Many efficient algorithms have been developed based on solving the induced optimization problems. However, there exists a gap between the theoretically unbeatable convergence rate and practically efficient learning speed. In this paper, we cast the regularized learning problem as a variational inequality (VI). Then, we solve the induced VI using the alternating direction method of multipliers (ADMM) in an online setting. For general convex problems, this new formulation enables our stochastic ADMM to achieve an $O(1/t)$ VI-convergence rate. The experiments demonstrate that the stochastic ADMM has almost the same performance as the state-of-the-art online algorithms but its $O(1/t)$ VI-convergence rate is capable of tightly characterizing the real learning speed."
327,Online Learned Discriminative Hidden Structural Part Model for Visual Tracking,"We present a discriminative hidden structural part-based approach for visual trackingwithout any prior assumptions about the target and scenarios. Unlike otherweak-constrained or manual labeling part generation strategy in the previous partbasedtrackers, the state (e.g. position, width and height) of each part is consideredas the latent variable in our model and is inferred automatically online withthe dual objective functions. The two objective functions respectively encode theappearance variations of the target and separate the target from the backgroundwith the max-margin. Specifically, the constraints between parts are integratedin graph model through the dynamically constructed pair-wise Markov RandomField (MRF). The part-based Support Vector Machine (SVM) and Reverse JumpMarkov Chain Monte Carlo (RJMCMC) algorithm is adopted to complete thelatent variable inference task. The experimental results on various challengingdatabase demonstrate the learned part-based tracker outperforms other state-ofthe-art trackers (both bounding-box-based as well as part-based)."
328,Variational Inference for Crowdsourcing,"Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al, while our MF method is closely related to a commonly used EM algorithm. In both case, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state of art algorithms based on more complicated modeling assumptions."
329,VLSI Implementation of a Coupled MRF Model Using Pulse-coupled Phase Oscillators,"This paper proposes efficient pixel-parallel image processing using a pulse-coupled phase oscillator model and its VLSI implementation. A processing unit which corresponds to a pixel of an image transmits spike pulses to other units, and updates its own analog state value at timing when spikes come from other units. From a VLSI implementation point of view, this mechanism is suitable to very low power operation because analog buffers are unnecessary for data transmission. On the basis of this model, we have designed and fabricated a VLSI image processor chip that performs a coupled Markov random field (MRF) model for image region segmentation. Very low power VLSI design has been achieved by combination of an analog oscillator and digital coupling function generator circuits with time-domain computation. The performance of this oscillator-based image processor chip can be superior to the existing digital processors.  Experiments using the fabricated chip show successful image region segmentation in 1D and 2D images. "
330,Convex Variational Image Restoration with Histogram Priors,"We present a novel variational approach to image restoration (e.g., denoising, inpainting, labeling.) that enables to complement established variational approaches with a histogram-based prior enforcing closeness of the solution to some given empirical measure. By minimizing a single objective function, the approach utilizes simultaneously two quite different sources of information forrestoration: spatial context in terms of some smoothness prior and non-spatial statistics in terms of the novel prior utilizing the Wasserstein distance between probability measures. We study the combination of the functional lifting technique with two different relaxations of the histogram prior and derive a jointly convex variational approach. Mathematical equivalence of both relaxations and optimality certificates are established. Numerical experimentsusing the basic total-variation based denoising approach as a case study demonstrate our novel regularization approach."
331,MCMC for continuous-time discrete-state systems,"We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process, while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler, and show its advantages."
332,A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling,"The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session."
333,Learning about Canonical Views from Internet Image Collections,"Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or ?canonical? view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views?We start by manually finding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to find the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories."
334,Context-Aware Wide-Area Activity Recognition,"In this paper, rather than modeling activities in videos individually, we propose a structural representation that jointly models related activities with both motion and context information. This is motivated from the observations that activities related in space and time rarely occur independently and can serve as context for each other. The spatial layout of activities and their sequential patterns provide useful cues for the understanding of activities. Given initial classifying information generated by a base classifier, our model aims to improve recognition accuracy by jointly modeling related activities using these preliminary results and various context features. The proposed model automatically captures and weights motion and context patterns for each activity class, as well as groups of them, from sets of predefined attributes during the learning process. Then the learned model is used to generate globally optimum labels for activities in the testing videos. We show how to learn the model parameter via an unconstrained convex optimization problem and how to predict the correct labels for a testing instance consisting of multiple activities. We show promising results on the VIRAT Ground Dataset that demonstrates the benefit of joint modeling and recognizing contextual activities in a wide-area scene."
335,Linking Heterogeneous Input Spaces with Pivots for Multi-Task Learning,"Most existing works on multi-task learning (MTL) assume the same input space for different tasks. In this paper, we address a general setting where different tasks have heterogeneous input spaces. This setting has a lot of potential applications, yet it poses new algorithmic challenges - how can we link seemingly uncorrelatedtasks to mutually boost their learning performance?Our key observation is that in many real applications, there might exist some correspondence among the inputs of different tasks, which is referred to as pivots. Forsuch applications, we first propose a learning scheme for multiple tasks and analyze its generalization performance. Then we focus on the problems where only a limited number of the pivots are available, and propose a general frameworkto leverage the pivot information. The idea is to map the heterogeneous input spaces to a common space, and construct a single prediction model in this space for all the tasks. We further propose an effective optimization algorithm to find both the mappings and the prediction model. Experimental results demonstrate its effectiveness, especially with very limited number of pivots."
336,Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data,"We propose an efficient, generalized, nonparametric, statistical Kolmogorov-Smirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efficient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods."
337,A Nonparametric Bayesian Classifier under a Mixture Loss Function,"Many classification problems can be conveniently formulated in terms of Bayesian mixture prior models. The mixture prior structure lends itself especially well for adapting to varying degrees of sparsity. Typically, parametric assumptions are made about the components of the mixture priors. In the following, we propose a parametric and a nonparametric classification procedures using a mixture prior Bayesian approach for a risk function that combines misclassification loss and an $L_2$ penalty. While the parametric procedure is closer to traditional approaches,in simulations, we show that the nonparametric classifier typically outperforms it when the parametric prior is misspecified; the two procedures have comparable performance even when the shape of the parametric prior is specified correctly.  We illustrate the properties of the two classifiers on a publicly available gene expression dataset."
338,Multiresolution Gaussian Processes,"We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity."
339,TRaNce: Trace Norm for Ranking,"Learning to rank objects, in order of their perceived importance, finds diverse applications in several domains. Typically, the goal of ranking is to learn a real valued function that induces an ordering over the entire object space. In this paper, we formulate query specific ranking as a matrix completion problem: given the relevance values for a (partial) collection of objects over a set of queries, expressed in terms of a matrix, our task is to predict the relevance of the remaining objects by filling in the missing entries of the matrix. Specifically, we develop a matrix factorization based framework, TRaNce, for learning to rank. To our knowledge, TRaNce is the first technique to investigate the applicability of the trace-norm for ranking.  We provide a rigorous theoretical analysis encompassing generalization bounds (including stability and consistency) for the proposed methodology. We also provide experimental evidence to corroborate the efficacy of our technique."
340,Displacement Determination by Motion Compensation,Motion determination from an image sequence has been an important and challenging problem in computer vision and remote sensing applications. A fully constrained nonlinear system of equations combing the Displacement Vector Invariant (DVI) equation for displacement determination from an image sequence are proposed without approximation and imposing any additional constraint and assumption. An adaptive framework for solving the nonlinear system of equations has been developed. This work is to seek motion fields that are consistent with the physical observation because the observation may not be consistent with the physical motion in a featureless image sequence. The estimated displacement field is based on a single minimized target function that leads to optimized motion-compensated predictions and interpolations in a wide class of applications of the motion-compensated compression without any penalty parameters. Experimental tests on synthetic and natural image sequences are presented. Applications of motion-compensated interpolation are also demonstrated.
341,Search-Based Understanding of Hierarchical Dirichlet Process Topic Models,"Bayesian nonparametric models are widely used to allow unsupervised learning of statistical model structure from data.  Conventional learning algorithms, based on Gibbs sampling or variational Bayesian approximations, can be undesirably prone to local optima and sensitive to initialization.  We explore this issue in the context of the hierarchical Dirichlet process, which with combined with Dirichlet-multinomial likelihoods provides a nonparametric topic model.  We present a more efficient learning algorithm based on the Maximization-Expectation (ME) algorithm, based on a novel combinatorial formulation of the HDP marginal likelihood and implemented with carefully crafted search moves.  Experiments on synthetic data with statistics similar to real text corpora, and four real document collections, show consistent improvement over sampling algorithms in both recovery of the true number of topics, and predictive likelihood of test data."
342,Localizing 3D cuboids in single-view images,"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners."
343,Active Metric Learning For Ground Level To Aerial Image Matching,"Image-based geolocation is a challenging problem that has recently captured the interest of computer vision and machine learning researchers.  In this work we focus on the specific problem of matching ground level images to 45 degree aerial images. Matching ground level and aerial images is a very hard problem due to wide disparities in viewpoint and imaging conditions, the combination of which leads to low level feature matching failure.  To overcome this problem, we propose an active metric learning framework that allows a human user to solve the problem collaboratively with the machine.  Our approach selects pairs of regions of interest based on an information gain criterion and asks the human user to establish correspondences between them.  Those pairs are subsequently used to update a metric to improve the correspondences. This process continues until the system finds the correct location of the ground level image.  We introduce a new Ground Level to Aerial Image Dataset (GLAID) to assess strengths and weaknesses of our proposed framework. Our experiments show that our system allows user to find the correct location with significantly reduced effort."
344,Newton-Like Methods for Sparse Inverse Covariance Estimation,"We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimationproblem. Comparisons with the method implemented in the QUIC software package are presented."
345,Learning to Align from Scratch,"  Unsupervised joint alignment of images has been demonstrated to  improve performance on recognition tasks such as face verification.  Such alignment reduces undesired variability due to factors such as  pose, while only requiring weak supervision in the form of poorly  aligned examples.  However, prior work on unsupervised alignment of  complex, real world images has required the careful selection of  feature representation based on hand-crafted image descriptors, in  order to achieve an appropriate, smooth optimization landscape.  In this paper, we instead propose a novel combination of  unsupervised joint alignment with unsupervised feature learning.  Specifically, we incorporate deep learning into the {\em congealing}  alignment framework.  Through deep learning, we obtain features that  can represent the image at differing resolutions based on network  depth, and that are tuned to the statistics of the specific data  being aligned.  In addition, we modify the learning algorithm for  the restricted Boltzmann machine by incorporating a group sparsity  penalty, leading to a topographic organization on the learned  filters and improving subsequent alignment results.  We apply our method to the Labeled Faces in the Wild database  (LFW). Using the aligned images produced by our proposed  unsupervised algorithm, we achieve a significantly higher accuracy  in face verification than obtained using the original face images,  prior work in unsupervised alignment, and prior work in supervised  alignment.  We also match the accuracy for the best available, but  unpublished method."
346,Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints,"Recent spiking network models of Bayesian inference and unsupervised learningfrequently assume either inputs to arrive in a special format or employ complex computations inneuronal activation functions and synaptic plasticity rules. Here we show in arigorous mathematical treatment how homeostatic processes,which have previously received little attention in this context, can overcome common theoretical limitationsand facilitate the neural implementation and performance of existing models.In particular, we show that homeostatic plasticity can be understood as the enforcement of a 'balancing'posterior constraint during probabilistic inference and learning with Expectation Maximization.We link homeostatic dynamics to the theory of variational inference, and show that, as a side effect, nontrivial terms which typically appear during probabilistic inference in a largeclass of models drop out.We demonstrate the feasibility of our approach in a spiking Winner-Take-All architecture of Bayesian inference and learning,and discuss general properties of the resulting network dynamics. Finally, we sketch how the mathematical framework can be extended to richer, recurrent network architectures.Altogether, our theory provides a novel perspective on the interplay of homeostaticprocesses and synaptic plasticity in cortical microcircuits, pointing to an essential computational roleof homeostasis during probabilistic inference and learning in spiking networks."
347,Multiclass Semi-Supervised Learning on Graphs,"We present a graph-based variational algorithm for multiclassclassification of high dimensional data. The variational energy is basedon a diffuse interface model, and we introduce an alternative measure ofsmoothness appropriate for the multiclass segmentation problem. Wedemonstrate that the multiclass diffuse interface model outperformsclassical spectral clustering methods, and that it obtains resultscompetitive with the state of the art among other graph-based algorithms."
348,Clustering Aggregation as Maximum-Weight Independent Set,"We formulate clustering aggregation as a special instance of Maximum-Weight Independent Set (MWIS) problem. For a given data-set, an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters. The vertices, which represent the distinct clusters, are weighted by an internal index measuring both cohesion and separation. The edges connect the vertices whose corresponding clusters overlap. Intuitively, an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together. We formalize this intuition as the MWIS problem on the attributed graph, i.e., finding the heaviest subset of mutually non-adjacent vertices.This MWIS problem exhibits a special structure. Since the clusters of each input clustering form a partition of the dataset, the vertices corresponding to each clustering form a maximal independent set (MIS) in the attributed graph. We propose a variant of simulated annealing method that takes advantage of this special structure. Our algorithm starts from each MIS, which is close to a distinct local optimum of the MWIS problem, and utilizes a local search heuristic to explore its neighborhood in order to find the MWIS. Extensive experiments on many challenging datasets show that: 1. our approach to clustering aggregation automatically decides the optimal number of clusters; 2. it does not require any parameter tuning for the underlying clustering algorithms; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance; 4. it is robust against moderate or even bad input clusterings."
349,Discriminative Hidden Kalman Filters,"In on-line classification, a real-time decision needs to be made based on signals observed so far. Generative models such as switching Kalman filters are good at describing signals but not classifying them. Discriminative approaches such as conditional random fields offer better classification performance. Nevertheless, they often require pre-defined features. We propose a discriminative hidden Kalman filter that jointly learns features and the classifier based on a novel discriminative training criterion. The variational Bayesian algorithm is employed for optimization. An extension to handle multi-class problems is also presented. "
350,Regularized Nonnegative Matrix Factorization using Minimum Mean Square Error Estimates under Gaussian Mixture Prior Models with online Learning for the Uncertainties,"We propose a new method to enforce priors on the solution of the nonnegative matrix factorization (NMF). The proposed algorithm can be used for denoising or single-channel source separation (SCSS) applications. The NMF solution is guided to follow the Minimum Mean Square Error (MMSE) estimate under Gaussian mixture prior models (GMM) for the source signal. In SCSS applications, the spectra of the observed mixed signal are decomposed as a weighted linear combination of trained basis vectors for each source using NMF. In this work, the NMF decomposition weight matrices are treated as a distorted image by a distortion operator, which is learned directly from the observed signals. The MMSE estimate of the weights matrix under GMM prior and log-normal distribution for the distribution is then found to improve the NMF decomposition results. The MMSE estimate is embedded within the NMF cost function as a regularized NMF, and their corresponding update rules are driven in this paper. Experimental results show that, the proposed regularized NMF algorithm improves the source separation performance compared with using NMF only."
351,Transaction-Based Link Strength Prediction Using Matrix Factorizations,"The revolution of online social networks and methods of analyzing them have attracted interest in many research fields. Predicting whether a friendship holds in a social network between two individuals or not, link prediction, has been a heavily researched topic in the last decade. In this paper we research a related problem, link strength prediction, which aims to assign ratings or strengths to friendship links. A basic approach would be matrix factorization applied to only friendship ratings. However, the existence of extensive transactions among users may be used for better predictions. We propose a new type of multiple-matrix factorization model for incorporating a transaction matrix. We derive gradient descent update equations for learning latent factors that predict values in the target rating matrix. To evaluate the model, we introduce data from Cloob which is a popular Iranian social network as well as synthetic data."
352,Generalized Conformal Prediction for Functional Data,"Conformal prediction is an approach to constructing distribution free predictions. In this paper we apply conformal method to develop a class of exploratory tools for functional data. We first describe a new conformal method and use it to construct simultaneous prediction bands for functional data. Then we use conformal methods with pseudo density to find anomalies, median sets, prototypes, and conformal cluster trees. In developing these tools, we give the first distribution free, finite sample valid prediction sets and prediction bands for functional data. Our construction also features a novel implementation of conformal prediction that greatly enhances the flexibility and computation efficiency of this general method."
353,Workflows for Computer Vision: Open Publication and Reproducibility of Experiments,"The inability to reproduce computational research is a rapidly growing area of concern in computer vision. In this paper, we incorporate a structured, end-to-end analysis methodology, based on workflows, to easily and automatically allow for standardized replication and testing of state-of-the-art models, inter-operability of heterogeneous codebases, and incorporation of novel algorithms. We demonstrate the utility of our approach by introducing a novel computer vision dataset and conducting an in-depth, comparative analysis of state-of-the-art methods on the new Atomic Pair Actions dataset using workflows. This allows us to re-use pre-existing workflows as well as incorporating new algorithms developed in heterogeneous codebases. The entire framework, including the workflows and the dataset, is then exported as web objects which can be executed via the web, or downloaded and imported into a compatible workflow system, by any user to re-create the full analysis or to change/extend the workflows as desired. In addition, we make the full dataset (the videos, their associated tracks with ground truth, and metadata) and all exported workflows widely available to the research community both as openly accessible web objects."
354,Sparse Manifold Alignment,"Previous approaches to manifold alignment are based on solving a (generalized) eigenvector problem. We propose a least squares formulation of a class of manifold alignment approaches, which has the potential of scaling better to real-world data sets. Furthermore, the least-squares formulation enables various regularization techniques to be readily incorporated to improve model sparsity and generalization ability. In particular, it enables using the $l_1$ norm regularization  framework to make previous manifold alignment algorithms more robust. The new approach can prune domain-dependent features automatically helping to improve transfer learning. This extension significantly broadens the scope of manifold alignment techniques  and leads to faster algorithms. We present detailed experiments to illustrate the approach using the domains of cross-lingual information retrieval and social network analysis."
355,Foveated Search Models That Learn Eye Movements,"This paper presents foveated search models that learn where to fixate inorder to improve perceptual performance on a simple visual search task. In particular, we combine models of eye movements during search with a perceptual learning model and a machine learning method for a task where the observer has to search for a target and then give a yes/no decision about its presence. We report simulation results for various eye movement models and learning methods. Foveated eye movements include maximum a posteriori (MAP), ideal searcher,random and systematic exploration models. We evaluate Bayesian and on-linelearning algorithms. Simulation results show that for the simple tasks the computationally tractable MAP model can approximate the ideal searcher irrespectiveof the learning algorithm. We also compare model performance (constrained toa human visibility map) to that of a human observer on the same task. Both human improvements of perceptual performance and convergence of eye movements towards the target location across sessions suggest that humans are not using either a random or systematic saccadic exploration or an on-line learning algorithm. Yet, humans are less efficient than the MAP eye movement model with Bayesianlearning."
357,Anatomy-guided Discovery of Large-scale Consistent Connectivity-based Cortical Landmarks,"Establishment of structural and functional correspondences across different brains and populations is one of the most fundamental issues in the brain imaging field. Recently, several multimodal DTI/fMRI studies have demonstrated that consistent white matter fiber connection patterns can predict brain function and represent common brain architectures across individuals and populations. Although a variety of approaches have been proposed to discover large-scale cortical landmarks with common structural connection profiles, the rich anatomical information such as gyral/sulcal folding patterns and structural connection pattern homogeneity have not been incorporated into existing DTI/fMRI studies yet. This paper presents a novel anatomy-guided discovery framework that defines and optimizes a dense map of cortical landmarks that possess group-wise consistent anatomical and connectional profiles. This framework integrates reliable and rich anatomical information for landmark initialization, optimization and prediction, which are formulated and solved as an energy minimization problem. Validation results based on fMRI data demonstrate that these landmarks are producible, predictable and exhibit accurate structural and functional correspondences across individuals and populations, offering a universal and individualized brain reference system for neuroimaging research."
359,Diagnosing learners' knowledge from their actions using inverse reinforcement learning,"The use of computerized environments in which students complete complex tasks is increasingly common in education. Data about students' actions in these environments has the potential to provide information about these students' knowledge, but can be difficult to interpret.In this paper, we focus on instances in which a student must take a series of actions to complete a goal, and develop a framework for automatically inferring the student's underlying beliefs based on these observed actions. This framework relies on modeling how student actions follow from beliefs about the effects of those actions. By framing the problem in terms of a Markov decision process, we specify a general model that can be applied to a wide range of situations. We first validate that this model can recover learners' beliefs in a lab experiment, and then use it to model data from an educational game. In the lab experiment, the model's inferences reflect participants' stated beliefs, and for the educational game, the model's inferences are consistent with conventional assessment measures."
361,Cluster-Based Active Learning to Address the Class Imbalance and Cold Start Problems,"Active learning (AL) has been used to improve the performance for supervised learning (SL) functions by selecting the training instances to be labeled.  However, there are two open problems which can actually make AL worse than passive learning:  class imbalance and cold start.  First, AL tends to ignore the minority label in the training instances making it unlikely that the function will ever predict the minority label.  Second, the quality for the instances selected depend on those previously selected.  This makes AL very sensitive to the order instances are selected for training.  To address this latter problem, cluster-based AL has been proposed, but they assume that instances in the same cluster have the same label, which is often untrue.  Further, cluster-based AL does not currently address class imbalance.  Therefore, we propose a novel new cluster-based AL powered by the Boundary of Use (BoU) framework which focuses on finding clusters which contain similar instances with multiple labels.  These BoU clusters are designed to capture the area around the decision boundary containing instances which are most informative for AL.  Our experiments, using 21 UCI datasets and four real-world datasets, show that cluster-based AL powered by the BoU framework improves test accuracy using three different SL systems.  Our approach is also more adept at addressing the two open problems than cluster-based AL."
362,A Framework for Enhancing Repair Mechanisms for Supervised Learning,"Repair mechanisms such as feature selection and noise correction have been used to improve the performance for supervised learning (SL) functions.  Furthermore, active learning has been used to identify cost-effective instances and could also be considered as a repair mechanism?removing less useful instances from training.  All these mechanisms have been enhanced with clustering to allow for more specific and localized repairs?hereby known as cluster-based repair?rather than performing repair on all the training data (i.e., universal repair).  However, traditional clustering?grouping similar instances with the same label?can make it difficult to know (for certain) whether a cluster still needs to be repaired or left alone.  Subsequently, repairs applied to these clusters can result in unnecessary repairs and overfitting which ultimately reduce function performance.  Therefore, we propose a novel framework called the Boundary of Use (BoU) framework which repairs clusters where there is a mixture of instances correctly and incorrectly labeled by the function and leaves correct clusters?no need for repair?and incorrect clusters?not repairable?alone.  Our experiments, using 21 UCI datasets, show that the cluster-based repair powered by the BoU framework reduces unnecessary repairs and overfitting and outperform traditional cluster-based repair and universal repair approaches on nine (repair mechanism ? SL system) configurations."
363,Topology Constraints in Graphical Models,"Graphical models are a very useful tool to describe and understand natural phenomena, from gene expression to climate change and social interactions. The topological structure of these graphs/networks is a fundamental part of the analysis, and in many cases the main goal of the study. However, little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data.In this work we propose extensions to the basic joint regression model for network estimation, which explicitly incorporate graph-topological constraints into the corresponding optimization approach. The first proposed extension includes an eigenvector centrality constraint, thereby promoting this important prior topological property. The second developed extension promotes the formation of certain motifs, triangle-shaped ones in particular, which are known to exist for example in genetic regulatory networks. The presentation of the underlying formulations, which serve as examples of the introduction of topological constraints in network estimation, is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge."
364,Fast Exact Gaussian Process Regression for Separable Covariance Functions,"Gaussian process regression (GPR) is a powerful non-linear technique for Bayesian inference and prediction. One drawback is its computational complexity, requiring O($n^3$) flops for both prediction and hyperparameter estimation over $n$ training points due to the need for inverting a covariance matrix. We show that when the problem has the structure that its covariance function is separable in the case of noisy observations, the costs can non-trivially be reduced to a potentially sub-quadratic function of $n$. Separable covariance functions commonly occur in spatial-spatial and temporal-spatial regression problems. Our method is demonstrated on problems in image resampling, denoising, and spherical-temporal kriging applications."
365,Hierarchical Identification of Leaves,"There is massive diversity among deformable biological objects such as cells and plants, including a large number of genetic categories and a large variation in appearance within categories. We present a novel method for determining the species of botanical objects from scanned images. We focus on leaves but the strategy is generic, based on a hierarchical representation of latent variables called identification keys which embody domain knowledge about taxonomy and landmarks. Classification proceeds systematically from coarse-grained to fine-grained characterizations. First, keys are estimated, one at a time, starting with landmarks and proceeding to the genus, and finally the individual species is identified. Each step is conditional on previous estimates. Two other main ingredients are multiple key-based local coordinate systems and likelihood ratios of discriminant scores. We obtain the best performance to date on several databases of scanned simple leaves."
366,Adaptive Compressive Network Modeling,"Network data is ubiquitous nowadays, such as social networks, bio-networks, computer networks, and complex information network. However, the research of network data is still mostly heuristic and lacks rigorous theoretical underpinnings. Until very recently, a new line of work named compressive network modeling sheds light on this problem.  Specifically, [13] proposes a general framework to use compressed sensing techniques to analyze network data. This method explores a large latent dictionary which is able to recover the hidden structure within the network. However, one limitation of this framework is that the dictionary used to model the network must be pre-given, e.g. clique spaces. Such a pre-determined dictionary is not adaptive to unknown datasets. In this paper, instead of assuming the dictionary is prefixed, we propose a method to automatically learn a dictionary based on the observed networks, which better adapts to the empirical data. Our approach, named adaptive compressive network modeling, is amenable to theoretical analysis, computationally tractable and can be successfully applied to many areas. "
367,Scalable Heterogeneous Transfer Ranking,"Learning to rank aims to automatically learn a ranking function with point-wise, pair-wise, or list-wise cost functions and has been demonstrated to achieve superior performance in many information retrieval applications. It is known that the ranking problem usually requires significantly more labeling efforts per task. However, in practical application we can often obtain abundant labeled documents in popular languages but very few or even no labeled documents in less popular languages.  In this paper, we propose to study the problem of heterogeneous transfer ranking, a transfer learning problem with heterogeneous features in order to utilize the rich labeled data in popular languages to help the ranking task in less popular languages. We develop a large-margin algorithm, namely LM-HTR, to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We analyze the theoretical bound of the prediction loss and develop fast algorithms via stochastic gradient descent so that our model can be scalable to large-scale applications. Experiment results on four application datasets demonstrate the advantages of our algorithms over other state-of-the-art methods."
368,Cosegmentation with Subspace Constraints ? Identifying Shared Structure in Related Image Sets,"We develop new algorithms to analyze and exploit the joint subspace structure of a set of related images to facilitate the process of concurrent segmentation of a large set of images. Most existing approaches for this problem are either limited to extracting a single similar object across the given image set or do not scale wellto a large number of images containing multiple objects varying at different scales. One of the goals of this paper is to show that various desirable properties of such an algorithm (ability to handle multiple images with multiple objects showing arbitary scale variations) can be cast elegantly using simple constructs from linearalgebra: this signi?cantly extends the operating range of such methods. While intuitive, this formulation leads to a hard optimization problem where one must perform the image segmentation task together with appropriate constraints which enforce desired algebraic regularity (e.g., common subspace structure). We propose ef?cient iterative algorithms (with small computational requirements) whose key steps reduce to objective functions solvable by max-?ow and/or nearly closed form identities. We study the qualitative, theoretical, and empirical properties of the method,and present results on benchmark datasets."
369,Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation,"Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available.Existing approaches strive to make the distribution of instances in one domain match that of the other domain.  Often, they are oblivious to individual differences of these samples and only tune to their statistical properties collectively as a population. Instead of this macroscopic view, we aim to be microscopic. In particular, we believe not all instances are created equally in terms of adaptability. Thus, it is beneficial to identify the most helpful instances for  adaptation. Mindful of this thought, we propose a landmark-based approach that broadens the notion of matching distributions  by examining it at the instance level. We define landmarks as a subset of labeled data instances in the source that are distributed most similarly to the target, thus presumably more amenable to being adapted. We then leverage the discovered landmarks to bridge the source to the target.  Specifically, we incorporate the landmarks in a cohort of auxiliary adaptation tasks that are provably easier to solve. The key intuition is that domain-invariant feature spaces for those auxiliary tasks form the basis to compose invariant features for the original problem. We show how this composition can be optimized discriminatively with the aid of landmarks,  requiring no labeled data from the target.  The proposed method is validated on standard benchmark datasets for object recognition. Empirical results show the proposed method outperforms the state-of-the-art significantly."
371,"Learning Feature Hierarchies from Recurring Coincidences in Images, Videos and Audios","Models for learning feature hierarchies in an unsupervised and online manner are not only imperative for real world machine perception applications but also helps to understand how the brain works. In this paper, we present a multilayered feedforward neural network model that learns a hierarchy of overcomplete and sparse feature dictionaries. A neuron is modeled as an integrate-and-fire neuron and is connected to its neighbors in its own layer as well as the layers above it by lateral and feedforward connections respectively. As we ascend up the layers, the spatial and/or temporal receptive field sizes of neurons increase. The feedforward weights are learnt from repeating spatial and temporal coincident patterns to encode features. We use the learning algorithm, without any alteration, for learning multiple layers of features from images, videos, and audios with no preprocessing. When applied to edge-detected images, the neurons learn to respond to edges/bars in different orientations at the first layer (similar to simple cells in primary visual cortex or V1), and combinations of these edges that depict parts of shapes or whole generic shapes in higher layers. When applied to action videos, the second layer neurons develop orientation- and direction-selective receptive fields (similar to complex cells in V1). When applied to speech data, the model learns a hierarchy of features that can be used to reconstruct unseen speech."
373,Stratified Tree Search: A Novel Suboptimal Heuristic Search Algorithm,"Traditional heuristic search algorithms use the ranking of states a heuristic function provides to guide the search. In this paper---with the object of improving suboptimality and runtime of search algorithms when only weak heuristics are available---, we present Stratified Tree Search (STS), a novel suboptimal heuristic search algorithm that uses a heuristic function to make a partition of the state space to guide search. We call the partition used by STS a type system. STS assumes that nodes of the same type will lead to solutions of the same cost. Thus, STS expands only one node of each type in every level of search. We empirically evaluated STS in heuristic search domains. Our results show that STS can find solutions of lower suboptimality in less time than standard heuristic search algorithms for finding suboptimal solutions."
374,A Polynomial Mechanism for Resource Allocation under Price Rigidities,"Computational issues on trading mechanisms for resource allocation have been studies intensively in the AI literature. The literature has focused almost entirely on economic models without price rigidities. However, it is ubiquitous in the real world that prices of items are not completely flexible but restricted to some admissible interval price rigidities for some economic or political reasons. This paper studies the computational issues ofdynamic mechanisms for selling multiple indivisible items under price rigidities. We propose a polynomial algorithm that can be used to find over-demanded sets of items, and then introduce a dynamic mechanism with rationing to discover constrained Walrasian equilibria under price rigidities in polynomial time."
375,Robust L1 Normalized Cut and Symmetric Nonnegative Matrix Factorization,"Spectral clustering is widely used in practice. However existing formulation is prone to large errors. In this paper, we first transform it into a matrix decompositionproblem and then propose a robust formulation using L1-norm. This leads to L1 normalized cut and L1 symmetric nonnegative matrix factorization (SNMF) models. We derive computational algorithms for L1 normalized cut. We also providevery efficient updating rules for L1 SNMF with rigorous convergence analysis. Extensive experiments on 6 datasets with significant data corruption/occlusionshow that L1 normalized cut and L1 SNMF provide consistently better clustering results as compared to standard methods and other L1-type functionals."
376,Multiview Spectral Clustering via Pareto Optimization,"Traditionally, the input of spectral clustering is limited to single-view data. However, many real-world datasets come with multiple heterogeneous feature sets, which provide multiple views of the same data. Such datasets include scientific data (fMRI scans of different individuals), social data (different types of connections between people), web data (multi-type data), and so on. How to optimally combine knowledge from multiple views to help spectral clustering find a better partition remains a developing area. Previous work formulates the problem as a single objective function to optimize, typically by combining the views under a compatibility assumption and requiring the users to decide the importance of each view a priori. In this work, we propose a multi-objective formulation and show how to solve it using Pareto optimization. The Pareto frontier captures all possible good cuts without requiring the users to set the ``correct'' parameter. The effectiveness of our approach is justified by both theoretical analysis and empirical results on benchmark datasets."
377,A New Formulation for Deep Neural Net Optimization,"Deep neural nets are very difficult to train, even with parallel computers, because of the ill-conditioned nature of their objective function, which involves a deeply nested mapping from inputs to outputs. We propose a new formulation to optimize deep nets that directly addresses the ill-conditioning problem, based on an idea of auxiliary variables. This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. We solve the constrained problem with a quadratic-penalty approach using alternating optimization over the weights and the auxiliary coordinates. This procedure decouples into many independent subproblems and allows a trivial parallelization. In experiments using autoencoders of varying depth with image and speech data, we show our algorithm far outperforms all leading methods for deep net optimization."
378,L$^{\natural}$-CCCP : L$^{\natural}$-Concave Convex Procedure,"L$^{\natural}$-Convexity is a discrete counterpart of convexity in a continuous function. In this paper, we propose L$^{\natural}$-CCCP (L$^{\natural}$-ConCave Convex Procedure); an approximation algorithm for minimizing the difference of two L$^{\natural}$-convex functions, which can formulate any discrete function optimization. L$^{\natural}$-CCCP is basically a discrete analog of CCCP (ConCave Convex Procedure) for D.C.\@ programing problems. L$^{\natural}$-CCCP is performed as a terminating iterative procedure, each of whose iterations can be computed in the same order as submodular minimization. We~describe the implementation of L$^{\natural}$-CCCP and prove termination at a stationary point. Moreover, we describe an application of L$^{\natural}$-CCCP to multi-label energy minimization with empirical examples."
379,"Combinatorial Multi-Armed Bandit: General Framework, Results and Applications","We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where multiple arms with unknown distributions organized in some unknown combinatorial structure to form super arms, each of which is a unit of play in each round. After a super arm is played, the outcome of related individual arms is revealed, and a reward for the super arm is given. The reward function only needs to satisfy a couple of mild assumptions and is otherwise unknown. Instead of knowing the specifics of the problem instance, we assume the availability of a computation oracle that takes the means of the distributions of arms and outputs a super arm that generates an $\alpha$-approximation of the optimal expected reward. The objective of a CMAB algorithm is to minimize {\em $\alpha$-approximation regret}, which is the difference in total expected reward between the $\alpha$ fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide two algorithms, CUCB and $\varepsilon_n$-greedy, and show that they achieve low regret of $O(\log n)$, where $n$ is the number of rounds played. We then demonstrate applications of our CMAB framework with several problem instances, namely probabilistic maximum coverage (PMC), social influence maximization, and matching bandit. The results on the first two problems are new, while the result on the last one matches the regret of a previous work that designed specifically for this problem."
380,High Dimensional Transelliptical Graphical Models,"We advocate the use of a new distribution family--the transelliptical--for robust inference of high dimensional graphical models. The transelliptical family is an extension of the nonparanormal family proposed by Liu et al. (2009). Just as the nonparanormal extends the normal by transforming the variables using univariate functions, the transelliptical extends the elliptical family in the same way. We propose a nonparametric rank-based regularization estimator which achieves the parametric rates of convergence for both graph recovery and parameter estimation. Such a result suggests that the extra robustness and flexibility obtained by the semiparametric transelliptical modeling incurs almost no efficiency loss. Thorough numerical experiments are provided to back up our theory."
381,Online Egocentric Models for Citation Networks,"With the emergence of large-scale longitudinal network data, dynamic network analysis(DNA) has become a very hot research topic in recent years. Although a lot of DNA methods have been proposed by researchers from different communities, most of them can only model snapshot data recorded at a very rough temporal granularity. Recently, a novel method, called dynamic egocentric model(DEM), has been proposed for DNA which can be used to model large-scale citation networks at a fine temporal granularity. However, DEM suffers from a significant decrease of accuracy over time because the learned parameters and topic features of DEM are static (fixed) during the prediction process of time-varying citation networks. In this paper, we propose an online extension of DEM, called online egocentric model (OEM), to learn time-varying parameters and topic features for dynamic citation networks. Experimental results on real-world citation networks show that our OEM can not only prevent the prediction accuracy from decreasing over time but also uncover the evolution of topics in the citation networks."
382,Adaptive Segmental Alignment for Improved Sequence Classification,"Traditional pairwise sequence alignment is based on matching individual samples from two sequences, under time monotonicity constraints. However, in some instances matching segments of points may be preferred and can result in increased noise robustness. This paper presents an approach to segmental sequence alignment based on adaptive pairwise segmentation. We introduce a distance metric between segments based on average pairwise distances, which addresses deficiencies of prior approaches. We then present a modified pair-HMM that incorporates the proposed distance metric  and uses it to devise an efficient algorithm to jointly segment and align the two sequences. Our results demonstrate that this new measure of sequence similarity can lead to improved classification performance, while being resilient to noise, on a variety of problems, from EEG to motion sequence classification."
383,Kernel Latent SVM for Visual Recognition,"Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning."
386,Learning Partially Observable Models Using Temporally Abstract  Decision Trees,"This paper introduces timeline trees, which are partial models of partially observable environments. Timeline trees are given some specific predictions to make and learn a decision tree over history. The main idea of timeline trees is to use temporally abstract features to identify and split on features of key events, spread arbitrarily far apart in the past (whereas previous decision-tree-based methods have been limited to a finite suffix of history). Experiments demonstrate that timeline trees can learn to make high quality predictions in complex, partially observable environments with high-dimensional observations (e.g. an arcade game)."
387,Sparse Principal Component Analysis with missing observations,"In this paper, we study the problem of sparse Principal Component Analysis(PCA) in the high-dimensional setting with missing observations. Our goal isto estimate the first principal component when we only have access to partial ob-servations. Existing estimation techniques are usually derived for fully observeddata sets and require a prior knowledge of the sparsity of the first principal compo-nent in order to achieve good statistical guarantees. Our contributions is threefold.First, we establish the first information-theoretic lower bound for the sparse PCAproblem with missing observations. Second, we propose a simple procedure thatdoes not require any prior knowledge on the sparsity of the unknown first principalcomponent or any imputation of the missing observations, adapts to the unknownsparsity of the first principal component and achieves the optimal rate of estima-tion up to a logarithmic factor. Third, if the covariance matrix of interest admits asparse first principal component and is in addition approximately low-rank, thenwe can derive a completely data-driven procedure computationally tractable inhigh-dimension, adaptive to the unknown sparsity of the first principal componentand statistically optimal (up to a logarithmic factor)."
388,Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form,"We consider minimizing convex objective functions in \emph{composite form}\begin{align*}  \minimize_{x\in\R^n} f(x) := g(x) + h(x),\end{align*}where $g$ is convex and twice-continuously differentiable and $h:\R^n\to\R$ is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efficiently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. Many problems of relevance in high-dimensional statistics, machine learning, and signal processing can be formulated in composite form. We prove such methods are globally convergent to a minimizer and achieve quadratic rates of convergence in the vicinity of a unique minimizer. We also demonstrate the performance of such methods using problems of relevance in machine learning and high-dimensional statistics."
389,Tumor Gene Expression Purification Using Infinite Mixture Topic Models,"There is significant interest in using gene expression measurements to aid in the personalization of medical treatment.  The presence of significant normal tissue contamination in tumor samples makes it difficult to use tumor expression measurements to predict clinical variables and treatment response.  We propose a probabilistic method, TMMPure, to infer the expression profile of the cancerous tissue using a modified topic model that contains a hierarchical Dirichlet process prior on the cancer profiles. We demonstrate that TMMpure is able to infer the expression profile of cancerous tissue and improves the power of predictive models for clinical variables using expression profiles. "
390,Manifold Alignment Preserving Global Geometry,"This paper proposes a novel algorithm for manifold alignment preserving global geometry. This approach constructs mapping functions that project data instances from different input domains to a new lower-dimensional space, simultaneously matching the instances in correspondence and preserving global distances between instances within the original domains. In contrast to previous approaches, which are largely based on preserving local geometry, the proposed approach is suited to applications where the global manifold geometry needs to be respected. We evaluate the effectiveness of our algorithm for transfer learning in two real-world cross-lingual information retrieval tasks."
391,Efficient Sparse Group Feature Selection via Nonconvex Optimization,"Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2)statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance."
392,A simple algorithm for learning a globally optimal chain event graph from data,"Chain event graphs are a model family particularly suited for asymmetric causal discrete domains. This paper describes a simple algorithm for exact learning of chain event graphs from multivariate data. While the exact algorithm is slow, it allows reasonably fast approximations and provides clues for implementing more scalable heuristic algorithms."
393,A scalable direct formulation for multi-class boosting,"We present a scalable and effective classification model for multi-class boosting classification. In [15], a direct formulation for multi-class boosting was introduced. Unlike many existing multi-class boosting algorithms, which rely on output coding matrices, the approach in [15] directly maximizes the multi-class margin. The major problem of [15] is its extremely high computational complexity, which hampers its application on real-world problems. In this work, we propose a scalable and simple stage-wise boosting method, which also directly maximizes the multi-class margin. Our approach has several advantages: (1) it is simple and computationally efficient to train. The approach can speed up the training time by more than two orders of magnitude without sacrificing the classification accuracy. (2) Like traditional AdaBoost, it is parameter free and empirically demonstrates excellent generalization performance. In contrast, one has to tune the regularization parameter for the multi-class boosting of [15]. Experimental results on challenging multi-class machine learning and vision tasks demonstrate that the proposed approach substantially improves the convergence rate and accuracy of the final visual detector at no additional computational cost."
394,Regularized Off-Policy TD-Learning,"We present a novel $l_1$ regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization.A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm."
395,Multi-criteria Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria."
396,Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes,"To learn reliable rules that can generalize to novel situations, the brain must be capable of imposing some form of regularization. Here we suggest, through theoretical and computational arguments, that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system. The functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise. Noise on the inputs is shown to impose regularization, and when synchronization upstream induces time-varying correlations across noise variables, the degree of regularization can be calibrated over time. The resulting qualitative behavior matches experimental data from visual cortex."
397,Calibrated Elastic Regularization in Matrix Completion,"This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a unified analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. "
398,Guided Network Discovery in fMRI Data via Constrained Tensor Analysis ,We investigate the problem of network discovery which involves simplifying spatio-temporal data into nodes and edges. Such problems naturally exist in fMRI scans of human subjects which consist of activations of thousands of voxels over time with the aim to simplify them into the underlying cognitive network being used. We propose supervised and semi-supervised variations of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least squares solver that is easily implementable. For real fMRI data our approach can reproduce well known results in neurology regarding the default mode network in resting state healthy and Alzheimer affected individuals. We show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive scores both by itself and with additional clinical information.
399,Link Prediction via Ranking with a Multiple Membership  Nonparametric Bayesian Model,"Link prediction in complex networks has found applications in a wide range of real-world domains involving relational data.  The goal is to predict some hidden relations between individuals based on the observed relations.  Existing models are unsatisfactory when more general multiple membership in latent groups can be found in the network data.  Taking the nonparametric Bayesian approach, we propose a multiple membership latent group model for link prediction.  Besides, we argue that existing performance evaluation methods for link prediction, which regard it as a binary classification problem, do not satisfy the nature of the problem.  As another contribution of this work, we propose a new evaluation method by regarding link prediction as ranking.  Based on this new evaluation method, we compare the proposed model with two related state-of-the-art models and find that the proposed model can learn more compact structure from the network data."
400,Modeling Visual Clutter Using Parametric Proto-object Segmentation,"Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects defined by superpixel similarity in intensity, color, and texture, features. We introduce a novel parametric method of merging superpixels using mixtures of Weibull distributions of edge weights, then take the normalized number of proto-objects following partitioning as our estimate of clutter. We validated the model using clutter ratings of 90 images (SUN Dataset) obtained from humans, and showed that our method not only predicted clutter extremely well (r=0.76, p<0.001), but also outperformed other clutter prediction methods."
401,Max-Margin Transforms for Visual Domain Adaptation,"We present a new algorithm for training linear support vector machine classifiers across image domains. Our algorithm learns a linear transformation that maps points from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce a novel cost function for transformation learning based on the misclassification loss of the target points transformed into the source domain. One advantage of our method over previous SVM-based domain adaptation algorithms is that it performs multi-task adaptation, learning a shared component of the domain shift across all categories.  Experiments on both synthetic data and real image datasets demonstrate strong performance and computational advantages compared to previous approaches."
402,Parameter Learning for Submodular Quadratic Pseudo-Boolean Functions,"Submodular quadratic pseudo-Boolean functions play an important role in many inference tasks, particularly in computer vision. Their importance in recent years has grown due to their modeling capability and the development of very fast inference techniques based on max-flow/min-cut algorithms. However, learning the parameters of these models remains a challenge. In this work, we present a simple and efficient algorithm for learning the parameters of a pseudo-Boolean function from training data. Our method directly adjusts edge capacities in the max-flow/min-cut graph so as to minimize the discrepancy between the minimum cut in the graph and the cut induced by the ground truth assignment. We show how our algorithm relates to the subgradient method for structured support vector machines and perform extensive experiments to evaluate variants of our approach."
403,A System for Predicting Action Content On-Line and in Real Time before Action Onset in Humans ? an Intracranial Study,"The ability to predict action content from neural signals in real time before action onset has been long sought in the neuroscientific study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a ?matching-pennies? game against either the experimenter or a computer. In each trial, subjects were given a 5s countdown, after which they had to raise their left or right hand immediately as the ?go? signal appeared on a computer screen. They won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The working hypothesis of this experiment was that neural precursors of the subjects? decisions precede action onset and potentially also the awareness of the decision to move, and that these signals could be detected in intracranial local field potentials (LFP).We found that low-frequency LFP signals from a combination of 10 channels, especially bilateral anterior cingulate cortex and supplementary motor area, were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5s before the go signal with 68?3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 channels simultaneously, and tested it on retrospective data from 6 patients. On average, we could predict the correct hand choice in 80% of the trials, which rose to 90% correct if we let the system drop about 1/3 of the trials on which it was less confident. Our system demonstrates ? for the first time ? the feasibility of accurately predicting a binary action in real time for patients with intracranial recordings, well before the action occurs."
404,Fast Exact MAP Inference by Passing Incomplete Messages,"We propose a novel approach to faster exact MAP inference that builds on max-product (min-sum) message passing on clique trees and exploits the branch-and-bound idea. The high level procedure is to propagate incomplete messages over the clique tree while maintaining local upper bounds at sepsets. Our algorithm is guaranteed to converge and find the global optimal solution. Empirically we show that our method consistently demonstrates large savings over state-of-the-art methods on several different models with both synthetic and real world data. Our approach also suggests an interesting connection between exact and approximate MAP inference: If we can find tighter relaxations (over sub-graphs), we can make use of the lower bounds to perform exact inference even faster."
405,"Constrain, Train, Validate and Explain: A Classifier for Mission-Critical Applications","Classifiers used in mission-critical applications, where misclassification errors incurhigh costs, should be robust to training-set artifacts, such as insufficient ormisrepresentative coverage, as well as high levels of missing values. As such,they are required to support intensive designer-control, and a range of validationprocedures that go beyond cross-validation. For such applications, we advocatethe use of a family of classifiers that employ a factored model of the posteriorclass probabilities. These classifiers are simple, interpretable, allow their designersto enforce a variety of domain-specific constraints, and can handle missingdata both during training and at prediction time. Such classifiers are also capableof explaining their decisions in terms of the basic measured quantities."
407,Emergence of Flexible Prediction-Based Discrete Decision Making and Continuous Motion Generation through Actor-Q-Learning,"In this paper, through the learning of invisible-target-capturing task by Actor-Q-learning with a recurrent neural network (RNN), the followings are shown.(1) Prediction-based continuous motions that should be varied complicatedly by several factors can be acquired only from rewards and punishments through reinforcement learning (RL) with a RNN.(2) Actor-Q-learning that is a RL method for both discrete decision(action) making and continuous motion generation works in a task other than active perception-and-recognition. It can also learn prediction-required action and motion by using a RNN.(3) As initial connection weights in RNN, local and regular connection and positive self-feedback connection improve the performance of learning with vision-like sensor inputs.(4) Two problem-solving strategies one of which is selectively used can be acquired in just one neural network as a parallel processing-and-learning system only from rewards and punishments through RL. The strategy-switching caused by situation changes also emerges without any explicit switching element or any prior knowledge."
408,Bayesian Estimation for Partially Observed MRFs,Bayesian estimation in Markov random fields is very hard due to the intractability of the partition function. The introduction of hidden units makes the situation even worse due to the presence of potentially very many modes in the posterior distribution. For the first time we propose a comprehensive procedure to approximate the evidence of partially observed MRFs based on the Laplace approximation. We also introduce a number of approximate MCMC-based methods for comparison but find that the Laplace approximation significantly outperforms these.
409,Efficient Random Walk with Gaussian Kernels,"Implicit manifolds is a technique used with random walk-based semi-supervised learning and graph clustering methods to implicitly construct a dense similarity matrix, reducing the cost of manipulations on the matrix from $O(n^2)$ to linear. Specifically, propagating labels through an $O(n^2)$ matrix can be replaced with propagating through a series of sparse matrices. While similarity functions such as cosine similarity are easily ``plugged into'' the implicit manifold framework, it is not straightforward to integrate the Gaussian kernel. In this paper we propose two methods to do this and provide experimental results."
410,Searching for objects driven by context,"The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired.We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image. Their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set.In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy."
411,Time-Dependent Subset Mining via iHMM,"When we analyze a time series data with large cardinality of item sets (or, high-dimensional feature vectors), it is often the case that a pile of non-informativeitems disturbs mining of hidden dynamic patterns behind the data. In such cases, we need to find and extract a tiny portion of informative items that are related(informative) to the change of hidden patterns. In this paper, we address the problem of modeling time series data that are characterized by the large cardinalityof item sets and sparseness of related items among them, where the relatedness of items are time-dependent. We propose an extension of the infinite Hidden Markov Model so that only related items of each hidden state be automatically selected. We can improve estimation of hidden states by excluding unrelated (non-informative) items and focusing on the latent structure analysis of extracted related items. Combined with the nonparametric Bayes approach, the proposed model simultaneously selects related items and finds hidden states without knowing thenumber of states beforehand. The proposed model is experimentally verified and we show that both relatedness and time dependency are important for mining related items of time series data."
412,Stationary Component Analysis for Video Classification,"Low-dimensional modelling of videos is key to the success of efficient video classification algorithms.From a signal processing perspective, video data is a mixture of stationary and non-stationary components.While videos belonging to the same class share the same stationary components,they may differ in the non-stationary ones.Existing video classification methods based on dimensionality reductiontypically fail to explicitly separate stationary parts of the signal from non-stationary ones.As a consequence, the low-dimensionality representation of the videos contains information that is not class-specific,and thus irrelevant for classification.We propose an approach to video modelling that overcomes this issue by explicitly separatingstationary from non-stationary parts of the signal.To this end, we make use of Stationary Subspace Analysis,which factorises multivariate time series into stationary and non-stationary components.Video classification can be then performed based only on the relevant, stationary part of the video signal.We demonstrate the benefits of our approach over state-of-the-art techniques on the tasks of near-duplicate video detection,dynamic texture classification and scene recognition.Our study shows that modelling videos with their stationary components not only dramatically improves recognition accuracy,but also yields a significant advantage in computational cost of classification over existing methods."
413,Timely Object Recognition,"In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the eminent PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains $66\%$ better AP than a random ordering, and $14\%$ better performance than an intelligent baseline. On the timeliness measure, our method obtains at least $11\%$ better performance. Our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning."
414,Multi-Domain Manifold Learning for Interaction Prediction,"Despite of the high dimensionality in many machine learning problems, data distributions in these high dimensional spaces usually span on certain low dimensional manifolds. In the last few years, extensive research efforts have been devoted to the utilization of manifold property on high dimensional data, e.g. dimension reduction methods preserving local structures of the manifolds. Motivated by the successes of these studies, we extend the manifold learning problem from single domain to multiple domain, especially on learning cross-domain interactive pairs. While many real-world applications (e.g. automatic image annotation) can be modelled as multi-domain interaction prediction, existing solutions to manifold learning fail to fully exploit the manifold property of the distributions. In this paper, we propose a general framework to bridge the gap, taking both manifold structures and known interaction/non-interaction information into account. To overcome the challenges of domain scaling and information inconsistency, we formulate the problem with Semidefinite Programming(SDP), including new constraints to improve the robustness of the learning procedure. A variety of optimization techniques are also designed to enhance the scalability of the problem solver. Effectiveness of the method is evaluated by experiments with two different tasks, including drug prediction and image annotation."
416,Model Selection in Markov Reward Processes,"Algorithms for solving Markov reward processes almost always start with the assumption that the state space is known. In this work we address the problem of how to use data to choose from a set of candidate discrete state spaces, where these spaces are constructed by a problem dependent domain expert. We discuss the difference between our proposed framework and the classical maximum likelihood framework, and give an example for such a criterion to fail. We propose alternative criterion and prove that it is consistent if the models are identifiable in an appropriate sense."
417,Nonparanormal Belief Propagation (NPBP),"The empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances. Yet, for continuous non-Gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models.  In this work we present Nonparanormal BP  for performing efficient inference on distributions parameterized by  a Gaussian copulas network and any univariate marginals. For  tree structured networks, our approach is guaranteed to be exact for  this powerful class of non-Gaussian models. Importantly, the method  is as efficient as standard Gaussian BP, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used."
418,Kernelized Bayesian Matrix Factorization,"We extend kernelized matrix factorization with a full-Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (a) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas full-Bayesian treatments are not computationally feasible in the earlier approaches. (b) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our proposed method outperforms alternatives in predicting drug-protein interactions on two data sets."
419,Fast Convolutional Sparse Coding,"Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many signals (e.g. visual and acoustic) however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimisation problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and augmented Lagrange multipliers (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence. "
420,Learning Non-Linear Sub-Spaces using K-RBMs,"The overall complexity in building descriptive or discriminative models is shared between the features derived from raw data and the models that use these features as inputs. Simple features require complex models while more sophisticated features require simpler models to achieve the same level of model quality. Learning semantically richer features is, therefore, the key to building simpler, more interpretable, and more accurate models. In domains such as images, where the data (image patches) might lie in multiple non-linear manifolds, feature learning becomes even more important. In this paper, we propose a framework that uses K Restricted Boltzmann Machines (K-RBMs) to learn non-linear manifolds in the raw image space. We solve the coupled problem of finding the right non-linear manifolds in the input space and associating image patches with those manifolds in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that such a framework outperforms the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks."
421,A New Fast Stochastic Bayesian Learning Automata: a Machine Learning Perspective,"One of the drawbacks of Learning Automata is having a relatively slow rate of convergence, thus the main challenge of Learning Automata theory is designing faster learning algorithms. In this paper, we propose a new fast learning algorithm from a machine learning perspective. The key idea is that the estimator which estimates the probability of stochastic environment rewarding each action, is considered as reconstructing Bernoulli distribution from sequential data, and is formalized based on exponential conjugate family which enables our designed Learning Automata having relatively simple format and hence easy to be implemented. We emphasize that this approach is quite generous and applicable to existing estimator based Learning Automata. Meanwhile,the -optimality of the proposed Learn-ing Automata referred to as Generalized Bayesian Stochastic Estimator Learning Automata is also presented. Extensive experimental results on benchmark environments demonstrate our proposed learning scheme is faster than the LA state of the art."
423,The Smoothness of The Stationary Distribution of Linear Predictive State Representations,"In this paper we consider linear predictive state representations (PSR), a modeling framework that has been proposed as an alternative to finite state partially observable Markov decision processes (POMDP), which is also capable of representing a strictly larger class of systems. Our main result is the proof of smoothness of the stationary distribution of a linear PSR with respect to the parameters of any finite memory policy. This property is important in cases when it is difficult to model the system accurately, while estimating expectations of different quantities from data is easy. Such result suggests that these estimates are robust with respect to slight changes in a policy. This result can also be seen as the first step in developing a perturbation theory for linear PSRs.In addition, we propose a new representation for the reward process, termed (linear) predictive-state reward process (PRP), which naturally extends a well known (linear) predictive state representations to capture a possibly continuous reward signal. Linear PRPs are, by design, suited for planning approaches that optimize average reward criterion. We then show that for a certain class of policies the average reward in linear PRPs is a smooth function of policy parameters. The result suggests that in fact the same should hold for any finite memory policies, remaining a subject of future work."
424,Deep Representations and Codes for Image Auto-Annotation,"The task of assigning a set of relevant tags to an image is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and fix an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of full sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we outperform or compete with existing annotation approaches that use over a dozen distinct image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efficient storage and fast comparisons. In our experiments, using deeper architectures always outperform shallow ones."
427,On High Dimensional Positive-definite Covariance Matrix Estimation,"We propose a novel approach called PLaCE (Positive-definite Large Covariance Estimation) for estimating high dimensional positive-definite covariance matrices. Our method can be viewed as an extension of the generalized thresholded operator (GTO, Rothman et. al 2010) with a positive-definite guarantee. Computationally, we derive an efficient algorithm named ISP (Iterative Soft-thresholding and Projection) based on the augmented Lagrangian method. Theoretically, we analyze the oracle properties of the PLaCE in the sparse covariance estimation under the different matrix norms. Empirically, we conduct the numerical experiments on both simulated and real data sets to illustrate the usefulness of the proposed method."
428,An Analysis on Ensembles of Neural Networks: a Motivation Behind Low Dimensional Networks,"The accuracy of ensembles have been shown to depend on the base classifier's accuracy and diversity. However, choosing good base classifiers to compose an ensemble is still a difficult task. The base classifier's accuracy-diversity trade-off composes a complex space to search. Moreover, searching for all possible classifiers is naturally unfeasible and some assumptions have to be taken. In this context, we present a motivation behind low dimensional classifiers. The experiments are conducted on varying levels of difficulty (time constraints and addition of irrelevant variables) and different datasets. Based on these experiments, it is verified that an evolutionary ensemble made of a certain portion of low dimensional neural networks can be more robust and accurate than high dimensional ones. Throughout the experiments the influences of diversity, base classifier's accuracy as well as the effects of the ensemble and evolution are analyzed and empirically evaluated. Thus, the use of low dimensional base classifiers is justified as a reasonable assumption for the construction of ensembles, which enables ensembles to achieve the maximum observed accuracy as well as surpass high dimensional ones in robustness and learning speed."
429,Identifying Unique Features in Latent Generative Models:  Medicinally-Induced fMRI Networks in Buproprion Trials,"Machine learning and feature evaluation algorithms often assume a direct correspondence of features across observations, with the features themselves well-established and directly measurable. Oftentimes though the true number and form of the features are unknown, and observations are merely a permuted assortment of distorted variables. Four-dimensional neuroimaging data, such as fMRI, can be understood as a collection of brain networks, operating over time.  These networks vary both within and across subjects'  brain scans, and not all brain networks operate during a given scan. Because of this, it is non-trivial to identify generative, consistent features that are unique to certain treatment conditions.  We present a method for learning the true form, uniqueness, and function of latent generative features in situations where the observations are a permuted selection of distorted features, obtained from a higher-level generative set of possible parents. Two competing basis sets are used to model the observed features within the desired treatment condition, and the distribution of the model fits are used to compute a posterior probability for whether the observed feature originated from the set of generative features common across all treatment conditions, or whether that feature exists only within that treatment condition. The observed unique features are then used to create a new generative set that is specific to the treatment condition.  We illustrate these methods by identifying the treatment networks associated with the drug buproprion during a smoking-cessation study, obtaining possible brain networks that exist only post-treatment for subjects who took the medication, but for none of the other treatment groups."
430,Enhanced statistical rankings via targeted data collection,"We study the dependence of the statistical ranking problem on the available pairwise data and propose a framework for which additional data may be collected to increase the informativeness of the ranking. Given a graph where vertices represent alternatives and pairwise comparison data is given on the edges, the \emph{statistical ranking problem} is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. For each edge $ij$, the pairwise comparison data consists of $w_{ij}$ comparisons between the alternatives $i$ and $j$ and  mean preference, written $y_{ij}$. Our goal, given an existing pairwise comparison dataset, $(w,y)$, is to augment this dataset, denoted $(\tilde w,\tilde y)$, so that  the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximally increases the informativeness of the ranking. Since there is a tradeoff between the amount of pairwise data to be ranked and the informativeness of the ranking, we constrain the total number of additional pairwise comparisons $\|\tilde{w}- w\|_{1}$. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding edge weights such that the $\tilde{w}$-weighted graph Laplacian has large second eigenvalue. This reduction of the data collection problem to spectral graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. "
431,Collaborative Filtering with Hybrid Restricted Boltzmann Machines,"We propose an novel framework for collaborative filtering based on RestrictedBoltzmann Machines (RBM), which extends previous RBM-based approaches inseveral important directions. First, we work with numbers as opposed to usingcategorical variables, which allows us to take the natural order between user-itemratings into account, e.g., we consider predicting a rating of 4 when the actual rat-ing is 5 to be better than predicting a rating of 2. More importantly, while previousRBM research modeled the correlation between item ratings only, we model bothuser-user and item-item correlations in a unified framework. Finally, we explorethe potential of combining the original training data with data generated by theRBM model itself in a bootstrapping fashion. The evaluation on the MovieLensdataset shows that our extended RBM model yields results that rival the predictionquality of the best previously-proposed collaborative filtering algorithms."
432,Local Learning with Local Patch Dissimilarity for Image Classification,"A new dissimilarity measure for images based on patches, called Local Patch Dissimilarity (LPD), was recently introduced in [1]. It was inspired from rank distance which is a distance measure for strings.There are many other patch-based techniques used in image processing. Patches contain contextual information and have advantages in terms of generalization. But most patch-based algorithms are heavy to compute with our current machines [2]. Despite LPD is also heavy computational, it has very promising results in image processing and works very well in image classification. In this paper we turn to unconventional learning methods to avoid the problem of the higher computational time on large sets of images. We conduct several experiments on large datasets using methods such as k-NN with filtering and local learning. All methods are based on LPD. The obtained results come to support the previous work of [1].  "
433,Revenue Maximization for Groupon-Style Websites by Dynamic Deal Allocation,"Groupon-style (GS) websites are very popular in electronic commerce and online shopping nowadays.  They provide one or multiple deals with significant discount at their websites every day. We study how to dynamically allocate the user traffic of a GS website to different deals to maximize its revenue. We first consider a simple scenario in which the GS website shows one deal to each visitor, and derive an efficient algorithm to find the optimal allocation. We then consider the scenario of showing multiple deals to each visitor, and propose an efficient algorithm that can achieve a 1/4 approximation of the optimal allocation in the worst case. With these results, we demonstrate that carefully designed allocation algorithms can help a GS website improve its revenue by better leveraging its user traffic."
434,Identifying Stationary Behavior in Externally Driven Environments,"  In real-world settings, environments and sensory data are typically  subject to change over time. Sensor readings are a mixture of  changes due to control on actors of a system and resulting changes  of its internal state but might also contain changes in the  properties of the dynamical system itself. Such changes are caused  by faults, wear, adjusted system configuration, or other permanent  changes. The task of separating these two sources is hard because  the resulting effects in measurable signals might live on similar  time scales, moreover the irrelevant signal components can have  significant internal structure. We present a new approach based on  Stationary Subspace Analysis to detect changes in such complex  scenarios reliably. We demonstrate its capabilities on synthetic  data. Finally, we consider a real-world application, a gas turbine  simulation where we detect changes of internal parameters under  heavily varying external conditions caused by transient operation."
435,Fast variational inference for stochastic differential equations,We introduce a Gaussian variational mean field approximation for inference in continuous time stochastic differential equations. This approach allows us to express the variational free energy as a functional of the marginal moments of the approximating Gaussian process. A restriction of moments to piecewise polynomial functions over time makes the complexity of approximate inference for stochastic differential equation models comparable to that of  discrete time hidden Markov models. We demonstrate the algorithm on state and parameter estimation for nonlinear problems with up to forty state variables. 
436,"Exploring Distinct, Alike and Identical Concepts for Transfer Learning","Transfer learning targets at leveraging the knowledge from source domains with different distributions, to train accurate models for test data coming from target domains. In spite of the different distributions in raw word features, which degrade the performance of traditional machine learning algorithms, the high level concepts(e.g., word clusters) are more appropriate for classification. Most previous works assume that the source and target domains share the same set of concepts, however, different domains may contain the distinct concepts. Along this line, we explore three types of concepts, namely distinct, alike and identical concepts for transfer learning and propose a generative model DAIC. Then an EM algorithm is developed to solve the optimization problem. Furthermore, we design two types of transfer learning tasks. One is the newly constructed classification problem, in which the distinct concepts may exist, and the other is the traditional transfer learning task. Finally,systemic experiments demonstrate the effectiveness of our model. It is worth mentioning that our model can gain 47.67% average accuracy improvement over the traditional machine learning algorithm, i.e., Logistic Regression, on the much more challenging transfer learning tasks."
437,Maximal Deviations of Incomplete U-statistics ?with Applications to Empirical Risk Sampling,"It is the goal of this paper to extend the Empirical Risk Minimization paradigm, from a practical perspective, to the situation where a natural estimate of the risk is of the form of a $K$-sample $U$-statistics, as it is the case in the $K$-partite ranking problem for instance. Indeed, the numerical computation of the empirical risk is hardly feasible if not infeasible, even for moderate samples sizes. Precisely, it involves averaging $O(n^{d_1+\ldots+d_K})$ terms, when considering a $U$-statistic of degrees $(d_1,\;\ldots,\; d_K)$ based on samples of sizes proportional to $n$. We propose here to consider a drastically simpler Monte-Carlo version of the empirical risk based on $O(n)$ terms solely, which can be viewed as an \textit{incomplete generalized $U$-statistic}, and prove that, remarkably, the approximation stage does not damage the ERM procedure and yields a learning rate of order $O_{\mathbb{P}}(1/\sqrt{n})$.Beyond a preliminary theoretical analysis guaranteeing the validity of this approach, numerical experiments are displayed for illustrative purpose."
438,Memory Constraint Online Multitask Classification,"We investigate online kernel algorithms which simultaneously processmultiple classification tasks while a fixed constraint is imposed onthe size of their active sets. We focus in particular on the design ofalgorithms that can efficiently deal with problems where the number oftasks is extremely high and the task data are large scale.  Two newprojection-based algorithms are introduced to efficiently tackle thoseissues while presenting different trade offs on how the availablememory is managed with respect to the prior information about thelearning tasks.  Theoretically sound budget algorithms are devised bycoupling the Randomized Budget Perceptron and the Forgetron algorithmswith the multitask kernel.  We show how the two seemingly contrastingproperties of learning from multiple tasks and keeping a constantmemory footprint can be balanced, and how the sharing of the availablespace among different tasks is automatically taken care of. We proposeand discuss new insights on the multitask kernel. Experiments showthat online kernel multitask algorithms running on a budget canefficiently tackle real world learning problems involving multipletasks."
439,Toward Optimal Uniform Stratification for Stratified Monte-Carlo Integration,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite budget n of noisy evaluations to the function. We tackle in this paper the problem of stratifying the domain in an efficient way. More precisely, it is interesting to refine the partition of the domain as much as possible, to have more flexibility on where to sample. On the other hand, having a (too) refined stratification is not optimal, since the more refined it is, the more difficult it is to estimate the variance of the noise and the variations of the function, in each stratum. We provide in this paper an algorithm, Adapt-MC-UCB, that is almost as efficient (up to a constant) as the algorithm MC-UCB (introduced in [Carpentier and Munos, 2011]) run on the best uniform partition defined in a hierarchical partitioning of the domain."
440,Phase vs. amplitude ? learning from subjective image quality assessment,"In frequency-based representation of images, phase and amplitude convey complementary information. Phase has been regarded as dominating the image appearance, however power (amplitude) spectra have recently been found useful for image classification. In the primary visual cortex (V1), simple cells are sensitive to and thereby encode the phase of visual stimuli, while complex cells, being majority in V1, show phase-invariance and encode the energy (magnitude) of simple cells? spikes. In this paper, we attempt to quantitatively exploit the relative importance of phase and amplitude to visual perception by learning from subjective image quality assessment. We designed an image quality metric based on the weighted combination of the amplitude and phase errors, and determined the weights so as to maximize the prediction accuracy of the metric over subjectively- rated databases, where a joint optimization over multiple databases strengthened the reliability of weights. The results confirm that: 1) both the phase and the amplitude are necessary for image quality assessment; 2) the amplitude becomes more important at the finest image scale while the phase dominates at the coarser scale. Moreover, the multiplicative combination of the amplitude and phase errors plausibly interprets the visual perception on negative images."
441,A Spectral Algorithm for Latent Dirichlet Allocation,"The problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that the observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem involving the estimation of  topic probability vectors (the distribution of words under each topic), when only the words are observed and the corresponding topics are hidden.We provide a simple and an efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which can be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order observed moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on $k \times k$ matrices, where $k$ is the number of latent factors (e.g., the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$)."
442,Identification of Consistent Brain Networks via Maximization of Predictability of Functional Connectivity from Structural Connectivity,"Recent studies have suggested that structural brain connectivity is strongly correlated with functional connectivity. However, it is still largely unknown what brain networks best possibly exhibit such close structural/functional connectivity relationship and how this close relationship can guide the identification of brain networks. This paper presents a novel framework to infer brain networks that are consistent across multiple neuroimaging modalities and across individuals. Our basic premise is that the predictability of functional connectivity from structural connectivity within each brain network should be maximized, which is formulated by and solved via a novel feedback-regulated multi-view spectral clustering algorithm. We applied and tested the proposed algorithm on the multimodal structural and functional brain networks of 50 healthy subjects, and obtained promising results. Our validation experiments demonstrated that the derived brain networks are in agreement with current neuroscience knowledge and offer novel insights into the close relationship between brain structure and function."
443,Music Generation with Weighted Finite-state Transducers,"We approach the task of musical style imitation by probabilistically modeling the melody and harmony of music pieces in the framework of weighted finite-state transducers (WFSTs), which have been used successfully for probabilistic models in speech and language processing. We divide the generation process into different steps, each performed by inference though transducers. We present a method to imitate local and global structure in the melody of music pieces, and a method for four-part harmonization that models vertical and horizontal structure in the generated harmonization. The weights of our transducers are learned with maximum likelihood estimation from a corpus of music pieces. We compare the predictive power of our models against that of existing approaches."
445,Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs,"Given $\alpha,\epsilon$, we study the time complexity  required to improperly learn a halfspace with misclassification  error rate of at most $(1+\alpha)\,L^*_\gamma + \epsilon$, where  $L^*_\gamma$ is the optimal $\gamma$-margin error rate. For $\alpha  = 1/\gamma$, polynomial time and sample complexity is achievable  using the hinge-loss. For $\alpha = 0$, \cite{ShalevShSr11} showed  that $\poly(1/\gamma)$ time is impossible, while learning is  possible in time $\exp(\tilde{O}(1/\gamma))$.  An immediate  question, which this paper tackles, is what is achievable if $\alpha  \in (0,1/\gamma)$.  We derive positive results interpolating between  the polynomial time for $\alpha = 1/\gamma$ and the exponential  time for $\alpha=0$. In particular, we show that there are cases in  which $\alpha = o(1/\gamma)$ but the problem is still solvable in  polynomial time. Our results naturally extend to the adversarial  online learning model and to the PAC learning with malicious noise  model."
446,Regularized Mapping to Latent Structures and Its Application to Web Search,"The task of matching data from two heterogeneous domains naturally arises in various areas, for example, in real-world web search. However, to our knowledge, there is no principled approach to learning a matching model. In this paper, we propose a framework for matching heterogeneous objects, which renders a rich family of matching models when different regularizations are enforced. With orthonormal constraints on the mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with $\ell_1$+$\ell_2$ type of regularization,  we obtain a new model called \emph{Regularized Mapping to Latent Structures} (RMLS).  RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization.  As another contribution, we give a generalizationanalysis of this matching framework, and apply it to both PLS and RMLS.  We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods,  while RMLS significantly speeds up the learning process."
447,Convex formulations of radius-margin based Support Vector Machines,"We consider Support Vector Machines (SVMs) learned together with linear transformations of the feature spaces on which they are applied. Under this scenario the radius of the smallest data enclosing sphere is no longer ?xed. Therefore optimizing the SVM error bound by considering both the radius and the margin has the potential to deliver a tighter error bound. In this paper we present two novel algorithms: R-SVM^+_{mu} ? a SVM radius-margin based feature selection algorithm, and R-SVM^+ ? a metric learning-based SVM. We derive our algorithms by exploiting a new tighter approximation of the radius and a metric learning interpretation of SVM. Both optimize directly the radius-margin error bound using linear transformations. Unlike almost all existing radius-margin based SVM algorithms which are either non-convex or combinatorial, our algorithms are standard quadratic convex optimization problems with linear or quadratic constraints. We perform a number of experiments on benchmark datasets. R-SVM^+_{mu} exhibits excellent feature selection performance compared to state of the art methods such as SVMRFE; in addition it determines automatically the appropriate sparsity level,unlike most existing feature selection algorithms, which require a sparsity constraint, such as the number of features to select. R-SVM+ has a predictive performance that is signi?cantly better than SVM and other state of the art SVM variants."
448,Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states."
449,Learning with Marginalized Corrupted Features,"An important goal of machine learning is to develop predictors that are robust to noise in the observations. In this paper, we consider a particular type of observation noise in which features are ``blanked out'' with some probability. Such blank-out noise occurs, \emph{e.g.} when sensors measuring features (temporarily) break down or when particular words related to a topic are not observed in a document. A simple way to train predictors that are robust to such noise is to extend the training data with training examples in which some of the variables are blanked out at random, but such an approach is computationally costly. This paper presents a new approach, called \emph{marginalized corrupted features} (MCF), that trains robust predictors by minimizing the expected value of the loss function under the blank-out noise model. Experimental evaluation of our approach reveals that the resulting predictors are not only more robust to sensors breaking down, but that they also perform substantially better on data with high-dimensional, heavy-tailed features, such as bag-of-words text documents. "
451,Controlling Transfer For Reinforcement Learning,"Recently, algorithms for transfer learning in reinforcement learning have been proposed that utilize a map between state-action pairs in the target and source tasks. This map is used to suggest possible values for states in the target task based on values of states in the source task. In this paper, we describe a generic transfer algorithm that, given any such map, determines online if the the map is correct and if not limits its use and minimizes {\em negative transfer}. We give bounds on the expected negative transfer and perform experiments to illustrate the usefulness of the algorithm."
452,Structured sparse coding via group gating,"Multiplicative feature learning models, like the Gated Boltzmann Machine,  are a recent extension of sparse coding for modeling relations and image transformations.  A potential shortcoming of these methods is that they do not allow any given feature to be transformed in multiple different ways, because features are matched in pairs.  We propose a ``group gating'' model that addresses this issue by allowing the re-use of filters.  In the model, features form groups, and multiplicative interactions are allowed between all features within each group. We demonstrate that the group gating extension can lead to improved performance in transformation extraction tasks.  We also show that learning on natural videos leads to filter-groups of similar frequency and orientation and of varying phase, as well as to ``pinwheel'' structures in the case of overlapping groups, providing a  new interpretation of this effect known previously from subspace energy models."
453,Directly Optimizing 0-1 Loss for Large-Scale Nonlinear Transductive Multiclass Classification,"We take a new look at graph-based transductive classifiers of min-cut type (MCCs), and we offer a new derivation for them as regularized risk minimizers for non-parametric, discrete-valued function spaces. MCCs directly optimize the expected 0\u20131 loss without need for a convex surrogate loss and they can be naturally formulated in the multi-class situation without need for one-vs-one or one-vs-rest constructions. Nevertheless, they can be trained efficiently (exactly in the two-class case, approximately with factor 2 guarantee in the multi-class case) using discrete energy-minimization techniques. This allows scaling them to large datasets, as we show in experiments on standard computer vision datasets"
454,Bayesian Conditional Tensor Factorizations for High-Dimensional Classification,"In many application areas, data are collected on a categorical response and highdimensional categorical features, with the goals being to build a parsimoniousmodel for classification while doing inferences on the important features. By using a carefully-structured Tucker factorization, we define a model that can characterizeany classification function, while facilitating variable selection and modeling of higher-order interactions. Following a Bayesian approach, we propose a Markov chain Monte Carlo algorithm for posterior computation accommodating uncertainty in the features to be included. Under near sparsity assumptions, the posterior distribution for the classification function is shown to achieve close tothe parametric rate of contraction even in ultra high-dimensional settings in which the number of candidate features increases exponentially with sample size. Themethods are illustrated through several applications."
455,Sample Bias Correction for Regression,"This paper presents a theoretical and empirical study of a discrepancy minimization (DM) sample bias correction algorithm in regression.  We give a theoretical analysis of sample bias correction for kernel ridge regression and prove new and more informative learning guarantees in terms of the \emph{discrepancy} of the empirical distributions. These results provide a strong theoretical support for the use and application of the DM algorithm. We have carried out an extensive empirical analysis of this algorithm both with artificial and real-world data sets and compared it with three other state-of-the-art sample bias correction algorithms applicable in regression. Till now, this algorithm had only been applied to the problem of domain adaptation and it was primarily evaluated for computational efficiency. Here we carry out a thorough comparative study of the algorithm used for the problem of sample bias correction. We report in detail the results of these empirical results which demonstrate the benefits of this algorithm."
456,Analog readout for optical reservoir computers,"Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers."
457,Robust Out-of-sample Extension for the Local and Global Consistency Algorithm,"The contribution of this paper is twofold. First, we reformulate the Local and Global Consistency (LGC) algorithm and show how it can be computed in linear time with the number of unlabeled examples combining an approximated eigendecomposition with Woodbury's formula. Last, we provide a robust out-of-sample extension for the LGC algorithm using this novel formulation. We provide a naive approximation to turn the proposed out-of-sample extension feasible for large-scale problems. Experiments on a number of benchmark data sets show the effectiveness of the proposed approach."
458,Visual Interestingness in Image Sequences,"Interestingness -- The power of attracting or holding one's attention(http://www.thefreedictionary.com/Interestingness, 2012/02/22).Visual attention is guided by experience with similar situations. Such experience can make us surprise and excite when unexpected visual events occur, or make us wait for an interesting upcoming event. Consider for example the image sequences in the figure on the left. The spider in front of the camera or the snow on the lens are examples of events that deviate from the context since they violate the expectations, whereas the egg in the stork's nest is an expected but exciting event that attracts huge attention. In this work we investigate what humans consider as interesting in image sequences and to what extent and why current state-of-the-art computer vision methods can automatically detect such interesting events."
459,Accuracy at the Top,"We introduce a new notion of classification accuracy based on the top $\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines. We define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems. We also present margin-based guarantees for this algorithm based on the $\tau$-quantile of the functions in the hypothesis set. Finally, we report the results of several experiments evaluating the performance of our algorithm. In a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top."
460,Minimizing Sparse High-Order Energies by Submodular Vertex-Cover,"Inference on high-order graphical models has become increasingly important in recent years. We consider energies with simple 'sparse'high-order potentials. Previous work in this area uses either specialized message-passing or transforms each high-order potential to the pairwise case.We take a fundamentally different approach, transforming theentire original problem into a comparatively small instance of a submodularvertex-cover problem. These vertex-cover instancescan then be attacked by standard pairwise methods,where they run much faster (4--15 times) and are often more effectivethan on the original problem.We evaluate ourapproach on synthetic data, and we show that our algorithmcan be useful in a fast hierarchical clustering and model estimation framework."
461,Learning in Real-Time in Repeated Games,"Despite much progress, state-of-the-art learning algorithms for repeated games still often require thousands of moves to learn effectively, even in two-agent, two-action games.  Our goal is to find algorithms that learn effective strategies in tens of moves against many kinds of associates.  Toward this end, we describe a new method designed to increase the learning speed and proficiency of expert learning algorithms.  We show that this method improves four expert learning algorithms, and produces an algorithm that quickly learns effective strategies in many two-agent, two-action repeated matrix games played against many associates."
462,On the Relation of Loss-Based Error Bounds to Discriminative Training Criteria,"General Bayes decision theory involves a loss function which is linked to the error rate of the corresponding classification task. Many pattern recognition tasks like automatic speech recognition, part-of-speech tagging, machine translation, and other string (word sequence) recognition tasks use the symbol (word, character, tag, etc.) error rate as evaluation measure based on a non-trivial loss function. Opposed to string recognition tasks, most pattern classification tasks assume classes having no symbol level and are evaluated using the classification error performance measure based on the simple 0-1 loss function (cost 0/1 for correct/false classification). We follow a principled approach which derives discriminative training criteria from bounds on the loss-based error. The derived criteria have a sound property ---- in case of infinite training data the corresponding loss-based error is minimized. Theoretical error optimal bounds and criteria are a novelty for loss-based string recognition tasks. Furthermore, the minimum phoneme error (MPE) criterion, which is the state-of-the-art discriminative training criterion in ASR, is shown to be an approximation to one of the proposed criteria. This result connects MPE to the corresponding loss-based error and gives a better and so far unknown theoretical justification for the practical effectiveness of MPE. The theoretical results are complemented by experiments comparing the novel and state-of-the-art discriminative training criteria on large scale ASR tasks."
463,State Abstraction in Reinforcement Learning by Eliminating Useless Dimensions,"Q-learning and other linear dynamic learning algorithms are subject to Bellman?s curse of dimensionality for any realistic learning problem. This paper introduces a framework for satisficing state abstraction ? one that reduces state dimensionality, improving convergence and reducing computational and memory resources ? by eliminating useless state dimensions. Statistical parameters that are dependent on the state and Q-values identify the relevance of a given state space to a task space and allow state elements that contribute least to task learning to be discarded. Empirical results of applying state abstraction to a canonical single-agent path planning task and to a more difficult multi-agent foraging problem demonstrate utility of the proposed methods in improving learning convergence and performance in resource-constrained learning problems."
464,Near-optimal Batch Mode Active Learning and Stochastic Optimization,"Active learning can lead to dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing,  surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance.In this paper, we consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some case, surprisingly, the use of batches only increases the cost by a constant factor independent of the batch size, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on active learning tasks, as well as adaptive influence maximization in social networks."
465,Learning Grouped Parameters in Undirected Graphical Models,"In large-scale applications of undirected graphical models, similar kinds of relations can occur frequently and give rise to repeated occurrences of similar parameters. Therefore, it is often beneficial to group the parameters for more effective learning. In cases when the grouping is unknown, allowing the parameter learner to automatically identify these groups can lead to a more accurate estimate and a better understanding of dependence among the variables. In this paper, we provide such a method to learn groups during parameter learning. Specifically, we place a Dirichlet process prior on the parameters in the graphical models, which can avoid the model selection problem, and is useful when we do not know the number of groups in advance. We solve the posterior inference problem with a Gibbs sampling algorithm integrated with classical parameter learning methods for undirected graphical models. Our parameter learning method does not only estimate parameters for undirected graphical models, but can also identify the possible latent groups among the parameters. We evaluate our grouping-aware parameter learning method and the classical grouping-blind parameter learning method on different undirected graphical models, and our method significantly outperforms the grouping-blind method when there indeed exist groups. When there are no groups among the parameters in the ground truth, our method does not lose much estimation accuracy. "
467,Extending Multi-Atlas Label Fusion to Groupwise Segmentation,"Groupwise segmentation that simultaneously segments a set of images and ensures the segmentations for all images are consistent with each other usually work better than segmenting each image independently. However, existing groupwise segmentation techniques are all developed based on simple and less powerful segmentation techniques, which limits the performance of groupwise segmentation when compared with other leading non-groupwise segmentation techniques. To address this problem, we develop a novel statistical model to extend the multi-atlas label fusion technique, which has shown to be very competitive for challenging biomedical image segmentation problems, for groupwise segmentation. Experiments on hippocampus segmentation in magnetic resonance images show the effectiveness of the new technique."
468,Learning mid-level representations by learning to relate viewpoints,"The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks such as stereopsis and motion analysis.  We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to observed transformations.  We show how this allows us to learn pose-invariant features of objects by observing how the objects change.  We also describe a dataset of objects undergoing 3-D transformations, that we use to evaluate the model."
469,Multi-Class Classification with Maximum Margin Multiple Kernel,"We present a new algorithm for multi-class classification withmultiple kernels. Our algorithm is based on a natural notion of themulti-class margin of a kernel. We show that larger values ofthis quantity guarantee the existence of an accurate multi-classpredictor and also define a family of multiple kernel algorithms based onthe maximization of the multi-class margin of a kernel.  Wepresent an extensive theoretical analysis in support of our algorithm,including novel multi-class Rademacher complexity margin bounds.Finally, we also report the results of a series of experiments withseveral data sets, including comparisons where we improve upon theperformance of state-of-the-art algorithms both in binary andmulti-class classification with multiple kernels."
470,Perfect Dimensionality Recovery by Variational Bayesian PCA,"The variational Bayesian (VB) approach isone of the best tractable approximations to the Bayesian estimation,and it was demonstrated to perform well in many applications.However, its good performance was not fully understood theoretically.For example, VB sometimes produces a sparse solution,which is regarded as a practical advantage of VB,but such sparsity is hardly observed in the rigorous Bayesian estimation.In this paper, we focus on probabilistic PCA andgive more theoretical insight into the empirical success of VB.More specifically, for the situation where the noise variance is unknown,we derive a sufficient condition for perfect recovery of the true PCAdimensionalityin the large-scale limitwhen the size of an observed matrixgoes to infinity with its column-row ratio fixed.In our analysis, we obtain bounds for a noise variance estimatorand simple closed-form solutions for other parameters,which themselves are actually very useful for better implementation of VB-PCA."
471,Mirror Descent Meets Fixed Share (and feels no regret),"Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters."
472,Downsampling a neighborhood graph,"Given a large neighborhood graph $G$, we would like to downsample it to some smaller graph $G'$ with much fewer vertices. This downsampling procedure should ``keep the geometry in the graph invariant''. To this end we define the notion of geometry-preserving downsampling. We then introduce a simple downsampling procedure that is based on a uniform subsample of vertices that is connected based on shortest path distances in the original graph. We prove that this procedure is geometry-preserving if it is applied to random geometric graphs like $k$-nearest neighbor graphs or $\eps$-graphs. We also show that some other popular downsampling algorithms are not geometry-preserving. "
473,Mining Frequent Sub-hypergraphs in an Uncertain Hypergraph for Knowledge Transfer,"The knowledge transfer learning can generalize across domains where the types of objects and variables are different. Previous studies ignored the connectivity and creativity of domain knowledge. Thus, these studies just transfer knowledge from a source domain to a target domain, and they do not fully utilize the knowledge from other domains. To address this problem, we proposed a method, called Multi-domain second order knowledge integration (MSKI), for integrating, hybridizing and creating new knowledge, which is formalized into an uncertain hypergraph. Then, we proposed a method to mining frequent sub-hypergraph from the uncertain hypergraph (MFS-UHG). Actually, the frequent sub-hypergraphs are pivot knowledge, which have to be transferred with high priority. We embed the pivot knowledge in the progress of MLN structure learning. The experimental evaluation on four domain datasets shows that our method outperforms state-of-the-art MLN-based transfer learning."
474,Indexed Optimization: Learning Ramp-Loss SVM in Sublinear Time,"Multidimensional indexing has been frequently used for sublinear-time nearest neighbor search in various applications. In this paper, we demonstrate how this technique can be integrated into learning problem with sublinear sparsity like ramp-loss SVM. We propose an outlier-free convex-relaxation for ramp-loss SVM and an indexed optimization algorithm which solves large-scale problem in sublinear-time even when data cannot fit into memory. We compare our algorithm with state-of-the-art linear hinge-loss solver and ramp-loss solver in both sufficient and limited memory conditions, where our algorithm not only learns several times faster but achieves more accurate result on noisy and large-scale datasets."
475,Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
476,Sparse Additive Matrix Factorization for Robust PCA,"Principal component analysis (PCA) can be regarded as approximating adata matrix witha low-rank one by imposing sparsity on its singular values,and its robust variant further captures sparse noise.In this paper, we extend such sparse matrix learning methods,and propose a novel framework called sparse additive matrix factorization(SAMF).SAMF systematically inducesvarious types of sparsityby the so-called model-induced regularization in the Bayesian framework.We  propose an iterative algorithm calledthe mean update (MU) for the variational Bayesian approximation to SAMF, which gives the global optimal solution for a large subset of parameters in each step.We demonstrate the usefulness of our  methodon artificial dataand the foreground/background video separation."
477,Novelty Detection with Extreme Function Theory,"We introduce extreme function theory as a novel method by which probabilistic novelty detection may be performed over functions, where the functions are represented by time-series of (potentially multivariate) discrete observations. We set the method within the framework of Gaussian processes (GP), which offers a convenient means of constructing a distribution over functions. Whereas conventional novelty detection methods aim to identify individually extreme data points, w.r.t. a model of normality constructed using examples of ?normal? data points, the proposed method aims to identify extreme functions, w.r.t. a model of normality constructed using examples of ?normal? functions, where those functions are represented by time-series of observations."
478,Design of Nonlinear Phase Oscillators with Custom Limit Cycle Shape and Convergence Behavior,In this contribution we present a general way to design low-dimensional nonlinear phase oscillators with arbitrary limit cycle shape and global asymptotic stability. We show examples for which the solution of the nonlinear oscillator with an arbitrary limit cycle shape can be obtained in closed form. The elegance of the oscillator allows for easy extension of the basic system to incorporate other interesting properties such as custom convergence behavior. Numerical simulation is used to show the properties of the proposed oscillator. We also demonstrate two example applications of the introduced oscillator: 1) We show how to couple a number of these oscillators to create a multidimensional Central Pattern Generator. We use this Central Pattern Generator to encode human kinematics as a dynamical system.  2) We present how an adaptive landscape shaping rule can be written for this oscillator to reduce the tracking error when controlling periodic movements of a mechanical system with a simple and suboptimal P-controller.
479,Convex Shape Priors for Isometry-Invariant Variational Image Segmentation,"Convex relaxations of variational approaches to image segmentation constitute an active field of research. Convex state-of-the-art functionals penalize segmentation boundaries in terms of length or curvature, whereas variational approaches that take into account more specific shape knowledge suffer from non-convexity, thus requiring careful initializations to converge. Moreover, invariant shape comparison is only implicitly achieved by additionally optimizing over transformation parameters, aggravating issues of non-convexity.This paper combines for the first time convex state-of-the-art variational segmentation in terms of continuous cuts with a variational shape prior, based on shape representation by metric structures that enables fully invariant shape comparison and matching, while preserving convexity of the overall variational approach. Thus standard convex programming techniques can be applied to variational segmentation enhanced by invariant shape priors."
480,Boundary Preserving Distance Metric for Superpixelization ,"Superpixel is a continuous pixel region sharing similar characteristics, such as color, texture, etc. In recent years, it has been widely accepted as an alternative to physically-defined grid pixels to aid and enhance further image analysis in various applications. In this paper, a generalized superpixelization algorithm with boundary preserving distance metric is proposed. It outperforms state-of-the-art superpixelization algorithms in three aspects: 1. It can be generalized to both color and highly-textured images; 2. Generated superpixels are more compact with less number of superpixels; 3. Less computational complexity and memoryconsumption."
482,Near-optimal Differentially Private Principal Components,"Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension.  Many current data sets of interest contain private or sensitive information about individuals.  Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs.  Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs.  In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output.  We demonstrate that on real data, there this a large performance gap between the existing methods and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling."
483,Graph Estimation From Multi-attribute Data,"Many real world network problems often concern multivariate nodal  attributes such as image, textual, and multi-view feature vectors on  nodes, rather than simple univariate nodal attributes. The existing  graph estimation methods built on Gaussian graphical models and  covariance selection algorithms can not handle such data, neither can  the theories developed around such methods be directly  applied. In this paper, we propose a new principled framework for  estimating multi-attribute networks. Instead of estimating the  partial correlation as in current literature, our method estimates  the {\it partial canonical correlations} that naturally accommodate  complex nodal features.  Computationally, we provide an efficient  algorithm which utilizes the multi-attribue  structure. Theoretically, we provide sufficient conditions which  guarantee consistent graph recovery. Empirically, we apply our  method on a genomic dataset to illustrate its usefulness. "
485,Inverse Reinforcement Learning: An Alternative Definition,"This paper discusses the foundations of the definition of the Inverse Reinforcement Learning (IRL) problem. Given a finite-Markov Decision Process (MDP) without reward function and an expert policy, IRL is usually stated as the problem of finding a reward function for which the expert policy is optimal. This problem is clearly ill-posed as the zero-reward function is always solution. Thus, in order to give a more mature definition of the IRL problem, we introduce the notions of set-policy and optimality for a set-policy. These notions allow us to create a particular partition of the space of reward functions for which each part corresponds to a unique set-policy and inversely. Moreover, each set-policy is optimal for each reward of its corresponding part of the partition and only them. Thanks to this partition, we give a new and well-posed definition of IRL. Based on this framework, we introduce a new algorithm."
486,Recovering Block-structured Activations Using Compressive Measurements ,"We consider the problem of detection and localization of a small block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. However, contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that in our problem the magnitude of the weakest signals one can reliably localize is strongly influenced by both structure and the ability to choose measurements adaptively. We derive tight upper and lower bounds for the detection and estimation problems, under both adaptive and non-adaptive measurement schemes. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation."
487,Random function priors for exchangeable graphs and arrays,"A fundamental problem in the analysis of relational data---graphs, matrices or higher-dimensional arrays---is to extract a summary of the common structure underlying relations between individual entities. A successful approach is latent variable modeling, which summarizes this structure as an embedding into a suitable latent space. Results in probability theory, due to Aldous, Hoover and Kallenberg, show that relational data satisfying an exchangeability property can be represented in terms of a random measurable function. In a Bayesian model, this function constitutes the natural model parameter, and we discuss how available latent variable models can be classified according to how they implicitly approximate this parameter. We obtain a flexible yet simple model for relational data by representing the  parameter function as a Gaussian process. Efficient inference draws on the large available arsenal of Gaussian process algorithms; sparse approximations prove particularly useful. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases."
488,Multi-Source Sensing with Group Reliablity Ranking,"Prediction and decision-making tasks are often negatively impacted by the low-quality of the data collected from multiple sensing sources. In order to address these issues, we study the joint ranking of the reliability of data sources and infer their true values from their observations. Data sources are very often highly interdependent. Clearly, this interdependence can play an important role in the reliability estimation process as well. This is achieved by latent grouping of the dependent sources, and analyzing the underlying reliability at this level. Different groups usually contain conflicting observations from one other. In order to resolve such conflicts and improve the data quality, we assume that the observations from each group are not equal, and can be ranked based on their reliability. For this purpose, we impose a prior for the group reliability in a Markov chain structure, and rank the group reliability levels of the observations in a Bayesian inference framework. The highly ranked groups in the chain provide more information about data objects, and thus play a more important role in the true value inference. Finally, we demonstrate the effectiveness of the proposed approach on two real data sets."
489,Learning a reward function from demonstrations: a cascaded supervised learning approach,"Discovering the reward function optimized by an expert in an MDP, usually referred to as the IRL problem, is addressed in this paper. The proposed generic model-free contribution consists in cascading a classification and a regression steps to produce a non-trivial reward function. We prove the expert policy to be near optimal with respect to this reward. The algorithm does not require solving any direct reinforcement learning problem (unlike most of IRL algorithms). With the help of some heuristics, it can be instantiated to solely rely on transitions from the expert (no need to sample trajectories according to other policies). All of this is illustrated through a standard benchmark."
490,A Bayesian Boosting Model,"We offer a novel view of AdaBoost in a statistical setting. We propose a Bayesian model for binary classification in which label noise is modeled hierarchically. Using variational inference to optimize a dynamic evidence lower bound, we derive a new boosting-like algorithm called VIBoost. We show its close connections to AdaBoost and give experimental results from four datasets."
491,Inverse Reinforcement Learning through Structured Classification,"This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called featureexpectation of the expert as the parameterization of the score function of a multiclass classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving a single time the direct RL problem. Moreover, up to the use of some heuristic, it may work with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator."
492,Classification with Asymmetric Label Noise,"We consider the problem of binary classification when the training labels are noisy. Previous theoretical work on this problem assumes that the two classes are separable, or that the label noise is independent of the class label. We present a general framework that allows for overlapping class-conditional distributions (so that the true labels are not deterministic) and that accommodates asymmetric label noise. This problem is motivated by several applications including nuclear particle classification, where background radiation and other environmental factors make it impossible to obtain pure training samples. We first identify a necessary and sufficient condition that makes the unknown noise proportions identifiable. Under this assumption, we describe estimation of the Type I and Type II errors, and use these estimates to design a classification rule that is consistent with respect to the minmax criterion. Our approach depends critically on recent results for mixture proportion estimation, which is the problem of estimating the percentage of one distribution that is present in another."
493,Episodic Risk-Sensitive Actor-Critic,"We present an episodic risk-sensitive actor-critic algorithm that is suitable for stochastic, continuous, and high-dimensional systems with policy-dependent cost variance. We generalize the simple stochastic gradient descent update to the risk-sensitive case, derive the minimum variance baseline, and show that, under certain conditions, it leads to an unbiased estimate of the gradient of the risk-sensitive objective. We show that the local critic structure used in the update can be exploited to interweave offline and online search to select local greedy policies or quickly change risk sensitivity. Our experiments include learning to lift a heavy, liquid-filled bottle with a dynamically balancing mobile manipulator and online learning of stiffnesses for fall bracing after very large impacts. "
494,Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics,"Non-linear dynamical systems (DS) have been used extensively for building generative models of human behavior. Its applications range from modeling brain dynamics  to encoding motor commands. Many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space. Although these enable the robots to react against sudden perturbations without any re-planning, the motions are always directed towards a single target. In this work, we focus on combining several such DS with distinct attractors, resulting in a multi-stable DS. We show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object. While exploiting multiple attractors provides more flexibility in recovering from unseen perturbations, it also increases the complexity of the underlying learning problem. Here we present the Augmented-SVM (A-SVM) model which inherits region partitioning ability of the well known SVM classifier and is augmented with novel constraints derived from the individual DS. The new constraints modify the original SVM dual whose optimal solution then results in a new class of support vectors (SV). These new SV ensure that the resulting multi-stable DS incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction. We show, via implementations on a simulated 10 degrees of freedom mobile robotic platform, that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations. "
495,Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search,"Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sampled-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration."
496,Mining the brain with a theory of visual attention,"We propose a new target objective for BCI systems in which a parametric model of early visual perception follows the EEG decoder stage. This approach enables the supervised extraction of EEG components that jointly predict behavioral responses. We analyze the pre-stimulus EEG activity from a letter-recognition task using two EEG decoders running in stereo, and detect distinct components of the EEG that predict separable attentional parameters on a single-trial level."
497,Dimensionality Dependent PAC-Bayes Margin Bound,"Margin is one of the most important concepts in machine learning. Previous margin bounds, both for SVM and for boosting, are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or infinite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors fixed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of finite dimension; and the two bounds tend to be equivalent as the dimension goes to infinity. In addition, we show that the VC bound for linear classifiers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classifiers."
498,Fast Exact Search in Hamming Space Using Anchors,"Recently there are a surge of approaches to learn a compact binary code representation of the data for large-scale nearest neighbor search. Once the binary codes are obtained, these methods usually employ the linear scan or hash table lookup to search in Hamming space, which are not efficient. There are some efforts trying to speed up the search in Hamming space, however, these techniques require specific data structure which consumes a lot of memory. In this paper, we propose a novel method for sub-linear exact search in Hamming space with only linear memory cost. The key idea of our method is reducing the search range using the triangle inequality induced by a small number of anchors, which are the representative samples of the database. An effective data structure is proposed to efficiently reject samples that are unnecessarily calculated. To further improve the performance, a MaxMin algorithm is presented to select the anchors. Both the theoretical analysis and empirical evaluation demonstrate that our approach is not only more efficient than the state-of-the-art methods but also consumes much less memory, which is essential to the large-scale problems."
499,A Linearly Convergent First-order Algorithm for Total Variation Minimization in Image Processing,We introduce a new formulation for total variation minimization in image denoising. We present a linearly convergent first-order method for solving this reformulated problem and show that it possesses a nearly dimension-independent iteration complexity bound.
500,Learning via Margin Maximization under Max-Mini Entropy,"This paper presents a new learning framework in the context of classification using nearest neighbors. The learning problem is mathematically formulated as a large margin problem under the principle of max-mini entropy. The margin of a sample is locally defined by its nearest neighbors, which allows the decomposition of any given sophisticated nonlinear separation problem into a series of local and simpler ones. Margin maximization directly leads to better generalizability, and the max-mini entropy principle makes the learned classifier more robust. An iterative algorithm was derived to implement this learning framework.  The convergence analysis of the algorithm is straightforward.  We demonstrate the power of our new approach by comparing it to a couple of widely used classifiers (such as Support Vector Machines, decision tree, na?ve Bayes classifier, k-nearest neighbors, and so on) and several other margin-based local learners (such as RELIEF, Simba, and G-flip, and so on) on a number of UCI and gene expression datasets."
501,Local stability and robustness of sparse dictionary learning in the presence of noise,"A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries and noisy signals, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations."
502,Unifying Common Multi-Task Multi-Kernel Learning Problems,"Over the past few years, Multi-Kernel Learning (MKL) has received significant attention among data-driven feature selection techniques. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including Multi-Task Learning (MTL). Soving different Multi-Task Multi-Kernel Learning (Multi-Task MKL) formulations usually involves designing algorithms that are tailored to the problem at hand, which is a non-trivial accomplishment. In this paper we present a general Multi-Task MKL framework that subsumes well-known Multi-Task MKL formulations, as well as several important MKL approaches on single-task problems. We then derive a simple algorithm that can solve the unifying framework. To furthermore underline the generality of the proposed framework, we formulate a new learning problem, namely Partially-Shared Common Space (PSCS) Multi-Task MKL, and demonstrate its merits through experimentation."
503,Error Correction in Learning Using SVMs,"This paper is concerned with learning binary classifiers under adversarial label-noise. We introduce the problem of error correction in learning where the goal is to recover the original clean data from a label-manipulated version of it, given (i)~ no constraints on the adversary other than an upper-bound on the number of errors, and (ii)~some regularity properties for the original data.  We present a simple and practical error-correction algorithm called SubSVMs that essentially learns separate SVMs on several class-balanced, small-size (often log-size), random subsets of the data and then reclassifies the training points using a majority vote. Our analysis reveals the need for the two main ingredients of SubSVMs, namely class-balanced sampling and subsampled bagging. We present experimental results on synthetic as well as benchmark data to demonstrate the effectiveness of our approach. In addition to the primary goal of noise-tolerance, log-size subsampled bagging also yields significant run-time advantages over standard SVMs."
504,Predicting a Neural Spiking Probability Map,"This paper models signals and noise for extracellular neural recording. Although recorded data approximately follow Gaussian distribution, there are slight deviations that are critical for signal detection: a statistical examination of neural data in Hilbert space shows that noise forms an exponential term while signals form a polynomial term. These two terms can be used to estimate a spiking probability map which tells the probability of spike presence in any time window. The predictions of the two terms and the spiking probability map are quantitatively assessed with both animal and synthesized data. To demonstrate the usefulness of the work, a few application examples are presented with quantitative experimental results that have shown improved signal processing reliability. Last, we report an algorithm implementation in a 130nm CMOS process for on-the-fly processing multi-channel neural data."
505,Exact Cheeger-Cuts through Discrete Newton Method,"The Cheeger-cut is one of the most popular formulations for {?it balanced clustering} that has been actively discussed in machine learning. In this paper, we propose an efficient algorithm for finding an exact Cheeger-cut. We formulate the Cheeger-cut as a parametric submodular-minimization problem and apply the discrete Newton method, whose convergence to an optimal solution is guaranteed. We derive the optimality conditions for the sub-problem solved at each iteration and develop an efficient algorithm for this problem, which is calculated as submodular minimization using the maximum-flow method. We also analyze the computational complexity of the proposed algorithm, and consider an extension to multiple clusters although this is no longer solvable as a convergent procedure. The performance of the proposed method is investigated through empirical experiments."
506,Temporal Abstraction in Reinforcement Learning based on Holonic Concept Clustering and Attentional System,"This paper proposes a new method which extracts bottleneck states automatically based on abstraction concepts for reinforcement learning agents in offline/online manner -incremental-. Generally, the existing mechanisms for creating temporally extended actions need to burdensome of calculations and eventually prone to error. Our approach is built on lines of researches from cognitive science and behavior analysis. We utilized attentional mechanisms as an effective tool to extract bottlenecks. Holonic concept clustering and attentional-system are the motivation and the core of the proposed method. The experimentations confirmed that the proposed method is able to identify bottlenecks with more precision and thereupon the speed of learning and the ability of knowledge transferring are improved significantly. Also, the time complexity of the proposed method is considerably less in comparison to other similar methods."
507,Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs,"Graphical model selection refers to theproblem of estimating the unknown graph structure givenobservations at the nodes in the model. We consider achallenging instance of this problem when some of thenodes are latent or hidden.  We  characterize  conditionsfor tractable graph estimation and develop efficient methods with provable guarantees. We consider the class of Ising models Markov on  locally tree-likegraphs, which are in the regime of correlationdecay. We  propose an efficient method for graph estimation, andestablish its structural consistency when the number of samples$n$ scales as $n = \Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where $\theta_{\min}$ is theminimum edge potential, $\delta$ is the depth (i.e.,distance from a hidden node to the nearest  observed nodes),and $\eta$ is a parameter which depends on the minimum andmaximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides  flexibility to control  the number of latent variables and the cycle lengths in the output graph.  We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound  on sample requirements.  "
508,Biased and Unbiased Natural Actor-Critics,"We show that NAC-LSTD and eNAC, two popular discounted-reward natural actor-critics, follow biased estimates of the natural policy gradient. We derive the first unbiased discounted-reward natural actor-critics using batch and iterative approaches to gradient estimation and prove their convergence to globally optimal policies for discrete problems and locally optimal policies for continuous problems. We discuss what the bias does to the system and suggest that in some situations it may be desirable."
509,Bayesian Nonparametric Image  Super-resolution,"Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data.  We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior.  However, this algorithm is not feasible for large-scale data.  To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries  in a fraction of the time needed by the Gibbs sampler. "
510,Learning Robust Low-Rank Representations,"In this paper we present a comprehensive framework for learning robustlow-rank representations by combining and extending recent ideas forlearning fast sparse coding regressors with structured non-convexoptimization techniques. This approach connects robust principalcomponent analysis (RPCA) with dictionary learning techniques andallows its approximation via trainable encoders. We propose anefficient feed-forward architecture derived from an optimizationalgorithm designed to exactly solve robust low dimensionalprojections. This architecture, in combination with different trainingobjective functions, allows the regressors to be used as onlineapproximants of the exact offline RPCA problem or as RPCA-based neuralnetworks. Simple modifications of these encoders can handlechallenging extensions, such as the inclusion of geometric data transformations.We present several examples with real data from image, audio, and videoprocessing. When used to approximate RPCA, our basic implementationshows several orders of magnitude speedup compared to the exactsolvers with almost no performance degradation. We show the strengthof the inclusion of learning to the RPCA approach on a music source separationapplication, where the encoders outperform the exact RPCA algorithms,which are already reported to produce state-of-the-art results on a benchmarkdatabase. Our preliminary implementation on an iPad showsfaster-than-real-time performancewith minimal latency."
511,A Max-K-Min Approach for Classification,"Over the past decades, Maximin/Minimax Classifiers have been proven to be of excellent performance in numerous applications. Nevertheless, the Maximin/ Minimax approach, which only considers the most boundary instance of each class, may be sensitive to the outliers/noisy points near the boundary. In this paper, a novel robust Max-K-Min approach for classification is proposed to make the K worst cases best. The original Max-K-Min formulation to maximize K-Min Margin, which concerns the classification confidence of the $K$ worst cases among total $N$ training instances, however, is an optimization problem with $C_N^K$ inequality constrains and is intractable when $N$ and $K$ are relatively large. In order to make the optimization of Max-K-Min approach tractable, a reformulation is adopted, which changes the original Max-K-Min formulation into a compact optimization problem with $2N$ inequality constrains. To verify the performance of Max-K-Min approach, by defining two kinds of K-Min Margin functions, a naive linear Max-K-Min classifier and a kernel Max-K-Min classifier are built respectively for 2-class classification. The extensive experiments on $10$ datasets show that the performance of Max-K-Min classifiers is competitive with the prestigious classifiers of Support Vector Machine (SVM) and Logistic Regression (LR)."
512,Learning Mixtures of Tree Graphical Models,"We consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with  provable guarantees.   Our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The   sample and computational requirements for our method scale as $\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs."
513,Break and Conquer: Efficient Correlation Clustering for Image Segmentation,"We present a probabilistic model for image segmentation and an efficient approach to find the best segmentation. The image is first grouped into superpixels and a local information is extracted for each pair of spatially adjacent superpixels. The global optimization problem is then cast as correlation clustering which is knownto be NP hard. This study demonstrates that in many cases, finding the exact global solution is still feasible by exploiting the characteristics of the image segmentationproblem that make it possible to break the problem into subproblems. Each sub-problem corresponds to an automatically detected image part. The reducedcomputational complexity of the proposed optimization algorithm and the improved image segmentation performance are demonstrated on manually annotatedimages."
514,Hamming Distance Metric Learning,"Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs.  We develop a new loss-augmented inference algorithm that is quadratic in the code length.  We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes."
515,Beyond Distance Metric: Learning Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community. For example, our method achieves 89.3\% accuracy on the ``Labeled Face in the Wild'' (LFW) dataset, which outperforms the best reported results under the same setting in recent years."
516,Symmetry Detection by Distributed Synchrony of Spiking VLSI Neurons,"The detection of geometrical symmetries in visual scenes plays a key role in both animal perception and machine vision. Such types of sophisticated pattern detection can be obtained via spike-to-spike synchrony in recurrent networks of Integrate & Fire (I&F) neurons. To determine the network properties and the conditions required for synchronization we apply a formal contraction theory analysis using weakly coupled oscillator models and show how, under these conditions, the stability of the synchronous state can be guaranteed. These conditions can be experimentally verified through the measurement of the Phase Response Curve (PRC). To demonstrate the reliability of the method and its robustness to noise and parameter variability we used it to implement a bilateral symmetry detection network in analog/digital neuromorphic hardware and applied the system to real-time symmetry detection in response to real-world sensory data, provided by an event-based silicon retina. The silicon neurons quickly synchronize when a symmetric object is aligned with respect to the scene vertical axis. The presence of a symmetric input stimulus is reported by a separate read-out network of I&F neurons used as coincidence detectors. Our results demonstrate how this theory can be used to successfully configure low-power neuromorphic system for robust real-time pattern detection, making a central use of precise spike timing to detect user-specified symmetries present in visual scenes."
517,Towards Practical Planning to Predict Intentions for Interacting with Self-Interested Agents,"A key challenge in non-cooperative multi-agent systems is that of developing efficient planning algorithms for intelligent agents to perform effectively among boundedly rational, self-interested agents (e.g., humans). The practicality of existing works addressing this challenge is being undermined due to either the restrictive assumptions on other agents' behaviors, the failure in accounting for their rationality, or the prohibitively expensive cost of modeling and predicting their intentions. To boost the practicality of research in this field, we investigate how intention prediction can be efficiently exploited and made practical in planning, thereby leading to efficient intention-aware planning frameworks capable of predicting the intentions of other agents and acting optimally with respect to their predicted intentions. We show that the performance loss incurred by the resulting planning policy is linearly bounded by the error of intention prediction. Empirical evaluations through a series of stochastic games demonstrate that our approach achieves better and more robust performance than the state-of-the-art algorithms."
518,Sampling based approximation schemes for Normalized Cuts,"The Normalized Cuts (NCut) objective seeks to partition a graph into roughly balanced clusters, and forms the cornerstone of a wide variety of applications in computer vision and machine learning. Finding the optimal normalized cut is NP-hard, and only few results are known in terms of approximation guarantees for various solution strategies. Relaxations like spectral clustering perform quite well in practice but are difficult to analyze in terms of approximation ratios. This paper provides a sampling based approximation scheme for the Normalized Cuts problem.  We derive a polynomial-time algorithm which yields an approximation ratio of $(1+\epsilon)$ with constant probability."
519,Spiking and saturating dendrites differentially expand single neuron computation capacity.,"The integration of excitatory inputs in dendrites is non-linear: multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input's response taken separately. If this depolarization is bigger than the arithmetic sum, the dendrite is spiking; if the depolarization is smaller, the dendrite is saturating. Decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity, as the neuron then maps onto a two layer neural network, enabling it to compute linearly non-separable Boolean functions (lnBFs). How can these lnBFs be implemented by dendritic architectures in practise? And can saturating dendrites equally expand computational capacity? To adress these questions we use a binary neuron model and Boolean algebra. First, we confirm that spiking dendrites enable a neuron to compute lnBFs using an architecture based on the disjunctive normal form (DNF). Second, we prove that saturating dendrites as well as spiking dendrites also enable a neuron to compute lnBFs using an architecture based on the conjunctive normal form (CNF). Contrary to the DNF-based architecture, a CNF-based architecture leads to a dendritic unit tuning that does not imply the neuron tuning, as has been observed experimentally. Third, we show that one cannot use a DNF-based architecture with saturating dendrites. Consequently, we show that an important family of lnBFs implemented with a CNF-architecture can require an exponential number of saturating dendritic units, whereas the same family implemented with either a DNF-architecture or a CNF-architecture always require a linear number of spiking dendritic unit. This minimization could explain why a neuron spends energetic resources to make its dendrites spike."
520,Probability Distribution Estimation by Aggregated One-Class SVMs,"In this paper, we present a probability distribution estimation algorithm using aggregated One-Class-SVM (AOC-SVM) classifiers.Our work is along the line of generative and discriminative learning, which has been a long standing problem in machine learning.We combine the nice property of one-class-SVM with the idea of ensemble learning and divide-and-conquer.Given a set of training samples, we draw a random subset each time and train a corresponding one-class SVM classifier;the final probability distribution is a normalized ensemble of one-class SVMs;for each randomly drawn subset, one alternative is to obtain a randomized tree with a one-class SVM classifier learned on each leaf node.Compared with the traditional density estimation (generative) algorithms, our method has less restriction in model assumptionsand it is efficient to learn;compared with discriminative approaches, our algorithm approximates the in-class distribution withoutthe ``negative'' examples. We provide justification to our approach and show encouraging experimental results."
522,EM-based Wall Fitting in Visually Extracted Noisy Point Clouds for UAV Exploration,"We introduce a robust and efficient technique for room fitting in noisy 2D point clouds. Our method simplifies the exploration problem for UAVs because it estimates the room geometry from visual SLAM feature locations. We use a single camera SLAM implementation, which generates features that roughly indicate the wall positions. To uncover these latent locations, we created a layered model that incorporates a sampling model and prior knowledge about walls. The Expectation Maximization algorithm for Gaussian Mixtures is extended to solve the problem, given this model. We demonstrate our technique on a popular low-cost quadcopter platform and show that it outperforms various alternative techniques."
523,Nonparametric Bayesian Clustering via Infinite Warped Mixture Models,"We introduce a flexible class of mixture models for clustering and density estimation. Our model allows clustering of non-linearly-separable data, produces a potentially low-dimensional latent representation, automatically infers the number of clusters, and produces a density estimate. Our approach makes use of two tools from Bayesian nonparametrics: a Dirichlet process mixture model to allow an unbounded number of clusters, and a Gaussian process warping function to allow each cluster to have a complex shape. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, and performs much better than infinite Gaussian mixture models at discovering meaningful clusters."
524,Clustering by Nonnegative Matrix Factorization Using Graph Random Walk,"Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity."
525,Multi-Label Classification with Relevance Ordering,"In many real multi-label tasks, it is often the case that the ordering of relevant labels for an example is important, while that of irrelevant labels is meaningless. Such a problem, however, could not be addressed by existing multi-label learning or label ranking approaches, because the former usually ignores the ordering among relevant labels while the latter often fails to explicitly distinguish relevance from irrelevance and involves more consideration on ordering irrelevant labels. Considering that there is no adequate criterion available, in this paper, we propose PRO Loss, a new criterion which concerns the ordering of only relevant labels and classification of all the labels. We then propose ProSVM that optimizes PRO Loss efficiently with the use of alternating direction method of multipliers. We further improve efficiency with an upper approximation that reduces the number of constraints from O(T^2) to O(T), where T is the number of labels. Experiments show that our proposals not only perform superior to state-of-the-art approaches on PRO Loss, but also achieve highly competitive performance on existing evaluation criteria."
526,Delay Compensation with Dynamical Synapses,"Time delay is pervasive in neural information processing. To achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system. In the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner. The state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents. The parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network. We also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude."
527,A Gaussian Latent Variable Model for Ranking,"We describe a Gaussian latent variable model for ranking. The model learns a linear scoring function to maximize the probability that randomly chosen pairs of examples are correctly ranked. We show how to perform inference in this model and derive the Expectation-Maximization (EM) algorithm that monotonically increases the likelihood of correct ranking. We also highlight the intuitive form of the EM algorithm: at each iteration, the weight vector is re-estimated by a simple least-squares update. Finally, we explore two extensions of the model, based on kernels and boosting, to learn nonlinear ranking functions. The model?s effectiveness is demonstrated on problems in AUC maximization and information retrieval."
528,An Anytime Algorithm for Exact Bayesian Network Structure Learning,"Learning a Bayesian network structure that optimizes a scoring function for a given dataset is NP-hard. In recent years, several exact algorithms have been developed for learning optimal Bayesian network structures. Most of these algorithms only find a solution at the end of the search, so they fail to find any solution if stopped early for some reason, e.g., out of time or memory. We present a novel anytime algorithm that also guarantees to find an optimal Bayesian network structure upon completion. We use a sparse representation for storing search information which often reduces the memory requirements by several orders of magnitude. The algorithm improves the runtime to find optimal network structures up to 100 times compared to existing state of the art methods. It is also shown to have excellent anytime behavior and often finds better network structures faster than existing local search techniques and other anytime exact algorithms."
529,Real-time online denoising and speaker identification by learning low-rank non-negative sparse models,"In this paper we present a new framework for real time speech denoising under non-stationary noise. We first propose a regularized version of nonnnegative matrix factorization (NMF) for modeling time-frequency representations of speech signals in which the spectral frames are decomposed as sparse linear combinations of atoms of an undercomplete dictionary. This regularization minimizes an upper-bound of the nuclear norm of the reconstruction. The proposed model outperforms standard NMF in speech denoising experiments and reduces the sensitivity of the obtained results with respect to the size of the dictionary. The main contribution consists of combining this model with recent developments in fast regressors for approximating sparse codes, to produce efficient feed-forward architectures that approximate the output of the exact algorithms with low latency and a fraction of the complexity. Incorporating dictionary update and elements of discriminative learning makes the proposed architecture full-featured low-rank non-negative sparse models, significantly outperforming exact NMF algorithms. We present several experiments in speech denoising and speaker identification in the presence of non-stationary noise that show successful results and the potential of the framework."
530,An Efficient Feature Selection Algorithm,"Many computer vision and medical imaging problems are faced with learning classifiers from large datasets, with millions of observations and features. In this paper we propose a novel algorithm for variable selection and learning on such datasets, coming from the field of penalized likelihood optimization. We pose the learning problem as a constrained penalized likelihood optimization and introduce a suboptimal algorithm that gradually removes variables based on a criterion and a schedule. The approach is generic, allowing the use of any differentiable prior on the coefficients. Experiments on real and synthetic data show that the proposed method outperforms Logitboost and L1 penalized methods for both variable selection and prediction while being computationally faster. "
531,A dynamic excitatory-inhibitory network in a VLSI chip for spiking information reregistration,"Inhibitory synapse is an important component both in physiology and artificial neural network, which has been widely investigated and used. A typical inhibitorysynapse in very large scale integrated (VLSI) circuit is simplified from related research and applied in a VLSI chip for spike train reregistration. The spike trainreregistration network is derived from a neural network model for sensory map realignment for network adaptation. In this paper, we introduce the design of spiketrain registration in CMOS circuit and analyze the performance of the inhibitory network in it, which shows representative characters for the firing rate of inhibitedneuron and information transmission in circuit compared to math model."
532,Cost-Sensitive Trees of Classifiers,"Recently, machine learning algorithms have started to successfully enter large-scale real-world industrial applications. In these settings, test-time CPU usage needs to be budgeted and accounted for. Addressing the trade-off between classifier accuracy and test-time cost in a principled fashion has become a major challenge for machine learning. This test-time cost consists of classifier evaluation time and feature extraction time, with the latter varying dramatically from feature to feature. In this paper, we propose a meta-learning algorithm that learns a tree of classifiers. Test-inputs traverse the tree along different paths and features are only extracted for subsets of inputs where they are beneficial. Experimental results on a real-world data set demonstrates that our algorithm significantly improves over the current state-of-the-art in test-time cost-sensitive learning. "
533,Random Projections for Support Vector Machines,"Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be a data matrix of rank $\rho$, representing $n$ points in $\mathbb{R}^d$. The linear support vector machine constructs a hyperplane separator that maximizes the  1-norm soft margin. We develop a new \emph{oblivious} dimension reduction technique which is precomputed and can be applied to any input matrix \math{\mathbf{X}}. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \math{\epsilon}-\emph{relative error}, ensuring comparable generalization as in the original space. We present extensive experiments in support of the theory."
534,ImageNet Classification with Deep Convolutional Neural Networks,"We trained a large, deep convolutional neural network to classifythe 1.3 million high-resolution images in the LSVRC-2010 ImageNettraining set into the 1000 different classes. On the test data, weachieved top-1 and top-5 error rates of 39.7\% and 18.9\% which isconsiderably better than the previous state-of-the-art results. Theneural network, which has 60 million parameters and 500,000 neurons,consists of five convolutional layers, some of which are followedby max-pooling layers, and two globally connected layers with a final1000-way softmax. To make training faster, we used non-saturatingneurons and a very efficient GPU implementation of convolutional nets.To reduce overfitting in the globally connected layers we employeda new regularization method that proved to be very effective. "
535,Recognizing Activities by Attribute Dynamics ,"The problem of modeling the dynamic structure of the attributes of human activities is considered. Video is first represented in a semantic feature space, where each feature encodes the probability of occurrence of an action attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining a binary observation variable with a hidden Gauss-Markov state process. In this way, it combines the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by the popular dynamic texture method for LDS learning, is proposed. A similarity measure between BDSs, which generalizes the Binet-Cauchy LDS kernel, is then introduced and used to design activity classifiers. These are shown to outperform similar classifiers derived from the kernel-LDS and state-of-the-art approaches to dynamics-based or attribute-based action recognition."
536,Compressive Sensing MRI with Wavelet Tree Sparsity,"In Compressive Sensing Magnetic Resonance Imaging (CS-MRI), one canreconstruct a MR image with good quality from only a small number ofmeasurements. This can significantly reduce MR scanning time.According to structured sparsity theory, the measurements can be further reduced to$\mathcal{O}(K+\log n)$ for tree-sparse data instead of$\mathcal{O}(K+K\log n)$ for standard $K$-sparse data with length $n$.However, few of existing algorithms has utilized thisfor CS-MRI, while most of them use Total Variation andwavelet sparse regularization. On the other side, some algorithmshave been proposed for tree sparsity regularization, but few of them has validated  the benefit of tree structure in CS-MRI. In this paper, we propose a fastconvex optimization algorithm to improve CS-MRI.  Waveletsparsity, gradient sparsity and tree sparsity are all considered in our model for real MR images.The original complex problem is decomposed to three simpler subproblemsthen each of the subproblems can be efficiently solved with an iterative scheme.Numerous experiments have been conducted and show that the proposedalgorithm outperforms the state-of-the-art CS-MRIalgorithms, and gain better reconstructions results on real MR images than generaltree based solvers or algorithms."
537,Graph Estimation with Joint Additive Models,"In recent years, there has been considerable interest in estimating conditional independence graphs in the high-dimensional setting in which the number of features exceeds the number of observations.  Most prior work in this area has assumed that the observations are drawn from a multivariate Gaussian distribution, or that conditional dependence relations among variables are linear, which as we will see are roughly equivalent. Unfortunately, if this assumption is violated, then the resulting conditional independence estimates can be inaccurate. Here we present a semi-parametric method, Sparse Conditional Estimation with Joint Additive Models (SpaCE JAM),  which allows for arbitrary additive conditional relationships among the features.  We present an efficient algorithm for its computation, and prove that our estimator is consistent.  We also  extend our method to estimation of  directed graphs.  SpaCE JAM enjoys superior performance to existing methods when there are non-linear relationships among the features, and is comparable to methods that assume multivariate normality when the features are linearly related."
538,A Study of Image Statistics through Higher-Order Interactions and Higher-Order Statistics,"  Nature image statistics is essential to the understanding of the  biological vision system, the Bayesian prior for image analysis,  texture analysis and synthesis, object detection, and scene  classification. The prior arts have mainly focused on the pairwise  interactions (e.g., correlation and 2-D joint histogram), and the  first and second order statistics (e.g. power spectrum and PCA). In  this paper, we use the multivariate (normalized) disjoint  information as a tool to capture the higher-order interactions  between two or more image subbands or transform coefficients. The  multivariate disjoint information satisfies the properties of  identity, non-negativity, symmetry, degeneracy, independence, upper  bounds, generalized triangle inequality, and simplex inequality. It  captures the higher-order interactions between an arbitrary number  of variables simultaneously, without simplification to the pairwise  interactions. In addition, the higher-order statistics, such as  bispectrum, are computed on the intensity images, wavelet  coefficients, and the disjoint information map. Preliminary results  based on the higher-order interactions and statistics are  demonstrated to differentiate the nature and man-made scenes."
539,Rates for Inductive Learning of Compositional Models,"Compositional Models are widely used in Computer Vision as they exhibit strong expressive power by generating a combinatorial number of configurations with a small number of components. However, the literature is still missing a theoretical understanding of why compositional models are better than flat representations, despite empirical evidence that compositional models need fewer training examples. In this paper we try to give the first theoretical answers in this direction. We focus on AND/OR Graph (AOG) models used in recent literature for representing objects, scenes and events, and bring the following contributions. First, we analyze the capacity of the space of AND/OR graphs, obtaining PAC (Probable and Approximately Correct) bounds for the number of training examples necessary to guarantee with a given certainty that the model learned has a given accuracy. We analyze both supervised and unsupervised learning approaches. Second, we observe that part localization leads to a reduction in the number of training examples required. Finally, we perform experiments for unsupervised learning of  AND/OR Graphs and part templates for objects and compare the theoretical bounds with the practical learning rates."
540,Training sparse natural image models with a fast Gibbs sampler of an extended state space,"We present a new learning strategy based on an efficient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models."
541,A Bayesian Approach for Policy Learning from Trajectory Preference Queries,"We consider the problem of learning control policies via trajectory preference queries to an expert. In particular,the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates thepreferred trajectory. The agent's goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problemwe propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries.Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and thatactive query selection can be substantially more efficient than random selection."
542,A Mixed-Initiative Approach to Solving  Correspondence Problems,"Finding correspondences among objects in different images is a critical problem in computer vision. Correspondence procedures can fail when faced with deformations, occlusions, and differences in lighting and zoom levels across images. We present a methodology for augmenting correspondence matching algorithms with a means for triaging the focus of attention and effort in assisting the automated matching. The mix of human and automated initiatives is guided by a speci?c computation of the expected value of resolving correspondence uncertainties. We introduce this measure and show how it can be used to guide efforts by human taggers. We explore the value of the approach with experiments on benchmark data. "
544,Evaluation of clustering stability using leave-one-out approach,"Currently, there are many clustering algorithms for the case of a known numberof clusters. Typically, clustering is a result of optimisation a quality criterionor iterative process. How to assess the quality of clustering obtained by somemethod? Is the data clustering corresponding to the objective reality or just astopping criterion of the method is made and obtained some partition? In thispaper, a practical approach and the general criterion based on an assessment ofthe stability of clustering are proposed. For the well-known clustering methods,efficient algorithms for computing the stability criterion according to the trainingset are obtained. We give illustrative examples."
545,ML Confidential: Machine Learning on Encrypted Data,"We demonstrate that by using a recently proposed somewhat homomorphic encryption (SHE) scheme it is possible to delegate the execution of a machine learning (ML) algorithm to a compute service while retaining confidentiality of the training and test data. Since the computational complexity of the SHE scheme depends primarily on the number of multiplications to be carried out on the encrypted data, we devise a new class of machine learning algorithms in which the algorithm's predictions viewed as functions of the input data can be expressed as polynomials of bounded degree. We propose confidential ML algorithms for binary classification based on polynomial approximations to least-squares solutions obtained by a small number of gradient descent steps. We present experimental validation of the confidential ML pipeline and discuss the trade-offs regarding computational complexity, prediction accuracy and cryptographic security."
546,Efficient Dimensionality Reduction for  Canonical Correlation Analysis,"We present the first sub-cubic time algorithm for Canonical Correlation Analysis. Given a pair of tall-and-thin matrices, our algorithm first employs a randomized dimensionality reduction transform to reduce the size of the input matrices, and then applies a standard SVD-based method to compute the canonical correlations. We prove that our algorithm computes an accurate approximation to the canonical correlations with high probability, and with asymptotic running times much better than the standard algorithm. We also show that our algorithm beats the standard algorithm in practice by 30-40% even on fairly small matrices."
547,A Mixed-Membership Model for Learning Genomic Subtype Signatures,We address the problem of identification of genomic signatures from mixed tumor samples. Most methods for identifying tumors on the basis of their genomic mutations are all-or-none classifiers whereas real tumors are complex mixtures. We present a method for identifying sparse subtype signatures from mixed samples using a hierarchical Bayesian model. We apply this method to identify signatures for subtypes of glioblastoma from RNA expression data obtained as part of the Cancer Genome Atlas (TCGA) project and find one subtype is associated with genes involved in DCX-mediated invasion and another subtype is associated with high POSTN expression and mesenchymal-like tumors.
548,Composite Discriminant Factor Analysis,"We propose a linear dimensionality reduction method, Composite Discriminant Factor (CDF) analysis, that searches for a discriminative but compact feature subspace that can be used as input to classifiers that suffer from problems such as multi-collinearity or the curse of dimensionality.  The subspace selected by CDF maximizes the performance of the entire classification pipeline, and is chosen from a set of candidate subspaces that are each discriminative by various local measures, such as covariance between input features and output labels or the margin between positive and negative samples.  Our method is based on Partial Least Squares (PLS) analysis, and can be viewed as a generalization of the PLS1 algorithm, designed to increase discrimination in classification tasks.  While our experiments focus on improvements to object detection, a computer vision task that often involves high dimensional features and benefits from fast linear approaches, we also demonstrate our approach on machine learning datasets from the UCI Machine Learning repository."
549,Submodular Function Maximization with Higher-Order Priors for Video Segmentation,"We propose a video segmentation algorithm by maximizing a submodular objective function subject to a matroid constraint. This function is similar to standard energy function in computer vision problems with unary terms, pairwise terms from Potts model, and a higher-order term. We show that the standard Potts model prior, which becomes non-submodular for multi-label problems, still induces a submodular function in a maximization framework. We propose a new higher-order prior to enforce consistency in the appearance histograms both spatially and temporally across the video. The matroid constraint leads to a simple greedy algorithm with a performance bound of $\frac{1}{2}$. We further develop a branch and bound procedure to improve the solution. We evaluate the proposed algorithm on a standard video segmentation dataset and achieve favorable performance."
550,Linear and Nonlinear Predictive Coding using Biologically Plausible Neuronal Circuits,"Predictive coding has previously been proposed as a useful model of redundancy reduction in sensory systems and served as a foundation for a normative theory of sensory processing. However, its discussion has been restricted to an abstract mathematical algorithm without a mechanistic implementation. In this paper, we demonstrate that simplified linear neurons in biologically plausible circuits can implement optimal predictive coding. Further, the addition of a neuron-like rectilinear nonlinearity into a feedback inhibitory neuronal circuit approximates the response of predictive filters optimized for various stimulus statistics, without the need to vary any parameters. This nonlinear network, therefore, provides a mechanism for dynamic gain control, which can operate in sensory neurons on time scales much faster than adaptation requiring synaptic plasticity."
551,GenDeR: A Generic Diversified Ranking Algorithm,"Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling,product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a wide range of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm."
552,A kernel independence test for time series, Testing for independence between random variables when samples are not independent identically distributed is a complex issue. Here we provide a solution to this problem when random variables a sampled from stationary time-series of arbitrary objects. To achieve this goal we define a test based on the statistics of the cross-spectral density operator induced by positive definite kernels. The performance of our test is compared to using i.i.d assumptions and show the interest of this approach for testing dependency in complex dynamic systems.
553,Scalarized Multi-Objective Reinforcement Learning: Novel Design Techniques,"In this paper, we use techniques from multi-objective optimization (MO), such as scalarization functions, to explore MO environments with reinforcement learning. A general framework for multi-objective reinforcement learning (MORL) is proposed. We call this class of algorithms scalarized MORL. A novel instance of scalarized MORL that uses the Chebyshev scalarization function is suggested. We experimentally compare scalarized MORL on three multi-objective simulation environments with two and three objectives. We show that the Chebyshev scalarization algorithm significantly outperforms MORLs using the linear scalarization function. This conclusion is in concordance with state-of-the-art off-line multi-objective optimization, proving our correct use of multi-objective techniques in the reinforcement learning field."
554,Online Learning of Rotations Using Geometric Structures,"This paper provides a solution to online learning for rotations by employing exponentials of sparse antisymmetric matrices. The method performs similarly to Riemannian gradient based methods but is derived using simple matrix algebraic techniques. We first show that a general optimization problem with a rotation constraint can be transformed into an equivalent problem in the space of antisymmetric matrices. An efficient approach is then introduced to iteratively solve the problem using antisymmetric matrices with one or more nonzero columns and an equal number of nonzero rows. Specially, we show that it is sufficient to employ antisymmetric matrices with only one nonzero column and row. Fast implementation is also presented to simplify the computation of sparse antisymmetric matrix exponentials involved in the algorithm. Experimental results obtained by using a variety of loss functions indicate that the algorithm converges quickly and estimates unknown rotations accurately."
555,Flexible Shift-Invariant Locality and Globality Preserving Projections,"In machine learning, the dimension reduction methods have commonly been used as a principled way to understand the high-dimensional data. To solve the out-of-sample problem, local preserving projection (LPP) was proposed and applied to many applications. However, LPP suffers two crucial deficiencies: 1) the LPP loses shift invariance property which is an important property of embedding methods; 2) the rigid linear embedding is used as constraint, which often inhibits the optimal manifold structures finding. To overcome these two important problems, we propose a novel flexible shift invariant locality and globality preserving projection method, which utilizes a newly defined graph Laplacian to make the projection shift invariant. Meanwhile, the relaxed embedding is introduced to allow the finding of more optimal manifold structures. We derive the new optimization algorithm to solve the proposed objective with rigorously proved global convergence. Extensive experiments have been performed on the machine learning benchmark data sets. In all empirical results, our method shows promising results."
556,On Multilabel Classification and Ranking with Partial Feedback,"We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show $O(T^{1/2}\log T)$ regret bounds, which improve in several ways on the existing results.We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance."
557,FAsT-Match: Fast Affine Template Matching,"FAsT-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. The smoother the image, the sparser the set of affine transformations we evaluate. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth these lead to a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We report quantitative results on a large data set of 9,500 images. To the best of our knowledge, this is the first template matching algorithm, which can handle arbitrary 2D affine transformations."
558,Adaptive Universal Linear Filtering,"We consider the problem of online estimation of an arbitrary real-valued signal corrupted by zero-mean noise using  linear estimators. The estimator is required to iteratively predict the underlying signal  based on the current and several last noisy observations, and its performance is measured by the mean-square-error. We design and analyse an algorithm for this task which achieves logarithmic adaptive regret against the best linear filter in hindsight. This bound is asymptotically tight, and resolves the question of \cite{MWPaper}. Furthermore, the algorithm runs in linear time in terms of the number of filter coefficients. Previous constructions required at least quadratic time. "
560,Dictionary Training with Side Information,"Recently, learning with side information, which incorporates information contained in the training data but not available in the testing phase to guide the learning process, attracts great attention in machine learning field. In this work, we propose a discriminative dictionary learning method that involves side information. In particular, we introduce a new soft constraint derived from side information and combine it with the reconstruction error and the classification error to form a unified objective function. The optimal solution to the objective function is efficiently obtained using the K-SVD algorithm. Our algorithm learns the dictionary and an optimal linear classifier jointly. We apply the proposed method to two pattern recognition problems, namely low resolution expression recognition and face recognition, where it demonstrates the effectiveness of the proposed method in classification performance."
828,Discriminative Restricted Boltzmann Machines for Regression,"The paper explores a variant of a discriminative Restricted Boltzmann Machine (RBM) model that performs regression, by defining an RBM with Gaussian output variables and conditioning on the data. The resulting model is intractable in general, but we derive a practical mean-field approximation to the predictive distribution of the model. We investigate two learning algorithms for discriminative RBM regressors, viz. one based on contrastive mean-field learning and one based on contrastive divergences. Experimental evaluation of the new discriminative RBM for regression illustrates the potential of the discriminative RBM regressor, in particular, for learning problems in which the data is high-dimensional."
829,Correlations strike back (again): the case of associative memory retrieval,"It has long been recognised that statistical dependencies in neuronalactivity need to be taken into account when decoding stimuli encoded ina neural population. Less studied, though equally pernicious, is theneed to take account of dependencies between synaptic weights whendecoding stimuli previously encoded in an auto-associative memory. Weshow that activity-dependent learning generically produces suchcorrelations, and failing to take them into account in the dynamics ofmemory retrieval leads to catastrophically poor recall. We deriveoptimal network dynamics for recall in the face of synaptic correlationscaused by a range of synaptic plasticity rules. These dynamics involvewell-studied circuit motifs, such as forms of feedback inhibition andexperimentally observed dendritic nonlinearities. We therefore show howassuaging an old enemy leads to a novel functional account of keybiophysical features of the neural substrate."
830,A spectral learning framework for graphical models,"This work draws on the previous works on spectral algorithm(\cite{Hsu:COLT09-long},\cite{DBLP:conf/alt/BaillyHD10},\cite{DBLP:conf/icml/SongSGS10},\cite{DBLP:conf/pkdd/BalleQC11}). We propose an extension of the\emph{Hidden Markov Models}, called \emph{Graphical Weighted Models  (GWM)}, whose purpose is to model distributions over labeledgraphs. We expose the spectral algorithm for GWM, which generalizesthe previous ones for sequences and trees. We show that this algorithmis \emph{consistant}, and we provide bounds for the statisticalconvergence for the parameters estimate and for the learned distribution."
831,High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning,"Joint sparsity regularization in multi-task learning has attracted much attentionin recent years. The traditional convex formulation employs the group Lasso relaxationto achieve joint sparsity across tasks. Although this approach leads to asimple convex formulation, we argue in this paper that the quadratic regularizerinduced by the group Lasso formulation is suboptimal. To remedy this problem,we view jointly sparse multi-task learning as a specialized random effects model,and derive a convex relaxation approach that involves two steps. The first steplearns the covariance matrix of the coefficients using a convex formulation whichwe refer to as sparse covariance coding; the second step solves a ridge regressionproblem with a sparse quadratic regularizer based on the covariance matrix obtainedin the first step. It is shown that this approach produces an asymptoticallyoptimal quadratic regularizer in the multitask learning setting if the number oftasks approaches infinity. Experimental results demonstrate that the convex formulationobtained via the proposed model significantly outperforms group Lasso."
833,Extending Fano's Inequality: Bounds on Balanced Error Rate and F-Score Using Conditional Entropy,"Fano's inequality lower bounds the probability of transmission errorthrough a noisy communication channel. Applied to classification problems, it provides a lower bound on the Bayes error and motivates the use of information theory in machine learning. In practice, we are often interested in more than just error rate;two popular measures are balanced error rate (BER) and F-score(also known as $F_1$-measure). In this work, we prove that the ``Bayes value'' of BER and F-score can be obtained by tuning a threshold on the posterior probability of the positive class. It is well known that a threshold of 0.5, i.e., where the posteriors cross, will minimize the error rate---the similar optimalthresholds are derived for BER and F-score. We then use a general defintion of conditional entropy that includes Shannon as a special case to derive lower/upper bounds on the Bayes BER and F-score, extending Fano's result. Our results show that the lower bound on the Bayes F-score is monotonically increasing with conditional entropy, meaning that when the entropy (amount of uncertainty) about class label becomes smaller, the performance of classifiers as measured by F-score could possibly degenerate. We discuss the underlying reason for this anti-intuitive phenomenon via a toy example and argue that the ratio of conditional entropy and the probability of the positive class might be a more accurate indicator for F-score."
834,The Mondrian hidden Markov model,"This paper discusses a novel extension of hidden Markov models that can capture clusters embedded in transitions between the hidden states. In our model, the state-transition matrices are viewed as representation of relational data reflecting a network structure between the hidden states. We specifically present a nonparametric Bayesian approach to the proposed state-space model whose network structure is represented by a Mondrian Process-based relational model. We describe an inference scheme, and also show an application of the proposed model to music signal analysis."
835,Causal discovery with scale-mixture model for spatiotemporal variance dependencies,"In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be representedas a function of the direct causes themselves. However, in many real world problems, there are significant dependencies in the variances or energies, which indicatesthat causality may possibly take place on the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporalvariance dependencies to represent a specific type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneousand temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR).We prove the identifiability of this model under the non-Gaussian assumption on the innovation processes. Wealso propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthesis and real world data areconducted to show the applicability of the proposed model and algorithms."
836,Fast Gaussian Process Uncertainty Approximation for Novelty Detection,"Gaussian processes offer the advantage of calculating the classification uncertainty in terms of predictive variance associated with the classification result.This is especially useful to spot samples of previously unseen classes known as novelty detection.However, the Gaussian process framework suffers from high computational complexity.Hence, we propose an approximation of the Gaussian process predictive variance leading to rigorous speedups. While the accuracy in novelty detection tasks remains the same or even increases, the complexity of both learning and testing the classification model as wellas the memory demand decreases by one magnitude with respect to the number of training samples involved.We also verify that our approximation is an upper bound for the exact variance and present bounds for the approximation."
837,Early Active Learning via Robust Representation and Structured Sparsity,"Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. The robust sparse representation loss function is utilized to reduce the effect of outliers and the structural sparsity regularization is adopted to find the most representative samples during the sparse representations. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost. We rigorously prove the global convergence of our solutions. Empirical results on multiple benchmark data sets show the promising results of our method."
838,Robust Rank-$k$ Matrix Completion,"Many applications can be formulated as reconstructing a matrix $M$ from noisy observations of a small, random subset of its entries. Much recent work has focused on the assumption that the data matrix has low rank and used its nuclear norm to approximate the rank. However, recent research casts doubts about this category of approach since such yielded solution could be indeed not low rank and unstable for practical applications. In this paper, we explicitly seek a matrix of \emph{exact} rank. Moreover, our method is robust to outlying or corrupted observations. We optimize the objective function in an alternative and asymptotic convergent manner, based on a combination of ancillary variables and augmented Lagrangian methods (ALM). We perform extensive experiments on three real world data sets and all empirical results demonstrate the effectiveness of our method."
839,Bayesian Games for Adversarial Regression Problems,"We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary's objective; instead, any knowledge of the learner about parameters of the adversary's goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression."
840,Online Optimization in Markov Decision Processes with Dynamic Uncertainty,"Markov Decision Processes (MDPs) are used to model many important and practical problems involving optimization under uncertainty. In online decision making this uncertainty may lead to an algorithm making irreversible sub-optimal decisions. Fortunately, in many natural scenarios some short-term `lookahead' is available into future rewards. Thus, the information available to the online algorithm changes from time step to time step as more information is revealed, leading to what we call \textit{dynamic} uncertainty. To study this class of problems we define a framework of adaptive optimization with dynamic uncertainty and provide online algorithms, based on discounting of future rewards, to handle time varying uncertainty in future reward structures and values. We derive theoretical bounds and establish that discounting is, in some sense, a method to effectively de-randomize against possible futures. We analyze the performance of our algorithms in terms of both regret and competitive ratio, under varying dynamics of uncertainty. In particular, we define a notion of \textit{effective uncertainty} that captures the accuracy of information, when it becomes available and how usable it is, and show how this plays a crucial role in our analyses."
842,Probabilistically Sampled Forests,"Random subspaces are the key idea in random forests.  While the optimal size s of random subspaces can significantly improve classification accuracy, a suboptimal choice can be detrimental, especially in high-dimensional feature spaces, which typically contain only a few discriminative features among a majority of (nearly) uninformative ones.  As to overcome this problem, we propose an alternative to the subspace method: learning a forest by probabilistically sampling trees from the Boltzmann distribution at temperature T.  This approach includes sampling from the posterior distribution as a special case for T=1.  An increased temperature T>1 serves as a conceptually well-established measure for the additional randomness introduced into the learning process.  Moreover, this approach also suggests a novel relationship between the bootstrap and the random subspace method.  Apart from this, in our experiments on UCI data sets, we also found improvements on the practical side: classification accuracy does not tend to suffer as much from a suboptimal T compared to a suboptimal s in high-dimensional feature spaces."
843,"A Mechanism of Generating Joint plans for Self-interested Agents, and by the Agents","Generating joint plans for multiple self-interested agents is one of the most challenging problems in AI, since complications arise when each agent brings into a multi-agent system its personal abilities and utilities. Some fully centralized approaches (which require agents to fully reveal their private information) have been proposed for the plan synthesis problem in the literature. However, in the real world, private information exists widely, andit is unacceptable for a self--interested agent to reveal its private information. In this paper, we define a class ofmulti--agent planning problems, in which self--interested agents' values are private information, and the agents are ready to cooperate with each other in order to cost efficiently achieve their individual goals. We further propose a semi--distributed mechanism to deal with this kind of problems. In this mechanism, the involved agents will bargain with each other to reach an agreement, and do not need to reveal their private information. We show that this agreement is a possible joint plan which is Pareto optimal and entails minimal concessions."
844,"Natural Images, Gaussian Mixtures and Dead Leaves","Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components --- including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models."
845,Shape Priors for Weakly Labeled Segmentation,"In this paper we tackle the problem of weakly labeled image segmentation.  Towards this goal, we propose a novel generative model of segmentation based on transformed hierarchical Pitman-Yor processes, where we augment each object class with a shape prior.  Our model exploits weakly label data, as it does not require a training set composed of  pixel-wise annotations. Instead, it learns appearance models for each object using  as labels only bounding boxes around the object of interest as well as a  shape prior. We demonstrate the effectiveness of our approach on the PASCAL 2010 dataset and show that we outperformed a set of baselines, improving $8\%$ absolute error over the unsupervised version of our model as well as $9\%$ over the detector, which is theinput to our approach. Importantly, our approach performs similarly to fully-supervised approaches.  "
846,Sparse coding for multitask and transfer learning,"We present an extension of sparse coding to the problems of multitask and transfer learning. The central assumption of the method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Preliminary experiments indicate the advantage of the sparse multitask coding method over single task learning and a previous method based on orthogonal and dense representation of the tasks."
847,Grounded Language Acquisition: A Cognitive Approach,"We detail a cognitively inspired primitive model for embodied language acquisition for an artificial agent with limited perceptual multi-modal input capabilities. We formalize the meaning space through unsupervisedly learned perceptual feature-classes, and demonstrate that noun/verb/relational linguistic elements can be acquired through similar correlation with the respective meaning spaces. Finally, through unsupervised syntactical knowledge discovery, we further refine the assigned labels and acquire argument structures to syntactical structure mapping for verbs/relations in a cognitive grammar framework.  "
848,Dual-Space Analysis of the Sparse Linear Model,"Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients.  These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters.  Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation.  The underlying cost functions can be related via a dual-space framework from Wipf et al. (2011), which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space.  This perspective is useful because some analyses or extensions are more conducive to development in one space or the other.  Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit.  As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I.  In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II.  For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular L1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations."
849,Fast Forward Feature Selection for Support Vector Machines,"When creating a pattern recognition system, the set of used features has typically a strong influence on the performance of the selected classifier. Automatic feature selection algorithms aim to select the best features of a given dataset. Whereas filter approaches do not take the target algorithm into account, wrapper approaches evaluate the target algorithm for each considered feature subset. Therefore, filter approaches are typically faster but wrapper approaches may deliver a higher performance.In this paper, we present an approach for reducing the run-time of the common wrapper approach forward selection for Support Vector Machines (SVM). The number of required evaluations of the SVM classifier is reduced by using experience knowledge gained during past feature selection runs.We evaluated the approach on 22 real world datasets and compared the results with state-of-the-art wrapper and filter approaches as well as one embedded method according to performance and run-time. The results show that the presented approach reaches the accuracy of traditional wrapper approaches requiring significantly less evaluations of the target algorithm. Moreover, the presented approach achieves statistically significant better results than the filter approaches."
850,Partition Tree Weighting,"This paper introduces Partition Tree Weighting, a  low-complexity probabilistic sequence prediction algorithm for piecewise stationary sources. It works by performing Bayesian model averaging over a large class of possible partitions of the data into stationary segments. Our prior is designed to be both efficiently computable and well suited towards data compression applications. We provide a competitive analysis of the redundancy (i.e. cumulative log-loss regret) of ourmethod, and show empirically that PTW consistently performs well relative to competing methods. We conclude by showing how to use PTW to improve the performance of a recently introduced universal data compression algorithm."
851,Active Comparison of Prediction Models,"We address the problem of comparing the risks of two given predictive models - for instance, a baseline model and a challenger - as confidently as possible on a fixed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values."
852,Low-Rank Affine Subspace Clustering,"We consider the problem of clustering high-dimensional data lying approximately in a union of affine subspaces. State-of-the-art methods solve this problem in two stages: learning an affinity matrix and applying spectral clustering. However, each stage is solved using a different optimization criteria. In this paper, we propose a unified approach in which we solve jointly for an affinity matrix and the segmentation of the data. We pose this problem as a rank minimization problem, which we solve using an alternating direction method, where the optimal solution at each iteration can be computed in closed form. Experiments on synthetic and real data demonstrate the superiority of our approach over state-of-the-art methods."
853,Picking the Low-Hanging Fruit First with Non-Convex Learning to Rank,"  In many learning to rank applications the main goal is to boost  precision at least at low recall. This can usually be done by  focusing the learning on the ``low-hanging fruit'', that is the  documents that are easier to model. In principle, this can be  obtained automatically by optimising appropriate ranking losses such  as AP that reward precision. However, we show that existing convex  surrogates of these losses, as used in practice by many  learning-to-rank methods as a convenient optimisation tool, may fail  to do so to the point that simple binary SVM can be competitive with  more advanced approaches.  We then show how to modify the latter to  better capture easy sub-populations in the training, improving  precision as part of a non-convex optimisation step. We demonstrate  this algorithm on a retrieval task in computer vision, including the  stage-wise construction of a mixture of linear models."
854,Incremental Online Boosting using Product of Experts,"We develop a novel incremental online algorithm for boosting based on a Product of Experts. The Product of Experts formulation allows for the first time, to devise rules to add experts to the ensemble thus incrementally adapt the model complexity of the ensemble along with learning in an online fashion. In this paper, we illustrate the efficacy of the online PoEBoost by comparing it with the previous approaches to online boosting on benchmark datasets."
855,Maximal Margin Learning Vector Quantisation,Kernel Generalised Learning Vector Quantisation (KGLVQ) was proposed to extend Generalised Learning Vector Quantisation into the kernel feature space to deal with complex class boundaries and thus yield promising performance for complex classification tasks in pattern recognition. However KGLVQ does not follow the maximal margin principle which is crucial for kernel-based learning methods. In this paper we propose a maximal margin approach to Kernel Generalised Learning Vector Quantisation algorithm which inherits the merits of KGLVQ and follows the maximal margin principle to favour the generalisation capability. Experiments performed on the well-known data sets available in UCI repository show promising classification results for the proposed method.
856,On the Consistency of AUC Optimization,"AUC has been widely used as an evaluation criterion in diverse learning tasks. Many learning approaches have been developed to optimize AUC; however, owing to the non-convexity and discontinuousness of AUC, almost all approaches work with surrogate loss functions. Thus, the consistency of AUC is a crucial issue. In this paper, we theoretically study the asymptotic consistency of learning approaches based on surrogate loss functions and provide a sufficient condition. Based on this result, we prove that the exponential loss and logistic loss are consistent with AUC, whereas the hinge loss is inconsistent. We then derive the {$q$-norm hinge loss} and {general hinge loss} that are consistent with AUC. We also derive the consistent bounds for the exponential loss and logistic loss. Additionally, we obtain the consistent bounds for many surrogate loss functions under non-noisy setting. Finally, we find an equivalence between the exponential surrogate loss of AUC and the exponential surrogate loss of accuracy, leading to a direct consequence that AdaBoost and RankBoost are asymptotically equivalent."
857,Stereopsis via deep learning,"Estimation of binocular disparity in vision systems is typically based on a matching pipeline and rectification. Estimation of disparity in the brain, in contrast, is widely assumed to be based on the comparison of local phaseinformation from binocular receptive fields. The classic binocular energy model shows that this requires thepresence of local quadrature pairs within the eye which show phase- or position-shifts across the eyes.  While numerous theoretical accounts of stereopsis have been based on these observations, there has been little work on how energy models and depth inference may emerge through learning from the statistics of image pairs.  Here, we describe a probabilistic, deep learning approach to modeling disparity and a methodology for generating binocular training data to estimate model parameters.  We show that within-eye quadrature filters occur as a result of fitting the model to data, and we demonstrate how a three-layer network can learn to infer depth entirely from training data. We also show how training energy models can provide depth cues that are useful for recognition. We also show that pooling over more than two filters leads to richer dependencies between the learned filters. "
858,Common Object Discovery via Relaxed Rank Minimization,"In this paper, we study the problem of discovering a common object within a set of images. We cast this problem as a robust subspace learning problem in presence of overwhelming outliers (negative instances) and corruptions. The additional knowledge of having one positive instance per image allows us to relax this highly combinatorial and prohibitive problem as a convex programming problem. Solving this program allows us to simultaneously identify the common objects in the image and learn a subspace model for its appearance. We give an efficient and effective algorithm based on the Augmented Lagrangian Multiplier method. We provide extensive simulations and experiments to verify the effectiveness of our method. Experimental results on challenging real-world datasets demonstrate significant advantages of our method over the state-of-the-art approaches. The  proposed scheme is not limited to solve object discovery and it can be applied to solve a wide class of high-dimensional combinatorial selection problems."
860,Learning in deep architectures with folding transformations,"Statistical Learning Theory establishes the consistency of a learning process as an upper bound that holds with certain probability.  We propose a folding transformation paradigm for deep architectures for supervised layer-wise learning that assures the reduction of the complexity of the task to be learned without compromising the learning consistency regardless of the additional VC-dimension due to the depth of the architecture.   We introduce concepts of internal decision making, mapping and shatter complexity that aid in the analysis of a contribution of an individual hidden transformation to the solution of a task in a deep architecture, and apply these methods to investigate the capabilities of the proposed folding transformations. We also provide possible implementation using a neural network and carry out a small test of the architecture's performance on a classification task."
861,Online Regret Bounds for Undiscounted Continuous Reinforcement Learning,"We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside a technical condition to ensure existence of an optimal policy, the only assumptions made are Hoelder continuity of rewards and transition probabilities. "
862,Bayesian Sparse Partial Least Squares,"Born in the bosom of the chemometrics discipline, partial least squares (PLS) is a class of methods that makes use of a set of latent or unobserved variables to model the relation between (typically) two sets of input and output variables, respectively. Several flavors, consisting on how the latent variables or components are computed, have been developed over the last years. In this paper, we propose a Bayesian formulation of PLS along with some extensions. In a nutshell, we provide sparsity at the input space level and an automatic estimation of the optimal number of latent components. We follow the variational approach to infer the parameter distributions. We have successfully tested the proposed methods on electrocorticogram data associated with several motor outputs in monkeys."
863,Hybrid Hierarchical Growing Neural Gas for Clustering of 3D Environmental Map,"This paper proposes a method of clustering of 3D environmental map. Our method is based on Growing Cell Structure and Growing Neural Gas. These methods are one of self-organizing neural network based on unsupervised learning First, we propose 3D map building method using Kinect that can get camera image and 3D distance information simultaneously. Next, we propose a method of clustering method based on hybrid hierarchical growing neural gas to cluster 3D environmental map. Finally, we show experimental results of the proposed method and discuss the effectiveness of the proposed method."
864,Efficient Inference for Robust GP Regression on Highly Contaminated Data,"We consider robust non-parametric regression on data where the scale of the outlier errors is larger than the amplitude of the oscillation of the inlier manifold. This phenomenon appears frequently in warp estimation for scattered data interpolation, where outliers are incorrect data assignments, and the scale of the outlier errors is usually larger than the warping effects of the underlying interpolant. In this context, we propose a fast inference algorithm for robust Gaussian Process regression which takes advantage of the above condition. Our idea is to putatively fit a simple parametric model to capture the global trend of the inlier distribution \emph{relative} to the outliers, and then guide the Gaussian Process inference using statistics collected from the fitted parametric model. Compared to standard inference algorithms, this combination of parametric and non-parametric techniques achieves significantly higher efficiency with less tendency to converge to local minima."
865,Networks of rate neurons with no propagation of chaos: when mean-field theory fails,"We show how to compute analytically the correlation between pairs of neurons in a linear stochastic network described by rate equations. We prove that correlation depends on the eigenvalues of the connectivity matrix and that in special cases it isn?t negligible (no propagation of chaos), preventing us from applying mean-field theories to describe the activity of the network."
866,Generalization Bounds for Domain Adaptation,"In this paper, we provide a new framework to study the generalization bound of thelearning process for domain adaptation. Without loss of generality, we considertwo kinds of representative domain adaptation settings: one is domain adaptationwith multiple sources and the other is domain adaptation combining sourceand target data. In particular, we introduce two quantities that capture the inherentcharacteristics of domains. For either kind of domain adaptation, basedon the two quantities, we then develop the specific Hoeffding-type deviation inequalityand symmetrization inequality to achieve the corresponding generalizationbound based on the uniform entropy number. By using the resultant generalizationbound, we analyze the asymptotic convergence and the rate of convergenceof the learning process for such kind of domain adaptation. Meanwhile, we discussthe factors that affect the asymptotic behavior of the learning process. Thenumerical experiments support our results."
867,Sampling with Deterministic Constraints,"Deterministic and near-deterministic relationships among subsets of variables in multivariate systems are known to causeserious problems for Monte Carlo algorithms. We examine the family of problems in whichthe relationship $Z = f(X_1,\ldots,X_k)$ holds and we wish to obtain exact samples from the conditional distribution $P(X_1,\ldots,X_k\mid Z= z)$. We begin with the case where $f$ is addition,showing that the problem is NP-hard even when the $X_i$s are independent and each has only two possible values.In more restricted cases---for example, i.i.d. Boolean or uniform continuous $X_i$s---efficient exact samplers have been obtained previously.For the case where each $X_i$ has a bounded range of integer values, we derive an $O(k)$ dynamic programming algorithm called {\em exact constrained sequential sampling} (ECSS).For the more general, continuous case, we propose a {\em dynamic scaling} algorithm (DYSC),a form of importance sampling. We evaluate these algorithms on several examplesand derive generalized forms that operate with any function $f$ that satisfies certain natural conditions."
868,Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning,"One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called \textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency."
869,fMRI Based Localization of EEG ,"This work introduces a novel EEG/fMRI integration approach that attempts to improve the spatial resolution of EEG using fMRI data, based on concurrent EEG/fMRI recordings. Advanced signal processing and machine learning, are used to improve EEG spatial resolution for specific regions based on simultaneous fMRI activity measurements of these areas.  We concentrate on demonstrating improved EEG localization in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model which is based on time/frequency representation of EEG data can predict the amygdala activity significantly better than traditional theta/alpha activity.  The important outcome is that with the proposed framework, the activity of sub-cortical regions can be analyzed with higher temporal resolution than obtained by fMRI. "
870,Bayesian Natural Actor Critic via Automatic Relevance Determination,"We present an algorithm which computes the posterior distribution of the natural policy gradient. The posterior mean of the gradient is used to update the parameters of the policy. The covariance matrix of the gradient can be used to evaluate the accuracy. Our algorithm can automatically prune out the irrelevant features in the value function approximation, resulting in a sparse model. This mechanism is achieved through automatic relevance determination which is similar with that in the relevance vector machine.  Finally, we demonstrate the performance of the algorithm experimentally and show that our work naturally combines the value basis function selection and the Bayesian natural policy gradient together."
871,Multi-Task Multi-Sample Learning,"In the ensemble of exemplar SVMs (E-SVMs) approach (Malisiewicz et al., ICCV 2011), each SVM is trained independently with only a single positive sample and all negative samples for the class . In this paper we borrow techniques from multi-task learning to develop a multi-sample learning (MSL) model which learns E-SVMs in a joint framework without any additional cost over the ensemble learning.In MSL, the degree of sharing between positive samples can be controlled, such that the classification performance of either an ensemble of E-SVMs (sample independence) or a standard SVM (all positive samples used) is reproduced. However, between the two limits the model can exceed the performance of either. We also introduce a further multi-task extension to MSL to multi-task multi-sample learning (MTMSL). This model encourages the sharing between both classes and sample specific classifiers within each class. Both MSL and MTMSL have convex objective functions.The MSL and MTMSL models are evaluated on standard benchmarks including the MNIST and Animals with attributes datasets. They achieve a significant performance improvement over both SVM and ensemble of E-SVMs."
872,Mixed Norm Regularization for Multi Class Learning,"Multiclass problems are everywhere, given an input the goal is topredict one of few possible classes. Most previous work reducedlearning to minimization the empirical loss over some training set,and an additional regulariztion term, prompting simple models, or someother prior knowledge. A common learning regulariztion promotessparsity, that is, small models or small number of features, asperformed in group LASSO. Yet, these assumption do not always hold, insome problems, for each class, there is a small set of features thatrepresents it well, yet the union of these sets is not small. Wepropose to use other regularizations that promptes this type ofsparsity, analyze the generalization property of such formulations,and show empirically that indeed, that not only perform well, but alsopromote such sparsity structure."
873,Privacy-preserving spectral analysis of large incoherent matrices,"In this paper we provide a highly efficient method capable of performingaccurate---yet privacy-preserving---spectral analyses of large matrices.Previously this was feasible only for small or moderately sized matrices. Incontrast, our algorithm benefits from the ``blessing of dimensionality''.Indeed, we prove that the error of our algorithm is bounded in terms of thecoherence of the input matrix. High-dimensional data tends to have lowcoherence resulting in greater accuracy of our algorithm. We evaluate ouralgorithm on both real and synthetic data sets where we obtain encouragingresults."
874,Non-accidental configuration representation in the human ventral visual pathway,"Non-accidental configurations have been proposed to help resolve ambiguities in the inputs to the visual system. Moreover, recent theoretical accounts propose that such configurations could be formally ordered by the degree of their 'non-accidentalness'. In this study, we wanted to explore the possibility that human visual system indeed assigns a special status to non-accidental configurations. In a fast blocked fMRI design, participants were presented with slowly moving stimuli composed of two lines in one of twelve configurations while performing a perceptual similarity task. Using multi-voxel pattern analysis we were able to reliably discriminate between most of the configurations. Moreover, our fMRI data were well modeled by a simple V1 filter bank model. Further analyses failed to reveal an special status of the configurations used. Our experiment sets a baseline for future experiments on non-accidental configuration representation in the visual system."
875,Bidirectional Noisy-Max Model,"Noisy-Or/Max models describe relations between graded variables in asymmetric directed relations, where multiple variables have effect on one variable through a Or/Max gate. However, it is common in real life that such relations exist in symmetric correlations. This paper presents a bidirectional noisy-Max model to represent it. The model is established on neighboring relations between variables. It shows how to obtain an exact joint distribution in the model. It also presents an inference algorithm based on message passing to compute marginal distributions of variables efficiently."
876,Boosting with Side Information,"In many problems in machine learning and computer vision, there exists side information, i.e., information contained in the training data and not available in the testing phase. This motivates the recent development of a new learning approach known as learning with side information that is aimed to incorporate side information for improved training of learning algorithms. In this work, we describe a new training method of boosting classifiers that uses side information.  In particular, we propose a novel feature space imputation method to construct extra weak classifiers from the available information that simulate the performance of better weak classifiers obtained from features in side information. We apply our method to two problems, namely handwritten digit recognition and facial expression recognition from low resolution images, where it demonstrates its effectiveness in classification performance."
877,Modeling Human-Object Interactions for Action Recognition in Real-World Videos,"This paper deals with the interesting problem of recognizing human actions in real-world videos. Such videos usually present large variation in background and camera motion, which makes the performance of low-level appearance and motion features unsatisfactory, particularly in the case of video classes sharing similar objects and background (e.g. ``snatch'' and ``clean-jerk'' weightlifting actions). In this paper, we tackle the problem through representation of action classes as human and object interactions (HOI). HOI is modeled as the spatio-temporal relationship between human and object tracks along with their appearance descriptions in a video. However, such a representation requires accurate detection of human and object tracks. This is a difficult task in its own right when dealt separately. We address the issue by extracting candidate tracks from a video and modeling the choice of correct tracks as latent variables in a latent SVM framework. This formulation enables the task of HOI modeling and action recognition without accurate initialization of human and object tracks. We demonstrate promising action classification results on the challenging Olympic Sports [1] and TRECVID11-MED [2] datasets, where our method outperforms state-of-the-art approaches."
878,Learning curves for multi-task Gaussian process regression,"  We study the average case performance of multi-task Gaussian process (GP)  regression as captured in the learning curve, i.e.\ the average Bayes error  for a chosen task versus the total number of examples $n$ for all  tasks. For GP covariances that are the product of an  input-dependent covariance function and a free-form inter-task  covariance matrix, we  show that accurate approximations for the learning curve can be  obtainedfor an arbitrary number of tasks $T$.  We use  these to study the asymptotic learning behaviour for large  $n$. Surprisingly, multi-task learning can be asymptotically essentially  useless: examples from other tasks only help when the  degree of inter-task correlation, $\rho$, is near its maximal value  $\rho=1$. This effect is most extreme for learning of smooth target  functions as described by e.g.\ squared exponential kernels. We also  demonstrate that when learning {\em many} tasks, the learning curves  separate into an initial phase, where the Bayes error on each task  is reduced down to a plateau value by ``collective learning''   even though most tasks have not seen examples,  and a final decay that occurs only once the number of examples is  proportional to the number of tasks."
879,Group Regularization of Correlated Features in Conditional Random Fields,"Conditional random fields allow extracting completely arbitrary linguistic dependencies between labels and observations. Optimization and even storage of billions of features is a problem; that is why a number of model selection approaches have been proposed. Although modern feature selection methods are efficient, they usually do not take into consideration correlation between  dependencies.  In this contribution we consider application of group and hierarchical linguistic dependencies using composite norms. We propose a new optimization approach which exploits the matrix of the second derivatives that keeps important information about correlations between model dependencies.  We illustrate by experiments on standard natural language learning data sets that the proposed approach is efficient. "
880,Nonsingleton Fuzzy Classifier for Attribute Noise Data,"Uncertain in data is an intrinsic problem to supervised machine learning.  Several can be the sources of problems and it is usually very hard to know, or assure, that a dataset is uncertainty free for real world data. One source of uncertainty is attribute noise that can be added by an instrument or by preprocessing data transformations that involve randomness or numerical imprecision. This work proposes to use a supervised fuzzy classifier based on a nonsingleton logic system to deal with attribute noise data. The method, firstly, models the data as fuzzy numbers and, secondly, the fuzzy data is mapped to a high dimensional space using a positive definite kernel whose domain is the fuzzy number space. Finally, the parameters are learnt by a fuzzy classifier using a support vector machine. To test the approach, experiments are done for fifteen attribute noise datasets. The results show that the nonsingleton fuzzy classifier performs well in this kind of noise datasets with no necessity of preprocessing data with noise filtering algorithms. "
881,New Relaxations of Graph Cuts for Clustering,"In recent clustering research, the graph cut methods, such as normalized cut and ratio cut, have been well studied and applied to solve many unsupervised learning applications. The original graph cut is an NP-hard problem. Traditional approaches used spectral relaxation to solve the graph cut problem. The main disadvantage of this approach is that the obtained spectral solutions %are not the final clustering results and the post-processing step has to be applied. Thus, the final results could severely deviate from the true solution.To solve this problem, in this paper, we propose a new relaxation mechanism for graph cut methods. Instead of minimizing the squared distances of clustering results, we use the $\ell_1$-norm distance. Meanwhile, considering the normalized consistency, we also use the $\ell_1$-norm for the normalized terms in the new graph cut relaxations. Due to the sparse result from the $\ell_1$-norm minimization, the solution of our new relaxed graph cut methods get discrete values with many zeros, which is close to the ideal solution. However, the new objectives are difficult to be optimized, because the minimization problem involves the ratio of non-smooth terms. The existing sparse learning optimization algorithms cannot be applied to solve this problem. In this paper, we propose a new optimization algorithm to solve this difficult non-smooth ratio minimization problem. The extensive experiments have been performed on three two-way clustering and eight multi-way clustering data sets. All empirical results show that our new relaxation methods consistently enhance the normalized cut and ratio cut clustering results."
882,Dynamic Bayesian Combination of Multiple Imperfect Classifiers,"Classifier combination methods aggregate the decisions of multiple imperfect classifiers to produce higher accuracy decisions. The imperfect classifiers can vary enormously in reliability and change over time. We present a Bayesian approach to classifier combination that allows us to infer and track the changing performance of individuals, using computationally efficient variational Bayesian inference. We apply the approach to real data from a large citizen science project, Galaxy Zoo Supernovae, showing superior performance compared to established classifier combination approaches. Using this dataset we demonstrate the ability to follow changes in performance of citizen scientists over time."
883,Transformed Poisson-Dirichlet Processes for Differential Topic Modeling,"We want to compare topics from a number of different document collections:some of these topics capture shared content, others capture the different andunique aspects that the collections may contain. We propose the transformedPoisson-Dirichlet process (TPDP), which is defined to be a class of hierarchicalPoisson-Dirichlet processes (HPDP) with transformed base measures, to build differential topic model among different groups of data. The main challenge of using the TPDP is the non-conjugacy between the prior and likelihood. We propose an efficient sampling algorithm by introducing auxiliary variables, which effectively resolve this problem. Experiment results show a dramatic reduced test perplexity compared to existing approximating methods on a variety of text and image collections. The model also gives an insightful analysis of the Democrat versus Republican blogs leading up to the 2008 USA election."
884,Kernel Hyperalignment,"We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We conducted experiments using real-world, multi-subject fMRI data."
885,Contextual Object Learning with Labeled Multi-LDA,"For many applications it is beneficial to model objects in terms of how they can be employed in human activity. In this paper we propose a Labeled Multi-Latent Dirichlet Allocation (LM-LDA) model that encodes both object appearance and how the object interacts with a human. As reported in related work, an appearance cue together with a contextual human action cue significantly improves recognition performance, compared to recognition based solely on object appearance. In addition to this, we show that our LM-LDA model encodes correlations between appearance and associated human actions, which improves classification of a previously unseen object instance, even if only a single image of the instance is available. Furthermore, given only a single view of a previously unseen instance of an object category represented in the model, the LM-LDA model can be used to infer actions that are commonly performed on objects of this category."
886,Universal restrictions on Natural Language determiners from a PAC-learnability perspective,"A classical conjecture in the generative linguistic literature is that universal restrictions on determiners in Natural Language (e.g.~{\em monotonicity}, {\em invariance}, and {\em conservativity}) serve the purpose of simplifying the language acquisition task. This paper formalizes this informal conjecture within the PAC-learnability framework.  "
887,Dictionary Learning from Ambiguously Labeled Data,"We propose a dictionary-based learning method for the problem of ambiguously labeled multiclass-classification, where each training sample has multiple labels and only one of the labels is correct. The dictionary learning problem is solved using an iterative alternating-algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. For each sample, a confidence/probability distribution is associated with the set of ambiguous labels. The dictionaries are updated using both soft and hard decision rules. Extensive evaluation on the Labeled Faces in the Wild, PIE, and Brodatz datasets demonstrates that the proposed method performs significantly better than state-of-the-art partially-labeled learning approaches."
888,Using Topic Models to Improve Object Detection in Context,"This paper describes a generative model of images, the objects they contain, their appearances, positions, sizes and locations, and the application of this model to the improvement of object detection. Our generative model has at its heart a topic model of object labels which enforces co-occurrence preferences. We demonstrate how this allows the model to take output from an object detector and enhance it with notions of context, and leverage this additional information to improve detection performance. Our proposed model exhibits key properties lacked by competing approaches: firstly, we are able to naturally model the repeat occurrence of objects in images; and secondly, our model is fully generative and thus easily interpreted and extended. We demonstrate the benefits of our model on the recently introduced SUN09 dataset, as well as the PASCAL 2007 challenge."
889,Dictionary-based Spatially-Variant Image Deblurring with Dual-Exposure Stereo,"Camera shake induces image blurring, which was assumed to be spatially  invariant in most traditional image restoration methods. State-of-the-art image deblurring methods utilize combination of projected views to handle camera rotation or approximate motion blur. Besides rotational motion, translation also induces non-uniform blurring because of non-uniform depth of the scene. Due to the complicated camera motion blur, a pair of images acquired with different settings could provide additional information to faciliate image deblurring. However, traditional methods consider image pairs which share almost the same viewpoint and mainly focus on uniform blur. In this paper, we use a stereo pair of images which contain one noisy image and one blurred image. We use sparse representation and kernel estimation techniques to decompose non-uniform motion blur and camera rotation blur. The proposed novel framework could be efficiently solved by convex optimization or compressive sensing algorithms, and it can be easily extended by increasing the size of the dictionary for sparse representation. The proposed algorithm can be used to recover the stereo pair with spatially varying blur. Experimental results show the proposed deblurring algorithm can effectively reduce the motion blur and provide better results than traditional methods."
890,Continuous Random Field Models,"  We study the problem of labeling random field models when the labels are  continuous. Many important problems fall under this category such as: stereo,  geolocation, optical flow, etc. We build on the well understood variational  convex formulation. We extend the convex variational formulation to perform  the labeling: (1) in a coarse to fine manner, (2) to handle product of labels,  (3) to apply a weighted median to obtain better results around discontinuities  and (4) to incorporate parameter learning. Our results show significant gains  over the baseline method and better scalability to larger problems."
891,Multiple Choice Learning: Learning to Produce Multiple Structured Outputs,"The paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade. Given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one. The standard approach for handling this scenario is to learn a single model and then produce M-best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we formulate this multiple {\em choice} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem. We present a max-margin formulation that minimizes an upper-bound on this loss-function. Experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this scenario and leads to substantial improvements in prediction accuracy."
892,Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions,"Conditional Markov Chains (also known as Linear-Chain Conditional Random Fields  in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables. Large-sample properties of Conditional Markov Chains have been first studied by Sinn and Poupart [1]. The paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given."
893,Deep Online Learning,"Learning an effective feature representation from data plays an important role for many machine learning tasks. Classical online learning methods typically operate directly on the input feature space. In this paper, we propose a novel framework of Deep Online Learning (DOL) which aims to improve online learning tasks by learning hierarchical feature representations from labeled and unlabeled data. We conducted extensive experiments by comparing the proposed online learning algorithms with several state-of-the-art online learning algorithms. The encouraging empirical results show that the DOL algorithms can effectively learn structure information on high-dimensional data to significantly boost online learning performance on real application with complex data including image and text data. These results indicate that exploring hierarchical representations learned from data is a promising direction for improving many existing online learning tasks."
894,Extracting Representative Joint Features,"In this paper we present an novel approach for automatically learning representative combinations of single features. The proposed method generates a summarized representation of the data which encodes statistical dependencies between individual features. This increases the discriminative quality while at the same time retaining low-dimensionality. Our approach is motivated by formulating a topological cover of possible joint representations. This allows us to circumvent the combinatorial complexity traditionally associated with generating joint features and learn representations of a cardinality previously deemed impossible. Our proposed method is general and can be used to learn a joint representations of any vector space. In the special case, where the cardinality of each set constituting the cover is one, the summarization corresponds to clustering and our method reduces to traditional k-means. We exemplify the benefits of the proposed method by learning a joint representation for an image classification task. We show that by increasing the size of the sets in the cover we achieve more accurate representation description. Our method significantly improves the quality of the representations in comparison with k-means at a significantly reduced cost. "
895,Persistent Homology for Learning Densities with Bounded Support,"We present a novel method for learning densities with bounded supportwhich enables us to incorporate `hard' topological constraints.In particular, we show how emerging techniques from computational algebraic topologyand the notion of Persistent Homology can be combined with kernel based methods from Machine Learning for the purpose of density estimation.The proposed formalism facilitates learning of models with bounded support in a principledway, and -- by incorporating Persistent Homology techniques in ourapproach -- we are able to encode algebraic-topological constraints which are notaddressed in current state-of the art probabilistic models. We study thebehaviour of our method on two synthetic examples for various samplesizes and exemplify the benefits of the proposed approach on areal-world data-set by learning a motion model for a racecar. Weshow how to learn a model which respects the underlying topologicalstructure of the racetrack, constraining the trajectories of thecar."
896,Dynamical models and tracking regret in online convex programming,"This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that capture a comparator's deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network."
897,Learning to Rank: from Batch to Online Learning,"Learning to rank represents a family of important machine learning algorithms for information retrieval applications. Traditional methods typically learn a ranking model from training data in a {\it batch} learning mode, which suffer from two major limitations: (i) they can {\it hardly} track fast-changing search intention of online users in a timely fashion; and (ii) their {\it scalability} is usually poor, especially for web-scale applications. To overcome these limitations, in this paper, we propose a novel framework of online learning to rank, which sequentially updates ranking models in an online learning fashion which thus can adapt the fast-changing search intention of online users, which is of particular importance for many online applications. Specifically, we propose two algorithms for online learning to rank, and theoretically analyze their IR measure bounds. We conduct extensive empirical studies on the LETOR testbed, in which our results show that our algorithms significantly outperform some existing online ranking technique, and achieve fairly comparable results with the state-of-the-art batch learning to rank algorithms, but enjoy significant advantage in scalability. Our framework enjoys a wide range of applications for many web-scale online services, such as personalized search, recommendation, and online advertising, etc."
898,Planning with Representation Adaptation in Continuous POMDPs,"Partially observable Markov decision processes (POMDP) provide an elegant mathematical formalism to describe decision problems in stochastic and incompletely perceivable domains. Discrete POMDPs can be solved in reasonable time. However, most applications are continuous and then the belief space is infinite-dimensional.In this paper the general concept of Planning with RepresentationAdaptation in POMDPs (PRAin) is proposed. It comprises two novel ideas to make continuous planning feasible: First, learning a problem specific,efficient space representation during value iteration to generalize results in the continuous space. Second, asserting consistency for the generalized results byself-correction. In order to constraint the POMDP models as little as possible, the concept is implemented with a decision tree learning algorithm and based on Monte Carlo approximation. In an experimental comparison PRAin converges to higher values in significantly shorter time than previous approaches. Further, thenegative influence of higher dimensions is effectively reduced to a minimum."
899,Multi-Modal Image Annotation and Retrieval with Multi-Instance Multi-Label LDA,"In image annotation tasks, one image is usually associated with multiple labels, and different image regions may provide different hints. By representing each image region as an instance, multi-instance multi-label (MIML) learning provides a natural formulation for such problems. It is worth noting that, in many real tasks such as web image applications, in addition to the image visual information, there is extra information that can be helpful, such as the surrounding texts or user tags for images. In this paper, we study a multi-modal setting where both visual and tag information are available for each image. We propose Multi-modal Multi-instance Multi-label Latent Dirichlet Allocation (M3LDA), where the model consists of a visual-label part, a tag-label part and a label-topic part. The basic idea is that the topic decided by the visual information and the topic decided by the tag information should be consistent, leading to the correct label assignment. Experiments show that M3LDA outperforms state-of-the-art approaches. Moreover, in contrast to existing approaches that can only give annotations to whole images, M3LDA is able to give annotations to image regions, providing a promising way to understand the relation between input patterns and output semantics."
900,Ants Crawling to Discover the Community Structure in Networks,"We cast the problem of discovering the community structure in networks as the composition of community candidates, obtained from several community detection base algorithms, into a coherent structure. In turn, this composition can be cast into a maximum-weight clique problem, and we propose an ant colony optimization algorithm to solve it. Our results show that the proposed method is able to discover better community structures, according to several evaluation criteria, than the ones obtained with the base algorithms. It also outperforms, both in quality and in speed, the recently introduced FG-Tiling algorithm."
901,A Method for Feature Induction in a Class of Higher-Order Random Fields,"Recently, there has been much interest, particularly in computer vision, in developing models which capture higher-order interactions between discrete random variables. Such interactions depend on the joint configuration of at least three variables, and may be described as `higher-order features' of a model, which may be data-dependent or structural, and can involve patterns over several variables, the counts of particular output values, subsets of output values in a solution, and the partition of variables by output value.  In computer vision these features have proved useful for tasks such as scene segmentation, image denoising and visual category discovery, where they can be used naturally to represent class co-occurrences, object histograms and power-law distributions of classes and segment sizes. Typically, particular higher-order features must be built into a model in advance, with the selection of the appropriate features for a given task being made manually.  We propose a feature induction method within a general class of random fields which we call `label-type networks', in which the automatic selection of higher order features of the kinds described can occur.  Our framework makes use of variational methods for both learning and inference, allowing us to draw on recent mean-field filtering techniques to construct dense models and provide the efficiency necessary to explore large feature spaces.  We demonstrate our approach on semantic segmentation using the PASCAL dataset."
902,Learning to Classify Actions using Image-level Labels,"We consider the problem of classifying whether a given person in an image is performing an action of interest. Unlike previous methods that rely on onerous and expensive annotations of training images (a tight bounding box of the person and his/her ground truth action label), we propose a  weakly supervised formulation that only requires image level labels indicating the presence or an absence of an action in an image. Specifically, we consider a set of candidate objects obtained automatically from an object detector, and treat the true action of each object as a latent variable. This allows us to adapt the recently proposed dissimilarity coefficient learning framework for our task. In order to address the commonly encountered problem of imbalance in the dataset---the negative samples far outnumber the positive samples---we extend our learning framework by introducing a relative weight for each candidate object. These relative weights are adaptively changed depending on the current estimate of the latent variables. Using the largest publicly available dataset, namely the PASCAL VOC action classification challenge, we show that our approach greatly reduces the burden of annotation while preserving the accuracy of the model."
903,Robust Manifold Learning via $l_1$ Minimization,"Previous research has shown that many existing manifold learning methods are sensitive to outliers in the data.  Although some robust extensions have been proposed, the techniques used by them are specifically tailored for a particular manifold learning method but cannot be applied readily to other methods.  In this paper, we propose a unified framework for robust manifold learning based on the $l_1$ norm.  By reformulating the optimization problems of several representative manifold learning methods as Rayleigh quotient maximization problems, we find that they can all be seen as minimizing the $l_2$ distance between a normalized similarity matrix and its reconstruction based on low-dimensional embedding.  Due to the sensitivity of the $l_2$ distance measure to outliers in the data, we replace it with an $l_1$ distance measure which is more robust.  To solve a convex relaxation of the optimization problem, instead of using off-the-shelf solvers, we devise an efficient algorithm based on the augmented Lagrange multipliers method by exploiting the special structure of the problem.  For experimental validation, we use the COIL image and UCI database and compare four representative manifold learning methods with their robust variants formulated based on the unified framework proposed."
904,The Incremental Risk Functional: Basics of a Novel Incremental Learning Approach,"Incremental learning gets increasing attention in research and practice as it has the advantages of continuous adaptation to nonstationary conditions with a low computation and memory demand at the same time. Several approaches have been proposed recently for specific approximators, but only few work has been done to pose the general problem of incremental learning. Hence, we introduce the \emph{incremental risk functional} describing the optimization problem to be solved for incremental learning, regardless of the specific task or approximator. Exemplarily, we apply this approach to regression estimation through linear-in-parameter approximators. We show that the resulting learning algorithm converges and changes the global functional behavior only as little as necessary with every learning step, thus resulting in a stable incremental learning approach."
905,Perceptual switching for a multistable motion stimulus: modelling and bifurcation analysis ,"Perceptual switches are known to occur during prolongedpresentations of barber pole stimuli viewed through a squareaperture due to competition between 1D motion cues aligned withthe grating's motion direction and 2D motion cues aligned withaperture edges. We study the temporal dynamics of this phenomenonwith a neural fields, population-level representation of activityin MT, a cortical area dedicated to motion estimation. Numericaltools from bifurcation analysis are used to investigate themodel's behaviour in the presence of different types of input;this general approach could be applied to a range of neuralfields models that are typically studied only in terms of theirspontaneous activity. The model reproduces known multistablebehaviour in terms of the predominant interpretations (percepts)of the barber pole stimulus. We describe key differences in thecharacteristic behaviour for two contrast regimes and we predicta change in the underlying distribution of percept switchingtimes for the different regimes."
907,Balanced kernel k-means for comparing large graphs with landmarks,"In this article, we study the problem of comparing two large geometric graphs with landmark points to each other via kernel functions. For instance, this problem arises in computer vision, brain imaging and bioimaging. Landmark points make the comparison more efficient, as they allow to cluster the nodes in the huge networks into segments around landmarks. However, the runtime still scales quadratically in the size of the largest segment that is created by clustering.Here we propose a graph kernel on size-balanced clusters around landmarks, which minimizes the size of the largest cluster, thereby optimizing the runtime of kernel computation. The resulting kernel on huge graphs with landmarks outperforms kernels based on unbalanced clustering in terms of runtime, while retaining high levels of accuracy on classification benchmarks."
908,On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes," We consider infinite-horizon stationary $\gamma$-discounted Markov  Decision Processes, for which it is known that there exists a  stationary optimal policy. Using Value and Policy Iteration with  some error $\epsilon$ at each iteration, it is well-known that one  can compute stationary policies that are $\frac{2\gamma{(1-\gamma)^2}\epsilon$-optimal. After arguing that this  guarantee is tight, we develop variations of Value and Policy  Iteration for computing non-stationary policies that can be up to  $\frac{2\gamma}{1-\gamma}\epsilon$-optimal, which constitutes a significant  improvement in the usual situation when $\gamma$ is close to  $1$. Surprisingly, this shows that the problem of ``computing near-optimal non-stationary policies'' is much simpler than that  of ``computing near-optimal stationary policies''."
909,Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model,"Neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli. While adaptation is an intrinsic feature of neuronal models like the Hodgkin-Huxley model, the challenge is to integrate adaptation in models of neural computation. Recent computational models like the Adaptive Spike Response Model implement adaptation as spike-based addition of fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents. Such adaptation has been shown to accurately model neural spiking behavior over a limited dynamic range. Taking a cue from kinetic models of adaptation, we propose a multiplicative Adaptive Spike Response Model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking. We show that unlike the additive adaptation model, the firing rate in the multiplicative adaptation model saturates to a maximum spike-rate. When simulating variance switching experiments, the model also quantitatively fits the experimental data over a wide dynamic range. Furthermore, dynamic threshold models of adaptation suggest a straightforward interpretation of neural activity in terms of dynamic signal encoding with shifted and weighted exponential kernels. We show that when thus encoding rectified filtered stimulus signals, the multiplicative Adaptive Spike Response Model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude, without changing model parameters. "
910,Turing Test Imaging based on Non-Artificial Evidence Criterion,"We in this paper present a Turing test image (TTI) generated method. Using our produced image, we can tell computers and humans apart and protectwebsite from attack. To this end, we propose a robust TTI generated framework based on novelNon-Artificial Evidence Criterion (NAEC). Extensive user study is conduced to prove our TTI is user-friendly. Experiments demonstrate that our proposed Turing test image can resist attack from state-of-the-art object detection and imageclassification system."
911,Regularization of Latent Variable Models to Obtain Sparsity,"We present a pseudo-observed variable based regularization technique for latent variable mixed-membership models that provides a mechanism to impose preferences on the characteristics of aggregate functions of latent and observed variables.  The regularization framework is used to regularize topic models, which are latent variable mixed membership models for language modeling.  In many domains, documents and words often exhibit only a slight degree of mixed-membership behavior that is inadequately modeled by topic models which are overly liberal in permitting mixed-membership behavior.  The regularization introduced in the paper is used to control the degree of polysemy of words permitted by topic models and to prefer sparsity in topic distributions of documents.  The utility of the regularization is evaluated internally using document perplexity and externally by using the models to predict star counts in movie and product reviews based on the content of the reviews.  Results of our experiments show that using the regularization to finely control the behavior of topic models leads to better perplexity and lower mean squared error rates in the star-prediction task."
912,Measuring Correlations Using Information Dimension,"We propose to measure the correlation between two continuous random variables by the mutual information dimension (MID), and present an efficient parameter-free estimation method of MID with linearithmical complexity with respect to the number of data points. Our method gives an effective solution, supported by sound dimension theory, to the problem of detecting interesting relationships of variables in massive data, which is now becoming a heavily studied topic not only in machine learning but also all over science. Different from the classical Pearson correlation coefficient, MID is zero if and only if two random variables are statistically independent and is translation and scaling invariant. We experimentally show superior performance of MID for various types of relationships in detection of them in the presence of noise."
914,MAP Inference in Chains using Column Generation,"Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning."
915,"Scoring Bayesian Networks with Informative, Causal and Associative Priors","A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. Currently however, there are limited practical ways of assigning priors to each possible network. In this paper, we present a method for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between a pair of variables X and Y. This type of knowledge naturally arises from prior experimental and observational datasets, among others. We show that incorporating such prior knowledge may not only improve the learning of the direction of the causal relations in the network, but also the learning of the network skeleton. This is particularly the case when sample size is low and thus prior knowledge increases in importance. Our approach is based on converting possibly-incoherent beliefs about marginals to joint distributions of priors by use of optimization theory."
916,Multi-Step Regression Learning for Compositional Distributional Semantics,"This paper presents a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it against two existing experiments, and find it to perform better than comparable methods in one, and on par with other best-performing models in the other. We argue in our analysis that the nature of this learning method renders it suitable for solving more subtle problems compositional distributional models might face, and suggest future work in this area."
917,Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ) for Multi-voxel Pattern Analysis in fMRI,"We present an efficient algorithm for simultaneously training elastic-net-regularized generalized linear models across many related problems, which may arise from bootstrapping, cross-validation and nonparametric permutation testing.  Our approach leverages the redundancies across problems to obtain approximately 10x computational improvements relative to solving the problems sequentially by the glmnet algorithm. We demonstrate our fast simultaneous training of generalized linear models (FaSTGLZ) algorithm,  for multivariate analysis of fMRI and run otherwise computationally intensive bootstrapping and permutation test analyses that are typically necessary for obtaining statistically rigorous classification results and  meaningful interpretation.  We also use our algorithmic framework to propose a new algorithm for stabilizing feature selection in sparse regression models."
918,Estimating Unknown Sparsity in Compressed Sensing,"Within the framework of compressed sensing, many theoretical guarantees for signal reconstruction require that the number of linear of measurements $n$ exceed the sparsity $\|x\|_0$ of the unknown signal $x\in\R^p$. However, when the sparsity parameter $\|x\|_0$ is unknown, the choice of $n$ remains problematic. In this paper, we consider the problem of directly estimating $\|x\|_0$ from a small number of linear measurements---without making any prior assumptions about the sparsity of $x$. Although we show that estimation of $\|x\|_0$ is generally intractable in this framework, we consider an alternative measure of sparsity $s(x):=\|x\|_1^2\big/\|x\|_2^2$, which is a sharp lower bound on $\|x\|_0$, and is more amenable to estimation. When $x$ is a non-negative signal, we propose a computationally inexpensive estimator $\hat{s}(x)$ for $s(x)$, and derive concentration bounds that imply $\hat{s}(x)/s(x)\to 1$ almost surely as $(n,p)\to\infty$. Remarkably, the quality of estimation is \emph{dimension-free}, which ensures that $\hat{s}(x)$ is well-suited to the high-dimensional regime where $n\ll p$. These results also extend naturally to the problems of using linear measurements to estimate the rank of a positive semidefinite matrix, or the sparsity of a non-negative matrix. Finally, we show that if no structural assumption (such as non-negativity) is made on the signal $x$, then the quantity $s(x)$ cannot generally be estimated when $n\ll p$."
919,Modelling Shapes and Segmentations by Fields of Expert Mixtures,"We introduce a Gibbs Random Field (GRF) for modelling segmentations, i.e., the shape and the spatial relations of segments. The latent variables of the model form a field of expert mixtures (i.e.~naive Bayes models). The resulting probability distribution for segmentations (obtained by marginalising over the expert field) is a GRF with factors of arity higher than two.We show how to formulate the corresponding learning and inference tasks for the resulting model and propose algorithms for their approximative solution. The resulting approach is experimentally analysed on the example of two prototypical segmentation tasks."
920,PRISMA: PRoximal Iterative SMoothing Algorithm,"Motivated by learning problems including max-norm regularized matrix completion and clustering, robust PCA and basis pursuit, we propose a novel optimization algorithm for minimizing a convex objective which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz part, and a simple non-smooth non-Lipschitz part. Our algorithm combines the methodology of forward-backward splitting, smoothing, and accelerated proximal methods."
921,Bayesian Nonparametric Modeling of Suicide Attempts,"The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the U.S. population. In the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts."
922,Using GSMDPs in Multi-Robot Decision-Making Problems,"Markov Decision Processes (MDPs) provide an extensive theoretical background for problems of decision-making under uncertainty. In order to maintain computational tractability, however, problems domains are typically discretized both in states and actions as well as in time. Assuming synchronous state transitions and actions at fixed rates may result in models which are not strictly Markovian, or where agents are forced to idle, losing their ability to react to sudden changes in the environment. In this work, we explore the application of Generalized Semi-Markov Decision Processes (GSMDPs) to a realistic multiagent scenario. A case study will be presented in the domain of cooperative robotics, where real-time reactivity must be preserved, and therefore synchronous discrete-time approaches are sub-optimal. By allowing asynchronous events to be modeled over continuous time, the GSMDP approach will be shown to provide greater solution quality, and reduced communication usage, than its discrete-time counterparts, while still being approximately solvable by existing methods."
923,Time Series Analysis - An Online Learning Approach,"In this paper, we address the problem of predicting a time series, under minimal assumptions on the noise terms. The focus of our work is on the ARMA model, which is a standard model for time series prediction. Using regret minimization techniques, we develop an effective online learning algorithm for the prediction problem, \emph{without} assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we perform an empirical study to verify the effectiveness of our algorithm."
924,Gradient Matching with Gaussian Processes,"Parameter inference in mechanistic models based on systems of coupled differential equations is a topical yet computationally challenging problem, due to the need to follow each parameter adaptation with a numerical solution of the differential equations. Techniques based on gradient matching, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differential equations, offer a computationally appealing shortcut to the inference  problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes. We show how the inference approach proposed in an earlier NIPS paper can be substantially improved, and we demonstrate the efficiency of the technique on three benchmark systems."
925,A Spectral Learning Approach to Range-Only SLAM,"We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our  approach does not need to linearize a transition or measurement model; such linearizations can cause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly for the highly non-Gaussian posteriors encountered in range-only SLAM. We provide a theoretical analysis of our method, including finite-sample error bounds.  Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost."
926,Maximally informative models and diffeomorphic modes,"Motivated by recent experiments in transcriptional regulation and sensory neuroscience, we consider the following general problem in statistical inference. A system of interest, when exposed to a stimulus $S$, adopts a deterministic response $R = \theta(S)$, of which a noisy measurement $M \sim \pi(M|R)$ is made. Given a large number of measurements and corresponding stimuli, we wish to identify the correct response function $\theta$. However, we do not know the conditional distribution $\pi(M|R)$ a priori. The standard regression approach is to maximize the likelihood of a model response function $\theta$ assuming a specific noise function $\pi$. However, an incorrect $\pi$ will typically lead to systematic error in the inferred $\theta$.  In the large data limit, we argue that one should instead maximize likelihood over both $\theta$ and $\pi$, and we show that this is equivalent to maximizing the mutual information $I[R;M]$ over $\theta$ alone. Moreover, when any candidate response model fully explains the data, maximizing mutual information becomes equivalent to simultaneously maximizing all dependence measures which obey the Data Processing Inequality. Even so, one typically finds not a single optimal $\theta$, but rather an extended subspace of optimal $\theta$. This is because experiments of the type considered cannot distinguish between different $\theta$ within certain equivalence classes. We present a formula for ``diffeomorphic modes'', directions in parameter space which lie within these equivalence classes and cannot, as a result, be constrained by data. We then derive the diffeomorphic modes of response models having either a general linear form or a specific linear-nonlinear form, and find that the number of diffeomorphic modes is often far less then the number of response model parameters. In such cases, maximizing mutual information will, in the large data limit, determine nearly all model parameters without any systematic error."
927,Learning to detect objects with high intra-class variation,"Objects such as humans exhibit large intra-class variations, posing significant challenges for visual object detection. State-of-the-art part-based models explicitly model object deformations, but are limited in their ability to handle image variations incurred by other geometric and photometric changes, such as object pose, lighting, occlusions, and large appearance variations. In this paper, we propose a novel approach which uses a spatially-biased hierarchical feature mapping scheme to map features into a high-dimensional space that better represents the rich set of object appearance and local deformation variations. We propose a new algorithm to jointly learn object detection and performfeature pooling in this high dimensional space, in a structured prediction setting. Our approach achieves the best detection performance on the INRIA pedestrian dataset."
928,Using Support Vector Machines for Solid State Materials Characterization,"Spectroscopic techniques like X-Ray diffraction and Nuclear Magnetic Resonance (NMR) are used in conjunction with empirical and ab-initio calculations to perform structural characterization for materials. Overall, these experimental and computational methods are generally expensive and time consuming, requiring much human input and interpretation, particularly with regards to novel materials. In this work, NMR spectral data for Titanium Oxide polymorphs are simulated and first principle calculations of NMR measurable quantities performed for these materials via batch processes. An array of machine learning kernels in the form of support vector machines (SVM) are then used to learn the complex mapping between structural details and simulated input NMR spectra. The SVM array when presented with input spectra for new but related polymorphs outputs structural details rapidly and accurately. This approach has the potential to reduce the time to discover structural details for new materials, by providing a viable method for solving the inverse problem, with minimal human intervention. A similar approach may be applied to other experimental characterization techniques and ab initio calculations, for example scanning tunneling microscopy images and density of states calculations, in order to build an expert system for solid state materials characterization."
929,Analysis on Co-Training with Insufficient Views,"Co-training is a famous semi-supervised learning paradigm which exploits unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sufficient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sufficient to correctly predict the label. First, we show that co-training might suffer from two limitations, i.e., label noise and sampling bias, when neither view is sufficient. We then define the diversity between the two views with respect to the confidence of prediction and prove that if the two views have large diversity, co-training suffers little from above two limitations and could succeed in improving the learning performance by exploiting unlabeled data even with insufficient views."
930,Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation,"This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture.Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP.Extensive experiments on different data sets demonstrate that EC-PBP achieves a higher topic modeling accuracyand reduces more than $80\%$ communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm."
931,Improving Academic Homepage Crawling with Unlabeled Data,"We investigate the design of a focused crawler for obtaining academic homepages: Given a set ofseed URLs that comprise hubs for academic homepages, how easy is it to filter out unwanted webpagesfrom the content being crawled? Academic homepages or professional homepages ofresearchers are rich sources of metadata from the perspective of digital libraries. Content-gatheringprocesses in digital libraries therefore need to track these resources on a frequent-basis necessitatingtechniques for obtaining accurate lists of academic homepages.We present results indicating that publicly-available datasets for building content-based focusedcrawlers for academic homepagessuffer from a high-degree of false positives. We trace this problem to the disparityin the training and deployment environments of the crawler. We propose URL-based hints ascomplementary evidence to address this issue and propose the use of content features and URL n-gramsas independent views for identifying homepages. Then, we illustrate that these independentviews can be effectively used with unlabeled examples in a co-training framework to adapt thecontent-based classifier to the changed environment. Experiments on our validation setsshow that this process remarkably decreases the false-positive rate of our content-basedclassifier from 34% to 15%."
932,Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling,"Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches."
933,Object Recognition with Boosted Collections of Parts,"We describe an approach for incrementally learning parts for object recognition.  We use category-independent object region proposals to pool learned parts, which removes the need for a structurally constrained spatial model and enables more flexible and extensible learning. Our base model uses an incremental learning approach to refine statistical templates of parts. Boosting is used to learn a strong classifier that scores collections of part responses into an object category. Our system achieves competitive accuracy with the best-existing systems, outperforming on more deformable categories, and is easy to extend to new features or categories."
934,Neurally Plausible Reinforcement Learning of Working Memory Tasks,"A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity. It is however not well known how these neurons acquire their task-relevant tuning. Here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error. We propose that the action selection stage feeds back attentional signals to earlier processing levels. These feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. A globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to (1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and (2) learn to optimally integrate probabilistic evidence for perceptual decision making."
936,Kernel Clustering: Speedup via Efficient and Accurate Eigenfunction Approximation,"Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data and achieve state-of-the-art clustering performance. However, they suffer from two major drawbacks: (i) Their runtimecomplexity and memory requirements increase quadratically with the number of data points, rendering them unsuitable for large data sets, and (ii) the resulting partitions cannot be used to efficiently handle out-of-sample data points. We address these limitations by developing an efficient kernel clustering algorithm based on approximate eigenfunctions. The eigenfunctions are found using a small number of randomly sampled data points, thereby avoiding the computation of the full kernel matrix. The eigenfunctions are then used to compute a low-dimensional representation ofthe cluster centers, which can later be used to assign any out-of-sample point to a cluster. Unlike the existing approaches that compute the eigenfunctions using either thekernel similarity between all the points in the data set or only the similarity between the sampled points, we use the similarity between the sampled points and all the data points, leading to a better trade-off between efficiency and accuracy in eigenfunction approximation, as supported by ouranalysis. Our empirical studies demonstrate that, while the performance of the proposed kernel clustering technique is similar to the performance of state-of-the-art algorithms for large-scale data clustering, its runtime is significantly lower. This allows us to perform kernel clustering efficiently on data sets with millions of points. "
937,Robust PCA from incomplete observations using l-0-surrogates,"Many applications in data analysis rely on the decomposition of a data matrix into a low-rank and a sparse component.Existing methods that tackle this task use the nuclear norm and l-1-cost functions as convex relaxations of the rank constraint and the sparsity measure, respectively, or employ thresholding techniques.We propose a method that allows for reconstructing a matrix of upper-bounded rank from incomplete and corrupted observations. It does not require any a priori information about the number of outliers. The core of our algorithm is an intrinsic Conjugate Gradient method on the set of orthogonal projection matrices, the so-called Grassmannian. Non-convex sparsity measures are used for outlier detection, which leads to improved performance in terms of robustly recovering the low-rank matrix.In particular, our approach can cope with more outliers and with an underlying matrix of higher rank than other state-of-the-art methods."
938,The flip-the-state transition operator for Restricted Boltzmann Machines,"Most learning algorithms for restricted Boltzmann machines (RMBs) rely onMarkov chain Monte Carlo (MCMC) methods using Gibbs sampling. The mostprominent examples are Contrastive Divergence learning (CD) and its variantsas well as Parallel Tempering (PT). The performance of these methods dependsseverely on the mixing properties of the Gibbs chain. We propose a Metropolis-type MCMC algorithm relying on a transition operator maximizing the probabilityof state changes. It is shown that the operator induces an irreducible and aperi-odic and, thus, a properly converging Markov chain. The transition operator canreplace Gibbs sampling in RBM learning algorithms without producing computa-tional overhead. It is shown empirically that this leads to faster mixing and in turnto more accurate learning."
939,Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions,"Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff."
940,Repulsive Mixtures,"Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.  Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set."
941,Non-Linear Dimensionality Reduction by Isometric Patch Alignment,"We propose a novel dimensionality reduction method which has low computational cost. This method is inspired by two key observations: (i) the structure of reasonably large patches of high-dimensional data can be preserved as a whole, rather than divided into small neighborhoods; and (ii) attaching two neighboring patches will align them such that the overall rank does not increase. In the proposed approach, first the data is clustered, so that it is conceptually reduced to a set of overlapping low-rank clusters. Each cluster is embedded into a low-dimensional patch and then all of the patches are rearranged such that their border points are matched. We show that the rearrangement can be computed by solving a relatively small semi-definite program. The embedding computed by this optimization is provably low-rank. The proposed method is stable, fast, and scalable; experimental results demonstrate its capability for manifold learning, data visualization, and even complex tasks such as protein structure determination."
942,Fully Bayesian inference for neural models with negative-binomial spiking,"  Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses.  The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability.  Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals.  This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efficient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models.  We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains."
943,Maximum Composite Likelihood Estimators for Gaussian Process Classification,"  Type-II Maximum Likelihood point estimation of covariance function  parameters is the inference framework of choice for the majority of  serious applications of Gaussian Process based discriminant analysis  and object classification. These estimators are typically derived  from approximations to the non-analytic marginal likelihood based on  Variational bounds or Expectation Propagation. However, the  frequentist properties of such estimators, such as the     unbiasedness, consistency and efficiency cannot  generally be determined analytically or indeed guaranteed. This  paper suggests that estimators with good frequentist properties for  binary Gaussian Process classifiers can be obtained by the use of  Composite Likelihoods for the estimation of the parameters of the  Gaussian Process. Furthermore, because of the structure of the  Composite Likelihood, the computational scaling achievable is vastly  superior to the typical ${\cal O}(n^3)$ scaling for Gaussian  Processes; the explicit computation of determinants, inverses and  derivatives of covariance matrices of arbitrary $n \times n$  dimension is obviated because exact analytical expressions can be  exploited in an independent and distributed manner."
944,Transductive Kernel Map Learning,"Transductive inference techniques are nowadays becoming standard  in machine learning due to their relative success in solving many real-world applications. Among them, kernel-based methods are particularly interesting but their success remains highly dependent on the choice of kernels. The latter are usually handcrafted or designed  in order to capture better similarity in training data. In this paper, we  introduce a novel transductive learning algorithmfor kernel design and classification. Our approach is basedon the minimization of an energy function mixing  i) a reconstructionterm that factorizes a matrix of  input data as a product of a learned dictionary  and a learned kernel map ii) a fidelity term thatensures consistent label predictions with those provided in a ground-truth and iii) a smoothness term which  guaranteessimilar labels for neighboring data and allows us to iteratively diffuse  kernel maps and labels from  labeled to unlabeled data. Solving this minimization problem makes it possible to learn both a decision criterion and a kernel map  that guarantee linearseparability in a high dimensional space and good generalizationperformance. Experiments conducted on object class segmentation, show improvements with respect to baseline as well as  related work on the challenging VOC database. "
945,Probabilistic Active Learning,"In the literature of active learning, most existing studies assume that the labels in the training dataset are deterministic, which is not quite realistic in many real-world applications. In many applications, however, the labels usually come in a probabilistic manner. Motivated by this observation, we propose a new frameworkwhere each label is enriched with a probability. Based on this framework, we propose an active learning algorithm. In addition, we show a theoretical bound on the label complexity of the proposed algorithm. Finally, we conducted comprehensive experiments in order to verify the effectiveness of our proposed algorithm."
946,Slice Normalized Dynamic Markov Logic Networks,"Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the domain of time points typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional randomfield for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues.It supports efficient online inference, and can directly model influencesbetween variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks."
947,Transductive Learning for Multi-Task Copula Processes,"We tackle the problem of multi-task learning with copula process. Multivariable prediction in spatial and spatial-temporal processes such as  natural resource estimation and pollution monitoring have been typically addressed using techniques based on Gaussian processes and co-Kriging. While the Gaussian prior assumption is convenient from analytical and computational perspectives, nature is dominated by non-Gaussian likelihoods. Copula processes are an elegant and flexible solution to handle various non-Gaussian likelihoods by capturing the dependence structure of random variables with cumulative distribution functions rather than their marginals.  We show how multi-task learning for copula processes can be used to improve multivariable prediction for problems where the simple Gaussianity prior assumption does not hold. Then, we present a transductive approximation for multi-task learning and derive analytical expressions for the copula process model. The approach is evaluated and compared to other techniques in one artificial dataset and two publicly available datasets for natural resource estimation and concrete slump prediction. "
948,EigenGP: KL-expansion based Gaussian process learning,"Gaussian processes (GPs) provide a nonparametric representation of functions. Given N training points, the exact GP inference incurs high computational cost. In this paper, we propose a sparse Gaussian process model, EigenGP, based on Karhunen-Lo`eve (KL) expansions of a GP prior. We use the Nystr?om approximation to obtain eigenfunctions of the covariance function and use an empirical Bayesian approach to select these eigenfunctions. To handle nonlinear likelihoods, we develop an efficient expectation propagation inference algorithm, and couple it with expectation maximization for evidence maximization. By selecting eigenfunctions of Gaussian kernels that are associated with data clusters, EigenGP is also suitable for semi-supervised learning. Our experimental results demonstrate improved predictive performance of EigenGP over alternative state-of-the-art sparse GP and semisupervised learning methods for regression, classification, and semisupervised classification."
949,Meta-Gaussian Information Bottleneck,"We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions witha Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuousdata and provides a solution more robust to outliers."
950,A Scalable Approach to Column-Based Low-Rank Matrix Approximation,"In this paper we address the column-based low-rank matrix approximation problem in a novel parallel approach. Our approach is based on the divide-and-combine idea that it performs column selection on each submatrices in parallel, and then combines results from them. Like many existing methods, our method enjoys a relative-error bound by the theoretical analysis. Importantly, our method is scalable on large-scale matrix compared with the traditional methods. In addition, our method is deterministic in data partition and free from any matrix coherence assumption. Finally, the experiments on both simulated and real data shows that our approach is both efficient and effective. "
951,The Variational Garrote,"In this paper, we present a new model for sparse regression using L0 regularization. The model introduces a sparseness mechanism in the likelihood, instead of in the prior, as is done in the spike and slab model. The posterior probability is computed in the variational approximation. The variational parameters appear in the approximate model in a way that is similar to Breiman's Garrote model.  We refer to this method as the variational Garrote (VG).  We show that the combination of the variational approximation and L0 regularization has the effect of making the problem effectively of maximal rank even when the number of samples is small compared to the number of variables.  The VG is compared numerically with the Lasso method and with ridge regression.  Numerical results on synthetic data show that the VG yields more accurate predictions and more accurately reconstructs the true model than the other methods. It is shown that the VG finds correct solutions when the Lasso solution is inconsistent due to large input correlations.  The naive implementation of the VG scales cubic with the number of features.  By introducing Lagrange multipliers we obtain a dual formulation of the problem that scales cubic in the number of samples, but close to linear in the number of features."
952,Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem,"Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have serious limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.We propose a simple framework for adaptive quality control in crowdsourced multiple-choice tasks. In it, we identify a novel problem we call the bandit survey problem. This framework is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations."
953,An Alternative Kernel Method for the Two-Sample Problem,"We present an alternative kernel method for the two-sample problem that is based on Friedman's approach of using any binary classification learning machine to score the data.  When the learning machine is chosen to be a support vector machine, we show that this approach is a generalization of the permutation $t$-test.  Previous work has yielded a normal rate of convergence bound using Stein's Method in the simple setting of univariate data and a linear kernel with simulations, suggesting that this proof technique may be extended to address a more general setting.  Despite a lack of tuning of the SVM parameters, this method is shown to be competitive with the Maximum Mean Discrepancy (MMD) test."
954,Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification,"This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification.  We show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation.  Applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification.  Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose five different criteria for adaptively acquiring nearest neighbors.  Experiments with both synthetic and real datasets demonstrate the effectivness of our classification criteria."
956,The Perturbed Variation,"We introduce a new discrepancy score between two distributions that gives an indication on their \emph{similarity}. While much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions. The new score gives an intuitive interpretation of similarity;  it optimally perturbs the distributions so that they best fit each other. The score is defined between distributions, and can be efficiently estimated from samples. We provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions. The statistical power of this procedures is presented in simulations. We also compare the score's capacity to detect similarity with that of other known measures on real data.  "
957,Learning with Feature Concatenation,"We propose a flexible hierarchical Bayesian framework for integrating multiple views based on feature concatenation, which can seamlessly incorporate any priors from generative model and any convex losses used in discriminative model. By virtue of the probabilistic interpretation of this framework, we uncover the connection from feature concatenation to multiple kernel learning (MKL) and weighted voting of view classifiers. This connection inspires a novel feature concatenation formula with the exponential distribution as the prior in the fully Bayesian inference, and also provides an elegant extension of many existing MKL formulations via the generalized maximum likelihood method. Moreover, under certain mild condition, the resulting optimization problems are convex. Experiments on image classification and UCI datasets illustrate the benefits of our proposed framework."
958,Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization,"We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\cite{kRegExpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice. "
959,Developmental Stage Annotation of Drosophila Embryos using Transfer-Active Learning,"Drosophila melanogaster is the major model organism for explicating the function and interconnection of animal genes and for establishing a better understanding of human diseases. Today, images capturing gene expression have unprecedented spatial resolution, resulting in high quality maps (expression images) in model organisms. Based on morphological landmarks, the continuous process of Drosophila embryo genesis is traditionally divided into a series of consecutive stages (e.g., 1-16). The manual annotation of developmental stage of an image of an embryo is done by an expert. This manual annotation process is very expensive and time consuming. It is, therefore, tempting to design computational methods for the automated annotation of gene expression patterns. Images of geneexpression patterns vary between different image databases based on their imaging techniques and resolutions, leading to distribution difference across databases.Hence, it is a challenge to directly use an available annotated image database for building a classifier for a new database of images. In this paper, we propose a transfer and active learning based computational method to develop a classification system for annotating the gene expression patterns. Transfer learning or domain adaptation is performed on the labeled database and active sampling is performed on the new set of images.The proposed framework performs domain adaptation and active sampling simultaneously by  minimizing a common objective of reducing distribution difference between the domain adapted source, the queried samples and the rest of the unlabeled target domain data. Our empirical studies on synthetic and the Drosophila image databases demonstrate the potential of the proposed approach."
960,Boosting for Multiclass Semi-Supervised Learning,"We focus on multiclass Semi-Supervised learning which islearning from a limited amount of labeled data and plenty of unlabeled data. Most of the existing semi-supervised algorithms use approaches such as one-versus-all to convert the multiclass problem to several binary classification problems which can have various problems, likeunbalanced classification problems. We propose amulticlass semi-supervised boosting algorithm that solves multiclass classification problems directly. Our algorithmworks based on two main semi-supervised assumptions, themanifold and the cluster assumption, to exploit the novel information from the unlabeled examples to improve the classification performance. It uses a novel loss functionconsisting of the margin cost on labeled data and two regularization terms on labeled and unlabeled data. Experimental results on a number of UCI datasets show that the proposed algorithm performs better than the state-of-the-art boosting algorithms for multiclass semi-supervised learning."
961,Analytic Tuning of the Elastic Net With an Application to Text Mining,"The elastic net is a proposed compromise between the stability of ridge regression and the model selection properties of the lasso. It is particularly useful for conservative model selection in the presence of data with highly correlated covariates, where the lasso is known to behave poorly. Tuning the regularization parameter in the elastic net via cross-validation, however, greatly reduces its model selection properties. We provide novel analytic tuning values for the elastic net estimator along with finite sample guarantees for parameter estimation. Our tuning method is applied to age prediction in a standard text corpus of internet blogs."
962,The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes,"Stochastic differential equations (SDE) are a natural tool for modelling systemsthat are inherently noisy or contain uncertainties that can be modelled as stochasticprocesses. Crucial to the process of using SDE to build mathematical modelsis the ability to estimate parameters of those models from observed data. Overthe past few decades, significant progress has been made on this problem, butwe are still far from having a definitive solution. We describe a novel methodof approximating a diffusion process that we show to be useful in Markov chainMonte-Carlo (MCMC) inference algorithms. We take the ?white? noise that drivesa diffusion process and decompose it into two terms. The first is a ?colourednoise? term that can be deterministically controlled by a set of auxilliary variables.The second term is small and enables us to form a linear Gaussian ?small noise?approximation. The decomposition allows us to take a diffusion process of interestand cast it in a form that is amenable to sampling by MCMC methods. We explainwhy many state-of-the-art inference methods fail on highly nonlinear inferenceproblems. We demonstrate experimentally that our method performs well in suchsituations. Our results show that this method is a promising new tool for use ininference and parameter estimation problems."
963,eXclusive Independent Component Analysis,"Independent Component Analysis (ICA) has been widely investigated for the both of the problems of blind source separation and efficient coding. In most of the relevant studies, the independent components of a signal are concerned. However, the exclusiveness of the components between two signals are rarely discussed. It is important to study the common basis set of two data sources as their complement basis sets. Thus, we introduce a method for finding the exclusive basis sets based on ICA, the eXclusive Independent Component Analysis (XICA). It aims to exclude the common features of two or more given signals from the the basis set in order to obtain more significant features of the given signals.In this paper, we investigated two image sets: nature images and urban images. The nature image set is composed by natural scene, and the urban image set contains artificial objects, such as buildings, cars, phones, etc. By using XICA, we will be able to see what is the essential difference between the nature and urban image sets in terms of their independent components."
964,Elite Opposition-based Particle Swarm Optimization,"Traditional particle swarm optimization (PSO) algorithm tends to suffer from premature convergence. We present a novel PSO in an attempt to solve this demerit based on a new elite opposition-based learning (EOBL) strategy. In EOBL, the elite particle, as a new concept, is introduced for highlighting the important role of particle?s best own experience. The aim of EOBL is to discover more useful information of elite particles by searching for their opposite solutions, which provide more chances of finding the global optimum in complex landscape of functions. This approach can be considered as an effective way to enhance the exploration ability of PSO. As another contribution of this paper, a new differential evolutionary mutation is presented to enhance the exploitation ability of PSO by searching for the neighbors of the global best particle, which is helpful to avoid the entire swarm being trapped into local optima. Experiments are conducted on a comprehensive set of benchmarks, the results compared with other nine state-of-the-art PSOs show that our proposed algorithm obtains promising performance."
966,Incorporating Hidden Knowledge through a Latent Variable,"A teacher provides a student prior knowledge such that the student learns more efficiently and makes fewer mistakes. Although prior knowledge plays significantly important role in human learning, it has not yet been thoroughly investigated in machine learning. Among different sources of knowledge, hidden knowledge, the information that is only available during training but not testing, has a wide variety of applications and great potential to bring huge benefits. But it remains challenging to capture the hidden knowledge and to incorporate them into the learning process. In this work, we take the first step to exploit and encode the useful information within the hidden knowledge into the learning task through a discrete latent variable. Experiments on gesture recognition demonstrated that hidden knowledge could be successfully captured and incorporated to learn more discriminative and generalizable hidden state models for classification."
967,Lazy Robust Derivative-Free Optimization for Hyperparameter Tuning,"Stochastic response surfaces pose a challenge to derivative-free optimization algorithmsthat rely on sampled gradients, empirical simplexes, or pointwise valuecomparisons. Achieving robustness can require averaging across many samples,which may be computationally expensive. This paper addresses both robustnessand efficiency in derivative optimization. It describes how the classic Nelder-Mead algorithm can be combined with statistical hypothesis testing to efficientlydeal with noisy evaluations that are commonplace when tuning machine learningalgorithm hyper-parameters. Employing lazy our lazy evaluation strategy leads toan efficient yet statistically sound algorithm, called Lazy Robust Nelder-Mead(LRNM). Empirical evaluation demonstrates the benefits of the approach overpopular alternatives."
968,Learning Low-rank Nonparametric Kernel Matrices From the Point of View of Matrix Completion,"Many existing nonparametric kernel learning methods suffer from high computational cost, which limits theirapplications to large scale real-world problems. In this paper, we propose a novel nonparametric kernel learning method based on the matrix completion technique, which emphasizes the low rank property of the learned kernel matrices. Given some pairwise constraints, we formulate the nonparametric kernel learning problem into a rank minimization problem with  constraints that enforce the learned similarities between the known data pairs equal the values of the known pairwise constraints. The resulting optimization problem can be relaxed to a convex optimization problem of minimizing nuclear norm with graph Laplacian regularization. We then develop a singular value thresholding like algorithm to solve the constrained convex problem using the similar techniques as in the matrix completion problems. Preliminary experimental results show that our algorithm performs better than or comparably to the existing best method BCDNPKL of [10] in terms of clustering accuracy and scalability."
969,Model checking For Improved Adaptive Behaviour,"Closed loop systems are traditionally analysed using simulation. We show how a formal approach, namely model checking, can be used to enhance and inform this analysis. Specifically, model checking can be used to verify properties for any execution of a system, not just a single experimental path. We describe how model checking has been used to investigate a system consisting of a robot navigating around an environment, avoiding obstacles using sequence learning. We illustrate the power of this approach by showing how a previous assumption about the system, gained though vigorous simulation, was demonstrated to be incorrect, using the formal approach."
970,Learning as MAP Inference in Discrete Graphical Models,"We present a new formulation for attacking binary classification problems. Instead of relying on convex losses and regularisers such as in SVMs, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but \emph{discrete} formulation, where estimation amounts to finding a MAP configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees. Due to the discrete nature of the formulation, it also allows for \emph{direct} regularisation through cardinality-based penalties, such as the $\ell_0$ pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner. We also outline a number of open problems arising from the formulation."
971,Parallel and Asynchronous Reinforcement Learning from Customer Interaction Sequences,"In this paper, we explore applications in which a company interacts with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur asynchronously and in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for reinforcement learning in this setting, using an asynchronous variant of temporal-difference learning to learn efficiently from partial interaction sequences. We applied our asynchronous TD algorithm to two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records. Our TD algorithm achieved good performance in both scenarios. It significantly outperformed comparable approaches that learn only from complete interaction sequences, as well as bandit-based approaches that do not exploit the sequential nature of the problem."
972,On a novel construction of the region-graph for the Generalized Belief Propagation,"Numerous inference problems in statistical physics, computer vision or error-correcting coding theory consist in approximating the marginal probability distributions on Markov Random Fields (MRF). The Belief Propagation (BP), an iterative message-passing algorithm, is an accurate solution that is optimal if the MRF is loopfree and supoptimal otherwise. The Low-Density Parity-Check codes used to protect sequences of bits sent through noisy channels, have a graphical representation, the Tanner graph, that is a particular MRF. This graph is a media for the BP algortithm that provides marginal probability distributions of the bits. Loops and combination thereof of the Tanner graph prevent the BP from being optimal, especially harmful topological structures called the trapping-sets. To circumvent this problem, the BP has been extended to the Generalized Belief Propagation (GBP), a message-passing algorithm that runs on a non unique mapping of the Tanner graph, namely the region-graph. Its nodes are gatherings of the Tanner graph nodes, that provides the possibility to absorb loops in a regiongraph, making the GBP more accurate than the BP. Relevant performance can be obtained by constructing the region-graph according to the topology of the Tanner graph. In this article, we expose a region graph construction suited to the Tanner code, an LDPC code whose Tanner graph is entirely covered by trapping-sets.Furthermore, we make use of classical and novel estimators to investigate the behavior of the GBP considered as a dynamic system in order to understand the way it evolves in terms of the Signal-to-Noise Ratio (SNR)."
973,Active Learning with Hinted Support Vector Machine,"The abundance of real-world data and limited labeling budget calls for active learning, which is an important learning paradigm for reducing human labeling efforts. Many recently developed active learning algorithms consider both uncertainty and representativeness when making querying decisions. However, considering representativeness with uncertainty concurrently usually requires tackling other sophisticated and challenging learning tasks, such as clustering. In this paper, we propose a new active learning framework, called hinted sampling, which takes both uncertainty and representativeness into account in a simpler way. We design a novel active learning algorithm within the hinted sampling frameworkwith an extended support vector machine. Experimental results validate that the novel active learning algorithm can result in a better and more stable performance than that achieved by state-of-the-art algorithms."
974,Nonparametric Bayesian Double Articulation Analyzer,"In this paper, we propose a new Bayesian model for fully unsupervised segmentation and chunking method for continuous time series data. The proposed method, nonparametric Bayesian double articulation analyzer, presume that there are double articulation structure inside of the observed time series data and estimated the hidden double articulation structure without knowing the number of hidden states (letters) and the number of chunk types (words). An efficient approximate sampling procedure is introduced to extracts hidden words and letters. Our experiments using synthetic data shows that the nonparametric Bayesian double articulation analyzer could find hidden word changing points.  "
975,A mechanistic model of early sensory processing based on subtracting sparse representations,"Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics."
976,Learning Canonical Correlations of Paired Tensor Sets via Tensor-to-Vector Projection,"Canonical correlation analysis (CCA) is a useful technique for measuring relationship between two sets of vectorial data. We propose a multilinear CCA (MCCA) method for paired tensorial data sets. Different from existing multilinear variations of CCA, MCCA extracts uncorrelated features under two architectures while maximizing paired correlations. One architecture imposes set-wise zero-correlation constraint while the other requires cross-correlation between different pairs to be zero. This is achieved through a pair of tensor-to-vector projections, estimated via a successive and iterative approach. We evaluate MCCA on matching facial images of different poses against competing solutions to show its superiority. We also study fusion of two architectures and observe small performance improvement, implying some complementary information captured."
977,Model Selection by Measuring Validated Information,"Which model best generalizes the available data?This study addresses the task of model selection.We introduce a general validation principle which is applicable to non-i.i.d. scenarios.Validated Model Information aims at selecting models whose predictions are optimally informative.The selection is performed by measuring quantifying predictive information, that is structure extracted from genuine regularities.This is in contrast to descriptive information, which includes accidental fluctuations due to noise.VMI exhibits stability with respect to resampling and Turing-universality.It is non-computable, yet effectively approximated by coding.Firstly, we evaluate the principle in a controlled setting with synthetic data.Then, we demonstrate its applicability with real biological data and external verification.Particular emphasis is given to the task of cluster model selection."
978,Multi-Stage Multi-Task Feature Learning,"Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex regularization problem, which is usually suboptimal, due to its looseness for approximating an $\ell_0$-type regularizer. In this paper, we propose a non-convex regularized formulation for multi-task sparse feature learning; we propose to solve the non-convex optimization problem by employing a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms."
979,Latent Configuration Clustering for Discovering Action Exemplars in Video,"We introduce Latent Configuration Clustering (LCC) for the efficient clustering of data elements residing in a latent space. We are given only a distance function with which to measure relative proximity between pairs of points, but do not know the global positions of the samples. Thus, spatial partitioning techniques can not be applied. LCC generates a sparse affinity matrix in O(N log N) time without having to map the points to a Euclidean configuration. Once the affinity matrix is computed, standard clustering techniques can be applied effectively. Demonstrating the value of LCC, we discover human actions in video data by clustering subspaces spanned by video segments. The proposed method identifies exemplars from the clusters that can provide insight into the data set and be used in reducing nearest-neighbor classification to nearest-exemplar. Through unsupervised exemplar selection, we demonstrate accurate action classification using only a few exemplars from each cluster. Our nearest-exemplar unsupervised action classification demonstrates superior performance to existing unsupervised methods on a popular benchmark data set."
980,Neural Network Models for Multilabel Learning,"Multilabel learning is an extension of standard binary classification where the goal is to predict a set of labels (which we call tags) for a given input example. There have been many models proposed for this problem, such as subspace learning, kernel methods, and nearest neighbour schemes. Recently, the probabilistic classifier chain method was proposed, which learns a series of probabilistic models that attempt to capture tag correlations. In this paper, we show how this model may be interpreted as a neural network with connections amongst output nodes. We argue that using an explicit hidden layer instead brings several advantages, such as tractable test-time inference, and removing the need for fixing a tag ordering. Further, the hidden units capture nonlinear latent structure that both improves classification performance and allows for interpretability. Compared to previous neural network models for multilabel learning, we discuss several design decisions that have a significant impact on training the network. Empirical results show that the model outperforms several existing methods."
981,Detection of collective temporary stationary regions using multi-resolution time analysis,"Detecting temporal stationary regions in stochastic data has great value in signal forecasting applications. It's uses include early detection of earthquakes by analyzing seismic signals, predicting epileptic seizures by processing EEG signals, and in this research forecasting short term trends in the financial market. In this work we formulate and empirically examine a detection scheme, aimed to detect collective temporary stationary regions using a multi-resolution cross-correlation approach. We demonstrate how by applying a multi-resolution approach to collective holographic techniques taken from the world of band-limited EEG signal, processing can be used to detect temporal stationary regions in non-band-limited signals, specifically in intra-day financial signals, and make short-term trend predictions. A modification of the Functional Holographic time domain analysis [2] is performed using a multi-resolution approach in order to adapt to the non-band-limited signal characteristics. Using the multi-resolution collective holographic approach, stochastic NYSE financial signals are presented as nodes in multiple reduced correlation spaces, where the number of spaces is the number of resolutions that are used in the multi-resolution analysis. To simulate and validate the proposed detection scheme, a market indicator is formulated and tested over historical intra-day NYSE data. The financial theory which is the foundation of the proposed indicator is the existence of stealth traders that use trading strategies such as small market injection that inherit a temporal collective behavior on certain high liquid financial assets. The findings in this work verify the computational applicability of the time domain multi-resolution analysis of non-band-limited signals and also statistically model the time duration where the market exhibits temporal stationary properties."
982,Recursive Gaussian Process Regression,"For large data sets, performing Gaussian process regression is computationally demanding or even intractable. If data can be processed sequentially, the recursive regression method proposed in this paper allows incorporating new data with constant computation time. For this purpose two operations are performed alternating on a fixed set of so-called basis vectors used for estimating the latent function: First, inference of the latent function at the new inputs. Second, utilize the new data for updating the estimate of the latent function at the basis vectors. By means of numerical simulations it is shown that the proposed approach significantly reduces the computation time compared to existing on-line and/or sparse Gaussian process regression approaches."
983,Robust Bijective Vector-Valued Function Learning by Jointly Learning Its Inverse,"We discuss about a quite challenging problem in this paper: given the train data with large proportion of the outliers, the goal is to robustly estimate a bijective vector-valued target function. The existing methods only learn the target function itself or learn its inverse respectively and thus can?t handle the outliers well. To address this problem, we propose a robust method for bijective vector-valued function learning. In this approach, the target function and its inverse are bounded together and then jointly optimized under the maximum likelihood estimation (MLE) framework. By associating each sample with a latent variable that indicates whether the sample is an inlier, Expectation Maximization algorithm is employed to solve the MLE problem. To show its usefulness, the proposed method is applied to solve a fundamental problem of computer vision tasks. In detailed, given a set of putative point correspondences between two images that large proportion of correspondences are mismatches, the objective is to estimate a mapping function which can identify correct matches as inliers and distinguish the mismatches as the outliers. The experimental results have demonstrated that our proposed method is very robust and outperforms the state-of-the-art methods."
984,A Coarse-to-Fine Approach to Flexible Activity Discovery and Data Segmentation,"The growing number of mobile sensors allow collection of activity data at differentlevels of complexity and at fine-grained temporal resolution. However,accurately translating unlabeled sensor streams into meaningful activity classesremains non-trivial. The primitives that comprise an activity are usually foundin multiple activity classes; activities exhibit high-order temporal dependenciesamong primitives; and these higher-order dependencies vary from activity to activityand are blended together in the data. This paper presents a novel unsupervisedcoarse-to-fine activity discovery framework that handles the temporal heterogeneitypresent in sequences of sensor data and automatically segments activitieseven when the dependency order is not known and not fixed across activities.Our framework designs a Mixed Memory Latent Dirichlet Allocation (MM-LDA)model that iteratively discovers sequential transition patterns, segments the sensordata accordingly, and groups contiguous activity samples using a locality metric.We demonstrate the effectiveness of the approach by empirical experimentationon real-world datasets."
985,Towards utilization of neural correlates of loss of control for enhanced human-machine interaction,"Perceived loss of control during human-machine interaction (HMI) is a well-known problem that reduces the usability of a technical device dramatically. Recently, it was proven that changes in cognitive user state can be detected by a passive Brain-Computer Interface (BCI) in a laboratory setup. Potentially, a passive BCI can significantly improve a given HMI in shared control systems. Applying it in real world applications implies its own  constraints,  e.g. reducing the number of  electrodes and limiting the time needed for calibrating the system for the sake of improved usability. Here, we investigate neural correlates of loss of control with an independent component analysis on a set of 32 channel EEG-datasets recorded from 12 subjects. Even though the number of channels is rather low for ICA, we could identify the level of mental workload on the channel level as well as in the source space to be correlated to loss of control. This outcome forms a first basis for selection of a neuropsychologically meaningful and low-dimensional feature space, which should lead to improved reliability and reduced calibration time for a passive BCI detecting loss of control in real world applications."
986,Removing Localized Corruption from Natural Images,"  Traditional approaches to removing image corruption such as blur or  noise combine a natural image prior with a reconstruction term. The  latter relies on a good generative model of the corruption --- which  may not exist for many distortions encountered in the real world. In  this paper we explore approaches for learning a direct mapping from  the corrupt input image to the clean image, obviating the need for  any kind of generative model. We evaluate the approaches on several  types of synthetic corruption, finding that neural-network based  models perform the best.  Our techniques can be used for many types of localized corruption. We  demonstrate this using photographs of real-world scenes taken behind a pane  of glass with water droplets, akin to a rainy window.  Our model removes most  of the raindrops without significant blur, the first such demonstration of  this application."
987,Semi-supervised Domain Adaptation on Manifolds,"In real-life problems the following semi-supervised domain adaptation scenario is often encountered: We have full access to some source data which is usually very large; The target data distribution is under certain unknown transformation of the source data distribution;Meanwhile only a small fraction of the target instances come with labels.The goal is to learn a prediction model by incorporating information from the source domain that is able to generalize well on the target test instances.We consider an explicit transformation function that maps examples from the source to the target domain, andwe argue that by proper preprocessing of the data from both source and target domains, the feasible transformation functions can be characterized by a set of rotation matrices.This naturally leads to an optimization formulation under the special orthogonal group constraints.We present an iterative coordinate descent solver that is able to jointly learn the transformation as well as the model parameters, while the geodesic update ensures the manifold constraints are always satisfied. Our framework is sufficiently general to work with a variety of loss functions and prediction problems. Empirical evaluations on synthetic and real-world experiments demonstrate the competitive performance of our method with respect to the state-of-the-art."
988,Moving Object Detection and Pixel-Level Localization From Semantic Priors and Topological Constraints,"We describe an approach to incorporate scene topology and semantics into pixel-level object detection and localization. Our method requires {\em video} to determine occlusion regions, and thence local depth ordering, and any visual recognition scheme that provides a score at local image regions, for instance detection probability.  We set up a cost functional that incorporates occlusion cues induced by moving objects, label consistency and recognition priors, and solve it using modern discrete optimization schemes. We show that our approach improves localization accuracy of existing recognition approaches, or equivalently provides recognition labels to pixel-level localization and segmentation."
989,From Deformations to Parts: Motion-based Segmentation of 3D Objects,"We develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3D) poses.  We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.  To allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.  By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty.  Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods."
990,Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,"This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets."
991,Operator-valued kernel-based autoregressive models with application to biological network inference,"Reverse-modeling of gene regulatory network from time-series of gene expression still remains a challenging problem in computational systems biology. Works concerning network inference from temporal data usually rely on sparse linear models or Granger causality tools. A very few address the issue in the nonlinear case. In this work, we propose a nonparametric approach to dynamical system modeling that makes no assumption about the nature of the underlying nonlinear system. We introduce a new family of vector autoregressive models based on operator-valued kernels to identify the dynamical system and retrieve the target network. As in the linear case, a key issue is to control the model's sparsity. We propose an alternate minimization procedure to learn both the kernel and the basis vectors. We show very good results both in  estimation on DREAM benchmarks as well as on the IRMA datasets."
992,Learning features for image classification from text,"The principle of cross-modal regularization, where data from an auxiliary modality is used to regularize classifiers of a principal modality, is exploited to improve image classification. Images and text are first represented by semantic descriptors, composed of their classification scores under classical image and text classifiers.A measure of cross-modal similarity, which defines the similarity of any image to the training texts, is then learned, and used to implement a soft label transfer mechanism, that transfers labels from training texts to the image. This mechanism is finally used to learn a set of cross-modal image classifiers, i.e. classifiers that classify images according to 1) their similarity to training text, and 2) the labels of the latter. The scores of these cross-modal image classifiers are used to augment the semantic image descriptors, acting as {\it cross-modalregularizing features\/}.  This regularization is shown to significantly improve the state-of-the-art semantics based image classification, on three challenging datasets."
993,What Can Pictures Tell Us About Web Pages? Improving Document Search using Images,"Traditional Web search engines do not utilize the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page. In this paper we study whether the {\em content} of the pictures appearing in a Web page can be used to enrich the semantic description of an HTML document and consequently boost the performance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using visual information extracted from the images contained in the pages. The reranker is query-independent and learned from labeled examples during an offline stage. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual information. We demonstrate our approach on the TREC 2009 Million Query Track, one of the premiere benchmarks in large-scale Web page retrieval. We show that the exploitation of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark. "
994,Structured Robust Subspace Learning: Efficient Recovery of Corrupted Low-rank Matrices,"In this paper, a Structured Robust Subspace Learning (SRSL) method is proposed for efficient low-rank recovery. Its main idea is a novel rank-minimization heuristic that imposes the group sparsity under orthonormal subspaces, which enables minimizing the rank by efficient sparse coding algorithms. Theoretical bounds of the group sparsity minimization under orthogonal subspace are given to validate this novel rank-minimization heuristic. In addition, a modified Nystrom method is introduced to further accelerate SRSL such that its sampling-based version (SRSL+) has linear complexity of the matrix size. Extensive experimental results demonstrate that SRSL and SRSL+ provide the state-of-the-art efficiency without compromising the recovery accuracy."
995,Convex Approximation to Mixture Models Using Step Functions,"The {\em parameter estimation} to  mixture models has been shown as a local optimal solution for decades. In this paper, we propose   a  {\em functional estimation} to  mixture models using step functions. We show that the proposed functional inference  yields  a  convex formulation and  consequently the  mixture models  are feasible for a global optimum inference with an asymptotic consistency. Furthermore, the simple gradient ascent method to optimize the convex formulation provides a theoretical explanation and clarification for  the heuristics of clustering by affinity propagation \cite{fdcpmbd07}. The proposed method opens a series of possibilities to achieve global optimal solutions to important  problems such as clustering, graph partitioning, and  finding information cascades in networks."
996,Bayesian estimation of discrete entropy with mixtures of Pitman-Yor process priors,"  We consider the problem of estimating Shannon's entropy H in the  under-sampled regime, where the number of possible symbols may be  unknown or countably infinite.  Pitman-Yor processes (a  generalization of Dirichlet processes) provide tractable prior  distributions over the space of countably infinite discrete  distributions, and have found major applications in Bayesian  non-parametric statistics and machine learning. Here we show that  they also provide natural priors for Bayesian entropy estimation,  due to the remarkable fact that the moments of the induced posterior  distribution over H can be computed analytically. We derive  formulas for the posterior mean (Bayes' least squares estimate) and  variance under such priors.  Moreover, we show that a fixed  Dirichlet or Pitman-Yor process prior implies a narrow prior on H,  meaning the prior strongly determines the entropy estimate in the  under-sampled regime. We derive a family of continuous mixing  measures such that the resulting mixture of Pitman-Yor processes  produces an approximately flat (improper) prior over H.  We  explore the theoretical properties of the resulting estimator, and  show that it performs well on data sampled from both exponential and  power-law tailed distributions."
997,A Geometric take on Metric Learning,"Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way.We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structuregives us a principled way to perform dimensionality reduction and regression according to the learned metrics.Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Combined these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data."
998,Low-rank Panoramas for Street View Videos,"In this paper, we address how to automatically generate panoramas for a street view from a long video sequence. We formulate the problem as one of robust recovery of a low-rank matrix from highly incomplete, corrupted, and deformed measurements (the video frames). We leverage powerful high-dimensional convex optimization tools from compressive sensing of sparse signals and low-rank matrices to solve this problem. In particular, we show how the new method can effectively remove severe occlusions or corruptions (caused by trees, cars, or reflections, etc.), and obtain street panoramas that have very clean global appearance and very accurate global geometry. We also show how our method can automatically and robustly establish pixel-wise accurate registration among all the video frames. We demonstrate the effectiveness of our method by conducting extensive experimental comparison with other state-of-the-art video stitching systems."
999,Novel Sparse Modeling by L2 + L0 Regularization,"We propose a novel sparse modeling method, the combination of L2 with L0 norms, to achieve feature selection while generating a well-predictive model at the same time.For many machine learning applications, feature selection is a crucial technique to construct the subset of features that is sufficient for prediction.We propose a novel sparse modeling framework, L0 elastic net.This framework encourages (1) reducing the redundancy of the predictive model and (2) searching for the most informative values and compact subset features.Furthermore, we propose the solver to search for the solution of L0 elastic net efficiently.As a theoretical analysis, we prove that the derived solution of L0 elastic net matches the minimal value of the L2-generalized objective function.Experimental results show that L0 elastic net is more suited to derive the compact predictive model than other previous regularization methods in a practical computational time."
1000,Cost-Sensitive Online Active Learning for Online anomaly detection,"Online anomaly detection is an important problem in data mining, which enjoys many real-world applications in a variety of domains. In literature, most of existing studies attempt to solve anomaly detection by formulating it as either a classical batch supervised classification task or an online unsupervised learning task. Both of approaches have their limitations in solving a real-world anomaly detection task. In this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL) for online anomaly detection, which goes beyond conventional approaches to solve anomaly detection in a natural, effective and scalable way. We propose two CSOAL algorithms under the proposed framework, and theoretically analyze their performance in terms of cost-sensitive measures, including (i) the lower bound for the weighted sum of sensitivity and specificity achieved by the first algorithm, and (ii) the upper bound for the weighted cost made by the second algorithm. We extensively examine the empirical performance of the proposed algorithms on several challenging anomaly detection tasks, in which encouraging results validate the efficacy of our proposed technique in solving anomaly detection with online active learning."
1001,Fast Hierarchical Topic Modeling via Nonnegative Matrix Factorization,"Hierarchical clustering is one of the cluster analysis tasks which aims at building a hierarchy of topics. Most state-of-the-art algorithms proposed in literature for hierarchical clustering are based on sampling. Their computational costs largely depend on the number of words in the given corpus, which is usually extremely. In this paper, we come up with an efficient hierarchical clustering method using non-negative matrix factorization (NMF), which is a dimension reduction technique that approximates a given matrix by a product of two low rank matrices. To maintain the hierarchy of topics, we design special constraints for low rank matrices, resulting in a novel optimization problem which we propose to solve using coordinate descent algorithms. Experiments on both artificial and real-world data sets demonstrate the effectiveness of our approach."
1002,Low-Rank Modeling via Capped-Trace Norm,"The problem of low-rank modeling has recently received increasing attentions in machine learning. Most problems that directly tackle the rank function are known to be NP-hard and thus many heuristics such as those based on the trace norm have been proposed for low-rank modeling. The convex relaxation based on the trace norm admits a global solution and has theoretical guarantees under certain assumptions. However, the trace norm may not be a good approximation of the rank function in practice, resulting in estimation bias. In this paper, we consider low-rank modeling via the capped trace norm (CTRN) which provides a better approximation of the rank function than the trace norm. The basic idea of the CTRN is to perform thresholding on singular values and reduce the dominating impact of the top singular values. Although the capped $\ell_1$ norm has been well studied in the vector case, to our best knowledge the extension to the matrix case has not been studied in the literature. The practical challenge lies in the efficient optimization associated with the CTRN which is non-convex. We employ the difference-of-convex (DC) programming by decomposing the non-convex CTRN into the difference of two convex functions. By applying the concave-convex procedure, the problem can be iteratively computed via solving convex optimization problems. We present a block coordinate descend algorithm for general problems where the objective and/or the constraint can be a difference-of-convex function, and we present the convergence property of the algorithm. Our convergence proof is much simpler and requires weaker assumptions than existing work. We perform extensive experiments using both synthetic and real data. Our results show that the proposed algorithms outperform many popular heuristics for low-rank modeling, including the trace norm and the Schatten-$p$ norm."
1003,Sample selection bias in unsupervised clustering,Sample selection bias has been studied in supervised settings when the training and test data are drawn from different distributions. We consider the effect of sample selection bias in an unsupervised setting when mixed-membership models are used for population stratification and topic modeling.We examined the effect of biased sampling on the accuracy of unsupervised clustering in terms of learning the low-dimensional representations of objects (documents or individual genotypes). We found that the accuracy of unsupervised clustering using a mixed-membership model is affected to a large extent by the sample used for analysis and how representative it is of the underlying populations. We also propose a correction for sample selection bias that is effective in real applications.
1004,Online Convex-Concave Optimization,"We study online convex optimization (OCO) and online convex-concave optimization(OCC) under a unified framework of variational inequalities (VI). The OCCyields projection-free online learning algorithms, breaking the bottlenecks in OCOwhere the projection onto the constraints set is challenging. We define a new typeof regret based on VI named VI-regret, which includes standard regret in OCO asa special case. We show the prox method has a variation-based VI-regret boundand online alternating direction method has a sublinear regret bound in OCC. Wealso give two examples to show the projection-free OCC."
1005,Learning and Decision Making,"This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization."
1007,On the Numerical Stability and Value Function Stability of Value-Directed Compression for POMDPs,"Value-directed compression (VDC) improves the tractability of a large scale POMDP by finding a basis to project its high-dimensional belief space into a low-dimensional approximation, where the problem can be solved with less computations. Our empirical findings indicate lossless VDC may sometimes produce larger compression errors than lossy VDC truncated to the same compression level due to the trade-off between residual threshold and numerical stability.  This paper analyses the numerical stability and residual error of the lossless and lossy VDC algorithms according to their column selection heuristics, and proposes a slight modification of lossless VDC that has a more tractable condition number. In addition, we show that the factorability of a problem is not the main determiner of the learnability of compressed problems. We discuss a built-indeficiency of VDC that can possibly magnify a distortion in the value function caused by compression errors to an infinitely great degree, which in the worst case will make the quality of the policy optimised based on the compressed POMDP arbitrarily low. This work contributes to the fundamental underlying theory of VDC, with supporting empirical evidence using benchmark POMDP problems."
1008,Training Structured Output Predictors with Heterogeneous Data Sources,"Conventional learning formulations for structured output prediction such as SSVM assume the availability of training instances with known features and structured labels of each instance. In this paper we address the problem of training structured output predictors where in addition to normal labelled training instances, we have instances where, instead of the true label, we know a set of feasible labels where the true label comes from. We show how learning with such heterogeneous training sets can be formulated using specific loss functions in the latent SSVM formulation. To demonstrate our framework, we consider a special case of the semantic image segmentation problem where either full pixel labelling, or image level annotations (weak labelling) might be available and derive max-margin learning procedure for it. Experiments reveal that addition of weakly labelled data increases the performance of SSVM, especially when the number of fully labelled images is small."
1009,Large-Scale Bandit Problems and KWIK Learning,"We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model  known as ``Knows What It Knows'' or KWIK learning. We give matching impossibility results showing that the KWIK-learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time."
1010,Robustness and Generalization for Metric Learning,"Metric learning has attracted a lot of interest over the last decade, but little work has been done about the generalization ability of such methods. In this paper, we address this issue by proposing  an adaptation of the notion of algorithmic robustness,  previously introduced by Xu and Mannor in  classic supervised learning, to derive generalization bounds for metric learning.  We also show that a weak notion of robustness is a necessary and  sufficient condition to generalize well, justifying that it  is fundamental for  metric learning. We provide some illustrative examples of our approach on a large class of existing algorithms."
1011,Wavelet Best-Basis Representation and Reconstruction for Earthquakes Prediction,"Recently, the destructive potential of earthquakes near nuclear power plants has been observed. Early prediction of such earthquake could have reduced the disaster size significantly. This paper introduces a novel machine learning algorithm for early prediction of earthquakes from seismic data. Our proposed method detects changes in the stationarity of the seismic signal by wavelet packet decompositions and reconstructions combined with advanced machine learning techniques. The algorithm is demonstrated on data from the July 10th, 2011 earthquake in Japan. "
1012,Learning the Architecture of Sum-Product Networks Using Clustering on Variables,"The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture significantly improves its performance compared to using a previously-proposed static architecture."
1013,Spike triggered covariance for strongly correlated Gaussian stimuli,"Characterizing feature selectivity is an important problem because it can shed light on how neurons process their inputs. The spike triggered covariance method (STCM) is a very commonly used method to extract the relevant set of stimulus features to which a neuron responds. One of the main advantages of STCM is that it can determine the dimensionality of the cell's relevant subspace. The method has been previously thought to be applicable when stimuli are drawn from a Gaussian ensemble, with or without stimulus correlations.  Here we use random matrix theory to show that when STCM is used with strongly correlated Gaussian stimuli, the null distribution of eigenvalues has a large outstanding mode. As a result, STCM can either yield an extra feature, which often corresponds to the strongest eigenvalue, or fail to yield any significant dimensions. We present a simple correction scheme that removes this artifact and illustrate its effectiveness by analyzing model neurons and recordings from retinal ganglion cells probed with correlated Gaussian stimuli whose second-order statistics was matched to natural stimuli. Our results can serve as guidelines for design of reverse correlation experiments that can help illuminate how neurons are optimized to code natural stimuli."
1014,A Sparsity Nonnegative Matrix Factorization Technique for Correspondences Problems,"Graph matching is an essential problem in computer vision and pattern recognition. In this paper, we present a robust graph matching method based on nonnegative matrix factorization with sparsity constraints. We show that our sparsity NMF based solution is sparse and thus naturally imposes the discrete mapping constraints strongly in the optimization process. Promising experimental results on both synthetic point matching and real world image feature matching tasks show the effectiveness of our graph matching method."
1015,Big-Five Personality Prediction from User Behaviors at Social Network Sites,"A central problem in psychology is the study of personality, which uniquely characterizes a human being through a set of psychological traits. Traditionally, personality assessment is performed by means of manually filling up a self-report inventory. However, its reliability could be influenced by subjective bias including e.g. social desirability bias, and subjective perception may also be unreliable, due to participants limited cognitive ability.In this paper, we propose a computational and objective approach to predict the so called Big-Five personality of an individual from his/her Social Network Site(SNS) behaviors. This is the first such attempt ever to objectively measure the Big-Five Personality from SNS. Inspired by the Multi-Task Learning (MTL) methods from machine learning community, we develop a dedicated learning method to address this problem. Empirical results on SNS behavior dataset demonstrate its superior performance comparing to the state-of-the-art MTL methods. Interestingly, the results suggest that features involving recent behaviors such as recent blog publishing frequency and frequency of making comments are more likely to be related to personality."
1016,Cooperating with a Markovian Ad Hoc Teammate,"This paper focuses on learning in the presence of a Markovian teammate in Ad hoc teams. A Markovian teammate's policy is a function of a set of discrete feature values derived from the joint history of interaction, where the feature values transition in a Markovian fashion on each time step. We introduce a novel algorithm ``Learning to Cooperate with a Markovian teammate'', or LCM, that converges to optimal cooperation with any Markovian teammate that satisfies certain assumptions, and achieves safety with any arbitrary teammate, in tractable sample complexity. The novel aspect of LCM is the manner in which it satisfies the above two goals via efficient exploration and exploitation. The main contribution of this paper is a full specification and a detailed analysis of LCM's theoretical properties. "
1018,Pointwise Tracking the Optimal Regression Function,"This paper examines the possibility of a `reject option' in the context of least squares regression. It is shown that using rejection it is theoretically possible to learn `selective' regressors that can $\epsilon$-pointwise trackthe best regressor in hindsight from the same hypothesis class, while rejecting only a bounded portion of the domain.Moreover, the rejected volume vanishes with the training set size, under certain conditions. We then develop efficient and exact implementation of these selective regressors for the case of linear regression. Empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error."
1019,"Phase-Invariance, Sparsity and Image Category Discrimination","Approaches to image object recognition and categorization have tended to favour Gaussian scale space over Gabor-like representations. This is largely due to the success of descriptors based on Histograms of Gradients (HoG) to create efficient ndexing schemes. In this paper, we look at the alternative epresentation of complex-valued directional filters, widely used in face and texture recognition, and as models for peak rate of firing for phase-invariant, complex cells in primary visual area V1. We ask the question: can such Gabor-like models  be equipped with focus-of-attention operators and effective region descriptors, as for Gaussian scale-space ? We present some conclusions to this research: first, optimizing the tiling of Fourier space to encourage sparse coding turns out to be important to achieving good performance in interest-point localization and scale estimation. We suggest a simple spatial pooler, and find that a Winner-Take-All (WTA) approach to encourage sparse descriptors, gives better performance than without this step. We use   Area-Under-Curve (AUC) measures applied to Receiver Operator   Characteristic (ROC) curves of descriptor distances within and  between image classes. Results are assessed on standard image atabases used for categorization performance in computer vision. The significance of this work is that systems based on complex direction-selective filters can, with suitable adjustments, achieve the scalability of keypoint-based approaches, and potentially yield performance that is state-of-the-art.  This would potentially remove the need for parallel feature stacks in general-purpose vision systems designed to handle a wide variety of object classes and vision tasks."
1020,Reconstructing ecological networks with hierarchical Bayesian regression and Mondrian processes,"Ecological systems consist of complex sets of interactions among species and their environment, the understanding of which has implications for predicting environmental response to perturbations such as invading species and climate change.  However, the revelation of these interactions is not straightforward, nor are the interactions necessarily stable across space. Machine learning can enable the recovery of such complex, spatially varying interactions from relatively easily obtained species abundance data. Here, we describe a novel Bayesian regression and Mondrian process model (BRAMP) for reconstructing species interaction networks from observed field data. BRAMP enables robust inference of species interactions considering autocorrelation in species abundances and allowing for variation in the interactions across space. We evaluate the model on spatially explicit simulated data, produced using a trophic niche model combined with stochastic population dynamics. We compare the model's performance against L1-penalized sparse regression (LASSO) and non-linear Bayesian networks with the BDe scoring scheme. Finally, we apply BRAMP to real ecological data."
1021,Bayesian nonparametric models for bipartite graphs,"We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, an Indian Buffet-like generative process for network growth, and a simple and efficient Gibbs sampler for posterior simulation. Our model is shown to be well fitted to several real-world social networks."
1022,Reducing statistical time-series problems to  binary classification,"We  show how binary classification methods developed to work on i.i.d.\ data can be used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solvging these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data."
1023,Tractable Objectives for Robust Policy Optimization,"Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance.  One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations.   In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty.  Instead we focus on identifying optimization objectives for which solutions can be efficiently approximated.  We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efficiently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP.  "
1024,Learning a manipulation task of cascaded blocks with passive joints from inexpert demonstrations using sample based stochastic policy,"Reinforcement learning (RL) methods have been successfully applied tothe learning of the robot controland becomes to allow a robot to learn various kinds of realistic tasksby designing the policy functionwith a prior knowledge about the control task.We present a novel representation of a stochastic policybased on a non-parametric model to obtain the knowledge directly from the demonstrations by human teacher.In this method, the action is selected from stored samples,and the importance of each sample is trainedby an RL method as a policy parameter.Since samples are generated from demonstrations,our method is expected to allow the robot to extractuseful knowledge included in human instructions.We applied our method to the manipulation task of a cascaded rigid linksand experimental results show that a good controller can be obtainedby our method."
1025,Estimating noise correlations under signal drift,"Large data in biological and information sciences often show nonstationary trends andare beyond the scope of the conventional methods under assumption of stationary.Especially, if infinitely many possible trends can occur unpredictably, it is difficult to tackle them with a single algorithm without previous knowledge.However, it is possible to estimate interesting statistical parameters from the data with unpredictable drifts for some semiparametric statistical models.In this paper, we consider a semiparametric, mixture of Gaussian models where the trend distribution is not restricted at all.We propose estimators of noise correlations for multivariate time series with brain signals in mind and demonstrate thatit works robustly against any unpredictable drift in signals (means) while the conventional correlogram shows spurious correlations contaminated by the temporal drift."
1026,Classification Calibration Dimension for General Multiclass Losses,"We study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of \emph{classification calibration dimension} of a multiclass loss matrix, which measures the smallest `size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al.\ (2010) for analyzing the difficulty of designing `low-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems."
1027,No voodoo here! Learning discrete graphical models via inverse covariance estimation,"We investigate the relationship between the support of the inverses of generalized covariance matrices and the structure of a discrete graphical model. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph. Our work extends results which were previously established only for multivariate Gaussian distributions, and partially answers an open question about the meaning of the inverse covariance matrix of a non-Gaussian distribution. We propose graph selection methods for a general discrete graphical model with bounded degree based on possibly corrupted observations, and verify our theoretical results via simulations. Along the way, we also establish new results for support recovery in the setting of sparse high-dimensional linear regression based on corrupted and missing observations."
1028,Online Information-Geometric Change Detection with Exponential Families,This paper studies online change detection with exponential families. We formulate a generic statistical framework for sequential abrupt change detection and introduce generalized likelihood ratio test statistics with arbitrary estimators. We show intrinsic links of these statistics with maximum likelihood estimates and interpret this within the context of information geometry. It provides a unifying view of change detection for many common statistical models and corresponding distances. We also derive a computationally efficient scheme for change detection based on exact generalized likelihood ratios with maximum likelihood estimators. This scheme is applied to synthetic and real-world datasets of various natures.
1030,Feature Selection at the Discrete Limit,"In this paper, we propose to use L2,p for feature selection with emphasis on small p. As p ? 0, feature selection becomes discrete feature selection problem. We provide two algorithms, proximal gradient algorithm and rank-one-update algorithm which is more efficient at large regularization \lambda. Experiments on real life datasets show that features selected at small p consistently out-perform features selected at p = 1, the standard L2,1 approach."
1031,Collaborative Gaussian Processes for Preference Learning,"We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms."
1032,Efficient 3D Kernel Estimation for Non-uniform Camera Shake Removal,"Non-uniform camera shake removal is a knotty problem which plagues the researchers due to the huge computational cost of high-dimensional blur kernel estimation. To address this problem, we propose to estimate a 3D blur kernel from its 2D projections, which are computed from image patches, by solving a linear equation system. Under this scheme, we propose a perpendicular acquisition system to increase the projection variance of 3D kernel and thus decrease ill-posedness. Correspondingly, the 3D kernel estimator is obtained by efficient intersection operation. Finally, a RANSAC-based framework is developed to raise the robustness to estimation error of 2D local blur kernels. In experiments, we test our algorithm on both synthetic and real captured data, and both results show that results validate the effectiveness and efficiency of our approach."
1033,Conditional gradient algorithms for regularized learning,"We consider the problem of optimizing learning objectives with a regularization penalty in high-dimensional settings. For several important learning problems, state-of-the-art optimization approaches such as proximal gradient algorithms are difficult to apply and do not scale up to large datasets. We propose new conditional-type algorithms, with theoretical guarantees, resp. for norm-minimization and penalized learning problems. Promising experimental results are presented on two large-scale real-world datasets. "
1034,Approximating Concavely Parameterized Optimization Problems,"We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\varepsilon >0$ by a set of size $O(1/\sqrt{\varepsilon})$. A lower bound of size $\Omega (1/\sqrt{\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\sqrt{\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion."
1035,Lasso Screening Rules via Dual Polytope Projection,"Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. Bytransforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose a fast and efficient screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. We have evaluated our screening rule using both synthetic and real data sets. Results show that our rule is more effective to identifyinactive predictors than existing state-of-art screening rules. We also extend our screening rule to identify inactive groups in group Lasso."
1036,Gradient-based kernel method for feature extraction and variable selection,"We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter finds a subset of predictors. First, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method.  In comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets.  Second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by Chen et al. (2010).  Experimental results show that the proposed methods successfully find effective features and variables without parametric models."
1037,Domain-Unifying Embedding,"We propose Domain Unifying Embedding, together with its kernelized version, as a consolidated framework for domain adaptation and cross-domain recognition. Our approach embeds samples from one or more source domains and a target domain into a single latent shared domain, with the embedding represented by a linear or kernel transformation. In addition to allowing an arbitrary number of source domains, the approach allows these sources to be heterogeneous in the sense of having different dimensions. It also allows simultaneously exploiting a variety of types of semi-supervision, including target samples with explicit class labels when available, and instances for which corresponding, but unlabeled, samples are available in two or more domains."
1038,Rational impatience in perceptual decision-making: a Bayesian account of discrepancy between two-alternative forced choice and Go/NoGo Behavior,"Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes."
1039,MEAN-FIELD ANNEALING BASED COMMITTEE MACHINES  Estimation of Weather-Related Outages in Power Distribution Systems,"The reliability of electrical delivery is an important concern for utility companies. Weather related outages have a significant impact on it. There are many regression based models to estimate outages from weather factors in overhead distribution system. This paper proposes the use of committee machines composed of multiple neural networks to estimate outages. A major challenge for using a committee machine is to properly combine predictions from multiple networks, since the performance of individual networks is input dependent due to mapping misrepresentation. This paper presents a new method in which the individual network predictions are combined dynamically. The error minimization is performed using the mean field annealing theory. Results obtained for the study area in Kansas are compared with observed outages to evaluate the performance of the model for estimating these outages. The results are also compared with previously studied regression and neural network models to determine an appropriate model to represent effects of wind and lightning on outages."
1040,Structuring Relevant Feature Sets with Multiple Model Learning,"Feature selection is one of the most prominent learning tasks, especially in high-dimensional datasets in which the goal is to understand the mechanisms that underly the learning dataset. However most of them typically deliver just a flat set of relevant features and provide no further information on what kind of structures, e.g.feature groupings, might underly the set of relevant features. In this paper we propose a new learning paradigmin which our goal is to uncover the structures that underly the set of relevant features for a given learning problem.We uncover two types of features sets, non-replaceable features that contain important information about the targetvariable and cannot be replaced by other features, and functionally similar features sets that can be used interchangeably in learned models, given the presence of the non-replaceable features, with no change in the predictive performance. To do so we propose a new learning algorithm that learns a number of disjoint models using a model disjointness regularizationconstraint together with a constraint on the predictive agreement of the disjoint models. We explore the behavior of ourapproach on a number of high-dimensional datasets, and show that, as expected by their construction, these satisfy a number of properties. Namely, model disjointness, a high predictive agreement, and a similar predictive performance to models learned on the full set of relevant features. The ability to structure the set of relevant features in such a manner can become a valuable tool in different applications of scientific knowledge discovery."
1043,Hypergraph-based Gaussian Process Models with Qualitative and Quantitative Input Variables,"Most existing Gaussian process models assume that all the input variables are quantitative, which makes them fall short in many applications that involve both qualitative and quantitative inputs. The fundamental challenge for enabling GP models on these applications is the design of a proper correlation function. We develop such a correlation function based on hypergraph and the associated Laplacian matrix. Compared with existing models, the proposed model requires much fewer free parameters and shows better performance on benchmark examples."
1044,Topographic Analysis of Correlated Components,"Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants area assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the data where the components can have linear or higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations that are sensitive to linear or higher order correlations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data."
1045,Exact inference in flipper graphs and their use for approximate inference using dual decomposition,"We show exact inference is possible for a subclass of binary labeled graphs that we call flipper graphs. These are graphs where the submodular and non-submodular edges follow a specific pattern-- essentially that an isomorphism exists to a fully submodular graph. Examples of flipper graphs include tree-structured graphs, submodular graphs, and bipartite graphs with all non-submodular edges. We investigate the use of flipper graphs in dual decomposition, based on two different decomposition strategies. Experimentally, we find that decomposition using flipper graphs outperform traditional tree-based dual decomposition."
1046,Toward automated semantic leaf categorization by image analysis,"Understanding the diversity of the plant community is a crucial issue for the development of many botanical industries as well as for the conservation of the ecosystem biodiversity. Traditionally, botanists have proposed detailed key descriptions (generally qualitative) or concepts about the morphology of plants and particularly of leaves that allow them to construct relationships between different plants and between them and their species. However, extracting these concepts is complicated, painstaking and can only be carried out by experts. One way to accelerate and broaden the use of these key descriptions is to automatically extract them directly from images. In this paper, we focus mainly on one of the most commonly used key leaf descriptions which is the foliage arrangement (simple, compound, pinnate, palmate). To do so, we analyse the spatial distribution of the leaf contour points and mainly its maxima (concave and convex) and inflexion points. For each category, we define a particular geometric feature that describes its point distribution. We test the proposed method on real world leaf images (Pl@ntLeaves scans). The experiments have demonstrated the robustness of our algorithm for a high number of leaf species (70 species) and even in the presence of some distortions (such as rotation, some leaf damage, partial leaf overlapping). In addition to its accuracy, the proposed approach satisfies real-time requirements with a low computational cost."
1047,3D Scene Grammar for Parsing RGB-D Pointclouds,"We pose 3D scene-understanding as a problem of parsing in a grammar.  A grammar helps us capture the compositional structure of real-word objects, e.g., a chair is composed of a seat, a back-rest and some legs. Having multiple rules for an object helps us capture structural variations in objects, e.g., a  chair can optionally also have arm-rests. Finally, having rules to capture composition at different levels helps us formulate the  entire scene-processing pipeline as a single problem of finding most likely parse-tree---small segments combine to form parts of objects, parts to objects and objects to a scene. We attach a generative probability model to our grammar by having a feature-dependent probability function for every rule. Our model can be trained very efficiently (within seconds), and it scales only linearly in with the number of rules in the grammar.   We show that we obtain good parse trees on 84 real point-clouds obtained from RGB-D cameras."
1048,On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks,"In this paper, we argue for representing networks as a bag of {\it triangular motifs},particularly for important network problems that current model-based approaches handle poorlydue to computational bottlenecks incurred by using edge representations.Such approaches require both 1-edges and 0-edges(missing edges) to be provided as input, and as a consequence, approximate inference algorithms for thesemodels usually require $\Omega(N^2)$ time per iteration, precluding their application to larger real-world networks.In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality.A triangular motif is a vertex triple containing 2 or 3 edges, andthe number of such motifs is $\Theta(\sum_{i}D_{i}^{2})$ (where $D_i$ is the degree of vertex $i$),which is much smaller than $N^2$ for low-maximum-degree networks. Using this representation, we develop a novelmixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networkswith high maximum degree, the triangular motifs can be naturally subsampled in a {\it node-centric}fashion, allowing for much faster inference at a small cost in accuracy.Empirically, we demonstrate that our approach, when compared to that of an edge-based model,has faster runtime and improved accuracy for mixed-membership community detection.We conclude with a large-scale demonstration on an $N\approx 280,000$-node network, which isinfeasible for network models with $\Omega(N^2)$ inference cost."
1049,"Feature Selection using L_{p,\infty } norm","In this paper, we present a feature selection method using  L_{p,\infty} norm as a regularization term. Compared with standard  L_{p,\infty} feature selection,  L_{p,\infty } feature selection gives more flexible to approach the number of non-zero features/variables, which is the desired goal in feature/variable selection tasks.   We use proximal gradient method to solve  L_{p,\infty } norm problem. An efficient algorithm is proposed to solve the associated proximal operator with rigorous analysis.    Extensive experiments on both multi-class and multi-label feature selection tasks demonstrate the effectiveness of our methods. The experiment results also suggest smaller $p$ values(say p=0.25) give relatively better results. "
1050,Classi?er Calibration: A Bayesian Non-Parametric Approach,"A set of probabilistic predictions is well calibrated if the events that are predicted to occur with probability p do in fact occur about p fraction of the time. Well calibrated predictions are particularly important when machine learning models are used in decision analysis. This paper presents two new non-parametric methods for calibrating outputs of binary classi?cation models: a method based on the Bayes optimal selection and a method based on the Bayesian model averaging. The advantage of these methods is that they are independent of the algorithm used to learn a predictive model, and they can be applied in a post-processing step, after the model is learned. This makes them applicableto a wide variety of machine learning models and methods. These calibration methods, as well as other methods, are tested on a variety of datasets in terms of both discrimination and calibration performance. The results show the methods either outperform or are comparable in performance to the state-of-the-art calibration methods."
1051,Local Features for Robust Manifold Modeling,"Despite the promise of low-dimensional manifold models for various vision and machine learning tasks, their utility has been hamstrung in practice by two fundamental challenges: practical image manifolds are plagued by a large number of nuisance variables and are non-isometric to the parameter space. In this paper, we develop a new manifold modeling, learning, and processing framework that directly addresses these challenges. At the heart of the framework are two key ideas: the use of the Earth Mover's Distance (EMD) to enable isometry of image manifolds to the underlying parameter space, and the use of local image features (such as SIFT features) to enable robust learning even to nuisance articulations.We analytically describe the performance of our approach and propose a fast kernel-based method for approximate EMD calculation to ensure computational efficiency. A powerful application of our approach is the automatic organization of large, unstructured collections of photographs gathered from the internet. "
1052,A non-parametric Bayesian prior for causal inference of auditory streaming,"Perceptual grouping of sequential auditory cues has traditionally been modeled using a mechanistic approach. The problem however is essentially one of source inference ? a problem that has recently been tackled using statistical Bayesian models in visual and auditory-visual modalities. Usually the models are restricted to performing inference over just one or two possible sources, but human perceptual systems have to deal with much more complex scenarios. We have developed a Bayesian inference model that allows an unlimited number of signal sources to be considered: it is general enough to allow any discrete sequential cues, from any modality. The model uses a non-parametric prior, so increased complexity of the signal does not necessitate more parameters. The model not only determines the most likely number of sources, but also specifies the source that each signal is associated with. The model gives an excellent fit to data from an auditory stream segregation experiment in which the pitch and presentation rate of pure tones determined the perceived number of sources."
1053,Low Rank Data Recovery Using Schatten p-Norm,"In this paper, we present Schatten $p$-Norm model for low-rank data recovery. Besides playing the role of data recovery, Schatten p-Norm model is more attractive due to its suppression on the shrinkage of singular values at smaller p.The proposed Schatten p-Norm model can be transformed into solving the proximal operator, and an efficient algorithm based on ALM method is proposed.  Another iterative algorithm is also presented to solve this problem with rigorous convergence analysis.Extensive experiment results on 6 occluded datasets indicate the relatively better data recovery results at smaller p values."
1054,Learning a network from multiple data sources,"Recent methods on estimating the structure of undirected Gaussian graphical models have focused on estimation from a single data source. However, in many real world applications, multiple data sources are available that give information about the same set of nodes. We propose NP-MuScL (non-paranormal multi source learning) to estimate the structure of a sparse undirected graphical model that is consistent with multiple sources of data, having the same underlying relationships between the nodes. We use the semiparametric Gaussian copula to model the distribution of the different data sources, and show how to estimate such a model in the high dimensional scenario. Results are reported on synthetic data, where NP-MuScL outperforms baseline algorithms significantly, even in the presence of noisy data sources. Experiments are also run on two different data sets of yeast microarray, where NP-MuScL predicts a higher number of known gene interactions than existing techniques. "
1055,Modeling 4D Human-Object Interactions for Object Detection,"In recent years, human-object context has been utilized for improving both object detection and action recognition, but most work has been primarily focused on modelling human-object relations in 2D static images and such contextual cues are often compromised due to their sensitivity to viewpoint changes. In this paper, we propose to build human-object interaction models in 3D space plus the time axis, using videos captured by Kineck cameras which provide rather accurate 3D human poses in time and the point clouds for the contextual objects.  Our contextual model are learned form such data and includes three components: i) co-occurrence of human action labels and object labels; ii) geometric compatibility between human body parts and object bounding boxes in 3D space; and iii) the sequence of actions/poses in time are grouped into events to provide temporal context. Such model provides much more reliable and accurate mutual context information for  object detection and action recognition through joint inference which resolve the ambiguities through a top-down and bottom-up computing process. In experiment, we show that our method achieve satisfactory results on an indoor dataset that we collect and will be released to the public. "
1056,Privacy preserving data publishing for Bayesian network structure learning,"In order to prevent the leak of personal electronic health record, the federal Health Insurance Portability and Accountability Act (HIPAA) has set a national standard to protect privacy of this kind of information. In this paper, we propose a privacy preserving data publishing method for Bayesian network structure learning. We come up with a new method that using simulated annealing to find an optimal noise adding to the original dataset so that the resulting dataset will be different from the original one. By publishing this noised dataset, other research institutes are able to use their own Bayesian network structure learning algorithms to build a network whose accuracy is comparable to the accuracy of network built with the original dataset."
1058,Parametric Learning of Generalized Decision Trees,"Decision trees are efficient models for representing piecewise-defined functions. We present gradient based learning algorithms to estimate the parameters for two general classes of decision trees. Decision trees can be distinguished based on their type of split functions. The first class divides the input space into disjunct regions by making hard splits. The second class, which we call generalized decision trees, relaxes the constraints of the split functions to enable a soft partitioning. This richer class is well suited to represent smooth functions, but has an increased computational complexity. The first contribution is a gradient based learning algo- rithm for generalized decision trees, that can learn in on-line settings with limited resources. The second contribution is a learning algorithm for decision trees with hard splits. We derive this learning scheme by combining the learning capabilities of the first algorithm with a continuation method. This is especially useful for post optimizing trees that are constructed with heuristic methods."
1059,Complex Activity Recognition using Granger Constrained DBN (GCDBN) in Sports and Surveillance Video,"Common approaches for modeling interactions of multiple interacting and co-occurring objects in complex activities can theoretically model any number of co-occurring agents or events. However, these can be intractable for complex representations, require manual structure definitions, and/or generatively learn their structures. Our approach involves automatically constraining the nodes and links of a Dynamic Bayesian Network (DBN) in an informative and discriminative manner, resulting in sparse models that are both tractable and improve classification. This is accomplished by explicitly constraining the temporal links based on a direct measure of temporal dependence using Granger Cause statistics, resulting in the Granger Constrained DBN (GCDBN). The experiments show how the GCDBN outperforms other state-of-the-art models in complex activity classification on both handball and surveillance video data."
1060,Robust exponential binary pattern storage in Little-Hopfield networks,"The Little-Hopfield network is an auto-associative computational model of neural memory storage and retrieval.  This model is known to robustly store collections of randomly generated binary patterns as stable-points of the network dynamics.  However, the number of binary memories so storable scales linearly in the number of neurons, and it has been a long-standing open problem whether robust exponential storage of binary patterns was possible in such a network memory model.  In this note, we design elementary families of Little-Hopfield networks that solve this problem affirmitavely."
1061,Shadow Densities for Speeding Up Kernel Methods ,"This paper presents an approach to improve the training and evaluation of kernel manifold learning algorithms relying on spectral decomposition. The approach, called the shadow method, exploits research regarding the spectral decomposi-tion of kernel operators applied to probability distributions. It is used to define the shadow of a kernel density estimate. The shadow density estimate (ShDE)in turn defines shadow KPCA (ShKPCA). For large, redundant datasets ShKPCA improves training and evaluation time of KPCA by an order of magnitude or more, each. A single parameter $\ell$ controls the computational gains. The shadow method is justified through bounds on the density estimate error, on the spectral decomposition error, and on the spectral operator error, all in terms of $\ell$. For low $\ell$ there are large improvements but the method is lossy. Increasing approaches baseline performance and leads to lower speed improvements. Experimentally $\ell$ =  4 works well across a broad spectrum of datasets. Modifications to  improve $\ell$ low performance are given, but with reduced computational gains during training."
1062,Linear Time Solver for Primal SVM,"Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, and designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with proved linear convergence for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easy parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state of the art solvers such as SVMperf, Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms."
1063,Parallel Execution of Self-Organizing Maps,"Self-organizing maps have been noted as useful tools for augmenting scientificdata visualizations, particularly in the case where visualization of multidimensionaldata is required. However, a chief disadvantage associated with the selforganizingmap in this capacity is its large runtime complexity, which may resultin impractical execution times in real-world use cases. Because of the propertiesassociated with neural networks, we tested the feasibility of applying parallel executionto the self-organizing map in an attempt to reduce execution time. Thoughwe had predicted that the parallelization of self-organizing maps would result innear-linear speedup, we have discovered evidence against this hypothesis."
1064,Gaussian Process Model Predictive Control: A Comparison with Conventional Techniques,"Gaussian processes are gaining increased popularity in the area of system identification for control. In this paper a new Model Predictive Control (MPC) algorithm based on a Gaussian process internal model is defined. The framework can be used for modelling and control of arbitrary nonlinear state-space systems, an extension of previous work which only considered modelling of ARMAX models. The performance of the algorithm is then compared with standard MPC and an adaptive Linear Quadratic Regulator (LQR), which use linearised models of the real system, on a benchmark industrial process control problem."
1065,Generic Active Appearance Models Revisited: The Active Orientation Models Paradigm ,"The proposed Active Orientation Models (AOMs) are generative models of facial shape and appearance. Their main differences with the well-known paradigm of Active Appearance Models (AAMs) are (i) they use a different statistical model of appearance (ii) they are accompanied by a robust algorithm for model fitting and parameter estimation and (iii) and, most importantly, they generalize well to unseen faces and variations. Their main similarity is computational complexity. The project-out version of AOMs is as computationally efficient as the standard project-out inverse compositional algorithm which is admittedly the fastest algorithm for fitting AAMs. We show that not only does the AOM generalize well to unseen identities, but also it outperforms state-of-the-art algorithms for the same task by a large margin. Finally, we prove our claims by providing Matlab code for reproducing our experiments."
1066,Compressible Motion Fields,"Traditional video compression methods obtain a compactrepresentation for image frames by computing coarse motion fieldsdefined on patches of pixels called blocks. This piecewiseconstant flow approximation reduces the size of the motion fieldbut introduces block artifacts in the decoded (warped) imageframe. In this paper, we address the problem of estimating densemotion fields that, while accurately predicting one frame from agiven reference frame by warping it with the field, are also\emph{compressible}. We introduce a representation for motionfields based on wavelet bases, and approximate thecompressibility of their coefficients with a piecewise smoothsurrogate function that is relatively easy to optimize. We thenshow how to quantize and encode such coefficients with adaptiveprecision. We demonstrate the effectiveness of our approach bycomparing its performance with a state-of-the-art videocompression algorithm. Experimental results reveal that ourmethod significantly outperforms classical block-based motioncompensation adopted in most modern video compression algorithms."
1067,Efficient and optimal Little-Hopfield auto-associative memory storage using minimum probability flow,"We present an algorithm to store binary memories in a Hopfield neural network using minimum probability flow, a recent technique to fit parameters in energy-based probabilistic models.  In the case of memories without noise, our algorithm provably achieves optimal pattern storage (which we show is at least one pattern per neuron) and outperforms classical methods both in speed and memory recovery.  Moreover, when trained on noisy or corrupted versions of a fixed set of binary patterns, our algorithm finds networks which correctly store the originals.  We also demonstrate this finding visually with the unsupervised storage and clean-up of large binary fingerprint images from significantly corrupted samples."
1069,Smooth Convolutional Stacked Autoencoder for Feature Learning in ECoG Based Brain Computer Interface,"Recent years have seen great interest in ECoG based Brain Computer Interface (BCI) systems. The performance of the BCI systems largely depends on the low-level features used in the decoding algorithm. The currently widely used features are based on frequency decompositions and designed manually, which are only justified empirically. In this work, we describe an automatic feature learning framework for ECoG based BCI decoding algorithms, using a multi-layer deep learning structure we termed as {\em smooth convolutional stacked auto-encoders (SCSA)}. One advantage of SCSA is that it is an open architecture that facilities incorporating various domain-specific constraints, e.g., smoothness of the extracted features over time.  Based on SCSA and data sets of ECoG signals, we demonstrate significant improvement in performance over the current state of the art methods based on manual features. Furthermore, the automatically extracted features from SCSA also shed light on the optimality of those features obtained empirically."
1070,Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL,"Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.  We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging."
1071,Active Learning on Low-Rank Matrices,"Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. With various criteria, we can actively choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many large points as possible. We evaluate our methods on simulated data and show their applicability on movie ratings prediction as well as discovering drug-target interactions."
1072,How safe is your room? 3D scene parsing using intuitive mechanics,"This paper proposes a new perspective for 3D scene parsing through reasoningabout object stability and safety using intuitive mechanics. Our approach utilizesa simple observation that, by human design, objects in static scenes should bestable and safe with respect to various disturbances, such as gravity, earthquakeand daily human activities. Given a 2.5D depth map captured by Kinect cameraor a 3D point cloud for the full scene by SLAM, firstly, our method performsvolumetric reasoning to recover the solid 3D volumes from defective point cloud.Secondly we introduce two new representations for intuitive mechanical reason-ing: i) A disconnectivity graph (DG) to represent the potential energy landscapeswith local minima (stable points), local maxima (unstable equilibrium) and energybarriers between them; and ii) A disturbance field (DF) to represent the possiblephysical work applied to each position in the scene. These allow us to evaluate theunsafeness as the expected risk, such as falling by gravity, knocked off by passing-by human etc. If a 3D entity (voxel in point cloud, shape primitive, or object) is atrisk, our algorithm applies two possible operators: a) attaching or connecting it toother objects to form a larger stable object; b) supporting it by hidden parts in theinvisible volume. By minimizing an energy function defined on both unsafenessand scene prior, our algorithm achieves the following objectives. i) Grouping theunstable primitives hierarchically to form stable objects in a parse graph; ii) Infer-ring hidden parts in the occluded areas; and iii) computing some cognitive maps,such as what area can a cup move freely on the scene, what objects are risky inthe room."
1073,Gradient-based Laplacian Feature Selection,"In many research fields, one is often confronted with very high dimensional noisy  data. Feature selection techniques are designed to find the relevant feature subset that can facilitate classification or pattern detection. Traditional feature selection methods utilize label information to guide the identification of relevant feature subsets. In this paper, however, we consider the unsupervised feature selection problem. Without the label information, it is particularly difficult to identify a small set of relevant features due to the noisy nature of real-world data which corrupts the intrinsic structure of the data. A Gradient-based Laplacian Feature Selection (GLFS) is proposed to select important features by minimizing the variance of the Laplacian regularized least squares regression model. With $\ell_1$ relaxation, GLFS can find a sparse subset of features that is relevant to the Laplacian manifolds. Through extensive experimental evidences using simulated, three real-world object recognition and two computational biology datasets, we illustrate the power and superior performances of our approach over multiple state-of-the-art unsupervised feature selection methods. Additionally, we show that GLFS selects a sparser set of more relevant features in a supervised setting outperforming the popular elastic net methodology."
1074,"Texture, Structure and Visual Matching","We propose a formal definition of ``visual texture'' and characterize it by the approximate sufficient statistics of the underlying process. These are inferred from data and used for compression, extrapolation, inpainting and segmentation. The formalization highlights relations between textures and other early vision operations, such as co-variant feature selection and correspondence. We show that co-variant frames (``structures'') are the complement of textures in an image. Unlike prior work on texture/structure partitioning, however, we show that such a decomposition requires multiple images: to attribute structures in the image to properties of the scene, a proper sampling condition has to be satisfied, which requires multiple realizations."
1075,Spectral Learning of General Weighted Automata via Constrained Matrix Completion,"Many tasks in text and speech processing and computational biology involve functions from variable-length strings to real numbers. A wide class of such functions can be computed by weighted automata. Spectral methods based on singular value decompositions of Hankel matrices have been recently proposed for learning probability distributions over strings that can be computed by weighted automata. In this paper we show how this method can be applied to the problem of learning a general weighted automata from a sample of string-label pairs generated by an arbitrary distribution. The main obstruction to this approach is that in general some entries of the Hankel matrix that needs to be decomposed may be missing. We propose a solution based on solving a constrained matrix completion problem. Combining these two ingredients, a whole new family of algorithms for learning general weighted automata is obtained. Generalization bounds for a particular algorithm in this class are given. The proofs rely on a stability analysis of matrix completion and spectrallearning."
1076,HGLMMF: Generalizing Matrix Factorization with Hierarchical Generalized Linear Model,"Matrix factorization (MF) has become the dominant method of collaborative filtering. Recently, various MF methods have been proposed and tried to jointly model multiple relations. However, such methods are vulnerable to the changes of data or sub-models. Moreover, data often follows the Pareto rule, which may lead to a poor result due to the global bias caused by such imbalanced data. To overcome these defects, we designed a generalized MF method based on hierarchical generalized linear models (HGLMMF) that augments knowledge with learned extra information from other related models. More specifically, HGLMMF uses one portion of the augmented knowledge to construct augmented covariates to better capture fixed effects while the other portion is used to model the cluster-specific random effects to adjust the global bias problem. We also demonstrate that a number of state-of-the-art MF models can be viewed as special cases of HGLMMF."
1077,"Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L_p$ Loss","In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general $L_p$ norm.  We generalize the Cramer-Rao lower bound and show how the $L_p$ loss can be written as a functional of the Fisher Information in the asymptotic limit, by proving the moment convergence of certain functions of Poisson random variables.  In this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing $L_p$ loss in the limit as $p$ goes to zero.  "
1078,Inferring Hidden Fluents Using Action and Causality,"In object and event detection, the identification of fluents (object statuses) that are not directly observable has been overlooked.  Inferring the values of such hidden fluents is important to fully understanding which actions are available to agents.  These values can be filled in by applying causal reasoning to video.  This paper presents a reasoning framework for the changes in, or maintenance of, the values of fluents over time by extending the Causal And-Or Graph, a stochastic grammar model for causality that integrates with current grammar models for event and object detection. The model incorporates causal reasoning with spatio-temporal detection to generate multiple interpretations of what is transpiring in a scene and to rank those interpretations according to probability.  We show that such interpretations can be used to correct (mis)detections of fluents and events in video; results are comparable to humans' performance in reasoning values of hidden fluents."
1079,Which classifiers are worth learning?,"Despite advances in model selection techniques,  choosing among different statistical models to select the ``correct'' one for the problem at hand remains a fairly subjective and even personal decision.  This short note suggests that so long as one is only interested inpolynomial-time algorithms then fairly mild assumptions about the learning problem ---namely, that the inputs are noisy---can lead to a simple understanding of what concepts are learnable even in principle.  In the case of uniformly distributed inputs and iid noise, there is a simple characterization of learnable concepts as well as a  simple universal learning algorithm that runs in polynomial time. The main technical observation involves the theory of {\em noise stable} functions (considered earlier in context of Fourier learning)."
1080,Inverse Reinforcement Learning for Dynamic Features with Applications to Socially Normative Robot Behavior Learning,"Inverse reinforcement learning (IRL) is a general framework for learning reward functions for Markov Decision Processes from demonstrated samples of its optimal policy. Prior work has assumed the reward features to be static or semi-static which prevented IRL to be effectively applied for agents in dynamic environments with features that continuously change over action executions. In this paper, we develop an approach to compute and match the estimated and predicted expectations of dynamic feature counts by taking a Monte Carlo-based path sampling approach. The method learns feature-based cost weights, induced by the modeled maximum entropy distribution, from many examples of locally optimal behavior. By joining local gradients and using alternative gradient descent methods, we obtain a particularly compact and efficient learning algorithm. Driven by the scenario of robots that learn socially normative behavior from demonstration, we apply the method to several examples of human-like navigation maneuvers in simulation and with a real robot and show how the proposed method outperforms alternative approaches."
1081,Monitoring Cardiac Stress Using Acoustic Heart Signals,"Cardiovascular complications arising from non-cardiac surgery exceed 1 million patients worldwide each year. Due to the increasing size of the elderly population it is predicted that during the coming decades, perioperative complications will increase by 100%. Current non-invasive cardiac monitoring techniques are based mostly on ECG signals. This kind of monitoring reflects cardiac electrical activity, but not its mechanical activity.It is known that acoustic heart sounds carry significant information about the cardiac state. In this work we present a novel monitoring that is based on the mechanical activity of the heart and is manifested by the sounds emitted from the heart. Two physiological features are extracted, which reflect cardiac morphology change from its baseline behavior. We use laparoscopic surgery as a model for a procedure which involves externally induced cardiac stress. The framework is applied to heart sounds recorded during laparoscopic surgeries of 25 patients. We demonstrate that suggested features change during cardiac stress, and are more significant for patients with cardiac problems. Furthermore, we show that other ECG morphology features are less sensitive during cardiac stress."
1082,Active Learning for Multiclass Cost-sensitive Classification Using Probabilistic Models,"Multiclass cost-sensitive active learning is a relatively new problem. In this paper, we derive the maximum expected cost and cost-weighted minimum margin strategy for multiclass cost-sensitive active learning. These two strategies can be seem as the extended version of classical cost-insensitive active learning strategies. The experimental results demonstrate that the derived strategies are promising for costsensitive active learning. In particular, the cost-sensitive strategies outperform cost-insensitive ones. The experimental results reveal how the hardness of data affects the performance of active learning strategies. Thus, in practical active learning applications, data analysis before strategy selection can be important."
1083,Bayesian Adaptive Mean Shift (BAMS),"The Adaptive Mean Shift (AMS) algorithm is a popular and simple non-parametric clustering approach based on Kernel Density Estimation. In this paper AMS is reformulated in a Bayesian framework, which permits a natural generalization in several directions that are shown to improve performance. The Bayesian framework considers the AMS to be a method to obtain a  posterior mode. This allows the algorithm to be generalized with three components which are not considered in the conventional approach: node weights, a prior for a particular location and a posterior distribution for the bandwidth. Practical methods to build the three different components are considered. The Bayesian Adaptive Mean Shift (BAMS) algorithm is evaluated with synthetic datasets and several real datasets."
1084,Algorithms for Learning Markov Field Policies,"We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications.The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects."
1085,Schizophrenia Detection and Classification by Advanced Analysis of EEG Recordings using a Single Electrode Approach ,"Electroencephalographic (EEG) analysis has emerged as a powerful tool for brain state interpretation and diagnosis. However, it has not emerged as a powerful tool in diagnosis of mental disorders. This may be explained by the low spatial resolution or depth sensitivity of EEG.  This paper concerns diagnosis of Schizophrenia using EEG, which currently suffers from few cardinal problems: it heavily depends on assumptions, conditions and prior knowledge of the patient. Additionally, the diagnostic experiments take hours, and accuracy of the analysis is low or unreliable.This article presents a novel approach for Schizophrenia detection showing great success in classification accuracy. The methodology is built for a single electrode recording attempting to make the data acquisition process feasible and quick for most patients."
1086,Affine Independent Variational Inference,"We present a method for approximate inference for a broad class of non-conjugate probabilistic models. In particular, for the family of generalized linear model target densities we describe a rich class of variational approximating densities which can be best fit to the target by minimizing the Kullback-Leibler divergence.  Our approach is based on using the Fourier representation which we show results in efficient and scalable inference."
561,Simulation of Database-Valued Markov Chains Using SimSQL,"This paper describes the SimSQL system, which allows for SQL-based specification,simulation, and querying of database-valued Markov chains, i.e., chainswhose value at any time step comprises the contents of an entire database. This isof particular interest in statistical machine learning, because SimSQL can easilybe used to declaratively specify Markov Chain Monte Carlo simulations that areautomatically parallelized to run on a large compute cluster."
562,When Does a Mixture of Products Contain a Product of Mixtures?,"We prove results on the relative representational power of mixtures of products and products of mixtures (restricted Boltzmann machines). Tools of independent interest are mode-based polyhedral approximations sensitive enough to compare even full-dimensional models, and characterizations of possible mode and support sets of both model classes. The title question is intimately related to questions in coding theory and the theory of hyperplane arrangements. In particular we find that an exponentially larger mixture model, requiring an exponentially larger number of parameters, is required to represent the distributions that can be represented by the restricted Boltzmann machine. "
564,Multi-Task Averaging,"We present a multi-task learning approach to jointly estimate the means of multipleindependent data sets. We prove that the proposed multi-task averaging (MTA) algorithmresults in a convex combination of the single-task maximum likelihood estimates.We derive the optimal amount of regularization, and show that it can be effectivelyestimated. Simulations and real data experiments demonstrate that MTAoutperforms both maximum likelihood and James-Stein estimators, and that ourapproach to estimating the amount of regularization rivals cross-validation in performancebut is more computationally efficient."
565,Functional Mesh Learning for Pattern Analysis of Cognitive Processes,"Here we propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning machine, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using functional neighborhood concept. In order to define functional neighborhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighboring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Local Relational Features (LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely K-Nearest Neighbor and Support Vector Machine, are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The proposed model exhibits classification performance superior to the state of the art methods available in the literature."
566,Block Coordinate Descent for Sparse NMF ," Nonnegative matrix factorization (NMF) is now an ubiquitous tool for  data analysis.  An  important variant is the Sparse NMF  problem which arises when we explicitly require the learnt features to be sparse. A natural  measure of sparsity  is the L$_0$-norm, however its  optimization is NP-hard.  Instead,  we consider the surrogate sparsitymeasure  linear in  the  ratio of  the  L$_1$ and  L$_2$ norms,  and  propose an efficient algorithm  to handle these norm constraints while  minimizing the reconstruction error.The  key  novelty  of  our algorithm is in its use of sequential updates  instead of the usual  batch  approach. Existing  algorithms for  solving this  problem are  typically inefficient.  We present experimental evidence  that our new algorithm performs an order of magnitude faster compared to the state-of-the-art solvers."
567,Tracking 3-D Rotations with the Quaternion Bingham Filter,"A deterministic method for sequential estimation of 3-D rotationsis presented.  The Bingham distribution is used to representuncertainty directly on the unit quaternion hypersphere.  Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions.  Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic.  We present two versions of the  QBF--suitable for tracking the state of first- and second-order rotating dynamical systems."
568,Robust Dictionary Learning by Source Decomposition,"It is now well established that sparse coding is well suited to many applications such as image restoration, denoising and classification. Especially, adaptive sparsemodels learned from data, or ?dictionary?, outperformfixed basis such as Discrete Cosine Basis and Fourier Basis. This paper extends this line of research to consider outlier elimination, proposing two methods to decompose the reconstructive residual into two components: a non-sparse component for small universal noises as well as a sparse one for the outliers respectively. In addition, further analysis reveals the connection between our model and the ?partial? dictionary learning approach, updating only part of the codewords (informative codewords DInfo)with remaining (noisy one DNoisy) fixed. We validate and evaluate our new approach on synthetic data as well as real applications and achieved satisfactory performance."
569,Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders,"Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density.  This paper contributes to the mathematical understanding of this phenomenon and helps define better justified sampling algorithms for deep learning based on auto-encoder variants. We consider an MCMC where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density. First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures estimators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed sampling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder."
570,Geodesic Distance Function Learning: Theory and an Algorithm,"Learning a distance function is of great importance in machine learning and pattern recognition. Geodesic distance, which has been widely used, is one of the most important intrinsic distances on the manifold. In this paper, we study the geodesic distance function $d(p, x)$ for a fixed point $p$. We provide two theorems to exactly characterize such a distance function. Our theoretical analysis shows if a function $r_p(x)$ is a Euclidean distance function in a neighborhood of $p$ in exponential coordinates and the gradient field of $r_p(x)$ has unit norm almost everywhere, then $r_p(x)$ must be the unique geodesic distance function $d(p,x)$. Based on our theoretical analysis, a novel approach from vector field perspective is proposed to learn the geodesic distance function. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm."
571,Measuring Reproducibility of High-throughput Deep-sequencing Experiments based on Self-adaptive Mixture Copula,"Measurement of the statistical reproducibility between biological experiment replicates is vital first step of the entire series of bioinformatics analyses for meaningful biology finding in mega-data. To distinguish the real biological relevant signals from artificial signals, irreproducible discovery rate (IDR) employing Copula, which can separate dependence structure and marginal distribution from data, has been put forth. However, the main disadvantage of IDR is that it assumes the data subject to normal distribution, which does not match the real data's feature. To address the issue, we propose a Self-adaptive Mixture Copula (SaMiC) to measure the reproducibility of experiment replicates from high-throughput deep-sequencing data. Simple and easy to implement, the proposed SaMiC method can self-adaptively tune its coefficients so that the measurement of reproducibility is more effective for general distributions.  Experiments in simulated and real data indicate that compared with IDR, the SaMiC method can better estimate  reproducibility between replicate samples."
572,Matrix Completion with Ordering Relation Constraints,"We relax the equality constraints in the very general and well-known affine Schatten p-norm minimization problem into one-side inequality constraints. Owing to the imposed equality con-straints, existing methods can only achieve some degree of denoising, via the optimization of the objective energy function. By our proposed re-laxation, the decision variables in the objective function possess flexible nonlinearity while maintaining their ordering relation constraints. We show that, our new objective function is convex, and its global minimum can be obtained by a more general form of the Fixed-Point Con-tinuation framework with almost the same com-putational cost. Experiments show that, our algo-rithm has good performance over various widely used datasets."
573,On the Equivalence of the Lasso and the SVM,"We investigate the relation of two fundamental tools in machine learning, that is the support vector machine (SVM) for classification, and the Lasso technique used in regression. We show that the resulting optimization problems are equivalent, in the following sense: Given any instance of one of the two problems, we construct an instance of the other, having the same optimal solution. In consequence, the two large classes of existing optimization algorithms for both SVMs and Lasso can also be applied to the respective other problem instances.Also, the equivalence allows for many known theoretical insights for SVM and Lasso to be translated between the two settings. One such implication gives a simple kernelized version of the Lasso, analogous to the kernels used in the SVM setting. Another consequence is that the sparsity of a Lasso solution is equal to the number of support vectors for the corresponding SVM instance.Furthermore, we can directly relate sublinear time algorithms for the two problems, and give a new such algorithm variant for the Lasso."
574,Fused Multiple Graphical Lasso,"In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which  encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a blockwise coordinate descent method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."
575,A polygon-based interpolation operator for super-resolution imaging ,"We outline the super-resolution reconstruction problem posed as a maximization of probability. We then introduce an interpolation method based on polygonal pixel overlap, express it as a linear operator, and use it to improve reconstruction.Polygon interpolation outperforms the simpler bilinear interpolation operator and, unlike Gaussian modeling of pixels, requires no parameter estimation. A free software implementation that reproduces the results shown is provided."
576,Discovering Common Functional Connectomics Signatures,"Based on the structural connectomes constructed from diffusion tensor imaging (DTI) data, we present a novel framework to discover functional connectomics signatures from resting-state fMRI (R-fMRI) data for the characterization of brain conditions. First, by applying a sliding time window approach, the brain states represented by functional connectomes were automatically divided into temporal quasi-stable segments. These quasi-stable functional connectome segments were then integrated and pooled from populations as input to an effective dictionary learning and sparse coding algorithm, in order to identify common functional connectomes (CFC) and signature patterns, as well as their dynamic transition patterns. The computational framework was validated by benchmark stimulation data, and highly accurate results were obtained. By applying the framework on the datasets of 44 post-traumatic stress disorder (PTSD) patients and 51 healthy controls, it was found that there are 16 CFC patterns reproducible across healthy controls/PTSD patients, and two additional CFCs with altered connectivity patterns exist solely in PTSD subjects. These two signature CFCs can successfully differentiate 85% of PTSD patients, suggesting their potential use as biomarkers."
577,Blind Image Deblurring by Spectral Properties of Convolution Operators,"In this paper, we study the problem of recovering a sharp version of a given blurry image when the blur kernel is unknown. Previous methods often introduce an image-independent regularizer (such as Gaussian or sparse priors) on the desired blur kernel. For the first time, this paper shows that the blurry image itself encodes rich information about the blur kernel. Such information can be found through analyzing and comparing how the spectrum of an image as a convolution operator changes before and after blurring. Our analysis leads to an effective convex regularizer on the blur kernel which depends only on the given blurry image. We show that the minimizer of this regularizer guarantees to give good approximation to the blur kernel if the original image is sharp enough. By combining this powerful regularizer with conventional image deblurring techniques, we show how we could significantly improve the deblurring results through simulations and experiments on real images, especially when the blur is large. In addition, our analysis and experiments help explaining why edges are good features for image deblurring."
580,Approximate l-fold cross-validation with Least Sqaures SVM and Kernel Ridge Regression,"Kernel methods have difficulties scaling to large modern data sets. The scalability issues are based on computational and memory requirements for working with a large matrix. These requirements have been addressed over the years by using low-rank kernel approximations or by improving the solvers' scalability. However, Least Squares Support Vector Machines (LS-SVM), a popular SVM variant, and Kernel Ridge Regression still have several scalability issues. In particular, the  $O(n^3)$ computational complexity for solving a single model, and the overall computational complexity associated with tuning hyperparameters are still major problems. We address these problems by introducing an $O(n\log n)$ approximate $l$-fold cross-validation method that uses a multi-level circulant matrix to approximate the kernel. In addition, we prove our algorithm's computational complexity and present empirical runtimes on data sets with approximately 1 million data points. We also validate our approximate method's effectiveness at selecting hyperparameters on real world and standard benchmark data sets. Lastly, we provide experimental results on using a multi-level circulant kernel approximation to solve LS-SVM problems with hyperparameters selected using our method."
581,Unsupervised Structure Discovery for Semantic Analysis of Audio,"Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines."
582,Identification of Spike-Processing Neural Circuits,"Reverse engineering of neural circuits requires the development of sound experimental and theoretical methods for determining the circuit connectivity and for estimating the processing of both spiking and continuous sensory signals. Here we present a new approach for identification of receptive fields in spiking neuron models that admit both continuous  signals and multidimensional spike trains as input stimuli. We consider circuit models of the sensory periphery in olfaction, audition and  vision as well as models of spike processing in higher brain centers, including models with lateral connectivity and feedback. We present algorithms for identifying temporal, spectrotemporal and spatiotemporal receptive fields directly from spike times produced by a neuron. The algorithms obviate the need to repeat experiments in order to compute the neuron's rate of response, rendering our methodology of interest to both experimental and theoretical neuroscientists."
583,A Marginalized Particle Gaussian Process Regression,"We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods."
584,Spherical Quantization based Binary Embedding,"This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods."
586,Structured Label Propagation in an Ensemble of Directed Acyclic Graphs,"This paper proposes a new approach to semi-supervised structured learning. Our structured learning formulation is based on energy minimization using graphic models that consist of overlapping local cliques. Local label patterns of cliques are propagated from training data to testing data, using an ensemble of directed acyclic graphs (DAGs). The key innovation in our approach is that the label propagation does not make the label smoothness assumption that  if two feature vectors are similar, then so should be their corresponding output labels. We argue that this assumption introduces a bias toward labels with dominant population because local cliques in a graphical model may and often exhibit similar weak local features but take different labels. In contrast, our label propagation makes a weaker label repetitiveness assumption that if one feature is similar to a few other features that may have different labels, then the label of this feature is one of the other features'; which label to use is determined by inferring the energy model. We present algorithms for structured label estimation marginalized over an ensemble of sampled DAGs. Our method compares favorably with the conventional approach that assumes label smoothness.  "
587,Sparse Correlation Estimation for Elliptical distributions,"We propose a semiparametric procedure---named RTK (Rank-based Thresholding via Kendall's tau)---to estimate the correlation matrix of high dimensional elliptical distributions. Unlike most existing methods that are based on Pearson's correlation, our approach exploits the nonparametric rank-based correlation estimator. Theoretically, our procedure achieves the optimal parametric rates of convergence for both parameter estimation and sparsity recovery in high dimensional settings; Empirically, our method always deliver a positive definite solution. Numerical results on both simulated and real datasets are also provided to support our theory."
588,Max-Margin Min-Entropy Hidden Conditional Random Fields,"We introduce the novel max-margin min-entropy hidden conditional random field (M$^3$E-HCRF), which encodes the conditional distribution over the latent variables and the single output variable given the input variables. The proposed M$^3$E-HCRF model provides a sparse and factorized representation of the conditional distribution. Given an observation, the M$^3$E-HCRF model infers the output by selecting the class label that minimizes the Renyi entropy of the unnormalized measure of the conditional distribution, which is equivalent to simultaneously (1) maximizing the conditional log-likelihood of the output given the inputs, and (2) minimizing the entropy of the conditional distribution of the hidden variables given the inputs and the output. The parameters of the proposed M$^3$E-HCRF model are learned by minimizing an $l_2$-regularized loss function, resulting in a non-convex optimization problem that can be solved by the non-convex bundle cutting plane algorithm. We evaluate our model's effectiveness on sequence labeling and structured learning using two public datasets, and demonstrate that our model achieves results comparable to the state of the art."
589,Growing a List,"We would like to intelligently grow a long list, starting from a small seed of examples. Our algorithm for solving this problem takes advantage of the wisdom of the crowd, in the sense that there are many experts who post lists of things on the Internet. We want to find these experts and aggregate their lists in an intelligent way in order to produce a single concise, complete, and meaningful list. Our solution to the list aggregation problem has several simple components: i) a combinatorial search over pairs of seed items, leveraging the speed of search engines, ii) a fast clustering algorithm (Bayesian Sets), and iii) an implicit feedback loop where the most relevant terms are added to the seed. The algorithm is extremely fast, and we show experimental results on two problems: creating a list of planned events in and around Boston, and creating a list of Jewish foods. We find that Bayesian Sets clusters well even when its underlying independence assumption is violated, and we provide a theoretical generalization bound to explain its ability to cluster well in general."
590,Building an Attribute based Semantic Hierarchy,"We propose a new framework to build attribute based hierarchies from visual datasets. Our desiderata is to construct a tree structure in which attributes which are used more frequently are associated with nodes which are closer to the root, whereas attributes which are used less frequently are associated with lower levels in the tree. Most of the existing works that are concerned with learning visual and semantic taxonomies are based on hierarchical topic models which entail the bag of features representation. Such approaches are therefore not suitable for dealing with an attribute based representation. An attribute based representation can facilitate information transfer from previously observed instances into new images, and therefore an attribute based hierarchy can capture richer semantics while limiting the use of costly annotation data. We develop a new generative model for hierarchical clustering of binary vectors, which we refer to as the attribute tree process (ATP), and which is based on a tree-structured stick breaking process. The ATP allows us to estimate the entire structure of the hierarchy and the model parameters in an unsupervised fashion. We evaluate the proposed framework using several widely available datasets, and demonstrate that the ATP is capable of constructing semantically meaningful hierarchical representations of the data. "
591,Refining Models for Percutaneous Coronary Intervention through Transfer Learning,"Identifying patients at risk of complications during percutaneous coronary intervention (PCI), such as arrhythmias and bleeding, is essential in guiding patient care at the bedside and in streamlining healthcare delivery across hospitals with varying clinical resources (e.g., with or without on-site cardiac surgery). The traditional approach to develop models for PCI care is largely centralized and uses patient data aggregated across a growing number of hospitals. While this approach is effective in increasing the amount of training data available for model training, and in improving the generality of the models learned, it suffers from the pooled data abstracting the PCI population in a way that fails to reflect variations across individual hospitals in terms of both patients and caregivers. We address this shortcoming by exploring the hypothesis that models for PCI care can be improved through the use of transfer learning. In particular, we study how transfer learning can be applied to adapt models derived from multi-hospital PCI data for use at individual hospitals, in an effort to simultaneously leverage the benefits of both aggregating clinical datasets and fitting to the specific characteristics of the patient/caregiver mix at individual hospitals. When studied on a registry of patients undergoing PCI, this approach of model adaptation through transfer learning improved reclassification at the individual healthcare provider level for many complications associated with PCI."
592,Optimal kernel choice for large-scale two-sample tests,"Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics."
593,Probabilistic Approximate Matrix Decomposition for Non-Negative Matrix Factorization,"This paper presents a new non-negative matrix factorization (NMF) algorithm called structured random (SR)-NMF. Probabilistic matrix decomposition is combined with NMF to generate a surrogate problem with a much lower dimension. After solving the compressed NMF problem, any standard NMF routine can be used to finish the factorization. The proposed method is evaluated in combination with several NMF algorithms on a variety of data sets. The new method combined with block principal pivoting (BPP) gives improved results. On the 300,000 article NY Times data set, in less than one tenth of the time, SR-NMF (followed by BPP) converges to the same accuracy as BPP alone. In general SR-NMF is faster than and at least as accurate as the existing evaluated methods. SR-NMF makes extremely large NMF problems tractable."
594,Laplacian Consistency,"Computing a faithful similarity/affinity metric is essential to many graph-based learning algorithms.In this paper, we propose a graph-based affinity learning method in an unsupervised scenario and show its application to shape retrieval, face clustering and web categorization.Our method, Laplacian Consistency  (LC), performs a dynamic diffusion process by propagating the similarity mass along the intrinsic manifold of data points.Convergence analysis is given and a closed-form solution is provided, making the LC process fast to calculate and easy to understand. Theoretical analysis shows our LC process only changes the eigenvalues gradually while keeping the eigenvector in the Laplacian spectral space. We also prove the superiority of our method from different points of views. Our method has nearly no parameter tuning and leads to significantly improved affinity maps, which help to greatly enhance the quality of various graph-based learning algorithms."
595,Learning to Classify From Multiple Experts,"Label is a critical component of the classification learning framework. However in many practical applications when labels are based on human  assessments, it is infeasible to assume one can obtain a perfect set of labels everybody agrees on. A solution that has been recently explored by the machine learning community is learning from multiple annotators: instead of collecting labels from a single expert/annotator, we collect labels from a number of annotators/experts. Since there may be substantial disagreements among labels of multiple annotators, some kind of a consensus model, that incorporates the characteristics (e.g. reliability) of each annotator, is sought.In this work, we study and develop a new approach for learning classification models from labels provided by multiple annotators. Our approach explicitly models and learns annotator-specific  model, reliability, and bias and incorporates them into the learning of the consensus model. Experimental results show that our approach outperforms commonly used multiple annotators baselines on multiple UCI-based datasets and a real-world medical data set."
597,Factoring nonnegative matrices with linear programs,"This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes."
598,Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
599,Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space,"This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications."
600,A Generalized Kernel Approach to Structured Output Learning,"We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. We show that the existing KDE formulations are special cases of our framework. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on two structured output problems, and compare it to the state-of-the-art kernel-based structured output regression methods."
601,Optical FLow Estimation by Adaptive Data Fusion,"Many state-of-the-art optical flow estimation algorithms are based on the variational method, which optimizes regularity and data terms simultaneously. This study presents a novel approach that provides weights to various data terms adaptively against a single data term in the conventional variational framework. In this study, a new optical flow estimation model with weighted sum of multiple data terms is introduced, and its optimization procedure is proposed. Competitive experimental results on the Middlebury optical flow benchmark show that the proposed method outperforms conventional methods with the aid of complementary data terms. In particular, this study is of importance for cases that incorporate various data terms into a unified variational framework."
602,Minimum Uncertainty Gap for Robust Visual Tracking,"We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at that state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the bounds, our method finds the confident state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood score, especially when there are severe illumination changes, occlusions, and pose variations.A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that minimizes the gap between the bounds. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods."
603,Learning from many experts: sparsity and model selection,"Experts classifying data are often imprecise. Recently, several models have been proposed to train classifiers using the noisy labels generated by these experts. Such models often have a large number of parameters, which can lead to overfitting. In order to avoid this and find better classifiers, we propose a new model which searches for sparse classifiers. We also develop a general method for model selection and apply it to optimize the tuning parameter in our model."
604,Soft Bounding Box Representation for Visual Tracking ,"A new tracking algorithm that tracks highly non-rigid targets robustly is proposed using a new bounding box representation called a soft bounding box (SBB).In the soft bounding box representation, the target is described as a range of the bounding box, which is bounded by an inner bounding box and an outer bounding box.In the paper, the inner and outer bounding boxes are theoretically constructed based on the theory of evidence.With these bounding boxes, the proposed method can solve the inherent ambiguity in a single bounding box representation for highly non-rigid targets. In addition, the method does not deal with the ambiguous region directly, which includes the foreground and the background at the same time. Hence, it robustly tracks highly non-rigid targets. In the soft bounding box representation, the best state of the target is efficiently found using a new Constrained Markov Chain Monte Carlo sampling method,which uses the constraint in which the outer bounding box must include the inner bounding box. Experimental results show that our method can track non-rigid targets accurately and robustly, and outperform even state-of-the-art methods."
605,Posterior contraction of the population polytope in finite admixture models,"We study the posterior contraction behavior of the latent population structure that arises in admixture models as the amount of data increases. An admixture model  --- alternatively known as a topic model --- specifies $k$ populations (or topics), each of which is characterized by vector of frequencies for generating a set of discrete values of observations. The population polytope is defined as the convex hull of the $k$ frequency vectors. Given a prior distribution over the space of population polytopes, we establish rates at which the posterior distribution contracts to $G_0$, under the Hausdorff metric and a minimum matching Euclidean metric, as the amount of data tends to infinity. Rates are obtained for the overfitted setting, i.e., when the number of extreme points of $G_0$ is bounded above by $k$, and for the setting in which the number of extreme points of $G_0$ is known. Minimax lower bounds are also established. Our analysis combines posterior asymptotics techniques for the estimation of mixing measures in hierarchical models with arguments in convex geometry."
606,Learning-based Stereo Method using MMSE Estimation,"We model the stereo problem using a product of Gaussian mixture models(PGMM). This enables efficient sampling and optimization, which makes general parameter learning possible. The learning procedure, along with the strong modeling power of PGMM,  helps us to find prior model for stereo problem using training data. Another important contribution of this work is that the proposed method computes its solution via minimum-mean-squared-error(MMSE) estimation instead of maximum-a-posteriori(MAP) estimation. The benefits of this approach is two-fold: it utilizes the learned characteristics of our model better than MAP and the result is more robust to subtle errors in stereo model itself. Experimental results show that the performance of our method based on MMSE estimation is far better than that of MAP estimation with the same model, while achieving competitive quantitative evaluation score in comparison with other learning-based stereo methods."
607,Teaching Classification Tasks to Humans,"Given a classification task, what is the best way to teach the resulting boundary to a human? While machine learning techniques can provide excellent techniques for finding the boundary, they tell us little about how we would teach a human the same task. We propose to investigate the problem of example selection and presentation in the context of teaching humans, and explore a variety of mechanisms in the interests of finding what may work best. In particular, we begin with the baseline of random presentation and then examine combinations of several mechanisms: the indication of an example?s relative difficulty, the use of the shaping heuristic from the psychology literature (moving from easier examples to harder ones), and a novel kernel-based ?coverage model? of the subject?s mastery of the task. From our experiments on 53 human subjects learning classification tasks via our teaching system, we found that we can achieve the greatest gains with a combination of shaping and the coverage model."
608,Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination ,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) isa fundamental component of a variety of studies in Neuroscience. The goal hereis to identify regions that exhibit changes as a function of the clinical conditionunder study. As the clinical questions of interest move towards identifying veryearly signs of diseases, the corresponding statistical differences at the group levelinvariably become weaker and increasingly hard to identify. Indeed, after a mul-tiple comparisons correction is adopted (to account for correlated statistical testsover all surface points), very few regions may survive. In contrast to hypothesistests on point-wise measurements, in this paper, we make the case for perform-ing statistical analysis on multi-scale shape descriptors that characterize the localtopological context of the signal around each surface vertex. Our descriptors arebased on recent results from harmonic analysis, that show how wavelet theoryextends to non-Euclidean settings (i.e., irregular weighted graphs). We providestrong evidence that these descriptors successfully pick up group-wise differences,where traditional methods either fail or yield unsatisfactory results. Other than thisprimary application, we show how the framework (i) allows performing corticalsurface smoothing in the native space (without a unit sphere mapping) and (ii)yields an efficient algorithm for perceptual shape segmentation of 3-D model thatcompares very favorably with the state of the art."
610,Generalized Ambiguity Decomposition for Convex Ensembles of Experts and Arbitrary Differentiable Loss Functions,"The squared error of a convex ensemble of regressors is related to the squared error of the individual regressors and the diversity of the ensemble as measured by the weighted sum of squared errors of each regressor's prediction from the ensemble's prediction. This relationship, also known as ambiguity decomposition, highlights the impact of diversity on ensemble's performance for least squares regression. In this paper, we present a generalization of ambiguity decomposition that can be applied to any convex ensemble of experts under a differentiable loss function. The proposed decomposition is applicable to both classification and regression, and provides a task-driven notion of diversity. It is shown that the diversity term in this decomposition is scaled by a factor dependent on the instance and the loss function in a classical supervised learning setting. This lends support to the intuition that not all instances are equally important from a diversity perspective. We then derive the decomposition for some common regression and classification loss functions, and demonstrate its accuracy on different UCI datasets."
611,Better Mixing via Deep Representations,"It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation.  To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples."
612,Predicting Functional Cortical ROIs via Joint Modeling of Anatomical and Connectional Profiles,"Localization of functional cortical ROIs (regions of interests) in structural data such as DTI and MRI images has significant importance in basic and clinical neuroscience. However, this problem is challenging due to the lack of quantitative mapping between brain structure and function, which relies on both the availability of benchmark training data such as task-based fMRI and effective machine learning algorithms. This paper presents a novel joint modeling approach that learns predictive models of functional cortical ROIs from multimodal task-based fMRI, DTI and MRI datasets. In particular, the effective generalized multi-kernel learning algorithm was tailored to infer the intrinsic relationships between anatomical/connectional MRI/DTI features and fMRI-derived functional localizations. Then, the predictive models of functional cortical ROIs were evaluated by cross-validation studies, independent datasets and reproducibility studies, and experimental results are promising. We envision that these predictive models can be widely applied in the future in scenarios that have only DTI and/or MRI data, but without task-based fMRI data. "
613,Maximum Weight Subgraphs with Mutex Constraints,"In this paper, we propose a novel algorithm for computing maximum weight subgraphs(MWSs) that satisfy mutex constraints on a weighted graph. As opposedto commonly used linear equality constraints, the mutex constraints expressedin a quadratic equality form allow for a greater modeling flexibility, which canbe beneficial in many applications. Although the proposed algorithm solves arelaxed formulation of MWS problem, it obtains a discrete solution in all ourexperiments on real data, which in turn guarantees that the solution satisfies themutex constraints. We evaluated our algorithm on two hard combinatorial problems:matching of salient points under perspective and nonrigid distortion andsolving image jigsaw puzzles. It significantly outperforms known state-of-the-artalgorithms, including loopy believe propagation and Integer Projected Fixed PointMethod (IPFP), even if it is restricted to using only constraints equivalent to linearequality constraints."
614,Pareto-Path Multi-Task Multiple Kernel Learning,"Traditional Multi-Task Multiple Kernel Learning (MT-MKL) methods routinely optimize the sum (thus, the average) of objective functions to simultaneously improve performances for all tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO) problem, which considers the concurrent optimization of all task objectives involved in the Multi-Task Learning problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel Support Vector Machine (SVM) MT-MKL framework, that considers an implicitly-defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of improving performances uniformly over tasks, when compared to the traditional MTL approach."
615,A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation,"A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lovasz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset."
616,Fused sparsity and robust estimation for linear models with unknown variance,"In this paper, we develop a novel approach to the problem of learning sparserepresentations in the context of fused sparsity and unknown noise level. We proposean algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes theaforementioned learning task by means of a second-order cone program. A special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers. We establish finite sample risk bounds and carry out an experimental evaluation on both synthetic and real data."
617,How Prior Probability Influences Decision Making: A Unifying Probabilistic Model,"How does the brain combine prior knowledge with sensory evidence whenmaking decisions under uncertainty? Two competing descriptive modelshave been proposed based on experimental data.  The first posits anadditive offset to a decision variable, implying a static effect ofthe prior. However, this model is inconsistent with recent data from amotion discrimination task involving temporal integration of uncertainsensory evidence. To explain this data, a second model has beenproposed which assumes a time-varying influence of the prior. Here wepresent a normative model of decision making that incorporates priorknowledge in a principled way.  We show that the additive offset modeland the time-varying prior model emerge naturally when decision makingis viewed within the framework of partially observable Markov decisionprocesses (POMDPs).  Decision making in the model reduces to (1)computing beliefs given observations and prior information in a Bayesianmanner, and (2) selecting actions based on these beliefs to maximize the expected sum of future rewards. We show that the model can explain both data previously explained using the additive offset model as well as more recent data on the time-varying influence of prior knowledge on decision making."
618,Exact and Efficient Parallel Inference for Nonparametric Mixture Models,"Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to sample from the true posterior in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods."
619,Iterative Learning in Modular Associative Memories,"The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a  retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance  can be made linear in the size of the network. "
620,Biased perception leads to biased action: Validating a Bayesian model of interception," We tested whether and how biases in visual perception might influence motor actions. To do so, we designed an  interception task where subjects had to indicate the time when a moving object, whose trajectory was occluded from the  subjects, would reach a target-area. Subjects made their judgements based on a brief display of the objects initial  motion at a starting point. Based on the known illusion that slow contrast stimuli appear to move slower than high  contrast ones, we predict that if perception directly influences motion actions subjects would show delayed  interception times for low contrast objects. In order to provide a more quantitative prediction, we developed a Bayesian  model for the complete sensory-motor interception task. Using fit parameters for the prior and likelihood on visual  speed from a previous study we were able to predict not only the expected interception times but also the precise  characteristics of response variability. Psychophysical experiments confirm the model's predictions. Individual  differences in subjects timing response can be accounted for by individual differences in the perceptual priors on  visual speed. Taken together, our behavioral and model results show that biases in perception percolate downstream to  bias action response in a predictable manner. Furthermore, our work emphasizes that the Bayesian model of speed  perception is generalizable to new domains."
621,High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer Disease Progression Prediction,"Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.  "
622,Discriminative Learning of Infinite Latent Variable Models,"We propose probabilistic models to infer discriminative latent variables in Hamming space from observed data. Our models allow for a simultaneous inference of the dimension of the binary latent variables, and their entries values. Further, the latent variables are discriminative in the sense that objects in the same category or semantic concept have similar latent values, and objects in different categories have dis-similar latent values. The inferred latent variables can be directly used to perform a nearest neighbour search for the purpose of classification or retrieval. We formulate this discriminative infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the proposed method is able to find semantically similar neighbours due to the discriminative nature of the latent space. The coupling structure of the inferred latent space lends itself to an application of extending hash codes in a discriminative way."
624,Compressed Sparse Concept Coding for Large Scale Data Representation,"Data representation is a fundamental problem in various research fields.When representing data as vectors, the feature space is usually of very high dimensionality, which makes itdifficult for applying learning algorithms for analysis.One then hope to apply matrix factorization techniques,such as Singular Vector Decomposition (SVD) to learnthe low dimensional hidden concept space. Among various techniques, sparse coding receives considerableinterests in recent years because its sparse representation leads to an elegant interpretation.However, most of the existing sparse coding algorithms are computational expensive since theycompute the basis vectors and the representations iteratively. Moreover, all the existing methodsare linear and not be able to capture the non-linear structure of the data. To tackle these issues, wepropose a novel sparse coding method, called {\em Compressed Sparse Concept Coding} (CSCC), for largescale data representation in this paper. Our method is non-linear and scales linearly with the numberof samples. Extensive experimental results on real world applicationsdemonstrate the effectiveness and efficiency of the proposed approach."
625,Multi-Relational Learning via Hierarchical Nonparametric Bayesian Collective Matrix Factorization,"Relational learning addresses problems where the data come from multiple sources and are linked together throughcomplex relational networks. Two important goals are pattern discovery (e.g. by (co)-clustering) and predicting unknown values of a relation, given a set of entities and observed relations among entities. In the presence of multiple relations, combining information from different but related relations can lead to better insights and improved prediction. For this purpose we propose a nonparametric hierarchical Bayesian model that improves on existing collaborative factorization models and frames a large number of relational learning problems. The proposed model naturally incorporates (co)-clustering and prediction analysis in a single unified framework, and allows for the estimation of entire missing row or column vectors. We develop an efficient Gibbs algorithm and a hybrid Gibbs using Newton?s method to enable fast computation in high dimensions. We demonstrate the value of our framework on simulated experiments as well as two real world problems: discovering kinship systems and predicting the authors of certain articles based on article-word co-occurrence features."
626,Functional Brain Interactions during Free Viewing of Video Stream,"Natural stimulus fMRI (N-fMRI) such as free viewing of video streams provides an uncontrolled environment to study the human brain's perception and cognition engaged in natural scene comprehension. Hence, it is receiving increasing interest in neuroimaging and multimedia analysis in recent years. In these fields, researchers rely on consistent and discriminative functional interactions such as functional or effective connectivity to measure the human brain's responses. However, the computational cost increases significantly in model-driven methods such as the dynamic causal modeling (DCM) when the cortical regions of interests (ROIs) are dense (e.g., 358 in this paper). In this paper, we present a data-driven computational pipeline to explore consistent and discriminative functional interactions during free viewing of video. The underlying premise is that the functional interactions, characterizing the semantic content of video samples in multiple categories and derived from N-fMRI data of multiple subjects, are simultaneously selected by multiple feature selection methods to pose both consistency and discriminativity. Then the spatial distribution of the ROIs involved in the identified interactions and the distribution of the functional sub-networks associated with the ROIs are assessed. Meanwhile, structural connectivity derived from diffusion tensor imaging (DTI) and video classification is used to evaluate the consistency and discriminativity of the identified functional interactions, respectively. Our findings provide new insights into the functional mechanism of the human brain in perception and cognition of complex natural scenes."
627,Constructing Deep Neural Networks via the Extended Restricted Boltzmann Machines,"We exploit an uniform training algorithm for the extended restricted Boltzmann machines (ERBM) to initialize the parameters of a deep neural network (DNN). Due to the conservative energy-based generative model and the intractable samples from the model distribution, the restricted Boltzmann machines (RBM) is extended as the following two aspects. Firstly, using a new method that introduces the free-weighting matrices, a novel energy-based generative model is proposed to establish an excellent RBM. To adapt to different high-dimensional data distribution and to speed up the Contrastive Divergence (CD), secondly we use the normal and no-sampling methods instead of the uniform-sampling method in CD. Our experiments finally confirm that the free-weighting matrices have important positive effects on DNN in face and MNIST datasets and DNN with no-sampling method can reconstruct better image than one with normal and uniform-sampling methods in face datasets."
628,Spatial-Visual Label Propagation for Local Feature Classification,In this paper we propose a novel approach to integrate feature similarity and spatial consistency of local features to achieve coherent and accurate labeling of feature points in a simple and effective way. We introduced our Spatial-Visual Label Propagation algorithm to infer the labels of local features in a test image from known labels. This is done in a transductive manner to provide spatial and feature smoothing over the learned labels. We show the value of our novel approach by a diverse set of experiments with successful improvements over previous methods and baseline classifiers.
629,Fixed-Point Model For Structured Labeling,"In this paper, we propose a simple but effective method to the structured labelingproblem: a fixed-pointmodel. Recently, layered models with sequential classifiers(regressors) have gained an increasing amount of interests for structural predictiondue to their capability in capturing a large degree of contexts and correlations. Inthis paper, we design an algorithm with a new perspective on layered models andstructured (contexts-based) learning problem; we provide a fixed-point functionwith the structured labelings being both the output and the input; it alleviates theburden in learning multiple/different classifiers in different layers. We devise atraining and testing strategy for our method and provide conditions/justificationsfor the fixed-point function for being a contraction mapping for the convergence.The learned function captures rich contextual information and is easy to train andtest. On three well-known structured prediction problems, the Optical CharacterRecognition (OCR) task, the Part of Speech (POS) task, and the Hypertext Classificationtask, the proposed fixed-point model observes significant improvement intraining efficiency and, at the same time, achieves comparable (most often better)prediction results over the state-of-the-art algorithms."
630,Decision-theoretic Sparsification for Gaussian Process Preference Learning,"We propose a decision-theoretic sparsification method for Gaussian process preference learning.  This method overcomes the loss-insensitive nature of popular sparsification approaches such as the Informative Vector Machine (IVM). Instead of selecting a subset of users and items as inducing points based on uncertainty-reduction principles, our sparsification approach is underpinned by decision theory and directly incorporates the loss function inherent to the underlying preference learning problem.  We show that by selecting different specifications of the loss function, the IVM's differential entropy criterion, a value of information criterion, and an upper confidence bound (UCB) criterion used in the bandit setting can all be recovered from our decision-theoretic framework.  We refer to our method as the Valuable Vector Machine (VVM) as it selects the most useful items during sparsification to minimize the corresponding loss.  Experiments show that variants of the VVM outperform  the IVM under similar computational constraints."
631,Dimension Independent Similarity Computation,"We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO)to compute all pairwise similarities between very high dimensional sparse vectors.All of our results are provably independent of dimension, meaningapart from the initial cost of trivially reading in the data, all subsequentoperations are independent of the dimension, thus the dimension can be very large.We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash.Our results are geared toward the MapReduce framework. We empirically validate ourtheorems at large scale using data from the social networking site Twitter."
632,Hippocampal CA3 Cells As Hidden Units of A Recurrent Neural Network,"Abstract Hippocampal cells are known for their spatial and temporal selectivity. However, it is unclear how such selectivity arises in different regions of the hippocampus and how it contributes to episodic memory. We simulate learning in a recurrent neural network (RNN) structurally similar to the neural circuit in area CA3. Our methods based on general temporal sequences can be extended to more specific inputs such as spatial and temporal correlated signals. Our simulation results provide a novel explanation of how multi-modal episodic memory is learned and suggest that the experimentally observed, sparse and selective tuning of CA3 cells facilitates the learning of temporal sequences as memory episodes."
633,Modeling Fashion,"We propose a method to try to model fashionable dresses in this paper. We first discover common visual patterns that appear in dress images using a human-in-the-loop, active clustering approach. A fashionable dress is expected to contain certain visual patterns which make it fashionable. An approach is proposed to jointly identify fashionable visual patterns and learn a discriminative fashion classifier. The results show that interesting fashionable patterns can be discovered on a newly collected dress dataset. Our model can also achieve high accuracy on distinguishing fashionable and unfashionable dresses. We test visual pattern centric dress retrieval, which is promising and interesting for visual shopping."
634,Symmetric Correspondence Topic Models for Multilingual Text Analysis,"Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models."
635,Infinite Structured Hidden Markov Model,"We present the infinite structured hidden Markov model (ISHMM). An ISHMM is an HMM that possesses an unbounded number of states, parameterizes state dwell-time distributions explicitly, and can constrain what kinds of state transitions are possible. We present two parameterizations of the ISHMM. The first is a novel construction for an infinite explicit duration HMM. The second is an entirely novel infinite left-to-right HMM. We provide inference algorithms for the ISHMM and show results from using the ISHMM to analyze both real and synthetic data."
636,Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data,"Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences."
637,Efficient coding connects prior and likelihood function in perceptual Bayesian inference,"  A common challenge for Bayesian approaches in modeling perceptual behavior is the fact that the two fundamental  components of a Bayesian model, the prior distribution and the likelihood function, are formally unconstrained. Here  we argue that a neural system that emulates Bayesian inference naturally imposes constraints by way of how it  represents sensory information in populations of neurons. More specifically, we propose an efficient encoding  principle that constrains both the likelihood and the prior based on low-level environmental statistics. The resulting  Bayesian estimates can show biases away from the peaks of a prior distribution, a behavior seemingly at odds  with the traditional view of Bayesian estimates yet one that has indeed been reported in human perception of visual orientation. We demonstrate that our framework correctly predicts these biases, and show  that the efficient encoding characteristics of the model neural population matches the reported orientation tuning  characteristics of neurons in primary visual cortex. Our results suggest that efficient coding can be a promising  hypothesis in constraining neural implementations of Bayesian inference."
638,Efficient Sampling for Bipartite Matching Problems,"Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in real-world applications of these problems is intractable, making efficient approximation methods essential for learning and inference. In this paper we propose a novel {\it sequential matching} sampler based on the generalization of the Plackett-Luce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems - ranking and image correspondence - which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches."
639,Learning visual motion in recurrent neural networks,"We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model."
640,Tensor Analyzers,"Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its \emph{additive} nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact \emph{multiplicatively}. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe a fairly efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches and of images containing a variety of simple shapes that vary in size and color. Tensor Analyzers can also accurately recognize a face under significantly pose and illumination variations when given only one previous image of that face. We also show that Mixtures of Tensor Analyzers outperform Mixtures of Factor Analyzers at modeling natural image patches and artificial data produced using multiplicative interactions."
641,Online Learning for Auction Mechanism in Bandit Setting,"This paper is concerned with the online learning of the optimal auction mechanism for sponsored search in a bandit setting. We point out that this task corresponds to a new type of bandit problem, which we call the \textit{armed bandit problem with shared information} (AB-SI). In the AB-SI problem, the arm space (corresponding to the parameter space of the auction mechanism which can be discrete or continuous) is partitioned into a finite number of clusters (corresponding to the finite number of rankings of the ads), and the arms in the same cluster share the explored information (i.e., the click-through rates of the ads in the same ranked list) when any arm from the cluster is pulled. We propose an upper confidence bound algorithm called UCB-SI to tackle this new problem. We show that when the total number of arms is finite, the regret bound obtained by our proposed algorithm is tighter than the classical UCB algorithm. In the continuum armed bandit setting, our algorithm can handle a larger classes of reward function and achieve a reasonable regret bound of $O(T^{2/3}(d\ln T)^{1/3})$, where $d$ is the pseudo dimension for the real-valued reward function class."
642,Large-Margin Tensor Decomposition for Multi-Relational Learning,"We propose a novel large-margin framework for multi-relational learning via tensor decomposition. In this setting, the training data consists of multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a transformed linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using a weighted objective function. The objective combines multiple task-specific loss functions, to accommodate different types of relations. For the typical cases of real-valued functions and binary relations, we propose a combination of quadratic and smooth hinge losses and derive the associated parameter gradients. We solve the resulting optimization problem using memory efficient quasi-Newton methods. We evaluate our method on synthetic and real data, showing that it obtains significant accuracy improvement over related techniques even when training data is limited. Further, we show that our decomposition is able to transfer information across the various relations, thus better exploiting the multi-relational structure."
643,The Interplay between Stability and Regret in Online Learning,"This paper considers the stability of online learning algorithms and its implications for learnability (bounded regret).  We introduce a novel quantity called {\em forward regret} that intuitively measures how good an online learning algorithm is if it is allowed a one-step look-ahead into the future.  We show that given stability, bounded forward regret is equivalent to bounded regret. We also show that the existence of an algorithm with bounded regret implies the existence of a stable algorithm with bounded regret and bounded forward regret. The equivalence results apply to general, possibly non-convex problems. To the best of our knowledge, our analysis provides the first general connection between stability and regret in the online setting that is not restricted to a particular class of algorithms. Our stability-regret connection provides a simple recipe for analysing  regret incurred by any online learning algorithm.  We illustrate our recipe by providing a novel dimension independent regret bound for the follow-the-perturbed-leader (FTPL) algorithm for online linear programming (OLP) over a hypersphere, a non-convex set. Using our framework, we analyse several existing online learning algorithms as well as the ``approximate'' versions  of algorithms like RDA that solve an optimization problem at each iteration. Our proofs are simpler than existing analysis for the respective algorithms, show a clear trade-off between stability and forward regret, and provide tighter regret bounds in some cases."
646,Understanding Indoor Scenes with Latent Interaction Template Models,"Visual scene understanding is a difficult problem, interleaving object detection, geometric reasoning and scene classification. In this paper, we present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the latent Interaction Template Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings, while also improving individual object detections."
647,Convex Tensor Decomposition via Structured Schatten Norm Regularization,"Conventionally, tensor decomposition has beenformulated as non-convex optimization problems, which despite theirempirical success, hindered the analysis of their performance. In thispaper, we propose structured Schatten norms for tensor decomposition based onconvex optimization. The proposed norms include two recently  proposedapproaches for convex tensor decomposition, which we call overlapped approach andlatent approach. Moreover, we mathematically analyze the performance of the latentapproach, which was empirically found to perform better than theother one in some settings. We show theoretically that this is indeedthe case. In particular, when the unknown true tensor is low-rank in a specific mode, the latent approach performs as good as knowing the mode with the smallest rank. We confirm through numerical simulations that our theoretical prediction can precisely predict the scaling behaviour of the mean squared error. "
648,Towards Sparse Representation on Cosine Distance,"Sparse code is a regularized least squares solution by $L_1$ or $L_0$ constraint, based on Euclidean distance between original and reconstructed signals with respect to a pre-defined dictionary.  The Euclidean distance, however, is not a good metric for many visual feature descriptors especially histogram features,~\eg~SIFT, HOG, LBP and Spatial Pyramid.  Instead, a cosine distance is a semantically meaningful metric for the visual features.  To leverage the benefit of cosine distance in sparse representation, we formulate a new sparse coding objective function based on approximate cosine distance by forcing a norm of reconstructed signal to be close to the norm of original signal.  We evaluate our new formulation on two datasets: Extended YaleB and AR dataset.  Our formulation shows consistent improvement over the traditional Euclidean distance based sparse coding formulation in our evaluations and achieve the state-of-the-art performance on the datasets."
649,Eliciting Predictions from a Connected Crowd of Traders,"We study an online trading community where traders can communicate with each other as well as perform trades. We discuss characteristics of social influence on trading decisions within this connected crowd. We discover traders are still heavily affected by social influence even when every trade is with their own money, and social influence often negatively affects their returns. Based on our observations, we implement three trading strategies to elicit the crowd?s prediction. In particular, we design a novel way of inferring predictions by modeling the crowd reasoning process under social influence. We find that even complex social dynamics can dramatically effect trades, it is still possible to infer knowledge from the crowd. Our novel algorithm achieves the best performance by modeling decision making processes under influence rather than the decisions from the crowd."
650,Learned Prioritization for Trading Off Accuracy and Speed,"Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines."
651,Generalized Classification-based Approximate Policy Iteration,"Classification-based approximate policy iteration allows us to benefit from the regularities of the optimal policy by explicitly controlling the complexity of the policy space. This leads to considerable improvements whenever the optimal policy is easy to represent. The conventional classification-based methods, however, do not benefit from the regularities of the value function as they often use a rollout-based estimate of the action-value function, which is rather data-inefficient and cannot generalize the estimate of the action-value function over states. In this paper, we introduce a general framework for classification-based approximate policy iteration, CAPI, that lets us benefit from the present regularities of both the policy and the value.Our theoretical analysis extends existing work by allowing the policy evaluation to be performed by any reinforcement learning algorithm, by handling nonparametric representations of policies, and by providing tighter convergence bounds on the estimation error of policy learning.A small illustration shows that this approach can be faster than purely value-based methods."
652,Another Nonparametric Functional Estimator that Achieves Asymptotic Optimality and Adapts to Irregular Domains,"We propose a new nonparametric functional estimation method. Existingstate-of-the-art methods are designed for regular domains (such as $\mathbb{R}^{d}$ or$[0,1]^d$) -- when the domain is irregular, they run intoimplementation difficulties; More specifically, one does not know the formulaof the corresponding reproducing kernels. Our newly designed method adapts to anyirregular domain. When some boundary conditionsare satisfied, it preserves the asymptotic optimality, which includesthe optimal convergence rate. The new methodalso achieves the asymptotic efficiency, however we did not include here. A significant advantage of the new approach is that it adapts to any domain, while traditional methodsrequire restrictive conditions on the domains."
653,Q-Learning on a Multi-state Markov Decision Process,"In this paper, we consider a different setting for the agent in Markov Decision Process (MDP):The agent can occupy multiple states at the same time and take an action from a set of states.We refer to this problem as multi-state MDP.This multi-state MDP has exponentially large state and action spaces which might be computationally expensive.However, we can take the advantage of the nondeterminism of the agent in multi-state MDP and propose two nondeterministic Q-learning algorithms.We show that the convergence and optimality of the standard Q-learning algorithm still holds.Furthermore, in the experiments we also show that the nondeterministic algorithms converge faster and are more robust with different learning rates than the standard Q-learning algorithm."
654,Value Pursuit Iteration,"Value Pursuit Iteration (VPI) is an approximate value iteration algorithm that finds a close to optimal policy for  reinforcement learning and planning problems with large state spaces.VPI has two main features: First, it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features. The algorithm is almost insensitive to the number of irrelevant features. Second, after each iteration of VPI, the algorithm adds a set of functions based on the currently learned value function to the dictionary. This increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function.We theoretically study VPI and provide a finite-sample error upper bound for it."
655,Link Prediction in Biological Networks using Penalized Multi-Attribute ERGMs,"Reconstruction of genetic networks is an important, yet challenging problem in systems biology. Gene networks often include different interaction mechanisms, such as transcriptional regulatory and protein-protein interactions. Further, different data sources provide valuable information about the relationships among genes, which motivate methods that allow for data integration.We propose a novel multi-attribute exponential random graph model for supervised prediction of gene networks, coupled with a penalized estimation framework for improved prediction performance. The proposed framework facilitates the analysis of gene networks with multiple edge types, and provides a systematic method for incorporating multiple sources of biological data, as well as diverse attributes regarding the function and location of genes, and structure of observed networks. Results of numerical experiments indicate that the method enjoys superior performance compared to state-of-the-art reconstruction methods."
656,"Compressive neural representation of sparse, high-dimensional probabilities","This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a high-dimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain."
658,Information Theoretic Pairwise Clustering,In this paper we develop an information-theoretic approach for pairwise clustering. The Laplacian of the pairwise similarity matrix can be used to define a Markov random walk on the data points. This view forms a probabilistic interpretation of spectral clustering methods. We utilize this probabilistic model to define a novel clustering cost function that is based on maximizing the mutual information between consecutively visited clusters of states of the Markovian process defined by the graph Laplacian matrix. The algorithm complexity is linear on sparse graphs. The improved performance and the reduced computational complexity of the proposed algorithm are demonstrated on several standard datasets.
659,Graphical Models via Generalized Linear Models,"Undirected graphical models, or Markov networks, such as Gaussian graphical models and Ising models enjoy popularity in a variety of applications.  In many settings, however, data may not follow a Gaussian or binomial distribution assumed by these models. We introduce a new class of graphical models based on generalized linear models (GLM) by assuming that node-wise conditional distributions arise from exponential families.  Our models allow one to estimate networks for a wide class of exponential distributions, such as the Poisson, negative binomial, and exponential, by fitting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We provide examples of high-throughput genomic networks learned via our GLM graphical models for multinomial and Poisson distributed data."
660,Shifted Subspace Tracking on Sparse Outliers,"In low-rank \& sparse matrix decomposition, the sparse part is often assumed to be generated by a random model. Analysis to its structure, which is of central interest in various problems, is rarely considered. One such example is tracking multiple object flows in video. We introduce ``shifted subspace tracking (SST)'' to both separate the object flows and recover their trajectories by exploring their shifted subspaces on the sparse outliers. SST can be summarized in two steps, i.e., background modeling and flow tracking. In step 1, we propose ``semi-soft GoDec'' to separate all the moving objects as the sparse outlier $S$ from the data matrix $X$. Its soft-thresholding of $S$ significantly speeds up GoDec and facilitates the parameter setting. In step 2, we treat the sparse $S$ in step 1 as the new $X$, and develop ``SST algorithm'' decomposing $X$ as $X=\sum\nolimits_{i=1}^k L(i)\circ\tau(i)+S+G$, wherein $L(i)$ denotes the subspace of the $i^{th}$ flow after transformation $\tau(i)$. The decomposition solves $k$ sub-problems of alternating minimization in sequel, each of which recovers a $L(i)$ and its $\tau(i)$ with randomized acceleration. Sparsity of $L(i)$ and smoothness between adjacent frames are explored to save computations. We justify the promising performance of SST on four surveillance video sequences."
661,CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem,"While compressive sensing (CS) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models. This limits its application and excludes many areas where CS ideas could make a difference. This paper presents a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose a novel solution using a lifting technique -- CPRL, which relaxes the NP-hard problem to a nonsmooth semidefinite program. Our analysis shows that CPRL inherits many desirable properties from CS, such as guarantees for exact recovery. We further provide scalable numerical solvers to accelerate its implementation. The source code of our algorithms will be provided to the public. "
662,Gene Context Analysis on Large-scale Genomic Data,"In this paper, we investigate one of the largest microarray datasets aggregated from the internet. We aim at detecting association between the variables (genes or gene pathways) and certain keywords of interest (tissue types or diseases, for example). We address the challenges of utilizing the text information and variable structure information in high dimensional feature selection and classification. To utilize the text information, we build keyword network borrowing the power of natural language processing and apply Nearest Shrunken Centroids (NSC) to context analysis. This procedure can fully utilize the text information and has the potential to scale up to the dimension of 1012 in minutes. To utilize the structure information, we develop a new discriminant analysis method called the group Nearest Shrunken Centroids (gNSC). By exploiting the text and variable structure information, our result verifies several biological associations and further leads to some new discoveries."
663,Incremental Beam Search,"Beam search is a widely applied heuristic search method. Given a beam-width, it explores that many nodes at each level until a goal node is found. However, the quality of the solution produced by beam search does not always monotonically improve with the increase in beam-width, which makes it difficult to choose an appropriate beam-width for effective use. We address this issue by proposing a new beam search algorithm called Incremental Beam search (IncB) which guarantees monotonicity. IncB is also an anytime algorithm. Experimental results on the sliding-tile puzzle problem and the traveling salesman problem show that IncB significantly outperforms iterative beam search as well as some of the state-of-the-art anytime heuristic search algorithms."
664,Visually-grounded Bayesian Word Learning,"Learning the meaning of a novel noun from a few labelled objects is one of the simplest aspects of learning a language, but approximating human performance on this task is still a significant challenge for current machine learning systems. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for given visual stimulus. Recent work in cognitive science on Bayesian models of word learning partially addresses this challenge, but assumes that objects are perfectly recognized and has only been evaluated in small domains. We present a system for learning words directly from images, using probabilistic predictions generated by visual classifiers as the input to Bayesian word learning, and compare this system to human performance in a large-scale automated experiment. The system captures a significant proportion of the variance in human responses. Combining the uncertain outputs of the visual classifiers with the ability to identify an appropriate level of abstraction that comes from Bayesian word learning allows the system to outperform alternatives that assume perfect recognition or use a more conventional computer vision approach."
666,Optimization of non-metric MRFs with QPBO via graph approximation,"Markov random field (MRF) has been used for many areas in computer vision. Many optimization methods were proposed to achieve good solutions on MRFs. Among them, graph cuts have gained widespread popularity. They achieve good approximated solutions when the energy function is metric using an $\alpha$-expansion scheme. However, if the energy function is non-metric, the conventional $\alpha$-expansion cannot solve the problem. In this case, the possible choice so far is the truncation, partial labeling with unlabeled nodes, and fusion move with heuristic proposals. In this paper, we propose a general way to handle non-metric MRFs with graph cuts using graph approximations. Extensive experiments support our claims and show that the proposed algorithm obtains better solutions than others both on synthetic and real problems."
667,Towards active event recognition,"Directing robot's sensors to anticipate events  like  goal-directed actions is complicated by intrinsic time constraints and spatially distributed sources of information.  The problem thus requires an integrated solution for tracking, exploration and recognition, which  traditionally have been seen as separate problems in active-vision.We propose a probabilistic generative framework  based on a mixture of Kalman filters  to use predictions in both recognition and sensor-control. This framework can efficiently use the observations of one element in a dynamic environment to provide information on other elements, and consequently enables guided exploration of the environment.Experiments on a humanoid robot  observing a human executing goal-oriented actions demonstrated improvement on recognition time and precision over baseline approaches."
668,A Sparse and Adaptive Prior for Time-Dependent Model Parameters,"We consider the scenario where the parameters of a probabilistic model are expected to vary over time. We construct a novel prior distribution that promotessparsity and adapts the strength of correlation between parameters at successivetimesteps, based on the data. We derive approximate variational inference procedures for learning and prediction with this prior. We test the approach on twotasks: forecasting ?nancial quanitities from relevant text, and modeling languagecontingent on time-varying ?nancial measurements."
669,Co-Regularized Hashing for Multimodal Data,"Hashing-based methods provide a very promising approach to large-scale similarity search.  To obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically.  In this paper, we study hash function learning in the context of multimodal data.  We propose a novel multimodal hash function learning method, called Co-Regularized Hashing (CRH), based on a boosted co-regularization framework.  The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized.  We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets."
670,Convergence and Energy Landscape for Cheeger Cut Clustering,"Unsupervised clustering of scattered, noisy and high-dimensional data points is an important and difficult problem. Continuous relaxations of balanced cut problems  yield excellent clustering results. This paper provides rigorous convergence results for two algorithms that solve the relaxed Cheeger Cut minimization.  The first algorithm is a new steepest descent algorithm and the second one is a slight modification of the Inverse Power Method algorithm \cite{pro:HeinBuhler10OneSpec}. While the steepest descent algorithm has better theoretical convergence properties,  in practice both algorithm perform equally.  We also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem, and relate this characterization to the convergence of the algorithms."
671,Leveraging for Fitting Linear Models in Large-scale Data,"Recent empirical and theoretical work has focused on using the empirical statistical leverage scores of data matrices in order to develop improved algorithms for common matrix problems such as least-squares approximation and low-rank matrix approximation.  Existing work focuses on algorithmic issues such as worst-case running times or on the usefulness of this approach in downstream data applications.  Here, we examine the statistical properties of this leveraging paradigm in the context of fitting a linear model to data.  We derive the mean squared errors for two related leveraging-based estimates and for uniform sampling estimates.  Depending on the the mean, variance and skewness of the leverage scores, one procedure or another is preferred.  We also describe the empirical behavior of these procedures on several synthetic and real data sets."
672,Symbolic Dynamic Programming for Continuous State and Observation POMDPs,"Partially-observable Markov decision processes (POMDPs) provide a powerfulmodel for real-world sequential decision-making problems. In recent years, point-based value iteration methods have proven to be extremely effective techniquesfor ?nding (approximately) optimal dynamic programming solutions to POMDPswhen an initial set of belief states is known. However, no point-based work hasprovided exact point-based backups for both continuous state and observationspaces, which we tackle in this paper. Our key insight is that while there maybe an in?nite number of possible observations, there are only a ?nite number ofobservation partitionings that are relevant for optimal decision-making when a?nite, ?xed set of reachable belief states is known. To this end, we make twoimportant contributions: (1) we show how previous exact symbolic dynamic pro-gramming solutions for continuous state MDPs can be generalized to continu-ous state POMDPs with discrete observations, and (2) we show how this solutioncan be further extended via recently developed symbolic methods to continuousstate and observations to derive the minimal relevant observation partitioning forpotentially correlated, multivariate observation spaces. We demonstrate proof-of-concept results on uni- and multi-variate state and observation steam plant control."
673,Bayesian Probabilistic Co-Subspace Addition,"For modeling data matrices, this paper introduces Probabilistic Co-Subspace Addition (PCSA) model by simultaneously capturing the dependent structures among both rows and columns. Briefly, PCSA assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two features, which distribute in the row-wise and column-wise latent subspaces. Consequently, it captures the dependencies among entries intricately, and is able to model the non-Gaussian and heteroscedastic density. Variational inference is proposed on PCSA for  approximate Bayesian learning, where the updating for posteriors is formulated into the problem of solving Sylvester equations. Furthermore, PCSA is extended to tackling and filling missing values, to adapting its sparseness, and to modelling tensor data. In comparison with several state-of-art approaches, experiments demonstrate the effectiveness and efficiency of Bayesian (sparse) PCSA on modeling matrix (tensor) data and filling missing values."
675,Incremental Learning Hierarchical Functional Categories for Interacting Objects,"We present an algorithm which has been designed as an online learning module in a cognitive system to perform two fundamental and often coupled tasks: category learning and value function approximation. There are four important features in our algorithm. First, categories are incrementally constructed without using externally provided category labels. Second, categories are organized into hierarchies, making the algorithm scalable with respect to diversity of the inputs. Third, different from unsupervised hierarchical clustering algorithms, categorization in our algorithm can be influenced by externally provided feedbacks (in form of utility values or rewards). Finally, when there are multiple interacting objects from different domains (such as prey and weapon), multiple category hierarchies are learned simultaneously. We use systematically generated synthetic data to evaluate our algorithm in a function approximation task, where our algorithm is shown to learn significantly faster than standard machine learning algorithms used in cognitive systems."
676,Identity maps and their extensions on parameter spaces: Applications to anomaly detection in video processing,"It is now commonplace for analysts to model data on non-Euclidean spaces.   The non-linear structure of these parameter spaces can accurately reflect non-linear nature of complex data, and analysis is performed using algorithms which respect the geometry of the parameter space.  Several well-known linear algorithms have been generalized to analogous algorithms on parameter spaces such as k-means and PCA. These generalized algorithms have been shown to be effective ways to cluster and classify data on parameter spaces.  It is the aim of this paper to generalize an established (Euclidean based) Identity map extension (MSET) to well-known parameter spaces.  An identity map extension (IME) is a function that acts as the identity mapping when restricted to a special subset, and this property has made the MSET algorithm useful for anomaly detection.  We define a generalization of this map to several parameter spaces, we prove it has the IME property, and we evaluate its performance as an anomaly detector on a real dataset."
677,Scaled Gradients on Grassmann Manifolds for Matrix Completion,This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices while maintaining established global convegence and exact recovery guarantees. We also show the connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure. Our conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods.
678,Dual Semi-Supervised Co-Clustering Informed by Geometry,"Co-clustering algorithms, which group a data matrix based on the similarities of both rows (samples) and columns (features), often yield impressive performanceimprovement over traditional one-side clustering approaches. Efficient utilizing partial supervision in the form of row labels as well as column labels is still a challenge, especially when the number of labels is small. Moreover, since many real world data are sampled from a low dimensional manifold, effective co-clusteringalgorithms will depend upon the intrinsic structures of rows as well as columns of the data matrix. In this paper we propose dual semi-supervised co-clusteringinformed by geometry (DSCIG) to address these two issues. First, we provide a general framework for co-clustering that incorporates partial supervision informationand preserve local geometry. Second, we augment this framework with an additional step for similarity propagation that generate richer supervision information.To manage the different applications of DSCIG, we derive an alternative optimization procedure and show the convergence is guaranteed theoretically ."
679,Online Self-Supervised Segmentation of Dynamic Objects,"We address the problem of learning models to automatically segment dynamic objects in an urban environment from a moving camera without manual labelling, in an online, self-supervised manner. We use input images obtained from a single uncalibrated camera placed on top of a moving vehicle, extracting and matching pairs of sparse features that represent the optical flow information between frames. This optical flow information is initially divided into two classes, static or dynamic, where the static class represents features that comply to the constraints provided by the camera motion and the dynamic class represents the ones that do not. This initial classification is used to incrementally train a Gaussian Process (GP) classifier to segment dynamic objects in new images. The hyperparameters of the GP covariance function are optimized online during navigation, and the available self-supervised dataset is updated as new relevant data is added and redundant data is removed, resulting in a near-constant computing time even after long periods of navigation. The output is a vector containing the probability that each pixel in the image belongs to either the static or dynamic class (ranging from 0 to 1), along with the corresponding uncertainty estimate of the classification. Experiments conducted in an urban environment, with cars and pedestrians as dynamic objects and no prior knowledge or additional sensors, show promising results even when the vehicle is moving at considerable speeds (up to 50 km/h), a scenario that produces a large quantity of featureless regions and false matches that is very challenging for conventional approaches. "
680,Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging ,"Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model?s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject?s conversion to Alzheimer?s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here, our new model outperforms the state of the art (p-values << 10?3 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity)."
681,Stochastic blockmodeling of relational event dynamics,"For continuous-time network data, several approaches have recently been proposed for modeling dyadic event rates conditioned on the observed history of events and nodal or dyadic covariates.  In many cases, however, interaction propensities -- and even the underlying mechanisms of interaction -- vary systematically across subgroups whose identities are unobserved.  For static networks, such heterogeneity has been treated via methods such as stochastic blockmodeling, which operate by assuming latent groups of individuals with similar tendencies in their group-wise interactions.  Here, we combine these two approaches by positing a latent partition of the node set such that event dynamics within and between subsets evolve in potentially distinct ways.  We illustrate the use of our model family by application to several forms of dyadic interaction data, including email communication and Twitter direct messages.  Parameter estimates from the fitted models clearly reveal heterogeneity in the dynamics among groups of individuals.  We also find that the fitted models have better predictive accuracy than either baseline models or relational event models without latent structure.  Our approach illustrates the utility of latent structure methods based on detailed dynamics, which can succeed even in the absence of differences in marginal interaction rates across groups. "
682,Privacy Aware Learning,"We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.  In this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator."
683,Alternating Update Procedures for Unconstrained and Constrained Binary Matrix Factorization,"In general, binary matrix factorization (BMF) refers to the problem of finding a matrix product of two binary low rank matrices such that the difference between the matrix product  and a given binary matrix is minimized. In the current literature on BMF,  the matrix product is not required to be  binary. We call this  unconstrained BMF (UBMF) and similarly constrained BMF (CBMF) if the matrix product is required to be  binary. In this paper, we first introduce two specific variants of CBMF and discuss the relationship between BMF and UBMF.Then we propose alternating update procedures for both UBMF and CBMF. In every iteration of the proposed  procedure, we solve a specific binary quadratic programming (BQP) problem to update the involved matrix argument. Two different algorithms are presented to cope with the BQP subproblem in the procedure. In particular, we show that the BQP subproblem can be reformulated as a specific clustering problem. Based on the clustering reformulation, we also derive an effective 2-approximation algorithm for CBMF. By exploring the interrelation between UBMF and CBMF, we show that  we can  obtain good  approximation to rank-1  UBMF. The complexity of the proposed algorithms is  discussed. Numerical results show that the proposed algorithms for UBMF are able to find better solutions in less CPU time than several other algorithms in the literature, and the solution obtained from CBMF is very close to that of UBMF."
684,Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
685,Multi-Armed Bandit Problem with Budget Constraint and Variable Costs,"In this paper, we study the multi-armed bandit problem with budget constraint and variable costs (MAB-BV). In this setting, pulling each arm is associated with an unknown and variable cost, and the objective of a learning algorithm is to pull a sequence of arms in order to maximize the expected total reward with the number of pulled arms complying with the budget constraint. This new setting describes many Internet applications (e.g., sponsored search and cloud computing) in a more accurate manner than previous settings that either assume the pulling of arms is costless or with a fixed cost. To tackle this new kind of multi-armed bandit problem, we extend the UCB algorithms by selecting arms according to the reward-cost ratio, and propose a new algorithm called UCB-BV. Our empirical results verify the effectiveness of this algorithm. Although the extension in UCB-BV seems natural and simple, and it is practically effective, the theoretical analysis on its regret bound turns out to be very difficult. We develop a set of new proof techniques and obtain a regret bound of $O(\ln B)$. Furthermore, we show that when applying the proposed algorithm to the setting with fixed costs (which is our special case), one can improve the corresponding regret bound obtained so far."
686,Optimal Computational Trade-Off of Inexact Proximal Methods,"In this paper, we investigate the trade-off between convergence rate and computational cost when minimizing a composite functional with proximal-gradient methods, which are popular optimization tools in machine learning. We consider the case when the proximity operator is approximated via an iterative procedure, which leads to an algorithm with two nested loops. We show that the computationally optimal strategy to reach a desired accuracy in finite time is to set the number of inner iterations to a constant, which differs from the strategy indicated by a convergence rate analysis. In the process, we also present a new procedure called SIP that is both computationally and practically efficient. Our numerical experiments confirm the theoretical findings and suggest that SIP can be a very competitive alternative to the standard procedure."
687,Smooth and Monotone Covariance Regularization,"The dangers of using the sample covariance matrix obtained from scarce data in high-dimensional settings are well recognized. In particular, the inconsistency of its eigenvalue spectrum has grave implications for modeling risk within the Markowitz portfolios framework. A variety of approaches to improve the covariance estimates exploit knowledge of structure in the data, including low-rank models (principal component and factor analysis), banded models, sparse inverse covariances, and parametric models. We investigate a different nonparametric prior for random vectors indexed along a low-dimensional manifold: we assume that the covariance matrix is monotone and smooth with respect to this indexing. This fits a variety of problems including interest-rate risk modeling in econometrics, and sensor array noise modeling. We formulate the estimation problem in a convex-optimization framework as a semidefinite-programming problem, and develop efficient first-order methods to solve it. We apply our framework on a number of examples with limited, missing and asynchronous data, and show that it has the potential to provide more accurate covariance matrix estimates than existing methods, and exhibits a desirable eigenvalue-spectrum correction effect."
688,Nearest Nonnegative Affine Subspace Classification Using Lotka-Volterra,"Nearest Subspace Classifier (NSC) is an important and well knownmethod in many multiclass classification problems. NSC uses theshortest projection distance between the testing data and thesubspaces spanned by the training samples of each class ascriteria for classification. To achieve good classificationresults, NSC requires that each test data is located on or atleast very close to the associated subspace. However, thisrequirement cannot be always met. Taking an example, objectclassification, it can be considered that an object withindifferent angles can span a subspace. There is an ideal propertyfor such images, i.e., the testing images from the same candidateare in fact located in the spanned subspace. In actualapplications, the image dimension is usually too high to dealwith. Reducing the dimension is necessary in many situations. Aproblem is that by reducing the dimension, the ideal propertycannot be guaranteed. This paper proposes a new classificationmethod: Nearest Nonnegative Affine Subspace Classifier (NNASC).Unlike the NSC, NNASC uses the shortest distance between thetesting data and the associated nonnegative affine subspace ascriteria for classification. Lotka-Volterra Recurrent NeuralNetworks(LV RNNs) are employed to solve the NNASC optimizationproblem. Three different kinds of databases are tested by theproposed algorithm. It demonstrates that NNASC outperforms the NSCand some other classifiers, especially, if the data is in lowdimensions."
689,Discovering Voxel-Level Functional Connectivity Between Cortical Regions,"Functional connectivity patterns are known to exist in the human brain at the millimeter scale, but the standard fMRI connectivity measure only computes functional correlations at a coarse level. We present the first method which identifies fine-grained functional connectivity between any two brain regions by simultaneously learning voxel-level connectivity maps over both regions. We show how to formulate this problem as a constrained least-squared optimization, with a spatial regularization term that allows connectivity maps to be learned much more efficiently. This optimization problem can be solved using a trust region approach, and can automatically discover connectivity between multiple distinct voxel clusters in the two regions. We validate our method in two experiments, demonstrating that we can successfully learn subregion connectivity structures from a small amount of training data. Our approach is shown to be substantially better at estimating fine-grained connectivity differences than state-of-the-art subregion connectivity methods, all of which learn maps over only one region at a time."
692,Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods,"We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{\dim}$ in convergence rate over traditional stochastic gradient methods, where $\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors."
693,The impact on mid?level vision of statistically optimal divisive normalization in V1,"The first two areas of the primate visual cortex (V1 and V2) provide a paradigmatic example of hierarchical computation in the brain. However both the interactions between the two areas, and the functional properties of V2, are not well understood. Here we present insights gained from statistical models of natural scenes. In particular, we study the impact of V1 output nonlinearities on the statistics seen by V2. We focus on divisive normalization, a canonical computation that has been found in many neural areas and modalities. We consider models of V1 complex cells with (and without) different forms of surround normalization derived from the Gaussian Scale Mixture (GSM) generative model of natural scenes, and a Mixture of GSMs that also accounts for image non?homogeneities. When surround normalization is used, followed by ordinary PCA to linearly combine V1 responses across space, then V2-like receptive fields emerge. To provide a more quantitative assessment, we compare the resulting 2?stage models on perceptual tasks of figure/ground judgment and object recognition; in both cases we find systematic advantages for using a V1 stage with statistically optimal surround normalization."
694,Near optimal policy decomposition for tasks with multiple goals explains human behavior,"The natural environment presents people with multiple potential goals that compete for action selection. Recent studies suggest that the brain generates several concurrent and partially prepared actions associated with alternative goals and use perceptual information to drive the goal competition, until a single action is selected. In the current study, we propose a near-optimal policy decomposition that the brain may use in visuomotor tasks with multiple competing goals. We show how human and animal strategies in the presence of competing goals can be expressed as a weighted mixture of multiple control policies, each of which produces a sequence of actions associated with a specific goal. We evaluate the performance of the proposed framework in a series of simulated reaching and saccade tasks with multiple targets in environments with and without presence of obstacles. The results show that the proposed model can qualitatively predict many aspects of human/animal strategies in goal-directed movements."
695,Hard and Easy Distributions of Bayesian Networks for Junction Tree Computation ,"The effort associated with Bayesian network computation is vital in many infer-ence and machine learning settings. In this paper, we introduce a novel algorithm,GPART, that generates synthetic Bayesian networks that reflect several input pa-rameters. Using the algorithm, we investigate how various parameters of Bayesiannetworks can affect junction tree characteristics and hence computation time. Wegeneralize previous approaches to randomly generating Bayesian network by (i)introducing a novel depth parameter as well as (ii) allowing state space size andnumber of parameters for a non-root node to be probability distributions. In ex-periments, we surprisingly find that increasing our novel depth parameter dramati-cally increases clique tree size and computation time. Using parameters computedfrom application networks as parameters in GPART, and comparing the resultingjunction trees, we better understand the similarities and differences between ap-plication and synthetic Bayesian networks."
696,Unidimensionality of sequential effects in human response times,Evidence is present showing that sequential effects occurring in 2-alternative forced-choice tasks are in fact a unidimensional phenomenon. Individual differences data from four different experiments was analyzed a multidimensional scaling analysis was performed on the distances between individual results. We found that the sequential effects described previously in the literature fit well in single dimension space. A dynamic belief model fit well to data from individual subjects and its parameters correlated strongly with sequential effects measures and distances within the one-dimensional space identified.
697,Hierarchical Optimistic Region Selection driven by Curiosity,"This paper aims to take a step forwards making the term ``intrinsic motivation'' from reinforcement learning theoretically well founded,focusing on curiosity-driven learning. To that end, we consider the setting where, a fixed partition P of a continuous space X being given,and a process \nu defined on X being unknown,we are asked to sequentially decide which cell of the partition to select as well as where to sample \nu in that cell,in order to minimize a loss function that is inspired from previous work on curiosity-driven learning.The loss on each cell consists of one term measuring a simple worst case quadratic sampling error,and a penalty term proportional to the range of the variance in that cell.The corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region,and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization canbe used in order to solve this problem. The resulting procedure,called Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C) is provided together with a finite-time regret analysis."
698,Sparse Prediction with the $k$-Support Norm,"  We derive a novel norm that corresponds to the tightest convex  relaxation of sparsity combined with an $\ell_2$ penalty. We show  that this new norm provides a tighter relaxation than the elastic  net, and is thus a good replacement for the Lasso or the elastic net  in sparse prediction problems.  But through studying our new norm,  we also bound the looseness of the elastic net, thus shedding new  light on it and providing justification for its use."
699,Robust Sparse Regression and Matching Pursuit,"In this paper we consider support recovery in sparse regression, when some number $n_1$ out of $n+n_1$ total covariate/response pairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interested in understanding how many outliers, $n_1$, we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables) provide guarantees on support recovery. Perhaps surprisingly, we also show that the natural brute force algorithm that searches over all subsets of $n$ covariate/response pairs, and all subsets of possible support coordinates in order to minimize regression error, is remarkably poor, unable to correctly identify the support with even $n_1 = O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in the basic setting we consider, where all authentic measurements and noise are independent and Gaussian. In this setting, we provide a simple algorithm that gives stronger performance guarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$ corrupted points, where $p$ is the dimension of the signal to be recovered."
700,Graphical Model Selection Using Junction Trees: Decompositions and Active Learning,"This paper proposes a framework for decomposing the undirected graphical model selection (UGMS) problem into multiple subproblems over clusters and separators of a junction tree.  Under certain conditions, we show that the junction tree framework significantly weakens the sufficient conditions on the number of observations required for high-dimensional consistent graph estimation.  When the conditions on the graphical model do not hold, we recover the standard conditions for high-dimensional consistency.  This motivates the use of our framework as a wrapper around algorithms for more accurate graph estimation.  Further, we show that the subproblems over the clusters and separators can be solved independently, which allows for using different regularization parameters or different UGMS algorithms to learn different parts of the graph.  Finally, the junction tree framework motivates active learning for UGMS that sequentially draws observations from the graphical model based on prior observations.  In the high-dimensional setting, we identify conditions under which the sufficient conditions on the number of scalar observations needed for an active algorithm is significantly less than that needed for a non-active algorithm.  Intuitively, the active learning algorithm draws more observations from parts of the graph that are difficult to learn and less measurements from parts of the graph that are easy to learn.  Our numerical results clearly identify the advantages of using the junction tree framework for both non-active and active learning for UGMS."
701,Active Learning of Multi-Index Function Models,"We consider the problem of actively learning \textit{multi-index} functions of the form $f(\vecx) = g(\matA\vecx)= \sum_{i=1}^k g_i(\veca_i^T\vecx)$ from point evaluations of $f$. We assume that the function $f$ is defined on an $\ell_2$-ball in $\Real^d$, $g$ is twice continuously differentiable almost everywhere, and $\matA \in \mathbb{R}^{k \times d}$ is a rank $k$ matrix, where $k \ll d$.  We propose a randomized, active sampling scheme for estimating such functions with uniform approximation guarantees. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive an estimator of the function $f$ along with sample complexity bounds. We also characterize the noise robustness of the scheme, and provide empirical evidence that the high-dimensional scaling of our sample complexity bounds are quite accurate."
702,Neurocomputational Model of Cognitive Process based on Working Memory,"Retaining information on previous stimuli and activities for short periods of time is crucial for processing complex tasks occurring in real life. In this paper, we propose to use working memory as a short-term storage for cognitive processes. We present two architectures for the working memory and implement those using spiking neurons. The results show that a temporal sequence of features are orderly captured and converted into a spatial pattern of persistent activities of neurons by the working memory."
703,Optimally Learning Hashing Functions Using Column Generation,"Fast nearest neighbor search is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learningdata-dependent hashing functions using machine learning techniques have been developed. In this work, we propose a column generation based method for learninghashing functions on the basis of proximity comparison information. Given a set of examples of proximity comparisons among triples of data points the methodlearns hashing functions which preserve the relative distances between them as well as possible within the large-margin learning framework. The learning procedureis implemented using column generation and hence is named CGHash. At each iteration of column generation procedure the best hashing function is selected. Unlike other recent hashing methods, our method generalizes to newpoints naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposedmethod learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on publicly available datasets."
705,Robust Structural Metric Learning,"Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degradesaccordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation,while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methodsin both high- and low-noise settings."
706,Learning Multiple Tasks using Shared Hypotheses,"In this work we consider a setting where we have a very large number  of related tasks with few examples from each individual task. Rather  than either learning each task individually (and having a large  generalization error) or learning all the tasks together using a  single hypothesis (and suffering a potentially large inherent  error), we consider learning a small pool of {\em shared    hypotheses}. Each task is then mapped to a single hypothesis in  the pool (hard association). We derive VC dimension generalization  bounds for our model, based on the number of tasks, shared  hypothesis and the VC dimension of the hypotheses  class. We conducted experiments with both synthetic problems and  sentiment of reviews, which strongly support our approach."
707,Differentially Private Learning with Kernels,"In this paper, we consider the problem of differentially private learning using kernel empirical risk minimization (ERM) where access to the training features is through a kernel function only. Existing work for this problem is either for the linear kernel or for translation invariant kernel, where (approximate) training features are available explicitly and furthermore their generalization error guarantees are dependent on the data dimensionality. Restricting access to data through kernel functions eliminates possibility of explicitly releasing the optima w^* to the kernel ERM. To alleviate this problem, we define three different models for differential private learning using kernel ERM. Our first model is an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. In the second model, learner sends back a differentially private version of the optimal parameter vector w^* but requires to see a small subset of unlabeled test set beforehand. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of w^*. For each of the model, we derive algorithms inspired by the technique for online database release by Gupta et al. 2011 and provide privacy as well as ``goodness'' guarantees. Furthermore, we show that our method can be applied to the setting of Rubinstein et al. 2009, Chaudhuri et al. 2011 also and obtain similar generalization error bounds with two distinctions: a) our bounds are independent of the data dimensionality, b) our sample complexity bounds have worse dependence on the required generalization error."
708,Semi-supervised Attribute Pattern Learning,"We focus on combining multiple views of object features to learn attribute patterns for semi-supervised classification. By formulating the problem as a semi-supervised classification with constrains: objects in the same class are similar to labeled data of the class upon each view of features and the quantized multi-modality attributes generated from the multiview features, an iterative attribute pattern learning method are proposed. In our approach, feature attributes are initialized by Euclidean Weighted Constrained-KMeans in each view, followed by attribute pattern discovery via Hamming Weighted Constrained-KMeans on the multi-modality attributes. These attribute patterns can further help to adjust the results of feature attributes in individual views by the feature co-occurrence. We therefore adopt a self-learning strategy to reconcile the disagreements between the classifications of the individual views of features and the multi-modality attributes with guaranteed convergence. On the other hand, we also achieve a variant of our method, where the supervision information is only imposed upon the multi-modality attributes for classification. The classifications on both synthetic and real-world datasets demonstrate that our attribute pattern learning method achieve better performance than the most informative view of features and concatenated features of all views by utilizing the complementary information from multiple views by feature co-occurrence."
709,On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization,"The ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge. This paper presents a new reinforcement-learning algorithm, called iKBSF, which extends the benefits of kernel-based learning to the on-line scenario. As a kernel-based method, the proposed algorithm is stable and has good convergence properties. However, unlike other similar algorithms,iKBSF's space complexity is independent of the number of sample transitions, and as a result it can process an arbitrary amount of data. We present theoretical results showing that iKBSF can approximate (to any level of accuracy) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator. In order to show the effectiveness of the proposed algorithm in practice, we apply iKBSF to the challenging three-pole balancing task, where the ability to process a large number of transitions is crucial for achieving a high success rate."
710,Semiparametric Bigraphical Models,"In multivariate analysis, a Gaussian bigraphical model is commonly used for modeling matrix-valued data. In this paper we propose a semiparametric extension of the Gaussian bigraphical model, called the nonparanormal bigraphical model. Nonparametric rank-based regularization estimators are exploited to  estimate the sparse precision matrices and graphs under a  penalized profile likelihood framework. Theoretically,  our semiparametric procedure achieves the  parametric rate of convergence for both parameter estimation and graph recovery. Empirically, our semiparametric approach significantly outperforms its parametric counterpart for non-Gaussian data and behaves competitive even for Gaussian data. "
711,Multiclass Clustering using a Semidefinite Relaxation,"Spectral and other cut-based relaxations have been applied to graph clustering problems. In this paper, we propose a novel semidefinite relaxation for graph clustering known as Max-cut clustering. The clustering problem is formulated in terms of a discrete optimization problem and then relaxed to a SDP. To make the optimization scalable, we represent the SDP by a low-rank factorized approximation that reduces the number of variables, and then use a simple projected gradient method to solve it. To obtain the clustering, we propose a reweighted rounding scheme to get integral solutions. We also extend this formulation to a global approach to multi-class clustering and MAP inference in graphical models. Experimental results indicate that we outperform state-of-art several clustering methods. The algorithm is extended to perform MAP inference in graphical models and outperforms competing methods."
712,Convex Loss Minimization with Noisy Labels,"We study supervised binary classification in the presence of random classification noise. This setting can be thought of as a particular instance of learning from partial information: the learner, instead of seeing the actual labels, sees labels that have been flipped with some small probability. Using a simple unbiased estimator of the gradient of the loss, we derive online regret bounds for convex loss functions. These bounds immediately lead to efficient algorithms for learning from iid data with noisy labels via a simple online-to-batch conversion. We point out an interesting situation for hinge loss: a batch method using unbiased estimates leads to a non-convex problem whereas online learning using unbiased estimates is still efficient and comes with theoretical guarantees. We show that convexity of the batch problem can be retained if the loss function satisfies a simple symmetry condition. We illustrate the usefulness of our techniques on synthetic and real data."
713,Higher-order Nonparametric Models for Recognition by Analogy,"Nonparametric classification methods such as nearest neighbor offer the ability tolearn by association, leveraging large amounts of data, avoiding a training phase,and placing no assumptions on the structure of label space. However, such meth-ods often perform poorly due to the limited ability of typical distance functions tocapture complex relationships in the data. We propose a method for learning dis-tance functions using higher-order nonparametric models, resulting in better per-formance while still learning by association with no training phase. Our methodreplaces single example association with pair association, and can be interpretedas finding analogies among training and test examples. We test our method onRGB-D [1], a multi-view object data set, which lets us learn implicitly when dif-ferent 2D shapes describe a similar 3D structure. We show that our method isparticularly beneficial in the one-shot transfer regime, where only one exampleis available for a test category. Where traditional supervised learning methodsperform poorly, our method can use the relationships between objects in differentcategories to learn the structure of categories with impoverished training data."
714,A Performance Function for Multi-class Classification,"A performance function for multi-class classification is proposed in this paper. This performance function takes Nearest Subspace (NS) residual together with Collaborative Representation (CR) residual as variables. Strong underlying geometric explanations make those well-known residual measurements effective for multi-class classification problem. Nearest Subspace Classification (NSC) is a local measurement that considers distance between testing sample and each class, while Collaborative Representation based Classifier (CRC) is a global method measuring both intra-class and inter-class measurements. These two measurements are independent to each other. The first and the second order Taylor series of the performance function are analyzed, which characterizes this function well in some degree. A Second Order Performance Function (SOPF) is derived by involving a quadratic term of the first order terms and a product term. The SOPF contains two parameters with a positive factor constraint. A classifier based on SOPF is proposed, which improves a recent reported classifier called CROC(Collaborative Representation Optimized Classifier). The proposed algorithm is tested against human face and handwritten digits recognition and it achieves competitive classification result comparing to baseline methods. A large range of parameter configuration is acceptable once the positive factor constraint is satisfied."
716,Communication-Efficient Algorithms for Statistical Optimization,"We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$. Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset."
718,Identifiability and Unmixing of Latent Parse Trees,"This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models."
719,A Novel Adaptive Geometric Mapping For Multi-Class Classification,"Solving multi-class problems is one of the challenging problems in machine learning field. Although several powerful binary classifiers have been developed, it is still an ongoing research issue to effectively extend these binary classifiers for multi-class cases.  In this paper we propose a new method for multi-class classification which is based on an adaptive mapping of data points into linearly separable classes. Our strategy, which is inspired from geometry laws, can handle multi-class cases directly. It has the capability of not increasing the dimension of data while separating them in a stable dimension. This algorithm generates the suitable mapping matrix using a vector of linear coefficients. We evaluate the performance of our classifier with different state-of-art support vector machine based methods and KNN on five data sets named as IRIS, WDBC, Glass, Wine, and Liver disorders. On most of these datasets, our implementation outperforms the best reported results and achieves comparable accuracy on the rest which demonstrates its effectiveness."
720,Preparing Deep Belief Networks for Pratical Tasks,"Deep Belief Networks (DBNs) is a probabilistic generative models composed of multiple layers of stochastic, latent variables. The network can learn many layers of features on various type of data such as gray scaled images, color images and acoustic data. This paper further examined the ability of DBNs to interpret the binary representation of data. The performance is validated by learning given distributions such as normal distribution, Poisson distribution and random number generator. We have shown that Deep Believe Networks can successfully learn the probability distribution with binary encoded dataset. With this property, we can further extend DBNs into states or properties prediction application,we will provide a example showing that DBNs can take multiple binary encoded parameter as input vector and predict the belong category of these input. Generally, the sensory input of DBNs contains information belong to a certain timestep, that is, the prediction depends only on the input. However, in some practical tasks, prediction often depend not only on the current state but also the history of states. We propose a method combining DBNs with Echo State Networks(ESNs), using the properties of ESNs' reservoir to encoded the history of previous states which gives us an idea of artificial dreaming."
721,Learning Hierarchical Spatial Tiling Representation for Scene Tagging,"In order to name and localize semantic tags/attributes on natural scene images, in this paper, we first propose a structure learning method to learn a novel representation for scene modeling, namely Hierarchical Space Tiling (HST). It is able to account for the structure variations of scene configurations using different parts/words in the learned tilling dictionary. Then, the association relationship between a part and a semantic tag/attribute is discovered by exploring their mutual information on scene images. Finally, given a naked image, we first parse it into a tree structure with the learned HST model, then assign tags to the terminal nodes/parts on the hierarchy. We evaluate the advantages of the proposed method from three aspects. (i) The proposed HST is compact and less ambiguous in constructing the compositions of scenes. (ii) The semantic tags are named and localized accurately on scene images. (iii) It has scalability potential to real applications by showing that the parsing + tagging is extremely fast. "
722,Encoding Natural Images with Mean and Covariance,"Here we show how the mean and covariance of the statistical structure of natural images can be learned efficiently by using maximum likelihood estimation. In particular, the parameters of mean and covariance are learned with two sets of latent variables independently. Simulation results demonstrate that the model is ableto successfully capture details of the natural image distribution not represented by either covariance or mean alone. The joint approach is thus a step towards a more realistic natural image representation."
723,Constructing ?2-graph for Clustering,"Constructing a sparse similarity graph is an important step in graph-oriented clustering algorithms. In a similarity graph, the vertex denotes a data point and the connection weight between two data points represents the similarity. Some recent works use ?1-minimization based sparse coefficients to construct the graph for various applications, and impressive results are achieved. This paper proposes a method to construct the similarity graph, called ?2-graph, by using ?2-minimization based representation coefficients. This graph can produce a block-sparse similarity graph via enforcing locality onto the non-sparse representation. The representation is derived via solving an optimization problem to obtain an interesting closed-form solution. Experimental results using several facial databases demonstrate that the proposed method outperforms two state-of-the-art ?1-minimization based clustering algorithms, i.e., Sparse Subspace Clustering [1, 2] and Low Rank Representation [3], in accuracy, robustness and time saving."
724,Bayesian nonparametric models for ranked data,"We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items.   Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process.  We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation.  We then develop a time-varying extension of our model, and apply our model to the New York Times lists of weekly bestselling books."
725,An Indexing Method for Efficient Model-Based Search,"Large databases of patterns, such as faces, body poses, fingerprints, or gestures, are becoming increasingly widespread, thanks to advances in computer technology. In this paper we focus on the problem of efficient search in such databases, when using model-based search. In model-based search, the user submits as a query a classifier, that has been trained to recognize the type of patterns that the user wants to retrieve. While model-based search can lead to good retrieval accuracy, the efficiency of model-based search can be inadequate if we need to apply the query classifier to every single database pattern. We propose a method for improving the efficiency of model-based search. The proposed method assumes that classifiers have been trained using JointBoost, and operates by defining an embedding, which maps both classifiers and database patterns into a common vector space. Using this embedding, the problem of finding the database patterns maximizing the response of the query classifier is reduced to a nearest neighbor search problem in a vector space. This reduction allows the use of standard vector indexing method to speed up the search. In our experiments, we show that the proposed embedding, together with a simple PCA-based indexing scheme, significantly improve the efficiency of model-based search, as measured on a database of face images constructed from the public FRGC-2 dataset."
726,Object Detection from Multiple Overlapping Views,"We present a method for object detection in a multi view 3D model. We use highly overlapping views, geometric data, and semantic surface classification in order to boost existing 2D algorithms. Specifically, a 3D model is computed from the overlapping views, and the model is segmented into semantic labels using height information, color and planar qualities. 2D detector is run on all images and then detections are mapped into 3D via the model. The detections are clustered in 3D and represented by 3D boxes. Finally, the detections, visibility maps and semantic labels are combined using a Support Vector Machine to achieve a more robust object detector."
727,Learning Granger Graphical Models via Alternating Direction Method of Multipliers,"This paper presents a recent powerful algorithm, namely, the alternating direction method of multipliers (ADMM) for solving topology selection problems in Granger graphical models of autoregressive processes. The existence of a directed edge from node $j$ to node $i$ in the graph can be specified by the nonzero $(i,j)$ entry of the autoregressive coefficients. The problem of estimating the graph topology is formulated as a least-squares problem with an $\ell_1$-type regularization and can be regarded as a variant of Group Lasso problem. The value of the regularization parameter which controls the density of the estimated graph can be determined by minimizing Bayes information criterion score. We illustrate the idea and verify the performance of the ADMM algorithm on randomly generated data sets. This approach is finally applied on Google Flu Trends data set to learn a causal structure of flu activities from $51$ states in the USA."
728,Feature-aware Label Space Dimension Reduction for Multi-label Classification,"Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets."
729,Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions,"We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a $\order(\pdim/T)$ convergencerate for strongly convex objectives in $\pdim$ dimensions and $\order(\sqrt{\spindex( \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our algorithm is based on successively solving a series of $\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error ofour solution after $T$ iterations is at most$\order(\spindex(\log\pdim)/T)$, with natural extensions toapproximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem."
730,Sparse Matrix based Random Projection for Face Recognition,"Random projection (RP) is a powerful method in dimensionality reduction for itsdata independence and lower computation requirement. For the application ofRP, the construction of random matrix is critical due to its instability in performance.However, there is few directional work in this respect. Although a fewtheoretical work has proposed some matrices in terms of performance distortionand computation cost in the past decade, to the best of our knowledge, there is nocomprehensive theoretical or experimental work to compare their performance. Inthis paper, we attempt to evaluate current popular RP matrices by extensive experimentswith face recognition, and propose one kind of most sparsest RP matrices,which shows better performance than existing RP matrices with nearly the lowestcomputation complexity."
731,Recovery Guarantees of Augmented Trace Norm Models in Tensor Recovery,"This paper studies the recovery guarantees of the models of minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ where $\mathcal{X}$ is a tensor and $\|\mathcal{X}\|_*$ and $\|\mathcal{X}\|_F$ are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors.In particular, they enjoy exact guarantees similar to those known for minimizing$\|\mathcal{X}\|_*$ under the conditions on the sensing operator such as its null-space property, restrictedisometry property, or spherical section property. To recover a low-rank tensor$\mathcal{X}^0$, minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ returns the same solution as minimizing $\|\mathcal{X}\|_*$ almost whenever$\alpha\geq10\mathop {\max}\limits_{i}\|X^0_{(i)}\|_2$."
732,Graphical Gaussian Vector for Image Categorization,"This paper proposes a novel image representation called a Graphical Gaussian Vector, which is a counterpart of the codebook and local feature matching approaches. In our method, we model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. We consider the parameter of GMRF as a feature vector of the image. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Finally we define a new image feature by embedding the metric into the parameters, which can be directly applied to scalable linear classifiers. Our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. As the proposed method simply calculates the local auto-correlations of local features, it is able to achieve both high classification accuracy and high efficiency."
733,Joint Modeling of a Matrix with Associated Text via Latent Binary Features,"A new methodology is developed for joint analysis of a matrix and accompanyingdocuments, with the documents associated with the matrix rows/columns. Thedocuments are modeled with a focused topic model, inferring latent binary features(topics) for each document. A new matrix decomposition is developed, withlatent binary features associated with the rows/columns, and with imposition of alow-rank constraint. The matrix decomposition and topic model are coupled bysharing the latent binary feature vectors associated with each. The model is appliedto roll-call data, with the associated documents defined by the legislation.State-of-the-art results are manifested for prediction of votes on a new piece oflegislation, based only on the observed text legislation. The coupling of the textand legislation is also demonstrated to yield insight into the properties of the matrixdecomposition for roll-call data."
734,High theta and low alpha waves may be a pattern for BCI illiteracy in motor imagery ,"While brain computer interfaces (BCI) can be employed for patients and healthy humans, there are problems that need to be solved before the technique will become a useful tool. In most BCI systems, the significant number of target users are not able to use BCI systems with a control paradigm such as motor imagery (MI), P300, and steady state evoked potential (SSEP). Such target users are called the BCI illiterate users. Only a few studies, however, investigated such phenomena and they have not provided a clear understanding of the BCI illiteracy mechanism or solution to this problem. Recently, alpha power in default mode network (DMN) was proposed to predict a user?s potential performance in MI BCI, and the causal relationship of gamma band to sensory motor rhythm was reported. However, what differences exist between BCI-literate and BCI-illiterate groups are not fully understood; moreover, the theta band has not been thoroughly investigated for BCI illiteracy. In this study, we sought to demonstrate the neurophysiological differences between two groups (literate, illiterate) among 52 subjects using a default mode network during the eyes-opened state. As a result, we found that high theta and low alpha waves are a pattern for BCI illiteracy relative to BCI-literate persons. Using an un-paired student t-test, we found that the spatially significant areas between the two groups were found in the frontal and post-parietal areas for theta, roughly in the overall area for alpha, and in the post-central area for gamma. In addition, from the results of the relationship between band power and offline accuracy, we propose a simple performance predictor using four band powers. This gives a Pearson correlation coefficient of r=0.59, indicating that our proposed predictor explains 35% of the variance in subject performance."
735,Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
736,Optimally fuzzy scale-free memory buffer,"Any system with the ability to learn patterns  from a time series of stimuli and predict the subsequent stimulus at each moment, should have  a buffer storing the stimuli from the recent past. In cases where the external environment generating the time series has a fixed scale, the buffer can be a simple shift register---a moving window of finite width extending into the past. However, such a traditional buffer is inappropriate for signals with scale-free long range correlations, which are found in many physical environments. We argue  for a scale-free fuzzy buffer that optimally sacrifices accuracy  in favor of  capacity to represent long time scales. Here we describe a neuro-cognitive model of internal time that satisfies these constraints."
737,Reinforcement Learning and Hierarchical State Representation for Modeling Incubation and Restructuring in Primate Insightful Problem Solving,The paper proposes a reinforcement learning model with hierarchical state representation for modeling the learning behavior observed in rhesus monkeys in a reverse-reward contingency task. The hierarchical state representation provides an ability to solve insightful problems by restructuring the internal belief representation of the environment through incubation in which the evidence of constructing an appropriate representation is accumulated. The experiment and simulation results show that the proposed model accounts for the three-stage learning patterns observed in experiments on rhesus monkeys. It also applies to behavioral results observed in rhesus monkeys on the same task during a transfer test when novel quantity combinations are presented.
738,Proper losses for learning from partial labels,"This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, establish a necessary and sufficient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. An interesting result is that the full knowledge of this matrix is not required, and losses can be constructed that are proper in a subset of the probability simplex."
739,Active Sensing as Bayes-Optimal Sequential Decision-Making,"Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience.  An important but poorly understood aspect of sensory processing is the role of active sensing: the use of self-motion to selectively process the most rewarding or informative aspects of the environment. Here, we present a Bayes-optimal inference and decision-making framework for active sensing, which directly minimizes a cost function that takes into account behavioral costs such as response delay, error, and effort. Unlike previously proposed algorithms that optimize heuristic objectives such as expected entropy reduction [Butko and Movellan, 2010] or one-step look-ahead accuracy [Najemnik and Geisler, 2005], this optimal policy can account for search duration as well as location, and is sensitive to contextual factors such as the relative importance of time, error, and effort.  We implement the optimal policy, along with the two heuristic policies, for an example visual search task, and illustrate how the heuristic policies deviate from optimal performance in various contexts.  We show that the discrepancy is especially large when the cost of time and the cost of switching between sensing locations are high. We demonstrate a potential route for overcoming the computational complexity of the optimal algorithm, especially problematic in large real-world applications, by exploiting the concavity and smoothness of the value function.  We show that a basis function approximation to the value function, several orders of magnitude reduced in dimensionality and complexity, achieves near-optimal performance."
740,Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation,"Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented."
741,Limiting memory improves online particle-?lter learners for word segmentation,"This paper shows that limiting the memory of an on-line particle-?lter algorithm for word segmentation improves its accuracy, yielding accuracies that are competitive with state-of-the-art batch algorithms. Our algorithm is derived by replacing the Chinese Restaurant Processes in a non-parametric Bayesian word segmentation model with distance-dependent Chinese Restaurant Processes (Blei and Frazier, 2011). This is easiest to do in a particle-?lter algorithm (B?rschinger and Johnson, 2011), and leads to a bounded-memory, on-line learning algorithm whose accuracy on standard evaluation data actually exceeds that of the corresponding algorithm without limited memory. We discuss the relevance of our results for ?over-learning? (speci?cally, under-segmentation) in Bayesian modelsand the ?less-is-more? effect (Newport, 1990)."
742,Selecting Diverse Features via Spectral Regularization,"We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse featuresthat can predict a given objective. Diversity is usefulfor several reasons such as interpretability, robustness to noise, etc.  We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized approximately by efficient greedy and local search algorithms, with provable guarantees.  We compare our algorithms to traditional greedy and $\ell_1$-regularizationschemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations."
743,Accelerating Graphical Models Inference via Iterative Algorithms,"In tree-structured graphical models, the max-product algorithm provides efficient and exact solutions to inference problems. However, as the number of states becomes large, the max-product algorithm becomes prohibitively slow. In this paper, we propose an iterative max-product algorithm to accelerate the inference. In addition, we apply iterative procedure to accelerate the k-best inference problem (in contrast to 1-best). We show the efficiency of these iterative based algorithms using both synthetic data and real world data."
744,Sample-based non-negative matrix tri-factorization,"We present a general dimensionality reduction algorithm applicable to non-negative bi-dimensional data sets composed of multiple samples. The new algorithm extends the standard non-negative matrix factorization to a constrained tri-factor decomposition. Our non-negative matrix tri-factorization has the unique feature of reducing dimensionality while identifying sample-independent factors in both the rows and columns of the input matrices, separately and simultaneously. This decomposition is motivated by neurophysiological data for which time-varying signals are typically recorded from several sources and in multiple samples. We derive the main algorithm, referred to as sNM3F, and present two possibly useful variants with orthogonality and time-shifts features. By applying the method to simulated muscle patterns we demonstrate that, compared to standard decompositions, our algorithm is effective in identifying reliably the underlying structure. Finally, we use it to decompose electromyographic signals recorded during arm reaching movements into separate spatial and temporal components."
745,Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
746,Partial Gaussian Graphical Model Estimation,"This paper studies the partial estimation of Gaussian graphical models from high-dimensional empirical observations. We derive a convex formulation for this problem using l1-regularized maximum-likelihood estimation, which can be solved via a block coordinate descent algorithm. Statistical estimation performance can be established for our method. The proposed approach has competitive empirical performance compared to existing methods, as demonstrated by various experiments on synthetic and real datasets."
748,Sparse Online Topic Models,"Probabilistic online topic models have been developed for discovering latent semantic representations from massive data corpora. However, due to normalization constraints, probabilistic topic models can be ineffective in controlling the sparsity of discovered representations. In this paper, we present a sparse online topic model, which directly controls the sparsity of word and document codes by imposing sparsity-inducing regularization. The topical dictionary is learned by an online algorithm, which is efficient and guaranteed to converge. We extensively evaluate the basic sparse online topic model as well as its collapsed and supervised extensions on large-scale data sets. Our results demonstrate appealing performance."
749,Discriminative Sub-categorization,"The objective of this work is to learn sub-categories. Rather thancasting this simply as a problem of unsupervised clustering, we investigatea weakly supervised approach using both positive and negativesamples of the category.We make the following contributions: (i) we introduce a new model fordiscriminative sub-categorization which determines clustermembership for positive samples whilst simultaneously learning amax-margin classifier to separate each cluster from thenegative samples; (ii) we show that this model does not suffer fromthe degenerate cluster problem that afflicts several competing methods(e.g., Latent SVM and Max-Margin Clustering); (iii) we show that the method is able to discoverinterpretable sub-categories in various datasets.The model is evaluated experimentally over several UCI datasets, andits performance advantages over $k$-means and Latent SVM are demonstrated. We also stresstest the model and show its resilience in discovering sub-categoriesas the parameters are varied.  "
750,Solving Relational MDPs with Exogenous Events and Additive Rewards,"We formalize a simple but natural subclass of service domains for relational planning problems with object-centered independent exogenous events and additive rewards, capturing, for example, problems in inventory control and fire and rescue operations. Focusing on this subclass, we then present the first complete symbolic solution for stochastic planning problems in relational domains that is able to handle exogenous events and additive rewards, and is independent of domain size.  Our planning algorithm provides a lower bound approximation on the optimal solution given by the true value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation that can represent and manipulate real-valued functions over relational world states. A preliminary experimental evaluation demonstrates the validity and potential of our approach.  "
751,Monte Carlo Methods for Maximum Margin Supervised Topic Models,"An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihood-based supervised topic models, of which posterior inference can be carried out using the Bayes' rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper, we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency."
752,Input Variable Selection for Linear Regression Model using Nearest Correlation Spectral Clustering,"Linear regression models have been widely accepted in many scientific and engineering fields for the estimation or interpretation of phenomena.  When a linear regression model is built, appropriate input variables have to be selected to achieve high estimation performance.  This work proposes new methodologies for selecting input variables for linear regression models using nearest correlation spectral clustering (NCSC), which is a correlation-based clustering method.  In the present work, NCSC is used for variable group construction, and a few variable groups are selected by their contribution to estimates or by group Lasso; they are referred to as NCSC-based variable selection (NCSC-VS) and NCSC-group Lasso (NCSC-GL). The performances of the proposed NCSC-VS and NCSC-GL are examined through a case study of chemometrics data."
753,Reinforcement Learning Behavior in Sponsored Search,"This paper is concerned with the modeling of advertiser behavior in sponsored search. Modeling advertiser behavior can help search engines better serve advertisers, improve auction mechanism, and forecast future revenue. Most of previous work assume that advertisers have perfect access to necessary information for making their decisions. However, as we know, there are many factors in the system which are hardly known to advertisers and make the above assumptions unreasonable. To tackle the challenge, we propose viewing sponsored search as a reinforcement learning (RL) system for each advertiser, and employing a RL behavior model to describe how each advertiser responds to the system. In the proposed model, advertisers estimate the utility for winning each rank position based on the signals provided by the search engine, target the most preferred position based on the estimation, and then adjust their bids accordingly to achieve this target. The proposed model does not assume perfect information access, but only assumes the observation of the signals provided by the search engine. Furthermore, it does not specify how advertisers actually utilize the observed information to make decision. Instead, one single parameter, the learning rate, is used to describe advertisers' ability in collecting and analyzing information, and learning how to appropriately behave. Our experiments show that our model outperforms previous models in the task of bid prediction and rank position prediction, demonstrating its advantage and flexibility in fitting real data. In addition to the short-term prediction capability, we also study the long-term outcome of the sponsored search system, if all the advertisers behave according to the proposed RL behavior model. Our theoretical analysis shows that under certain conditions, the dynamic system of sponsored search will converge to a locally envy-free equilibrium, which verifies the soundness of our model from another angle."
754,On Smoothness in Low-Rank Models,"We propose the Smooth Low-Rank Models (SLRM) to address problems in applications where the data matrix is not only low-rank, but also has a small total variation. Low-rank models are important in a number of problems such as matrix completion, denoising, and motion recovery. However, exact recovery of a low-rank matrix from a set of randomly sampled entries is not guaranteed when an entire column or row is not sampled. The problem can be alleviated if  prior information, such as smoothness in data, is available. This can be formulated as a nuclear-norm minimization problem, regularized by a Total Variation (TV) constraint.   Alternating Direction Method of Multiplier (ADMM) is used for solving the model. We studied the problems of matrix completion, denoising  and  motion capture data reconstruction.  Experiments on synthetic data, motion capture data and background modeling datasets demonstrated that SLRMs can significantly improve upon the original low-rank models and superior than other state-of-the-art models, especially when the set of sampled entries are insufficient or corruption is heavy. "
756,Path Integral Control by Reproducing Kernel Hilbert Space Embedding,"We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided."
758,A Linear Time Active Learning Algorithm for Link Classification,"We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V,E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\log|V|$."
759,How can probable mechanisms of object recognition in the visual cortex be determined by visual features?,"There are lines of evidence demonstrating that the fusiform face area (FFA) preforms processes related to specific object recognition, which requires expertise (e.g. facial recognition). It seems that our brain utilize different mechanisms for generic object recognizing (e.g. face vs. non-face) and specific recognition (e.g. two different faces). To address this issue, we first introduce a biologically inspired object recognition model and then examine it in two experiments. The model has a hierarchical structure that employs a biologically plausible approach for visual feature extraction. Our results suggest that the mechanisms underlying generic and specific object recognition are performed distinctly. The results are in agreement with evidence that indicates inferotemporal cortex (IT) is responsible for object recognition and FFA is involved in tasks that requires expertise. Furthermore, the results suggest that the important factor which makes these two mechanisms different may lie under the visual feature extraction. We attempt to propose a mechanism for these two different kinds of recognition. We also investigate the influence of extracted visual features on view-invariant object recognition and show that while the target objects and distractor are very similar to each other, a moderate degree of view invariance can be achieved without making association between views. It seems that by making use of some visual features which are common between near views of objects we can obtain moderate level view invariant object recognition."
760,Analysis of Differential Privacy Based on Importance Weighting,"This paper introduces and analyzes a novel data-publishing mechanism based on computing weights that make an existing dataset, for which there are no confidentiality issues, analogous to the dataset that must be kept private. The existing dataset may be genuine but public already, or it can be synthetic. The only necessary requirement is that it have similar schema as the private dataset. The weights are importance sampling weights, but they are regularized and have noise added. The weights allow statistical queries to be answered approximately while provably guaranteeing differential privacy. We derive expressions for the variance of the approximate answers. Experiments show that the new mechanism performs well even when the public dataset is quite different from the private dataset,and the privacy budget is small."
761,Bayesian Meta-classifier Learning from Biased Multiple Predictions,"We propose a probabilistic generative model for combining the predictions of multiple classifiers to form a meta-classifier that provides high classification accuracy.  The key feature of the model is the introduction of a latent variable that can identify whether the classifier in the ensemble is {\it related or unrelated} to each of the classes.  Our modeling is motivated by the idea that classifiers which provide incorrect predictions but are informative in terms of discriminating one class from others can also be effectively utilized for learning a meta-classifier.  The proposed meta-classifier learningscheme is particularly useful when the performance of each classifier is biased toward some specific class.  We perform empirical evaluations using both synthetic and real data. As a real case study of a combination of biased classifiers, we show its application to the high-level recognition of actual nursing activity by using accelerometers."
763,Challenge Mode Training,"This paper proposes a new training method called Challenge Mode Training which increases the accuracy as well as the robustness against irrelevant attributes of any classifier. Challenge mode training works by replacing some values with missing values. Actually, by removing information the proposed method create a different and more difficult problem which has less degrees of freedom and therefore fewer plateaus. Experiments are conducted with the challenge mode training applied to neural networks over traditional and complex datasets as well as datasets with the addition of irrelevant variables. For all of the tests, challenge mode training surpassed or performed similarly well in relation to the usual training method, in fact, the gain in accuracy provided by the proposed method was higher for complex datasets or datasets with irrelevant variables."
764,Towards An Order Preserving Approximation of LASSO Programs,"Sparse representation (SR) has recently drawn extensive attention in the signal processing, machine learning and machine vision communities. General SR theory emphasizes the importance of the \emph{sparsity} of a solution, as a natural regularization term for certain optimization problems. Exploiting sparsity has been shown to dramatically improve performance in specific real-world problems, e.g. \emph{face recognition}. Despite their proven success in enhancing classification performance, SR-based techniques are not readily applicable to large-scale problems because of the high computational burden incurred by solving sparse optimization programs, namely LASSO programs. As a result, there has been a recent push towards efficient techniques that adequately approximate the LASSO solution. In this paper, we propose such an approximation that serves the purpose of ordering a set of LASSO programs according to their optimal solutions. The need to order LASSO programs (e.g. to select the one with smallest objective) arises in important retrieval applications, from which we focus on two: online face recognition and visual object tracking. For such tasks, we propose a novel sampling-based algorithm that efficiently estimates the optimal solution for each LASSO program, while sufficiently preserving the relative order of these programs with high probability. To demonstrate the effectiveness and efficiency of the proposed method, we apply it to face recognition and object tracking on benchmark datasets. Our experiments show that it not only achieves state-of-the-art performance but also allows for significant computational speedup over baseline methods. %These results suggest that the proposed method can be a key enabler for such time-critical applications. "
765,Bayesian Warped Gaussian Processes,"Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classification, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets."
766,Optimized dictionary based sparse representation for robust speaker recognition,"The mismatch between the training and the testing environments greatly degrades the performance of speaker recognition. Although many robust techniques have been proposed, speaker recognition in mismatch condition is still a challenge. To solve this problem, we propose an optimized dictionary based sparse representation for robust speaker recognition. To this end, we first train a speech dictionary and a noise dictionary, and concatenate them for sparse representation; then design an optimization algorithm to reduce the mutual coherence between the two learned dictionaries; after that, utilize mixture k-means to model speaker corresponding to sparse feature; and finally, present a distance divergence to measure the similarity. Compared with the standard Universal Background Model and Gaussian Mixture Models based speaker recognition, our preliminary experiments show that the proposed recognition framework consistently improve the robustness in mismatched condition."
767,Nonparametric Reduced Rank Regression,"We propose an approach to multivariate nonparametric regression thatgeneralizes reduced rank regression for linear models.  An additivemodel is estimated for each dimension of a $q$-dimensional response,with a shared $p$-dimensional predictor variable.  To control thecomplexity of the model, we employ a functional form of the Ky-Fanor nuclear norm, resulting in a set of function estimates that havelow rank.  Backfitting algorithms are derived and justified using anonparametric form of the nuclear norm subdifferential.  Oracleinequalities on excess risk are derived that exhibit the scalingbehavior of the procedure in the high dimensional setting.  Themethods are illustrated on gene expression data."
768,Temporal Coding of Local Spectrogram Features for Robust Sound Recognition,"There is much evidence to suggest that the human auditory system uses localised time-frequency information for the robust recognition of sounds. Despite this, conventional systems typically rely on features extracted from short windowed frames over time, covering the whole frequency spectrum. Such approaches are not inherently robust to noise, as each frame will contain a mixture of the spectral information from noise and signal.Here, we propose a novel approach based on the temporal coding of Local Spectrogram Features (LSFs), which generate spikes that are used to train a Spiking Neural Network (SNN) with temporal learning. LSFs represent robust location information in the spectrogram surrounding keypoints, which are detected in a signal-driven manner, such that the effect of noise on the temporal coding is reduced. Our system models characteristic clusters of LSFs in an unsupervised way, using tonotopic learning based on Self Organising Maps (SOMs).Our experiments demonstrate the robust performance of our approach across a variety of noise conditions, such that it is able to outperform the conventional frame-based baseline methods."
769,Lifted Variable Elimination: A Novel Operator and Completeness Results,"Various methods for lifted probabilistic inference have been proposed, but our understanding of these methods and the relationships between them is still limited, compared to their propositional counterparts. The only existing theoretical characterization of lifting is for weighted first-order model counting (WFOMC), which was shown to be complete domain-lifted for the class of 2-logvar models. This paper makes two contributions to lifted variable elimination (LVE). First, we introduce a novel inference operator called group inversion. Second, we prove that LVE augmented with this operator is complete in the same sense as WFOMC."
770,The Gaussian Nonlinear Poisson Process,"Recently, intracellular recordings of neurons in vivo have become increasingly available. There is a pressing need to develop models which can be used to characterize the statistical properties of the temporal dynamics of single cells. We propose a doubly stochastic continuous-time model of a single neuron which is supposed can capture both the membrane potential dynamics and the process of spiking. The model consists of a gaussian process which is transformed through a nonlinearity, providing the firing rate for an inhomogeneous Poisson process. We highlight the use of the moment-generating functional in order to derive the previously known $n$-point function and devise a method for fitting this model to in vivo data."
772,Large Margin Metric Learning for Sparse Representation-Based Classification,"Sparse representation-based classification (SRC) has achieved great successes in many visual recognition tasks in recent years, such as face recognition and image classification. However, SRC is usually performed in the original feature space, which may not be discriminative enough for some classification problems. In this paper, we propose a large margin metric learning method to learn a discriminative distance metric to calculate the sparse reconstruction errors. The distance metric is learned by enforcing a margin between intraclass reconstruction error and interclass reconstruction error, for each training example. Experiments conducted on face recognition, gait recognition, and palmprint recognition show the efficacy of our proposed method. This approach has the potential to be used to enhance the SRC method in many applications."
773,Multiresolution analysis on the symmetric group,"There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of O(n^p) complexity with small p for sparse signals (in contrast to the O(n^q n!) complexity typical of FFTs). We discuss potential applications in ranking, sparse approximation, and multi-object tracking. "
776,Isotropic Hashing ,"Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances."
777,Gaussian Message Passing for Non-Gaussian Vision: Non-Lambertian Shape from Shading,"Although Expectation Propagation has become a popular and successful inference method for continuous probabilistic models, it is rarely applied to computer vision. One disadvantage is that for grid-shaped models, Gaussian EP requires quadratic space and cubic time in the number of pixels. However, run times for EP are independent of clique size, and depend only on the rank, or intrinsic dimensionality, of potentials. This property would be highly advantageous in computer vision.Here, we propose two variations of EP suitable for visual problems. The final approach exploits commonalities in natural scene statistics to achieve run times that are linear in both number of pixels and clique size.We test these methods on shape from shading. To demonstrate that the method performs well in problems with highly non-Gaussian potentials, we show performance for surfaces with arbitrary non-Lambertian reflectance and lighting. In each case, inferred shapes closely obey constraints imposed by the image and prior."
779,PCA transform via Partial Rotation,"We present a relaxed version of high-dimensional rotation called \emph{partial rotation}. For two $d\times k$ matrices $S_1$ and $S_2$, each consisting of $k$ selected columns from two orthonormal bases in $\RR^d$, respectively, a partial rotation of degree $k$ is a $d\times d$ orthonormal matrix $R$ such that $R S_1=S_2$ and the null space of ${S_1}^T$ coincides with the null space of ${S_2}^T$ under $R$.  We show that such a rotation can be represented by a sequence of $k$ Givens rotations and provide an efficient algorithm to find the rotation in $O(k^2d)$ time. Since a partial rotation of degree $k$ is represented by a sequence of $k$ Givens rotation, it takes only $O(kd)$ time to rotate a $d$ dimensional vector by the rotation. This is faster than the standard rotation algorithm of time complexity $O(d^2)$ by orders of magnitude when $k$ is much smaller than $d$. Partial rotation is especially useful to principal component analysis (PCA) with $k$ principal components. By substituting the standard projection method of PCA with our partial rotation method, PCA transform can be done without any information loss. Our empirical results show that even a simple brute-force algorithm using PCA with partial rotation outperforms the state-of-art techniques in high-dimensional nearest neighbor search."
780,Sparse Optimal Control Signals for Natural Human Movements Using the Infinity Norm,"Optimal control models have been a successful tool in describing many aspects and characteristics of human movements. While such models have a sound theoretical foundation, their interpretation and neuronal implementation in the Central Nervous System (CNS) is not clear. We propose that the CNS not only utilizes control policies that are optimal with respect to a criterion, but also satisfy sparsity constraints. In recent years sparsity has played a pivotal role in theoretical neuroscience for information processing (such as vision). Typically, sparsity is imposed by introducing a cardinality constraint or penalty measured or approximate by the one-norm. In this work, to obtain sparse control signals, however, the $L_{\infty}$ norm is used as a penalty on the control signal. Even though such sparse control signals are discontinuous, the movements that result are continuous and smooth.  In addition, such sparse control signals are more biologically realistic and have a clear neuronal interpretation with a sequence of neuronal spikes. We show that moreover sparse optimal control signals quantitatively describe real human arm movements with high accuracy. "
781,Local Learning Algorithms for Multi-Task Learning,"Multi-task learning is to improve the performance of one task by utilizing information from other related tasks. Almost all existing multi-task learning methods belong to global learning approach. In this paper, different from existing methods, we propose local learning methods for multi-task classification and regression problems by extending some single-task local learning methods. For classification problems, we extend k-nearest-neighbor classifier by formulating the decision function on each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific. By defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one, a regularized objective function is proposed and we develop an efficient coordinate descent method to solve it. For regression problems, we extend kernel regression to multi-task setting in a similar way to the classification case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods."
782,A population search algorithm for clustered connectivity patterns,"The identification of time, frequency and spatial locations between which connectivity occurs within the brain is traditionally done via a brute force search of all available locations in a specified range. However, this is inefficient and slows down progress in the identification of connectivity patterns related to previously unexplored cognitive processes. Therefore, a novel, population based, search algorithm is proposed based upon the behaviour of foraging animals.The method is evaluated on both a simple grid search problem and on the identification of time-frequency locations of statistically significant phase synchronisation in the EEG. The method is shown to exceed brute force searches in terms of speed by several times while identifying a large proportion of available solutions."
783,A statistic for testing equality of distributions in metric space,"Deciding whether two sets of samples originate from the same probability law is essential in many fields of science and engineering such as economics, biology, medicine and neuroscience. Although this problem has been studied extensively when the underlying random variables are categorical or real valued, it remains largely unexplored when the random variables have more exotic domains such as time series, graphs, probability measures, and spike trains. A common aspect of these latter domains is that they can be assigned appropriate distance metrics, such as edit distance. In this paper, we exploit this natural characteristic to develop a computationally efficient, and parameter free statistic for testing equality of distributions. We compare the proposed approach with other state-of-the-art methods, and demonstrate that it performs equally well."
784,On Lifting the Gibbs Sampling Algorithm,"Statistical relational learning models combine the power of first-order logic, the de facto tool for handling relational structure, with that of probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the speed, accuracy and scalability of existing graphical models' inference algorithms by exploiting symmetry in the first-order representation. In this paper, we consider blocked Gibbs sampling, an advanced variation of the classic Gibbs sampling algorithm and lift it to the first-order level. We propose to achieve this by partitioning the first-order atoms in the relational model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing such clusters and determining their complexity and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy and convergence."
785,Structured Message Passing,"Almost all message-passing based approximate inference approaches proposed to date, e.g., belief propagation (BP), its generalizations and expectation propagation (EP), use tabular representation of messages and potentials. In this paper, we argue that this limits their accuracy in practice. To remedy this, we propose structured message passing (SMP), a unifying framework for taking advantage of structured representations. Within this framework, we investigate two structured approaches: sparse hash tables and algebraic decision diagrams for representing and manipulating messages, and propose a new message-passing algorithm that is an instance of SMP. The key idea in our new SMP algorithm is to artificially introduce determinism and context-specific independence in the messages which enables us to exploit the power and compactness of structured representations. We investigate our new algorithm both theoretically and experimentally. Our experimental results show the power and superiority of SMP over tabular message passing algorithms."
786,Perplexity on Reduced Corpora,"This paper studies a relationship between perplexity and vocabulary size on a corpus (or documents),which is reduced to improve computational performance.We prove that perplexity of k-gram models and topic modelsroughly follows a power law with respect to reduced vocabulary size,when a corpus follows Zipf's law.This gives a theoretical evidence for our intuition thatlow-frequency words may not make a large contribution to learning results.We verify the correctness of our theory on synthetic corporaand examine a gap between theory and practice on real corpora."
787,"Dual Decomposition from the Perspective of Relax, Compensate and then Recover","Relax, Compensate and then Recover (RCR) is a paradigm for approximateinference in probabilistic graphical models that has previouslyprovided theoretical and practical insights on iterative beliefpropagation and some of its generalizations.  In this paper, wecharacterize the technique of dual decomposition in the terms of RCR,viewing it as a specific way to compensate for relaxed equivalenceconstraints.  Among other insights gathered from this perspective, wepropose novel heuristics for recovering relaxed equivalenceconstraints with the goal of incrementally tightening dualdecomposition approximations, all the way to reaching exactsolutions. We also show empirically that recovering equivalenceconstraints can sometimes tighten the corresponding approximation (andobtaining exact results), without increasing much the complexity ofinference."
788,"A meta algorithm making centralized graph computation faster, distributed and at times better","In this paper, we present a meta algorithm that takes existing centralized algorithms for graph computation and makes them distributed and faster. In a nutshell, the meta algorithm creates a randomized partition of  the graph, with each partition being a small subgraph, and it then runs the centralized algorithm on each partition separately and stitches the resulting solutions to produce a global solution. We illustrate this meta algorithm with two popular problems: computation of Maximum A Posteriori (MAP) assignment in an arbitrary pairwise Markov Random Field (MRF), and modularity optimization for clustering and community detection.We show that the resulting distributed algorithms for these problems essentially run in linear time  and that they perform as well -- or even better -- than the original centralized algorithm as long as the graphs have geometric structure. More precisely, if the centralized algorithm is a constant factor approximation, the resulting distributed algorithm is also a constant factor approximation with constant slightly bigger; but if the centralized algorithm is a non-constant (e.g. logarithmic) factor approximation, then the resulting distributed algorithm becomes a constant factor approximation. For general graphs (not necessarily geometric), we  compute explicit bounds on the loss of  performance of the distributed algorithm with respect to the centralized algorithm."
789,Optimizing a Multiple Linear Regression-based Approach for a Priori Decision Threshold Estimation in Biometric Recognition,"Biometric recognition is a complex classification problemwhere the goal is to classify a pattern (biometric sample) as belonging or not to a certain class (user). As in other pattern recognition problems, a correct estimation of the decision threshold is essential for optimizing the biometric system's performance. A successful new approach for this estimation (prediction) based on Multiple Linear Regression has been proposed by us in a previous work. Here, we go into this proposal in greater depth, optimizing the independent variables selection by meansof the use of their matrix correlation, and only the uncorrelated ones are incorporated to the model. A study of the threshold estimation accuracy with regard to the data set size used to train the linear model is alsoperformed. Other related works have focused on a single biometric and classifier. However, our proposal is applied to different biometrics (signature and speech) and with different classifiers (Artificial Neural Network and Dynamic Time Warping), showing a good accuracy in all the tested scenarios."
790,On the connections between saliency and tracking,"A model connecting visual tracking and saliency has recently been proposed. Thismodel is based on the saliency hypothesis for tracking which postulates that trackingis achieved by the top-down tuning, based on target features, of discriminantcenter-surround saliency mechanisms over time. In this work, we identify threemain predictions that must hold if the hypothesis were true: 1) tracking reliabilityshould be larger for salient than for non-salient targets, 2) tracking reliabilityshould have a dependence on the defining variables of saliency, namely featurecontrast and distractor heterogeneity, and must replicate the dependence ofsaliency on these variables, and 3) saliency and tracking can be implemented withcommon low level neural mechanisms. We confirm that the first two predictionshold by reporting results from a set of human behavior studies on the connectionbetween saliency and tracking. We also show that the third prediction holds byconstructing a common neurophysiologically plausible architecture that can computationallysolve both saliency and tracking. This architecture is fully compliantwith the standard physiological models of V1 and MT, and with what is knownabout attentional control in area LIP, while explaining the results of the humanbehavior experiments."
791,Infinite EFH: an Infinite Undirected Latent Variable Model,"Bayesian nonparametrics has been promising in learning Bayesian networks, but very few attempts have been made under the context of undirected Markov networks. This paper presents infinite exponential family Harmoniums (iEFH), an attempt to broaden the use of Bayesian nonparametrics to automatically resolve the unknown number of hidden units in undirected latent variable models. We further generalize iEFH to the supervised infinite max-margin Harmoniums (iMMH), which directly regularizes the latent representations via imposing max-margin constraints for discovering predictive latent representations that are good for classification. We use the sparsity-inducing Indian buffet process prior to select latent units from an infinite pool. Our extensive experiments on real text and image datasets appear to demonstrate the benefits of iEFH and iMMH inherited from both Bayesian nonparametrics and max-margin learning."
792,"Learning to Grasp using Vision, Haptics and Proprioception","The ability to grasp and manipulate objects is an integral part of a robot's physical interaction with the environment. Humans alike, robots are expected to grasp and manipulate objects in a goal-oriented manner. In other words, objects should be grasped so to afford subsequent actions: if I am to hammer a nail, the hammer should be grasped so to afford hammering. Most of the work on grasping, commonly addresses only the problem of finding a stable grasp without considering the task/action a robot is supposed to fulfill with an object.In this paper, we present work on modeling of goal-directed robot grasping tasks based on integration of multisensory data using probabilistic generative models.  Our probabilistic framework facilitates assessment of grasp success in a goal-oriented way, taking into account both geometric constraints imposed by the task and fulfilling grasp stability requirements.  The conditional relations between tasks and the sensory data (vision, haptics and proprioception) are modeled using graphical models. We integrate high-level task information introduced by a teacher in a supervised setting with low-level stability requirements acquired through a robot's self-exploration.  The generative modeling approach enables inference of appropriate grasping configurations, as well as prediction of grasp success. The framework provides insights into dependencies between variables and features relevant for object grasping in general."
793,Cross-Domain Information Role Classifier for Profiling Users in Online Q&A Forums,"This paper presents a novel application of text classification techniques for analyzing forum dynamics and profiling discussants in online Q&A discussions. Building on the existing Speech Act research, we identify dialogue features that capture true information seeking and providing roles of the discussants and their messages within threaded discussions. As message-level lexical information is not enough in capturing true information roles, we include additional thread-level information such as author turns and message positions in the thread. We generated and evaluated user information roles across four different Q&A forums including student group project forums and industry troubleshooting forums. The current result indicates that information role classifiers are robust across several different domains. We also found that the role-based user profiles are useful in predicting user performance or user expertise within the community."
794,Transfer learning in the blind with the personal perceptron,"We consider the transfer learning scenario where test data is randomly sampled from several, possibly unknown distributions. This scenario is interesting for many natural language processing services, for example, where the domain, genre and style of the next input example is not known in advance. We introduce a perceptron learning algorithm that learns so-called 'error prints' for intermediate models that are averaged at test time to adapt the final model to new data points. An error print marks a region where a model is more likely to misclassify examples. We evaluate our algorithm on 6 sentiment analysis datasets (12 problems) and show that the method is effective, in particular when test data is not sampled from the source distribution. We show that the personal perceptron is more expressive than other large-margin perceptron algorithms and establish an relation to nearest neighbor methods. "
795,On Generalization Performance of Unified Learning Model,"Recently, A.Anonymous [1] showed a unified formulation based on robust optimization thatembraces several kinds of classification methods and gave a geometric interpretation and statistical interpretation for the unified formulation. This paper extends the unified model to cover not only  binary classification but also  regression and outlier (or novelty) detection. Then we show that the unified model minimizes a well-known financial risk measure. Moreover, after deriving generalization bounds using such risk measures, we prove that the unified model gives a solution that minimizes the generalization bounds. "
796,Clustering Probability Densities,"We describe a clustering algorithm where each example is represented as a probability density and distortion is measured by Kullback-Leibler divergence. This setup is relevant to many applications where an example is better characterized by a probability distribution, to capture the underlying uncertainty of interest, than a finite-dimensional feature vector in Euclidean space. We first derive a k-means variant for clustering Gaussian densities which has a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalizes a single Gaussian and is typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. We report empirical results on a successfully deployed application: query clustering based on bid landscape for sponsored search auction optimization."
797,Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method,"We develop new stochastic optimization methods that are applicable to a wide range of {\it structured regularizations}.Basically our methods are a combination of stochastic optimization techniques and Alternating Direction Multiplier Method (ADMM).ADMM is a general framework for optimizing a composite function,and has a wide range of applications.We propose two types of online variants of ADMM corresponding to onine proximal gradient descent and regularized dual averaging respectively.The proposed algorithms are computationally efficient and easy to implement.It is shown that our methods yield $O(1/\sqrt{T})$ convergence of the expected risk.Numerical experiments show effectiveness of our methods in learning tasks with structured sparsitysuch as overlapped group lasso."
798,Locally Optimized Hashing,"Fast nearest neighbor search is becoming more and more important to utilize massive data. Recent work shows that hash learning is effective for nearest neighbor search in terms of computational time and space. Existing hash learning methods try to convert near samples to near binary codes, and their hash functions are globally optimized on the data manifold. However, such hash functions often have low resolution of binary codes; each bucket, a set of samples with same binary code, may contain a large number of samples in these methods, which makes it infeasible to obtian the nearest neighbors of given query with high precision. As a result, existing methods require long binary codes for precise nearest neighbor search. In this paper, we propose Locally Optimized Hashing to overcome this drawback, which explicitly partitions each bucket by solving optimization problem based on that of Spectral Hashing with stronger constraints. Our method outperforms existing methods in image and document datasets in terms of quality of both the hash table and query, especially when the code length is short."
799,Ellipsoidal Multiple Instance Learning,"We propose a large margin method for learning with ellipsoids that isparticularly suited to asymmetric detection tasks such as multipleinstance learning (MIL). In contrast to current approaches for solving MILthat involve complex computationally expensive algorithms, ourapproach is a direct geometric one.We consider the distance between ellipsoidsand the hyperplane, generalising the standard support vector machine.Negative bags in MIL contain all negativeinstances, and we treat them akin to the robust optimisationframework. However, our method allows positive bags to cross themargin, since it is not known which instances within are positive. We derive agradient descent approach to solve the resulting bilevel program, andapply it to several MIL datasets. Surprisingly, our geometric approachresults in state of the art performance."
800,A Multiscale Composite Dirichlet Process for modelling rhythm tracks,"This paper introduces a novel non-parametric Bayesian model for hierarchically structured, pseudo-repetitive data, where the distribution of data on each scale of the hierarchy is generated by a Dirichlet Process.  The generality of this model makes it easily applicable to a wide range of data types, but it is particularly successful for composing melody or drum tracks.  In music, relative positional information is important, as patterns tend to be aligned to a metrum, a regular and hierarchical division of the sequence which generates an audible impression of a regular beat.  The model described in this paper is suitable for data with such properties, and can be trained using a Gibbs sampling algorithm.  The generative process is fast enough to compose melody or rhythm tracks in real time."
801,Simple Models for Shunting Inhibition,"The integration of excitatory and inhibitory inputs at a neuron follows a nonlinear process, which is generally termed shunting inhibition. The experimental data has revealed that the effect of shunting inhibitionon the somatic potential can be largely expressed as a simple arithmetic rule, in which the contribution of shunting inhibition is proportional to the product between the contributions of excitatory and inhibitory inputs when they are applied individually. In this study, we develop simple neuron and network models for shunting inhibition. Our simple neuron models reproduce the experimental results qualitatively. We show that shunting inhibition can provide a mechanism to retain persistent activity in a network, andcan be well approximated as divisive normalization in describing the stationary states of continuous attractor neural networks."
802,Global Multi-view Subspace Learning,"Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results, particularly for high dimensional data."
803,Constrained fractional set programming - Tight relaxations and efficient optimization,"The problem of minimizing a ratio of set functions subject to constraints appears in several machine learning applications e.g. clustering and community detection. Convex relaxations of these NP hard problems, although having global optimality guarantees,are often too loose and do not perform well in practice. In this paper, we show that a tight continuous relaxation exists for the minimization of any ratio of non-negative set functions subject to constraints. Although global optimality cannot be guaranteed for the resulting non-convex problem, the efficient minimization scheme that we present here has a provable quality guarantee. The experiments performed on problems in local clustering and community detection clearly demonstrate the superiority of our approach. "
804,Variational Bayesian Matching,"Matching of samples refers to the problem of inferring unknown co-occurrence or alignment between observations in two data sets. Given two sets of equally many samples, the task is to find for each sample a representative sample in the other set, without prior knowledge on a distance measure between the sets. Recently a few alternative solutions have been suggested, based on maximization of joint likelihood or between-data dependency. In this work we present a variational Bayesian solution for the problem, learning a   Bayesian canonical correlation analysis model with a permutation parameter for re-ordering the samples in one of the sets. We approximate the posterior over the permutations, and demonstrate that the resulting matching algorithm clearly outperforms all of the earlier solutions."
805,A Hierarchical Approach to Modeling Human Visual Chunk Learning,"How humans form chunks, or combinations of elementary features, is a fundamental question in visual perception and cognition. We propose a novel hierarchical chunk learning framework to detect suspicious coincidences of elements during training, and to form efficient representations of complex visual scenes. First, we utilize a hierarchical model structure to capture part-to-whole relations between visual chunks. Second, we adopt a data-driven maximum-likelihood approach to learn this model, reducing running time from several days to less than one minute. We demonstrate that our model can account for a range of experimental data concerning human chunk learning. In addition, we designed a new experiment using a paradigm that enabled human observers to form explicit representations of subparts, so that the observer could induce untrained sub-configurations in a learned hierarchical chunk. Our model based on a hierarchical approach successfully accounted for this new phenomenon, whereas previous models of chunk learning failed to predict it."
806,Spectral learning of linear dynamics from generalised-linear observations with application to neural population data,"Latent linear dynamical systems with generalised-linear observation models arise in a variety of applications, for example when modelling the spiking activity of populations of neurons.  Here, we show how  spectral learning methods for linear systems with Gaussian observations   (usually called subspace identification in this context) can be extended to estimate the parameters of dynamical system models observed through non-Gaussian noise models. We use this approach to obtain estimates of parameters for a dynamical model of neural population data, where the observed spike-counts are Poisson-distributed with log-rates determined by the latent dynamical process, possibly driven by external inputs. We show that the extended system identification algorithm is consistent and accurately recovers the correct parameters on large simulated data sets with much smaller computational cost than approximate expectation-maximisation (EM) due to the non-iterative nature of subspace identification. Even on smaller data sets, it provides an effective initialization for EM, leading to more robust performance and faster convergence. These benefits are shown to extend to real neural data."
807,Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model,"We consider linear regression in the high-dimensional regime in which the number of observations $n$ is smaller than the number of parameters $p$. A very successful approach in this setting uses $\ell_1$-penalized least squares (a.k.a. the Lasso) to search for a subset of $s_0< n$  parameters that best explain the data, while setting the other parameters to zero. A considerable amount of work has been devoted to characterizing the estimation and model selection problems within this approach. In this paper we consider instead the fundamental --but  far less understood-- question of statistical significance. Roughly speaking, when the Lasso estimates a specific parameter to be zero (or non-zero), \emph{how certain is this conclusion}? We study this problem under the random design model in which the rows of the design matrix are i.i.d. and drawn from an unknown high-dimensional Gaussian distribution. This situation arises --for instance-- in learning high-dimensional Gaussian graphical models. Leveraging on an asymptotic distributional characterization of regularized least squares estimators, we develop a procedure for computing p-values and hence assessing statistical significance for hypothesis testing. We characterize the power of this procedure, and evaluate it on synthetic and real data, comparing it with earlier proposals."
808,Statistics of edge co-occurences are sufficient to categorize natural images,"The analysis and interpretation of a visual scene to extract its category, such as whether it contains an animal, is typically assumed to involve higher-level associative brain areas.  Previous proposals have been based on a series of processing steps organized in a hierarchy that would successively interpret the scene at different levels of abstraction, from contour extraction, to low-level object recognition, to object categorization. We explore here an alternate hypothesis that second-order statistics of edges are sufficient to perform a rough yet robust (translation, scale and rotation invariant) scene categorization. The method is based on a realistic model of image analysis in the primary visual cortex that extends previous work from Geisler et al. (2001). Using a scale-space analysis coupled with a sparse coding algorithm, we achieved detailed and robust extraction of edges in different sets of natural images. This edge-based representation allows for a simple characterization of the ``association field'' by computing the second-order statistics of edge co-occurences. We show that the geometry of angles is sufficient to distinguish different sets of natural images taken in a variety of environments (natural, man-made, or containing an animal). This is quantitatively illustrated by using a na?ve classifier that allows to classify images solely on the basis of this geometry which performs at similar levels to hierarchical models. Such results call for the importance of the relative geometry of local image patches and its possible applications for image analysis, for instance to improve the efficiency of visual analysis systems. Most importantly, it challenges assumptions about the flow of computations in the visual system and emphasizes on the relative importance of associative connections, and in particular of intra-areal, lateral connections, in this process."
809,Thompson Sampling for Complex Online Problems,"We study stochastic multi-armed bandit settings with complex actionsover the basic arms, where the decision maker has to select a subsetof the basic arms or a partition of the basic arms at every round(rather than only selecting a single basic arm). The reward of thecomplex action is some function of the basic arms' rewards, and thefeedback observed may not necessarily be the reward per-arm. Forexample, when the complex action is a subset of the arms, we may onlyobserve the total reward or the maximum reward over the chosensubset. We use Thompson sampling to decide which complex action toselect. We prove a general theorem showing that a variant of Thompsonsampling with uniform exploration obtains logarithmic regret, and weshow how the regret depends explicitly on the information gain fromthe observations. As applications, we obtain several corollaries forspecific complex bandit problem setups with improved rates. Usingparticle filters for computing posterior distributions, we devise andsimulate Thompson-sampling algorithms for subset selection andjob-scheduling problems."
810,Direct 0-1 Loss Minimization and Margin Maximization with Boosting,"We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensembled classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples. Once the training classification error is reduced to its minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching the maximum of the margins. Experimental results on a synthetic data and a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost and LogitBoost."
811,Mixability in Statistical Learning,"Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability."
812,Reliability weighted belief consensus of experts,"The belief consensus algorithm for a network of interacting experts (such as sensors or classifiers) is based on the result that for a given hypothesis of interest, the belief of each expert converges to a sum of the log-likelihoods for that hypotheses from all experts. However, the algorithm does not address the realistic possibility of certain experts being less accurate than the others. We propose a reliability-weighted version of the belief consensus algorithm and prove that the individual expert beliefs still converge to a stable solution given by a reliablity-weighted sum of the log-likelihoods from all experts. We also propose a scheme for learning the reliabilities of individual experts using two smooth approximations (softmax and exponential) of the 0-1 reward function with the empirical risk minimization principle. Experiments on four UCI classification datasets suggest that the proposed reliability-weighted belief consensus algorithm with reliability estimation performs better than the standard unweighted version."
813,Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
814,Learning pattern-based CRF for predicting the protein local structure,"We describe a pattern-based conditional random field. Such CRFs appear naturally in sequence labeling problems of bioinformatics and can be considered as relative to Hidden Markov Models. In this model, factors that participate in conditional probability are nontrivial only if their argument belong to a certain set of sequences. We describe an inference algorithm based on dynamic programming that is optimized to the structure of the sequence set. This algorithm becomes preferable as lengths of patterns become long. Then our model is applied to predicting $\phi,\psi$ angles of all-alpha proteins. Learning of parameters for this problem was done by structural SVM technique. The accuracy in prediction of dihedral angles $\phi$ and $\psi$ we achieved was 21.1 and 48.4 degrees respectively. The MDA score, defined as the percentage of residues that are found in correctly predicted eight-residue segments, attained 57.2\%."
815,Periodic Spatial Filter for Single Trial Classification of Event Related Brain Activity,"Because of the small amplitudes of event related potentials (ERPs), they are usually hidden in electroencephalogram (EEG) recordings. This is particularly a problem when analyzing single-trial data.A spatial filtering method for P300 detection in oddball paradigm is proposed in this paper which is based on the assumption that brain responses to the same stimulus look the same (or at least do not change significantly over trials). Therefore, the sequence generated by concatenating all the responses to the same type of stimulus has a hidden periodicity. Enhancing the periodic structure of this sequence, a transformation is found to project the data into a lower dimensional subspace. Experiments show that even with a small subspace of the projected data, the classification performance in single-trial P300 detection is still high."
816,Poset View and Energy Distribution Criteria for Monotonic Dual Decomposition,"Dual decomposition algorithms based on block coordinate descent are efficient techniques for approximate MAP inference in graphical models. They optimize a local dual function at each step to monotonically increase the dual function value. In this paper, we present a unified framework for constructing and optimizing the local dual function based on the partially ordered set (poset). To maximize the local dual function, we first introduce the concept of the energy distribution ratio, and then derive an explicit and globally optimal solution, which covers all the existing algorithms. We show that the differences of the monotonic algorithms can be summarized in the local dual functions and the energy distribution ratios. Furthermore, we investigate the effect of energy distribution ratios on convergence and introduce energy distribution criteria for fast convergence. New algorithms are proposed based on the criteria, and the experimental results show they outperform the existing algorithms on convergence performance."
817,A lattice filter model of the visual pathway,"Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (LGN) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons. We propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals. The stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> LGN) and its filter weights can be learned using Hebbian rules in a stage-wise sequential manner. Moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in LGN neurons and fruit fly second-order visual neurons. Therefore, the lattice filter model is a useful abstraction that may help unravel visual system function."
818,Gossip-based On-Line Learning in Multi-Agent ?Systems with Local Decision Rules,"This paper is devoted to investigate binary classification in a distributed and on-line setting. The framework considered accounts for situations where both the training and test phases have to be performed by taking advantage of a network architecture by the means of local computations and exchange of limited information between neighbor nodes. An online learning gossip algorithm (OLGA) is introduced, together with a variant which implements a node selection procedure. Beyond a discussion of the practical advantages of the algorithm we promote, the paper proposes an analysis of the accuracy of the rules it produces, together with preliminary experimental results."
819,Constrained stochastic gradient descent for large-scale least squares problem,"The least squares problem is one of the most important regression problems in statistics and machine learning. In this paper, we present a Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by using the constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound O(log{T}), and fastest convergence speed among all first order approaches. Empirical studies confirm the effectiveness of CSGD by comparing with SGD and the averaged SGD."
820,Bayesian Fusion of Image Modalities with Disjoint Attributes,"Large scale monitoring of spatial phenomenon often produces image data with multiple modalities. In the computer vision literature, fusion of images is usually formulated as a principled sensor inversion problem, with a wide variety of techniques applicable to the case where multiple observed images have the same underlying attributes. On the other hand, in many remote sensing applications the acquired modalities have no common attribute channels. For this case, we seek to use the detail of the high-resolution image modalities to enhance the low resolution modalities, considering that the data may be related through their content structure rather than the sampling process alone. This paper presents a Gaussian Process (GP) formulation to transfer spatial structure from the high-resolution image into the low-resolution modality without assuming a value mapping. Experimentation suggests the proposed approach is able to reconstruct local detail across large resolution differences, and we present fusion results from real aerial data with modalities from both a low flying unmanned robotic aircraft and a high altitude commercial hyperspectral imager."
821,Estimating Node Labels via Feature Propagation,"We propose a new method for estimating node labels from given instances (nodes) with a graph, particularly focusing on the estimation of the graph edge weights. We estimate edge weights through hyper-parameter optimization of a harmonic Gaussian field model for feature vectors, which we call feature vector propagation (FVP). FVP defines edge weights as a parameterized similarity function and optimizes edge hyper-parameters by cross-validation over feature vectors of all nodes. That is, the optimization is independent of labeled instances, leading to several important advantages, such as the robustness against sparsely labeled graphs and the applicability to multi-class problems. FVP can also capture the local structure of data by the objective function which shares the same form as the local reconstruction error in locally linear embedding. Experimental results demonstrated the effectiveness of FVP both in synthetic and real datasets."
822,How to sample if you must: On Optimal Functional Sampling,"Abstract. We examine a fundamental problem that models various active sampling setups, such as network tomography. We analyze sampling of a multivariate normal distribution with an unknown expectation that needs to be estimated: in our setup it is possible to sample the distribution from a given set of linear functionals, and the difficulty addressed is how to optimally select the combinations to achieve low estimation error. Although this problem is in the heart of the field of optimal design, no efficient solutions for the case with many functionals exist. We present some bounds and an efficient sub-optimal solution for this problem for more structured sets such as binary functionals that are induced by graph walks. "
823,Infinitesimal Annealing for Training Semi-Supervised Support Vector Machine,"The semi-supervised support vector machine(S^3VM) is a popular classification algorithm for finding the maximum-margin separating hyper-plane for both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good(local) solution of S^3VM is to gradually increase the effect of unlabeled data, `a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches. "
824,Learning Collaborative Behaviors Through Observation: A Contextual Case-Based Planning Approach,Learning teamwork behaviors has been an increasingly important issue for different applications in multi agent systems. Observational learning of teamwork behaviors provides an intuitive and effective method to imitate the behaviors of experts. However the agents should have the ability to generalize observed behaviors for unseen situations. In this paper a new framework is employed for learning collaborative behaviors from an expert team. Our method enhances observational learning method with explicit domain knowledge. Agents observe an expert team perform some tasks to learn plans from observational data. Tasks are defined with the help of some explicit domain knowledge. A contextual case-based planning system uses learned plans to enable learners to imitate expert?s behaviors by performing tasks for different contexts. This paper describes an implemented soccer prototype built to evaluate the effectiveness of our hybrid approach by some experiments. The results of experiments show improved teamwork performance and generalization ability in comparison with other alternative methods.
825,Semantic Kernel Forests from Multiple Taxonomies,"When learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient.  While an \emph{object taxonomy} specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that \emph{are} relevant.  In light of these issues, we propose a discriminative feature learning approach that leverages \emph{multiple} hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenic ties, while another could reflect their habitats).  For each taxonomy, we first learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes.  Then, using the resulting \emph{semantic kernel forest}, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class.  To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure.  We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements."
826,Data Representation with Rank Regularized PCA,"Trace-norm is often used in low-rank data representation models. In this paper, we point out some drawbacks of the trace norm based approach and propose a rank regularized formulation which can be solved very efficiently. We did extensive experiments on six datasets. Experiments show the advantage of the proposed approach."
827,Diversity and Capacity Control in Boosting,"Various explanations have been proposed for understanding the great success of boosting algorithms, particularly AdaBoost, mainly including the statistical view and the margin theory. However, there are still observations out of the explanations. In this paper, we investigate the learning capacity (also known as the hypothesis space complexity and structure risk) of AdaBoost, which has not been well investigated in existing explanations. Previously, the learning capacity of boosting was canonically measured by the VC-dimension of the convex hull of the linear combination of hypotheses, which shows that the capacity grows exponentially to the number of base hypotheses. This paper proves the connection between the learning capacity and the diversity among base hypotheses, which discloses that the learning capacity can be small if the diversity is large. We then reveal that AdaBoost can automatically maximize diversity while optimizing its loss function, and therefore, implicitly controls its learning capacity. The investigation on diversity of AdaBoost may provide a clue to complement the understanding of AdaBoost and boosting algorithms."
1087,Gradient-Boosted Adaptive Codes for Classification and Embedding,"Discriminative classifiers pursue simultaneous embeddings of observations and classes in a shared space such that the embedding of each observation is more similar to the embedding of its associated class than to that of any other class. Boosting-based classifiers are often partitioned into two sets: those which assume a fixed embedding of the classes and those which concurrently learn to embed both the observations and classes. The classifier we introduce falls into the latter camp; it learns all dimensions of the observation and class embeddings concurrently. With L1 regularization applied to the class embedding, our method outperforms existing boosted classifiers on standard benchmarks. Using Euclidean distance to measure class-observation similarity, rather than the typical dot-product, and with additional regularization controlling the spread of each class' embedded representation, our method also produces meaningful embeddings of labeled data. We begin our presentation by recapitulating boosting as functional gradient descent and then examining a weakness in one frequently cited theorem concerning the convergence of gradient-based boosting."
1088,Bandit Market Makers,"We propose a flexible framework for profit-seeking market-making, using a sequence of cost-function based automated market-makers with bandit learning algorithms. We do this by considering the magnitude to which a cost-function extends beyond the simplex as a bandit arm, and the minimum-expected profits consistent with a no-arbitrage condition as the rewards. This allows for the creation of market-makers that can adjust bid-asks spreads dynamically, maximising worst-case-expected profits. "
1089,Clustered Bandits,"We consider a multi-armed bandit setting that is inspired by real-world applications in e-commerce.  In our setting, there are a few types of users, each with a specific response to the different arms. When a user enters the system, his type is unknown to the decision maker.  The decision maker can either treat each user separately ignoring the previously observed users, or can attempt to take advantage of knowing that only few types exist and cluster the users according to their response to the arms. We devise algorithms that combine the usual exploration-exploitation tradeoff with clustering of users and demonstrate the value of clustering.  In the process of developing algorithms for the clustered setting, we propose and analyze simple algorithms for the setup where a decision maker knows that a user belongs to one of few types, but does not know which one."
1090,Discovering Latent Styles in Human Movements,"There are often latent styles underlying the dynamics of human movements; for instance, table tennis strokes can be executed with forehand push or backhand chop. Modeling latent styles in human movements is crucial for learning prior models in many applications. We propose a latent style dynamics model to capture the generative process of human movements from latent styles. As efficient inference is desired in practice, we introduce an approximate inference method based on proxy variables, which, despite its low complexity, also takes into account the uncertainty in latent style variables. On both synthetic data and human table tennis stroke data, our method successfully discovers interpretable latent styles and provides reliable modeling of human dynamics."
1091,Learning from the Wisdom of Crowds by Minimax Conditional Entropy,"We consider the multiclass crowd labeling issue.  Each instance is labeled several times by different workers,  while one instance might be labeled more times than another. We propose a minimax conditional entropy principle to simultaneously estimate  worker expertise,  task ambiguities, and true labels. We also suggest an objectivity requirement for reasonably measuring worker expertise and task ambiguities, and show that the proposed method is unique in meeting  the objectivity requirement. Experimental results are presented for both synthetic and real data."
1093,Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Hamiltonian Monte Carlo Sampling,"This paper proposes a novel framework for multigroup shape analysis relying on a hierarchical graphical statistical model on shapes within a population. Under the proposed hierarchical model, individual shapes are represented as pointsets, derived from their group shape model. Similarly, each group shape model is derived from a single population shape model. The hierarchical model follows the natural organization of population data and enables comparison of shape models between groups, via hypothesis testing, by proving a common frame of reference for the shape models. Unlike typical approaches for shape modeling, the proposed model is a generative statistical model that defines a joint distribution of objectboundary data and the model variables in the hierarchical model. Furthermore, it naturally enforces optimal correspondences during the process of model fitting and thereby subsumes the correspondence problem. The proposed optimization framework employs an expectation maximization (EM) algorithm that treats the individual and group shape variables as hidden random variables and integrates them out before estimating the parameters (population mean and variance and the group variances). The underpinning of the EM algorithm is the sampling of shapes from their posterior distribution, for which the paper exploits a highly efficient scheme based on Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted hierarchical model to perform hypothesis testing for comparison between pairs of groups using permutation testing. The paper validates the proposed framework on simulated data and demonstrates results on real data."
1094,Adaptive Training for Online Transfer Learning ,"Training an effective prediction model using small amount of data is important topic in machine learning.For this purpose, one of the most widely studied frameworks is transfer learning.To achieve scalability and reduce memory, we focus on online transfer learning. Although most works on online learning intends to minimize cumulative errors throughout online training,we introduce a novel online algorithm for transfer learningto obtain the best prediction adaptivelyand improve generalization error rapidly rather than cumulative errors.We give a strong theoretical support on the predictive accuracy of our algorithm.Numerical experiments for several datasets demonstrates that our algorithm always have optimal accuracy in course of online training."
1095,Learning Representations for Detecting and Recognizing Sequences of Animated Motion - A Neural Model,"The detection and categorization of animate motions is a crucial task underlying social interaction and perceptual decision-making. Neural representations of perceived animate objects are built in the primate cortical region STS which is a region of convergent input from intermediate level form and motion representations. Populations of STS cells exist which are selectively responsive to specific animated motion sequences, such as walkers. It is still unclear how and to which extent form and motion information contribute to the generation of such representations and what kind of mechanisms are involved in the learning processes. The paper develops a cortical model architecture for the unsupervised learning of animated motion sequence representations. We demonstrate how the model automatically selects significant motion patterns as well as meaningful static form prototypes characterized by a high degree of articulation. Such key poses are selectively reinforced during learning through a cross-talk between the motion and form processing streams. Next, we show how sequence selective representations are learned in STS by fusing static form and motion input from the segregated bottom-up driving input streams. Cells in STS, in turn, feed their activities recurrently to their input sites along top-down signal pathways. We show how such learned feedback connections enable making predictions about future input as anticipation generated by sequence-selective STS cells. Network simulations demonstrate the computational capacity of the proposed model by reproducing several experimental findings from neurosciences and by accounting for recent behavioral data."
1097,The Design of a Vibro-tactile Watch for Long-term Use and Learning,"In this article, we present a mechanical tactile device that emulates a watch. This device has many practical applications but requires further study. We discuss its design, benefits, and flaws. We then discuss possible solutions to specific problems, as well as future uses of the device. We also discuss the implications this device may have on haptic research."
1098,An Iterative Soft-Hard Thresholding Method for Low-Rank Matrix Recovery and Completion,"Low-rank matrix recovery and completion problems can be solved via their convex relaxations, which minimize the nuclear norm instead of the rank function, and have to be solved iteratively and involve singular value decomposition (SVD) at each iteration. Therefore, those algorithms suffer from high computational cost of multiple SVDs. In this paper we propose an efficient iterative soft-hard thresholding (ISHT) method to approximate the original nuclear norm minimization (NNM) problem and mitigate the computational cost of performing SVDs. The proposed ISHT method can be used to address a wide range of low-rank matrix recovery and completion problems such as low-rank representation (LRR), robust principal component analysis (RPCA) and low-rank matrix completion (MC). The ISHT method relies on first order optimization with orthogonal constraint. Furthermore, we also prove that the proposed algorithm converges to local minima. Experimental results validate the efficiency, robustness and effectiveness of our ISHT method comparing with the state-of-the-art NNM algorithm."
1101,Spectral Differential Privacy,"Positive semidefinite matrices are important for a number of machine learning applications. We consider the problem of differentially private publication of positive semidefinite matrices computed from private information. Differential privacy is typically achieved by adding random noise.However, when the outputs form positive semidefinite matrices, element-wise additive randomization causes problems. First, when not a single element, but the entire matrix is released, the scale of noises to provide differential privacy can be too large. Second, such randomization not only destroys the positive semidefiniteness, but may be statistically denoised in some cases.For these problems, we introduce a new randomization mechanism which separately randomizes eigenvectors and eigenvalues so that the randomization does not completely destroy the spectral features. Furthermore, noting that low-rank approximation preserves useful information of matrices while discarding unnecessarily details, we incorporate low-rank approximation into randomization.We prove that the scale of perturbation required to guarantee differential privacy is inversely proportional to the rank of the output matrices in the proposed randomization mechanism. Thus, if a data analyst does not need the output matrix itself, but needs only a low-rank approximation, the scale of perturbation can be relatively smaller without sacrificing privacy. This is convenient for machine learning applications which work well even with lower-rank approximation.We experimentally demonstrate that low-rank approximation helps to implicitly control the accuracy-privacy trade-off with  a collaborative filtering example."
1102,Sketch-Based Linear Value Function Approximation,"Hashing is a common method to reduce large, potentially infinite feature vectors to a fixed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and fifty-five Atari 2600 games to highlight the superior learning performance of tug-of-war hashing."
1103,Nearly Optimal Algorithm for Euclidean Projection onto a Nonnegative Max-Heap,"Optimization problems with non-negative max-heap constraints arise in many regression/classification applications because the features often exhibit some hierarchical relationships. Euclidean projection is one essential step in the optimization scheme, so it is of great interest to compute the projection of a vector (of length $p$) onto a non-negative max-heap. In this paper, we develop a greedy merge-based algorithm (GMBA) for computing such projection. The algorithm benefits from powerful data structures, including Fibonacci heaps for children retrieve, heap for greedy selection, and disjoint-set for super-node finding. The proposed algorithm runs in $O(p \log p)$ for an arbitrary tree, which is much faster than the best algorithm previously known whose time complexity is $O(p^2)$. Numerical simulations show that the proposed algorithm is efficient and is significantly faster than previous one."
1105,Multimodal Learning with Deep Boltzmann Machines,"We propose a Deep Boltzmann Machine for learning a generative model of multimodal data. We show how to use the model to extract a meaningful representation of multimodal data. We find that the learned representation is useful for classificationand information retreival tasks, and hence conforms to some notion of semantic similarity. The model defines a probability density over the space of multimodal inputs. By sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval fromboth unimodal and multimodal queries. We further demonstrate that our model can significantly outperform SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains."
1106,Learning with Target Prior,"In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\y$ can be modeled with a prior model $p(\y)$ and the relations between data and target variables are estimated through $p(\y)$ and a set of uncorresponded data $\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\t$ that maximizes the log likelihood of $f_\t(\x)$ on a uncorresponded training set with regards to $p(\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video."
1107,Sparsest Combination under Linear Transformation,"We consider the following signal recovery problem: given a measurement matrix $\Phi\in \mathbb{R}^{n\times p}$ and a noisy observation vector $c\in \mathbb{R}^{n}$ constructed from $c = \Phi \theta^* + \epsilon$ where $\epsilon\in \mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal $\theta^*$ if $D\theta^*$ is sparse where $D\in\mathbb{R}^{m\times p}$? One natural method using convex optimization is to solve the following problem: $$\min_{\theta}~{1\over 2}\|\Phi\theta - c\|^2 + \lambda\|D\theta\|_1.$$ This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix $\Phi$ is a Gaussian random matrix:1) in the noiseless case, if the condition number of $D$ is bounded and the measurement number $n\geq \Omega(s\log(p))$ where $s$ is the sparsity number then the true solution can be recovered with high probability; and2) in the noisy case if the condition number of $D$ is bounded and the measurement increases faster than $s\log (p)$ (that is, $s\log(p)=o(n)$), the estimate error converges to zero with probability 1 when $p$ and $s$ go to infinity. Our results are consistent with those for the special case $D=\bold{I}_{p\times p}$ (equivalently LASSO) and improve the existing analysis for the same formulation. The condition number of $D$ plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if $m/p$ is larger than a certain constant. Numerical simulations are consistent with our theoretical results."
1108,On Pre-training Shallow Networks with Support Vector Machine Primals,"We present a methodology to pre-train shallow neural networks with Support Vector Machine primals. We train a Support Vector Machine and extract the primal weights to embed them as pre-trained prior knowledge in a shallow neural network; we then proceed to apply backpropagation to leverage and fine tune this knowledge. This contrasts with previous work on pre-training, in which unsupervised pre-training has been used as feature extractors in deep learning. In our MNIST experimental results, we find that using even only $\frac{1}{60}$ of the original dataset for the Support Vector Machine primal pre-training yielded a consistently faster convergence in the network. We believe this paper opens up interesting opportunities for pre-training shallow networks using prior knowledge."
1109,Slice sampling normalized kernel-weighted completely random measure mixture models,"A number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality.  However, the inference algorithms are often slow and unwieldy, and are in general highly specific to a given model formulation. In this paper, we describe a wide class of nonparametric processes, including several existingmodels, and present a slice sampler that allows efficient inference across this class of models.  "
1110,Semi-Supervised Learning with Probabilistic Smoothness on Graphs,"We study graph-based semi-supervised learning by exploiting the smoothness constraint with respect to the intrinsic structure that exists among labeled and unlabeled data. Unlike previous works that define the smoothness constraint by different cost functions, we formulate it under a probabilistic framework instead. Interestingly, our probabilistic smoothness constraint can be tied to and hence justifies a new cost function. Based on this probabilistic smoothness constraint, we further derive an algorithm PSmooth for semi-supervised learning, which can also be interpreted as a random walk on graphs. Finally, our experiments show that PSmooth consistently outperforms existing state-of-the-art algorithms on various public benchmark datasets."
1111,Deep Attribute Networks for Attribute-based Classification,"Obtaining compact and discriminative features is one of the major challenges in many of the real-world image classification tasks such as face verification and object recognition. One possible approach is to represent input image on the basis of high-level features that carry semantic meaning that humans can understand. In this paper, a model coined deep attribute network (DAN) is proposed to address this issue. For an input image, the model outputs the attributes of the input image without performing any classification. The efficacy of the proposed model is evaluated on unconstrained face verification and real-world object recognition tasks using the LFW and the PASCAL VOC datasets. We demonstrate the potential of deep learning for attribute-based classification by showing comparable results with existing state-of-the-art results. Once properly trained, DAN is fast and does away with calculating low-level features which maybe unreliable and computationally expensive."
1113,Learn to rank Based on Topic Relation Models,"Most of learning to rank algorithms use word based features to train ranking models. Topic features are seldom considered. In this paper we aim at building a topic based ranking model which uses the relational topic model (RTM) to learn ranking function. The original RTM is a one-class model. To make RTM learn from data with different ranking labels, we extend RTM to two-class model and regression model. We employ variational inference algorithm to compute the posterior distribution of the latent variables and use the Expectation Maximization algorithm to estimate the topic vectors and parameters of ranking function. Our experiments on OHSUMED data set show that the proposed models have better accuracy than word based ranking model(SVM) and the original RTM model."
1114,Scalable Inference of Overlapping Communities,"We develop a scalable algorithm for posterior inference of overlappingcommunities in large networks.  Our algorithm is based on stochasticvariational inference in the mixed-membership stochastic blockmodel.It naturally interleaves subsampling the network with estimating itscommunity structure.  We apply our algorithm on ten large, real-worldnetworks with up to 60,000 nodes. It converges several orders ofmagnitude faster than the state-of-the-art algorithm for MMSB, findshundreds of communities in large real-world networks, and detects thetrue communities in 280 benchmark networks with equal or betteraccuracy compared to other scalable algorithms."
1115,Maximize Short-term Memory in Direct Model of Echo State Networks,"Echo state networks (ESN) are a kind of novel recurrent neural network (RNN) which has a large number of randomly connected neurons (called ?reservoir?) and an adaptable output. The short-term memory (STM) of ESN is the ability of storing information about recent inputs in the reservoir's transient response. It is indispensable for the time varying information processing. Previous work suggested that for i.i.d. input, the upper bound of memory capacity (MC) is N, where N is the number of neurons in the reservoir. In this paper, we show that this is not always the case. We transform the iterative mathematical model of ESN to direct one. In this model, we establish a direct relationship between memory capacity of ESN and its connectivity. We find that some reservoir topologies proposed by previous papers are the special solutions of our method. Furthermore, our experimental results show that the maximum MC in ESN can exceed the upper bound N even with i.i.d. input."
1116,Online L1-Dictionary Learning with Application to Novel Document Detection,"Given their pervasive use, social media, such as Twitter, have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge, we introduce the problem of online L1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the L1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data, shows that this online L1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results. Our algorithm for online L1-dictionary learning could be of independent interest."
1117,Learning from Data and Constraints: an Unified Probabilistic View of Clustering,"In this paper we introduce an unified view of clustering, i.e. learning from unlabeled data and constraints. Clustering is considered as a prediction problem for the states of latent variables, i.e. unknown labels. Unlabeled data and constraints are seen as two main sources to provide related information of latent variables. We present a probabilistic clustering model based on Hidden Markov Random Fields (HMRFs), which can embed different related information together to guide theclustering process. We also present a novel constrained clustering method, i.e. a simplified version of HMRF-based clustering model. Unlabeled data and a few pairwise constraints are combined to generate the neighborhood system between latent variables. One of the main limitations of existing constrained clusterings, which is the requirement of a large amount of constraints, can be significantly alleviated. Further, connections between HMRF-based clustering model and manyexisting clusterings are established to demonstrate the inclusiveness and flexibility of the proposed model. Experiments on synthetic and real data are also performed to demonstrate the benefits of the proposed constrained clustering method."
1118,A systematic approach to extracting semantic information from functional MRI data,"This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure."
1120,Why MCA? Nonlinear Spike-and-slab Sparse Coding for Neurally Plausible Image Encoding,"Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level components, e.g. edges. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA). The major challenge is parameter optimization because a model with either (1) or (2) results in a strongly multimodal posterior. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posterior. We design an exact piecewise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric, bimodal, and sparse activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, due to the nonlinearity, the model predicts a large number of globular receptive fields (RFs), another significant difference from standard SC. The inferred prior and the high proportion of predicted globular fields make the model more consistent with neural data than previous SC models, suggesting closer tuning of simple cells to visual stimuli than has been predicted until now. "
1121,Learning optimal spike-based representations,"How do neural networks learn to represent information? Here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder. We define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss. The dynamical equations yield a network of integrate-and-fire neurons undergoing Hebbian plasticity. We show that, through learning, initially regular and highly correlated spike trains evolve towards Poisson-distributed and independent spike trains with much lower firing rates. The learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder. We show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs. In turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme. Our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code."
1122,Collaborative Ranking With 17 Parameters,"The primary application of collaborate filtering (CF) is to recommend a small set of items to a user, which entails ranking. Most approaches, however, formulate the CF problem as rating prediction, overlooking the ranking perspective. In this work we present a method for collaborative ranking that leverages the strengths of the two main CF approaches, neighborhood- and model-based. Our novel method is highly efficient, with only seventeen parameters to optimize and a single hyperparameter to tune, and beats the state-of-the-art collaborative ranking methods. We also show that parameters learned on one dataset yield excellent results on a very different dataset, without any retraining."
1123,Rational inference of relative preferences,"Statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions. However, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented. In this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects. Complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function. "
1124,Multiresolution Value Function Approximation in Reinforcement Learning using the Wavelet Basis,"We present the wavelet basis, a linear value function approximation scheme that enables multiresolution value function approximation in continuous state spaces. We apply the wavelet basis to two standard reinforcement learning domains, and show that it performs as well as or better than existing commonly used basis functions when used as a fixed basis.We also briefly demonstrate how it can be used to add representational power to better represent spatially local detail."
1125,Sparse Reward Processes,"  We introduce a learning problem where the agent is presented with a series of tasks. If they are related, information gained during execution of one has value for future tasks. Thus, the agent must be ``curious'' and explore its environment beyond the degree necessary to solve the current task.  We develop a decision theoretic setting capturing this intuition, the sparse reward process:  This is a multi-stage stochastic game between a learning agent and an opponent. The agent acts in an unknown environment, according to a utility that is arbitrarily selected by the opponent at each stage. We link our setting to other problems, examine its properties and examine the behaviour of two learning algorithms, for different opponent types."
1126,Off-Policy Actor-Critic with Function Approximation,"We present a new off-policy learning algorithm with an actor-critic architecture that is convergent to a locally optimal solution. Off-policy learning---learning about a policy different from the one being followed---plays an important role in reinforcement learning (RL) due to exploration-exploitation tradeoff. Recent advances in off-policy Temporal-Difference (TD) learning, such as Greedy-GQ, have been hitherto  limited  to value-function based methods and have not been fully extended to policy gradient methods,  which can represent a larger class of policies and also can handle problems with large (or continuous) action space.  Among policy gradient methods, actor-critic methods substantially have been considered for large-scale applications due to their desirable algorithmic features---e.g., they use bootstrapping methods such as TD learning that can reduce variance, and generally are easy to use with function approximation. The critic in our algorithm is based on recent gradient-TD prediction (GTD) methods with linear function approximation and the actor updates the policy parameters via stochastic gradient-ascent of a performance measure. Recently, Degris et al. (2012) have presented an off-policy actor-critic algorithm (OPAC) with similar objectives. However, OPAC does not update the actor via gradient-ascent and, as we will establish, does not always converge. In this paper, we address this issue by proposing an algorithm, called GTD-AC, that shares several of OPACs desirable features: online operation, incremental updating, linear complexity both in terms of memory and per-time-step computation, and in addition it maintains the same number of tuning parameters. Most importantly, we establish a convergence guarantee."
1127,Minimum Distortion Sketches for Learning,"The number of unique features can be prohibitively large in various classification problems, especially in document analysis. For such problems, storing all coefficients of a classifier model would require massive amount of memory on a single machine. Therefore, in this work, we propose a new sketching technique to approximately store the coefficients of a classifier model in constant memory budget. The proposed technique cuts down the space requirement by a factor of sketch depth (typically 3-7 in practice) in the best case, while matching the best known bounds of the existing sketching technique in the worst case. We also demonstrate the performance improvement on various large-scale classification problems."
1128,Extrinsic Support Vector Machines for Riemannian Manifolds,"In computer vision and pattern recognition applications, the features often lie on Riemannian manifolds. In such cases, one needs good classification techniques that make use of the underlying manifold structure. Due to its superior generalization properties, in this paper, we focus on developing support vector machine (SVM) classifier for such features. For Riemannian manifolds, the popular approach for classification is to project the data onto the tangent space and learn classifiers in this space. However, the tangent space need not be optimal for classification. Furthermore, it does not even preserve the global structure of the manifold. Hence, we learn a feature space suitable for support vector classification, by minimizing the structural risk of SVM, and using the manifold structure as a regularizer. We formulate this problem of learning the optimal space as a kernel learning problem, which results in an instance of semidefinite programming, that can be solved using standard semidefinite programming solvers. We also discuss a computationally more efficient solution using the multiple kernel learning framework. Experimental evaluation on synthetic and real data sets clearly demonstrate the effectiveness of the proposed approach."
1129,Recognizing Human Activities from Incompletely Observed Videos,"In this paper, we present a novel method for handling the problem of recognizing human activities from incompletely observed videos. Compared with the similar problem of human activity prediction from unfinished activities [12], in an incompletely observed video an un-observed subsequence of frames may occur any time with any duration and yield a temporal gap in the video. In practice, incompletely observed videos may occur when the video signal drops off, when camera or objects of interest are occluded, or when videos are composited from multiple sources. In this paper, we formulate the problem of human activity recognition from incompletely observed videos in a probabilistic framework. In this framework, we take a set of training video samples (completely observed) of each activity class as the basis, and then use sparse coding to derive the likelihood that an incompletely observed test video belongs to a certain activity class. Furthermore, we propose to divide each activity into multiple temporal stages, apply sparse coding to derive the activity likelihood at each stage, and finally combine the likelihoods at each stage to achieve a global posterior for the activity. We evaluate the proposed method on both the widely used UT-Interaction human activity dataset and a new human activity dataset selected from the Year-1 corpus of the DARPA Mind's Eye program [4]. For the new DARPA dataset, both the activities and the videos show very large within-class temporal, spatial, and background variation. Our results demonstrate that the proposed method performs substantially better than several competing methods on both datasets."
1130,Clustered Approximation for Gaussian Kernel Support Vector Machines,"The Gaussian kernel support vector machine (SVM) is one of the most widely used classification methods. However, scalability of this method is a big issue when facing millions of samples. Recently, many papers have suggested tackling this problem by using a low-rank approximation for the Gaussian kernel matrix. In this paper, we first show that the structure of the Gaussian kernel matrix changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter $\gamma$ in the Gaussian kernel. Based on this observation, we propose a Clustered Approximation (CA) framework for Gaussian kernel matrix approximation. For many non-linearly separable datasets, we find that the best parameter $\gamma$ for classification usually lies in the region where CA achieves lower approximation error than low-rank approximation when they both use the same amount of memory.Moreover, when we apply CA methods to scale and speed up the Gaussian kernel SVM training, the resulting algorithms outperform state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on a non-linearly separable dataset \covtype with half-a-million samples, our proposed method Clustered Block Approximation SVM (CBA-SVM) achieves 96.03\% testing accuracy with training time of less than 3 minutes on a single workstation with 4G RAM. In comparison, on this problem, \LIBSVM takes more than 13 hours to get 96.08\% accuracy, while the fast low-rank approximation based method, low-rank linearized SVM (LLSVM), takes 1 hour but only gets 85.05\% testing accuracy. "
1131,Sensitivity analysis of neuronal dynamics under additive STDP rule,"Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learning rule. The basis of STDP has strong experimental evidences and it depends on precise input and output spike timings.  In this paper we show that under biologically plausible spiking regime, slight variability in the spike timing leads to drastically different evolution of synaptic weights when its dynamics are governed by the additive STDP rule."
1134,Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. "
1135,Online computation of sparse representations of time varying stimuli using a biologically motivated neural network,"Natural stimuli are highly redundant, possessing significant spatial and temporal correlations. While sparse coding has been proposed as an efficient strategy employed by neural systems to encode sensory stimuli, the underlying mechanisms are still not well understood. Most previous approaches model the neural dynamics by the sparse representation dictionary itself and compute the representation coefficients offline. In reality, faced with the challenge of constantly changing stimuli, neurons must compute the sparse representations dynamically in an online fashion. Here, we describe a leaky linearized Bregman iteration (LLBI) algorithm which computes the time varying sparse representations using a biologically motivated network of leaky rectifying neurons. Compared to previous attempt of dynamic sparse coding, LLBI exploits the temporal correlation of stimuli and demonstrate better performance both in representation error and the smoothness of temporal evolution of sparse coefficients."
1136,The topographic unsupervised learning of natural sounds in the auditory cortex,"The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efficient coding strategy and that the disorder in A1 reflects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reflected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner."
1137,Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand,"In many large economic markets, goods are sold through sequential auctions. Such domains include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions. In this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown."
1138,Safe Policy Iteration,"This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on a simple test domain."
1139,A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation,"In this paper, we consider the $\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables. We further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure. As an example, a recent state-of-the-art method, QUICrequires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem."
1140,Importance Sampling Active Learning Algorithm,"This paper presents the importance sampling active learning (ISAL) algorithm, which can be very sample efficient by drawing instances to label from some appropriate distribution.  In particular,  ISAL  begins with a set of user-specified instance distributions; on each iteration, it identifies  a distribution that puts large weight on instances whose labels are  uncertain, then requests the label of an instance drawn from that distribution. We prove that ISAL can be more sample-efficient than passive learning, and that it  can achieve an exponential convergence rate to the Bayes classifier on noise-free data.  We also provide empirical studies that show  ISAL   is more efficient than many other active learning algorithms. "
1141,Fast Manifold Learning with Unsupervised Nearest Neighbors,"In this paper we introduce a simple and extremely fast dimensionality reduction method for point-wise embedding of patterns in continuous latent spaces. The approach is an iterative method, which fits nearest neighbors into the framework of unsupervised regression. We introduce unsupervised nearest neighbors for continuous latent spaces. Latent points are iteratively embedded with a stochastic approach: distances in data space are employed as standard deviation for Gaussian sampling in latent space, neighborhood relations are preserved with a nearest neighbor regression-based data space reconstruction error. We extend the approach to handle missing data, and analyze the employment of kernel functions for computation of the data space reconstruction error. Experimental studies show that kernel unsupervised nearest neighbors is an an efficient method for embedding high-dimensional patterns on artificial test data, and real-world data from astronomy."
1142,Conditional Distance Variance and Correlation,"Recently a new dependence measure, the distance correlation, has been proposed to measure the dependence between continuous random variables. A nice property of this measure is that it can be consistently estimated with the empirical average of the products of certain distances between the sample points. Here we generalize this quantity to measure the conditional dependence between random variables, and show that this can also be estimated with a statistic using a weighted empirical average of the products of distances between the sample points. We demonstrate the applicability of the estimators with numerical experiments on real and simulated data sets."
1143,A Simple and Practical Algorithm for Differentially Private Data Release,"We present a new algorithm for differentially private data release, based on a simple combination of the Exponential Mechanism with the Multiplicative Weights update rule.  Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques."
1145,Weighted Likelihood Policy Search with Model Selection,"Reinforcement learning (RL) methods based on direct policy search (DPS)have been actively discussed to achieve an efficient approach to complicatedMarkov decision processes (MDPs).Although they have brought much progress in practical applications ofRL, there still remains an unsolved problem in DPS related to model selection for the policy.In this paper, we propose a novel DPS method,{\it  weighted likelihood policy search (WLPS)}, where a policy isefficiently learned through the weighted likelihood estimation.WLPS naturally connects DPS to the statistical inference problemand thus various sophisticated techniquesin statistics can be applied to DPS problems directly.Hence, by following the idea of the {\it information criterion},we develop a new measurement for model comparison in DPSbased on the weighted log-likelihood."
1147,Implementing Attention Focus in Model-based Reinforcement Learning,"We propose a new framework for learning the world dynamics of model-based reinforcement learning (RL) in high-dimensional, feature-rich environments. We model the world dynamics through predicting changes with the action effects, and differentiating the model features involved.  We present a factored transition function representation that supports efficient learning of the relevant features. We also introduce an online sparse coding learning technique to implement attention focus in learning the transition models. We provide theoretical analyses and empirical evaluation of our new RL algorithm, loreRL, in two benchmark domains."
1148,Learning the Dependency Structure of Latent Factors,"In this paper, we study latent factor models with the dependency structure in the latent space.  We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance."
1149,"Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders","We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ is chosen uniformly at random from $\{+1, -1\}^n$, $\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers $A$ and $\Sigma$ up to an additive $\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \epsilon$. To accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search."
1150,Cellular Neural Networks: A Scalable Architecture for Learning MIMO Systems,"Cellular neural networks (CNNs) are a class of sparsely connected dynamic recurrent networks (DRNs). Neural networks for implementing large complex interconnected systems consist of multiple inputs and multiple outputs. Many outputs lead to more number of parameters to be adapted. Each additional variable increases the dimensionality of the problem and hence learning becomes a challenge. By proper selection of a set of input elements that affect a particular output variable in a given application, a DRN can be modified into a CNN which significantly reduces the complexity of the neural network and allows use of simple training methods such as backpropagation for independent learning in each cell thus making it scalable. The paper demonstrates this concept of developing CNN using dimensionality reduction in a DRN for scalability and better performance. The concept has been empirically verified through applications."
1151,Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins,"While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice. However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent. In this work we propose to augment these algorithms with an $\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers."
1152,Approximate Factored Real-time Dynamic Programming,"Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when there is information about the initial state. Traditional RTDP approaches work best on problems with sparse transition matrices where they can often efficiently achieve e-convergence without visiting all states; however, the advantages of traditional RTDP are often lost on problems with dense transition matrices where most states are reachable in one step (which is the case of a variety of control problems with exogenous events), as we demonstrate in this paper. One approach to overcome this caveat is to exploit the regularities in the domain dynamics, reward and value function throughout factored representation and calculations. In this paper, we propose a new  factored RTDP algorithm, called FactRTDP, and its approximate version, called aFactRTDP, which is the first straight forward factored version of enumerative RTDP, i.e., without performing generalized updates. Experiments show that these new algorithms can deal with dense transition matrices and have good online behavior when compared to the best probabilistic planning systems, without engineering optimizations, but by simply exploiting factored backups."
1153,When Block Meets Group: Structured Sparse Modeling For Learning,"This paper proposes a novel framework, the {\it block/group sparse coding} ($\bg$), for dictionary learning.  Two important features distinguish $\bg$ from other existing methods in that all dictionary blocks are trained simultaneously with respect to each data group and instead of the inter-block coherence, the intra-block coherence is explicitly minimized as an important objective.  We provide both empirical and heuristic evident for this latter novel feature that can be regarded as the consequence of using the group structure for the data and the block structure for the dictionary.  The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-coordinates descent, and the details of the optimization algorithms are presented.  We evaluate the proposed method on several classification (supervised) and clustering (unsupervised) problems using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed $\bg$ algorithm."
1154,Recurrent Gradient Temporal Difference Networks,"Temporal-difference (TD) networks (Sutton and Tanner, 2004) are a predictive representation of state in which each node is an answer to a question about future observations or questions. Unfortunately, existing algorithms for learning TD networks are known to diverge, even in very simple problems. In this paper we present the first sound learning rule for TD networks. Our approach is to develop a true gradient descent algorithm that takes account of all three roles performed by each node in the network: as state, as an answer, and as a target for other questions. Our algorithm combines gradient temporal-difference learning (Maei et al., 2009) with real-time recurrent learning (Williams and Zipser, 1994). We provide a generalisation of the Bellman equation that corresponds to the semantics of the TD network, and prove that our algorithm converges to a fixed point of this equation. We also demonstrate empirically that our learning algorithm converges to the correct solution in a benchmark problem for which prior learning rules diverge."
1155,Split and Approximate in Monte Carlo,"We advance a general approach to increase the efficiency of Monte Carlo algorithms for estimating multi-dimensional integrals: split the integral into two nested parts; treat the outer integral by sampling while computing the inner integral to within a guaranteed approximation ratio. We give a unifying view of some previously presented specializations of this approach, provide analytical justifications and guidelines for algorithm design, and demonstrate the power of the approach in Bayesian structure discovery in Bayesian networks."
1156,Decision Tree Algorithm for Data Streams based on Alternate Formulation of Mutual Information,"Learning problems with large and streaming data sets are becoming prevalent now a days. Such large data sets require specialized algorithms that are able to learn in one pass through the data set. Standard Decision Tree algorithms are not suitable for data streams. This paper describes and evaluates IQ Tree, a novel decision tree construction algorithm that can handle data streams. We derived an alternate formulation of Mutual Information that facilitates variable levels of approximation of Information Gain. Based on this formulation, our algorithm goes through the data only once and uses pre-calculation and sampling to achieve fast decision tree construction that closely approximates standard decision tree algorithm. We demonstrate detail error analysis that shows the superiority of our algorithm. Detailed empirical evaluation also shows that IQ Tree is better than state of the art decision tree algorithms that can handle data streams."
1158,Algorithms for finding the source of an outbreak and other epidemic inference problems,"During the course of a disease outbreaka fundamental problem in public health planning is to use surveillance data(e.g., partial subset of people who became infected) to solve variousepidemic inference problems, e.g., what is the probability that a givenindividual was the source of the outbreak, or is infected, or the expectednumber of infections (conditional on the observed infections). We developa systematic approach to formulate these problems in terms of random generationof subgraphs with specific connectivity constraints, by means of an equivalencebetween disease transmission in complex networks and the edge percolationprocess. We then use a Markov chain approach for random sampling of suchsubgraphs. We develop efficient analytical bounds on the mixing time, i.e., thetime needed for the Markov chain to reach close to its stationary distribution.We study the empirical performance of our approach on different graphs, andfind that it is able to determine the node infection probabilities quiteaccurately."
1159,Robust Joint Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix,"We propose a robust framework to jointly perform two critical tasks of high dimensional modeling in synergy: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among the responses while adjusting for their predictors. This framework is relevant to many applications. In computational biology, for instance, it enables the integration of genomic and transcriptomic datasets. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecification. This issue is exacerbated when dealing with high dimensional noisy data. We therefore adopt an alternative approach to minimizing a regularized distance criterion, which is motivated by minimum distance estimators used in nonparametric methods. The proposed method yields an efficient algorithm that alternates between weighted versions of lasso and graphical lasso, where the sample weights intuitively explain the robustness of our method. We demonstrate the value of our framework through extensive simulation and real eQTL data analysis."
1160,Limits of Adaptation in Crowdsourcing,"Crowdsourcing systems, where numerous tasks are electronically distributed to an unidentified pool of workers through an open call, has emerged as an effective tool for human-powered solving of data intensive tasks such as image classification, video annotation, product categorization, and transcription. Since these low-paid workers can be unreliable, all crowdsourcers need to devise a way to cope with the errors and ensure a certain reliability in their answers. A common solution is to add redundancy by asking each question to multiple workers and combining their answers using some scheme such as majority voting. A fundamental question of interest for such systems is how much redundancy is necessary to achieve a certain accuracy in our answers? In this paper, we investigate the fundamental limit on the minimum number of queries necessary to achieve the target error probability. In particular, we want to identify how much we can gain by switching to an adaptive algorithm from an existing low-complexity and non-adaptive algorithms. To establish  this result, we provid a lower bound on the probability of error achieved by the optimal adaptive algorithm.  Compared to a known upper bound for a practical and non-adaptive algorithm, this shows that there is no significant gain in using adaptive algorithms. In terms of the budget required to achieve the target error probability, the gain of using an adaptive scheme is at most a constant factor. "
1161,No-Regret Algorithms for Unconstrained Online Convex Optimization,"Some of the most compelling applications of online convexoptimization, including online prediction and classification, areunconstrained: the natural feasible set is R^n.  Existing algorithmsfail to achieve sub-linear regret in this setting unless constraintson the comparator point x* are known in advance.  We present analgorithm that, without such prior knowledge, offers near-optimalregret bounds with respect to _any_ choice of x*.  In particular,regret with respect to x* = 0 is _constant_.  We then prove lowerbounds showing that our algorithm's guarantees are optimal in thissetting up to constant factors."
1162,A Gaussian Approximation of Feature Space for Fast Image Similarity,"We introduce a fast technique for the robust computation of image similarity. It builds on a re-interpretation of the recent exemplar-based SVM approach, where a linear SVM is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. Although it is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of-the-art performance on the PASCAL VOC 2007 detection task despite its simplicity. We re-interpret it by viewing the SVM between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. We show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. We then use a simple Gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. Given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. This allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. We further show that our approach is equivalent to feature-space whitening and has links to image saliency."
1163,Distributed Reinforcement Learning for Policy Synchronization in Infinite-Horizon Dec-POMDPs,"In many multi-agent tasks, agents face uncertainty about the environment, the outcomes of their actions, and the behaviors of other agents. Dec-POMDPs offer a powerful modeling framework for sequential, cooperative, multiagent tasks under uncertainty. Solution techniques for infinite-horizon Dec-POMDPs have assumed prior knowledge of the model and have required centralized solvers.  We propose a method for learning infinite-horizon Dec-POMDP solutions in a distributed fashion.  We identify the issue of policy synchronization that distributed learners face and propose incorporating rewards into their learned model representations to both ameliorate this issue and to improve the quality of the agents' learned models. Most importantly, we show that even if rewards are not visible to agents during policy execution, exploiting the information contained in reward signals during learning is still beneficial."
1164,Video-based Object Recognition by Sets of Sets,"We address the problem of automatic object recognition in videos, where users move their mobile camera around an unknown object of interest in order to capture more information in a random manner. Using videos that capture variations in an object's appearance due to camera motions (viewpoints and scales), cluttering and lighting conditions, can accumulate evidences and improve object recognition accuracies. Most previous works have taken a single image as input, or tackled a video by a collection i.e. sum of frame-based recognition scores. In this paper, we explore two novel representations and matching methods of videos for object recognition beyond frame-based recognition: 1) Video is first represented as a set of frames, in which each frame itself is a set of detected feature points (SURF in our experiments); 2) Each feature point in the initial frame is tracked in following frames, which forms one trajectory containing a set of similar feature points, therefore a video is a set of trajectories. Each representation forms sets of sets. We combine bag-of-words (for a set of data spatially distributed) and manifolds method (for a set of data with temporal smooth changes) to depict the two set-of-set representations. Also we propose how to match such representations for object recognition. The proposed representation and matching techniques are evaluated on our video data sets, which contain 830 videos of ten objects and four environment variations. The experiments on the challenging data set show that our proposed solution significantly outperforms the traditional frame-based methods."
1165,Bayesian models for Large-scale Hierarchical Classification ,"A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods. "
1166,Recovery of Sparse Probability Measures via Convex Programming,"We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. It's well-known that the classical $\ell_1$ regularizer fails to promote sparsity on the probability simplex since $\ell_1$ norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on $\ell_1$ norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm."
1168,Rule Extraction from Neural Networks Using an Ensemble of Stacked Decision Trees,"Neural Network is a powerful pattern recognition algorithm capable of learning complex non linear patterns. However, Neural Networks have a well-known drawback of being a ?Black Box? learner that is not comprehensible or transferable thus making it unsuitable for many high risk tasks that require a rational justification for making a decision. Rule Extraction methods can resolve this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. In this paper, we present an algorithm called HERETIC that uses a symbolic learning algorithm (Decision Tree) on each unit of the Neural Network. This stacked ensemble of Decision Trees is shown to be a good approach of approximating a Neural Network. We also present HERETIC+, an extension of the basic algorithm that exploits Neural Network connection weights to attain better accuracy. Experiments and theoretical analysis show HERETIC and HERETIC+ generates highly accurate rules that closely approximates the Neural Network."
1169,Efficient Learning in (Recurrent) Helmholtz Machines,"In this paper, we revisit the Helmholtz machine architecture as a generative model for both stationary data and sequences with long term time dependencies. We pro- pose an efficient method for calculating the gradient of the free energy by back- propagating its first and second derivatives through the model?s Gaussian genera- tive and recognition passes, as such providing an alternative to both the previous wake-sleep algorithm and REINFORCE. While not necessarily competitive with recent techniques from the deep learning community on stationary distributions, the resulting algorithm?s quick unbiased sampling procedure renders the method tractable on complicated sequential video tasks."
1170,Decomposing information,"How can the information contained in a group of random variables be decomposed? Ideally, we would like to understand to what extent different subgroups provide the same, i.e. redundant, information, carry unique information or interact for the emergence of synergistic information.  So far, no convincing solution has been found, that captures our intuitions behind these concepts.Motivated by recent results due to Williams and Beer we discuss natural properties that such an information decomposition should have. We proof that some of these properties contradict each other, and we illustrate further puzzling aspects of shared information. We conclude that intuition and heuristic arguments might not suffice when thinking about information."
1172,Multiple Operator-valued Kernel Learning,"Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces."
1173,MAP Inference on Million Node Graphical Models: KL-divergence based Alternating Direction Method,"We consider the problem of maximum a posteriori (MAP) inference in graphical models with millions of nodes. We present a parallel primal MAP inference algorithm called KL-ADM based on two ideas: tree-decomposition of a graph, and the alternating direction method (ADM). However, unlike the standard ADM, we use an inexact ADM augmented with a Kullback-Leibler (KL) divergence based regularization. Theunusual modification leads to an efficient iterative algorithm while avoiding double-loops. We rigorously prove global convergence of the KL-ADM algorithm. The proposed algorithm is extensively evaluated on simulated datasets and compares favorably to existing approximate MAP inference algorithms. We also implement parallel KL-ADM using Open MPI and the experimental results on a drought detection problem withmore than 7 million variables demonstrate that the algorithm scales nicely in the multicore setting."
1174,An efficient feature allocation for parallel stochastic optimizations with lazy updates,"  This paper proposes an efficient feature allocation algorithm to  accelerate parallelized stochastic optimization algorithms using  lazy updates for large-scale sparse data. Our key observation is  that a feature allocation governs efficiency of parallelized lazy  update algorithms. In fact, in the worst case, the total  computational cost of the parallelized algorithm is the same or even  worse than that of non-parallelized (single-core) algorithms. This  paper formulates the feature allocation problem as a specific form  of variable assignment problem whose optimal feature allocation  minimizes the computational cost for parallelized lazy  updates. Since the assignment problem itself requires large  computational cost, we propose two efficient algorithms using  randomization and a greedy search."
1175,Explicit Embedding Learning with Kernel and Boosting Frameworks for Image Categorization,"In this article, we propose a method to learn a kernel function following a two-stage schema: one for kernel learning, and one for image categorization. We adopt a Boosting framework to design and combine weak kernel functions targeting an ideal kernel. The weak kernel selection criterion adapted to kernel combination and the weight of this combination are computed thanks to an analytic solution. We show that our method actually builds mapping functions which turn the initial input space to a new feature space where categories are better classified. We propose to learn a single kernel/mapping for all categories."
1177,{Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning,"We consider the estimation of an i.i.d.\ vector $\xbf \in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possiblynonlinear) measurement channel. We present a method, calledadaptive generalized approximate message passing(Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\xbf$.The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriorsin the E-steps are computed via approximate message passing.The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yieldasymptotically consistent parameter estimates, which implies that the algorithmachieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic,general and computationally efficient methodapplicable to a large range of complex linear-nonlinearmodels with provable guarantees."
1178,A Better Way to Pre-Train Deep Boltzmann Machines,"We describe how the pre-training algorithm for Deep Boltzmann Machines (DBMs) is related to the pre-training algorithm for Deep Belief Networks and we show that under certain conditions, the pre-training procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pre-training DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pre-training algorithm allows us to learn better generative models."
1179,EEG single-trial detection in a rapid serial visual presentation paradigm task with supervised spatial filtering,"The detection of single-trial event related potentials (ERPs) in the electroencephalogram (EEG) is a difficult problem that requires efficient signal processing and machine learning techniques due to the poor spatial resolution and low signal-to-noise ratio of the EEG signal. Among the different steps that are typically used for the detection, spatial filtering is an important part. Spatial filtering allows enhancing the relevant information in the signal by combining the signal recorded across the different sensors. We propose a neural network with a convolutional layer dedicated to spatial filtering (CNN) for the detection of ERPs. The method is compared with a method based on the maximization of the signal-to-signal-plus-noise ratio (xDAWN) and common spatial pattern (CSP) that maximizes the discriminative activity to the common activity ratio for the creation of spatial filters. The two latter methods are combined with a neural network (MLP) for the classification. We have compared these methods with an MLP without spatial filtering as pre-processing. These techniques were evaluated on a rapid serial visual presentation (RSVP) task where eight participants had to detect faces from car images. The mean area under the ROC curve (AUC) is 0.843, 0.810, 0.753, and 0.820 for CNN, xDAWN+MLP, CSP+MLP, and an MLP without spatial filtering, respectively."
1180,Modular Value Iteration Through Regional Decomposition,"To quickly solve large Reinforcement Learning problems involving complex reward functions (e.g., multiple reward sources), we decompose Markov decision processes (MDP) into regions. We introduce a novel modular version of Least Squares Policy Iteration (LSPI), called M-LSPI, which 1. breaks up the MDP states into a set of mutually exclusive regions, 2. leverages the regional decomposition to efficiently solve the MDP --- all values of each region are updated by a single matrix inversion; regional information is then propagated by value iteration. As the number of states increases, on both structured and unstructured MDPs, this yields substantial improvements over other algorithms in terms of time to convergence to the value function of the optimal policy, especially at a higher discount factor."
1181,Optimal Stochastic Convex Optimization Through The Lens Of Active Learning,"The large fields of convex optimization and active learning have been developed fairly independent of each other, from the design of algorithms to the techniques of proof. Given the growing literature in both these subjects, we believe that understanding the connections between them is important to people in both areas. Here, we establish few such interesting relationships in upper and lower bound techniques that bring out these similarities. Our prime result is showing upper and lower bounds for precisely how the minimax rate for optimizing a given function depends solely on a flatness/noise condition for the function around its minimum."
1182,Towards a learning-theoretic analysis of spike-timing dependent plasticity,"This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli."
1183,"On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution","We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition."
1184,"Halo, Hyperbole, and the Pragmatic Interpretation of Numbers","Numbers are interpreted flexibly in everyday language: imprecision, exaggeration, and hyperbole are everywhere. We propose a computational model of the pragmatic interpretation of numbers, building upon recent models of pragmatics as rational inference. We assume that speaker and listener perform a social inference regarding the intended meaning, precision, and affective subtext of a numerical utterance. This model predicts two pragmatic effects, pragmatic halo and hyperbole, and their interaction. We demonstrate that the model accurately predicts the qualitative effects of human interpretation of number words in five real-world domains."
1186,"Predicting Human Gaze Using Low-, Object- and Social- Saliency: A Dataset and Computational Models","Previous models to predict where people look in natural scenes focused on low-level image features. To bridge the semantic gap between the predictive power of computational saliency models and human behavior, we propose a new saliency architecture that incorporates information at three layers: low-level image features, object-level features, and social-level features. Object- and social-level information is frequently ignored, or only a few sample object categories are discussed where scaling to a large number of object categories is not feasible nor neutrally plausible. To address this problem, this work constructs a principled vocabulary of basic attributes to describe object- and social-level information thus not restricting to a limited number of object categories. We build a new dataset of 700 images with eye tracking data of 15 viewers and annotation data of 5551 segmented objects with fine contours and 12 social attributes (publicly available with the paper). Experimental results demonstrate the importance of the objectand social-level information in the prediction of visual attention."
1187,"Sliding global attractors of learning, memory and innovation by synapse and membrane plasticity","Employing conductance-based models of neural firing and synaptic plasticity, it is shown that the noninvertible, sometimes chaotic, firing process associated with neuronal learning is accompanied by the formation of a single global memory attractor in the composite space of synaptic weights, membrane activation and firing rate. The neuronal global attractors and the corresponding firing modes form six types.  Changes in membrane conductance or activation have a sliding effect on the global attractor, changing its parameter values, but not its dynamic nature. The sliding effect is eliminated by membrane conductance memory, yielding non-spurious, globally-stable retrieval. Selective membrane activation and lateral feedback from interacting neurons creates a shunting effect, yielding exponential capacity of innovation, manifested by combinatorial retrieval of neuronal firing patterns, modulated by the sliding effect. "
1188,Taxonomic Prediction with Tree-Structured Covariances,"Taxonomies are natural structures for representing the relationships between concepts, and are useful sources of prior information to learning algorithms.  The use of taxonomies may give a statistical improvement, in that training data present in nearby classes may be leveraged to effectively increase the sample size of all classes.  Taxonomies may improve performance by serving as a modified regularizer: the taxonomic structure may guide selection from the set of possible prediction functions by indicating that risky sets of functions are those that have very different values for nearby classes.In this work, we explore taxonomic prediction in the structured output setting using joint kernel maps following Cai and Hofmann (2004).  In particular, we relate taxonomic structured prediction to two key concepts, (i) tree structured covariance matrices, and (ii) non-parametric dependence measures.  We show that the joint kernel map for taxonomic prediction is tightly coupled to the concept of a tree-structured covariance matrix, and that Tikhonov regularization results in regularization by a special case of the Hilbert-Schmidt Independence Criterion (HSIC).  Using these concepts, we derive a family of highly computationally efficient algorithms for learning with arbitrary covariance matrices over output classes and evaluate its computational and empirical performance in a number of structured prediction settings."
1189,Detecting Local Manifold Structure for Unsupervised Feature Selection,"Unsupervised feature selection is fundamental in statistical pattern recognition, and has drawn persistent attention in the past several decades. Recently, much work have shown that feature selection can be formulated as nonlinear dimensionality reduction with discrete constraints. This line of research emphasizes the manifold learning techniques, where the Laplacian eigenmap has been extensively studied. In this paper, we propose a new feature selection perspective from locally linear embedding (LLE), which is another popular manifold learning method. Our algorithm, called locally linear selection (LLS), can select the feature subset which optimally represents the underlying data manifold. We further develop a locally linear rotation-selection (LLRS) algorithm which extends LLS to identify the optimal coordinate subset from a new space. Experimental results on five real-world datasets show that our method can be more effective than Laplacian eigenmap based feature selection methods. "
1190,Fitting community models to large sparse networks,"Many algorithms have been proposed for fitting network models with communities but most of them do not scale well to large networks, and often fail on sparse networks.   Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees.   We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs.  We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood.   "
1191,Learning Manifolds with K-Means and K-Flats,"We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-?ats, and analyze their performance. We extend previous results  for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-?ats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-?ats, both the results and the mathematical tools are  new."
1192,Multivariate discrete kernels and representations for sequence data,"String kernel-based machine learning methods have yielded great success in practical tasks of structured/sequential data analysis. They often exhibit state-of-the-art performance on tasks such as document topic elucidation, music genre classification, protein superfamily and fold prediction.However, typical string kernel methods rely on analysis of discrete 1D (univariate) string data (e.g., amino acid sequences, word sequences, codeword sequences, etc).This work introduces new {\em multivariate (2D)}  representations and {\em multivariate (2D) string kernel} methods for data in the form of sequences of feature vectors(as in music MFCC sequences, biological sequence profiles, or image sequences).On three music classification tasks as well as protein sequence classification proposed multivariate (2D) representations and kernels show significant 25-40\% improvements compared to traditional codebook learning and existing state-of-the-art sequence classification methods."
1193,Convex Collective Matrix Factorization,"In many realistic applications, multiple interlinked sources of data are available and they cannot be easily represented in the form of a single matrix. Collective matrix factorization has recently been introduced to improve generalization performances by jointly factorizing multiple relations or matrices. In this paper, we extend the trace norm for matrix factorization to the collective matrix factorization case. This norm defined on the space of relations is used to regularize the empirical loss, leading to a convex formulation of the problem. Similarly to the trace norm on matrices, we show that the collective-matrix completion problem admits afast iterative singular-value thresholding algorithm.The collective trace norm is also characterized as a decomposition norm, usefulto find an optimal solution thanks to an unconstrained minimization procedure. Empirically we show that stochastic gradient descent suits well for solving theconvex collective factorization even for large scale problems. We also show thatthe proposed algorithm directly solving the convex problem is muchfaster than unconstrained gradient minimization optimizing in the space of low-rankmatrices."
1194,KernelUCB for Contextual Bandits,We tackle the problem of online reward maximization over a large but finite set of actions that are described by contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume  that we have cheap access to the similarities between actions' contexts and that the expected reward is a linear function of the contexts' images in the related reproducing kernel Hilbert space. We propose a kernelised UCB algorithm based on the contextual linear bandit algorithm LinUCB and give a cumulative regret bound. We present experiments showing the benefit of a kernelised approach to contextual bandits by comparing KernelUCB with LinUCB on both synthetic and real-world data. 
1195,Non-parametric Bayesian Clustering with Noisy Side Information,"In clustering tasks, the incorporation of side information can usually offer substantial benefits. In many practical applications the side information is extracted following empirical rules and is thus likely contaminated by errors. In this paper we propose a non-parametric Bayesian framework Two-View Clustering (TVClust) to incorporate noisy side information into clustering. We model the data instances and constraints as two independent sets of outcomes, or two views, generated from the latent cluster structure, and try to seek a consensus between the observed data and the noisy side information. Specifically, the data instances are modeled using the Mixture of Dirichlet Process and the side information is modeled as a random graph. For the estimation of model parameters and related posterior inference, we present an efficient Gibbs sampler. Experiments on six real datasets and one social media dataset demonstrate that our method achieves significant improvement over the other methods we compared to."
1196,Iterative ranking from pair-wise comparisons ,"The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR?s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, finding ?scores? for each object (e.g. player?s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efficacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1]."
1197,A Polynomial-time Form of Robust Regression,"Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach compared to standard methods."
1198,Learning Probability Measures with respect to  Optimal Transport Metrics,"We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures."
1199,A Flexible Integer Linear Programming Formulation of Hierarchical Clustering,"In this paper we formulate hierarchical clustering as an integer linear programming (ILP) problem.  We present algorithmic results for our objective, showing that  a special case can be solved exactly in polynomial time (using an linear programming relaxation) and provide approximationschemes for the general case.  We show the flexibility of our approach by removing the transitivity constraint typically required of hierarchies, so we can learn hierarchies that contain overlapping clusterings.  Our experiments showed that our formulation is capable of outperforming standard agglomerative clustering algorithms in a variety of settings, including traditional hierarchical clustering as well as learning overlapping clusterings."
1200,Label Ranking with Partial Abstention based on Thresholded Probabilistic Models,"Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classification, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach."
1201,An Online Learning Algorithm for Multi-valued Function Learning,"In this paper we introduce a learning algorithm based on an infinite mixture of linear experts (IMLE) that is able to properly learn multi-valued functions. It consists of a generative model similar to the one found in Xu et al.~\cite{xu1995ame}, together with a set of priors that improve the algorithm versatility and performance, while providing some regularization of the parameters being learned. It is trained by a generalized Expectation-Maximization algorithm in an online, incremental fashion that can automatically grow the number of active components of the mixture as needed. Contrary to most state-of-the-art function approximation algorithms, IMLE can successfully learn multi-valued functions, and it equals or even outperforms popular online learning algorithms in single-valued prediction tasks."
1202,Fast and Scalable Online CCA for Realtime Impact Analysis of Social Media Data,"The dynamics of temporal dependencies in time series of web graphs can be used to study the influence of single web sources on other web sources. Previous approaches to analysis of temporal dynamics in web graph data were either based on simple and manually tuned heuristics or not designed for online applications with massive amounts of data. Here we propose a simple but efficient and robust online learning approach to canonical correlation analysis. We show that the algorithm converges to the optimal canonical correlations and canonical variates. Using online CCA for canonical trend analysis we can a) assess the impact of a single node on all other nodes, b) explore the temporal dynamics of this impact and c) interpret the features (e.g. in BoW space) that gave rise to an information cascade. We provide preliminary results showing that we can efficiently estimate  canonical trends and thus assess the impact of single users in realtime on large data streams of web data obtained from the social network Twitter. Our results represent a first step beyond simple heuristics and towards an automatized content based realtime impact analysis for large scale data such as social networks activity."
1203,Robust elastic-net nonnegative matrix factorization with box constraints,"In this paper, we propose an elastic-net nonnegative matrix factorization (NMF) with box constraints to remove grouped outliers and recover the inherent nonnegative low-rank structure of the given high dimensional noisy image data. Based on the augmented Lagrangian framework, we solve the linearly constrained minimization reformulation of the elastic-net NMF with the successive overrelaxed outer product iteration (SOOPI). We evaluate the performance of the proposed method for the background modeling of video image sequence and removal of varying illumination and grossly corrupted artifacts in face images. The numerical results show that our proposed elastic-net NMF model does better recover low-rank structure than the state-of-the-art nuclear norm based robust principal component analysis (PCA) and other robust NMF models."
1204,Higher-order decorrelation of receptive fields using support vector machines,"Characterization of neural response properties by means of the receptive field corresponds to estimation of the linear part of a combined linear-nonlinear system. It commonly involves estimation of the stimulus auto-covariance matrix as in the reverse correlation method as a way to remove second-order stimulus correlations that occur in many stimuli of interest, in particular natural stimuli. However, non-Gaussian stimulus distributions and higher-order stimulus correlations in conjunction with nonlinear response properties result in biased estimates of the true receptive field for covariance-based approaches. We show that the problem of receptive field estimation may be reformulated in terms of a binary classification problem, an approach that alleviates the aforementioned problems.  In contrast to regression-like modification of the reverse correlation method, it works on single spikes, and unlike the spike triggered average (STA), it uses spike-eliciting and non-spike-eliciting stimulus portions for receptive field estimation. Using simulations and recordings from inferior colliculus neurons of mongolian gerbils, we show that receptive field estimates obtained using the support vector machine (SVM) classification based receptive field estimator show better decorrelation properties of higher-order stimulus correlations and are more robust against asymmetric stimulus intensity distributions than typical covariance-based approaches. The results obtained for receptive field estimation may imply relevance for general linear-nonlinear systems estimation using large-margin approaches."
1205,Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting,"This paper proposes an efficient on-line learning algorithm to track the smoothing functions of additive models. The key idea is to combine the linear representation of additive models with a recursive least squares filter. In order to quickly track model changes and put more weight on recent data, the recursive least squares filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behavior is further enhanced by using an adaptive forgetting factor which is updated based on the gradient descent method, with the maximum admissible value of the learning rate provided by Lyapunov stability theory. The algorithm is applied to additive models tracking 6 years of electricity demand data from the French utility company EDF (Electricite de France). Compared to state-of-the-art methods, it achieves a superior performance in terms ofprediction accuracy."
1206,Toward adaptive brain computer interfaces,"We consider the question of on-line multinomialclassification in non-stationary environments with non-reliable rewards.We present a policy gradient approach that appear to be effectivein the context of the ``oddball'' classification framework.Then, we present two series of experiments reproducing the conditions of brain computerinterfaces and compare the policy gradient to a more genuine classifier update, and showthe benefit of using the two methods simultaneously for optimal recovery."
1207,Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High Dimensional Results,"Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. This paper develops efficient OMP-like algorithms to deal with precisely this setting. Our algorithms are as efficient as OMP, and improve on the best-known results for missing and noisy data in regression, both in the high-dimensional setting where we seek to recover a sparse vector from only a few measurements, and in the classical low-dimensional setting where we recover an unstructured regressor. In the high-dimensional setting, our support-recovery algorithm {\it requires no knowledge} of even the statistics of the noise. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise."
1208,Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs,"We describe an approach to speed-up inference with latent variable PCFGs, which have been shown to  be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature.  We also describe an error bound for this approximation, which bounds the difference between the probabilities calculated by the algorithm and the true probabilities that the approximated model gives. Empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance."
1209,Semi-supervised Eigenvectors for Locally-biased Learning,"In many applications, one has information, e.g., labels that are provided in a semi-supervised manner, about a specific target region of a large data set, and one wants to perform machine learning and data analysis tasks nearby that pre-specified target region.  Locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools.At root, the reason is that eigenvectors are inherently global quantities.In this paper, we address this issue by providing a methodology to construct semi-supervised eigenvectors of a graph Laplacian, and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning.These semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance, conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner.We also provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning."
1210,Restricting exchangeable nonparametric distributions,"Distributions over exchangeable matrices with infinitely many columns, such as the Indian buffet process, are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution."
1212,Statistical inference in compound functional models,"We consider a general nonparametric regression model called the compoundmodel. It includes, as special cases, sparse additive regression and nonparametric(or linear) regression with many covariates but possibly a small number of relevantcovariates. The compound model is characterized by three main parameters: thestructure parameter describing the macroscopic form of the compound function,the microscopic sparsity parameter indicating the maximal number of relevant covariatesin each component and the usual smoothness parameter corresponding tothe complexity of the members of the compound. We find non-asymptotic minimaxrate of convergence of estimators in such a model as a function of these threeparameters. We also show that this rate can be attained in an adaptive way."
1213,Graph Denoising,"The paper is motivated by real-world applications for denoising graph data. We show that this problem amounts to solving matrix recovery for an adjacency matrix which is both sparse and low-rank under a random perturbation matrix which is also sparse. We formulate the problem as the minimization of a regularized convex objective with an $\ell_1$ loss. We present two methods: an exact method based on Douglas-Rachford splitting, and an approximate method using matrix factorization and rank-one updates which offers better scalability. Numerical experiments confirm the relevance of the approach compared to state-of-the-art methods such as robust PCA."
1214,Exponential Concentration for Mutual Information Estimation with Application to Forests,"We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph."
1215,Minor Surfaces for Clustering and Manifold Learning,"We show that mode-based cluster boundaries exhibit themselves as minor surfaces of the data distribution. Following this observation, we propose a connectivity metric based on the minor surface search between samples. This extends the mode-seeking procedure into a connectivity graph representation, which could be used in many machine learning applications. The use of the graph construction is particularly demonstrated in clustering and manifold learning problems. The experiments are carried out on synthetic and real datasets using Gaussian mixture models and kernel density estimates. Instead of climbing the mode for each sample, we perform connected component analysis in the proposed graph and achieve the same performace as mean-shift algorithm. The minor surface search discards the connections that does not pass through the data cloud, which makes it possible to use in manifold learning problems. We employ a distance matrix masked by our connectivity graph as an input to the Isomap algorithm, and show that the dependence of the performance on the $knn$ and $\epsilon$-ball generalizations decreases with our approach."
1216,A Soft-Label Model with Impact for Active Graph Search,"We consider the problem of active search on a graph where we seek nodes belonging to a certain positive class by iteratively selecting nodes to query for their class label. The problem has similarities with active learning on a graph except that the performance is measured by number of positives identified rather than classification accuracy. Good solutions must tradeoff exploration to better fit a model against exploitation to collect likely positives and thus the problem has similarities with bandit problems as well. However, bandit algorithms are hard to adapt to the  problem since we will never choose the same node more than once.Previous work showed that the optimal active search algorithm requires a look ahead evaluation of expected utility that is exponential in the number of node selections to be made and considered heuristics that do a truncated look ahead [1]. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs.  We test the algorithm empirically on citation and wikipedia graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps."
1217,Augment-and-Conquer Negative Binomial Processes,"By developing augment-and-conquer methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters."
1218,Transferring Expectations in Model-based Reinforcement Learning,"We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without pre-defined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. "
1219,Minimization of Continuous Bethe Approximations: A Positive Variation,"We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions.  While existing message passing algorithms define fixed point iterations corresponding to stationary points of the Bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge. For continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as Gaussians with negative variance. Conversely, our approach leverages multiplier methods with well-understood convergence properties,and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random fields, showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation."
1221,Synergy is the whole minus the union of the parts,"Quantifying cooperation among random variables in predicting a single target random variable is an important problem in biological systems with 10s to 1000s of co-dependent variables.  A common definition of synergy within these system is based on the intuition that synergy is the difference between the whole and the sum of the parts. We introduce *synergistic mutual information* as the sum minus the union of the parts. We compare both measures against a set of pedagogical examples. We conclude that in the presence of redundant information, subtracting the sum underestimates true synergy."
1222,Object Classification with Attributes as Side Information,"This paper proposes a new approach to incorporate attributes as side information for object classification. Attributes are a list of semantically meaningful properties that are available only during training. Instead of predicting these attributes during testing with attribute classifiers, we utilize attributes as side information to improve learning the category classifier on the primary features in a learning with side information paradigm. We propose a novel approach for learning with side information based on the assumption that the posteriors of the label using side information and primary features are close. With the proposed approach, we develop the corresponding learning methods for logistic regress model with L2 regularization. Experiments demonstrate the effectiveness of our approach in classification performance."
1223,Non-linear Metric Learning,"In this paper, we introduce two novel metric learning algorithms, ?2-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: ?2-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear ?2-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of ?2-LMNN, obtain best results in 19 out of 20 learning settings. "
1224,Factorial LDA: Sparse Multi-Dimensional Text Models,"Multi-dimensional latent variable models can capture the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional latent variable model in which a document is influenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientific discipline, and focus (e.g. methods vs. applications.) Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors."
1225,Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
1226,Automating Collusion Detection in Sequential Games,"Collusion is the practice of two parties deliberately cooperating to the detriment of others.  While such behavior may be desirable in certain circumstances, in many it is considered dishonest and unfair.  If agents otherwise hold strictly to the established rules, though, collusion can be challenging to police.  In this paper, we introduce an automatic method for collusion detection in sequential games.  We achieve this through a novel object, called a collusion table, that aims to capture the effects of collusive behavior, i.e., advantage to the colluding parties, without committing to any particular pattern of behavior.  We demonstrate the effectiveness of this method in the domain of poker, a popular game where collusion is prohibited."
1227,A Unified Framework for Probabilistic Component Analysis,"In this paper we attempt to unify many very popular and well-studied componentanalysis algorithms, such as Principal Component Analysis (PCA), LinearDiscriminant Analysis (LDA), Locality Preserving Projections (LPP) and SlowFeature Analysis (SFA) under a single, probabilistic framework. We firstly showthat the projection directions produced by all the above mentioned methods arealso produced by the Maximum Likelihood (ML) solution of a single joint probabilitydensity function (pdf), just by choosing the appropriate prior over the latentspace. Subsequently, we propose novel Expectation Maximization (EM) algorithmsutilising the proposed joint pdf. Experimental results show the usefulnessof the proposed EM framework in both simulated and real world data."
1229,Semisupervised Classifier Evaluation and Recalibration,"How many labeled examples are needed to estimate a classifier's performance on a new dataset? We study the case where data is plentiful, but labels are expensive.  We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier's confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by re-estimating the class-conditional confidence distributions."
1230,Ancestral Sampling for Particle Gibbs,"We present a novel method in the family of particle MCMC methods that we refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the existing PG with backward simulation (PG-BS) procedure, we use backward sampling to (considerably) improve the mixing of the PG kernel. Instead of using separate forward and backward sweeps as in PG-BS, however, we achieve the same effect in a single forward sweep. We apply the PG-AS framework to the challenging class of non-Markovian state-space models. We develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the PG-AS framework. In particular, as we show in a simulation study, PG-AS can yield an order-of-magnitude improved accuracy relative to PG-BS due to its robustness to the truncation error. Several application examples are discussed, including Rao-Blackwellized particle smoothing and inference in degenerate state-space models."
1231,Modelling Reciprocating Relationships,"We present a Bayesian nonparametric model that discovers implicit socialstructure from interaction time-series data.Social groups are often formed implicitly, through actions among members ofgroups.Yet many models of social networks use explicitly declared relationships toinfer social structure.We consider a particular class of Hawkes processes, a doubly stochastic pointprocess, that is able to model reciprocity between groups of individuals.We then extend the Infinite Relational Model by using these reciprocatingHawkes processes to parameterise its edges, making events associated with edgesco-dependent through time.Our model outperforms general, unstructured Hawkes processes as well as structuredPoisson process-based models at predicting verbal and email turn-taking, andmilitary conflicts among nations."
1232,Expectation Propagation in Gaussian Process Dynamical Systems,"Rich and complex time-series data, such as those generated from engineering sys-tems, financial markets, videos or neural recordings are now a common feature ofmodern data analysis. Explaining the phenomena underlying these diverse datasets requires flexible and accurate models. In this paper, we promote Gaussianprocess dynamical systems as a rich model class appropriate for such analysis. Inparticular, we present a message passing algorithm for approximate inference inGPDSs based on expectation propagation. By phrasing inference as a general mes-sage passing problem, we iterate forward-backward smoothing. We obtain moreaccurate posterior distributions over latent structures, resulting in improved pre-dictive performance compared to state-of-the-art GPDS smoothers, which are spe-cial cases of our general iterative message passing algorithm. Hence, we providea unifying approach within which to contextualize message passing in GPDSs."
1233,A quasi-Newton proximal splitting method,"We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification."
1234,Hessian-Free Optimization for Long Short-Term Memory,"The application of $2^{nd}$-order optimization techniques to overcome the inherent limitations of gradient-based learning in Recurrent Neural Networks have proven to be quite succesful. However, comparisons between Hessian-Free optimization and LSTM trained by stochastic gradient descent represent a false equivalence. The many non-linear units and gating cells in LSTM can greatly benefit from $2^{nd}$-order learning methods, without negating the powerful role played by the constant error carousel in providing . In this paper, we have presented HF-optimization for LSTM, in order to more accurately compare performance with standard RNNs which have been similarly trained. On the chosen test set, we find that LSTM outperforms RNNs by solving the task up to a full order of magnitude more quickly. Admittedly, this is a narrow comparison overall, and we intend to run experiments across a number of pathological test cases, as well as real world examples. "
1235,Multi-Task Active Learning for Hierarchical Classification," In this paper, we present a novel combination of Active Learning and Multi-Task Learning for minimizing the training data required for effective Hierarchical Classification. For Multi-Task Learning, we describe a novel hierarchical regularization strategy that utilizes the learnt parameters of a parent category as regularizers for its children categories. For Active Learning, we leverage the multi-task relationships to selectively acquire training data that is effective for improving classification at a category as well as other categories that it influences through the regularization framework. We formulate a stochastic gradient descent solution for Multi-Task learning and an online decision criterion for Active learning to make our approach scalable for large-scale deployment of Active Hierarchical Classification. In spite of being a jointly learnt multi-task model, the approach can be easily adapted to the popular MapReduce, OpenMP and MPI frameworks for large-scale learning through differential message passing amongst categories. Through experiments on well-known hierarchical classification datasets, we demonstrate the superior performance of our approach as compared to learning the hierarchical categories in isolation (single-task setting), especially for categories with limited positive training instances. Our experiments also show significant reduction in the amount of required training data when it is selected with our novel multi-task active learning approach as compared to conventional active learning approaches that select instances for each category in isolation."
1236,Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential ?1-Minimization,"We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where $A_k$ and $e_k$ denote the measurement matrix and noise, respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ \emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity."
1237,Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems ,"We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes.More recently, an asymptotic regret bound of $\tilde{O}(\sqrt{T})$ was shown for $T \gg p$ where $p$ is the dimension of the state space.In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large.We present an adaptive control scheme that for $p \gg 1$ and $T \gg \polylog(p)$ achieves a regret bound of $\tilde{O}(p \sqrt{T})$.In particular, our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$.This is in comparison to previous work on the dense dynamics where the algorithm needs $\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy.We believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks."
1238,Analysis of Algorithms for the Memory Hierarchy,"Batch gradient descent looks at every data point for everystep, which is wasteful for early steps where the current position isnowhere near optimal.There has been a lot of interest in warm-start approaches to gradientdescent techniques, but little analysis. In this paper, we formallyanalyze a method of warm-starting batch gradient descent using smallbatch sizes. We argue that this approach is fundamentally differentthan mini-batch, in that after an initial shuffle, it requires only sequential passes over thedata, improving performance on datasets stored on a disk drive. We also analyze sequential gradient descent."
1239,Supervising Unsupervised Learning: Alleviating label noise in behavioural EEG-BCI experiments,"Behavioural experiments in the neurosciences require the assessment of a given stimulus by a subject. We will study the audio signal quality judgements of subjects and their respective neural correlates as measured by an EEG-BCI.  At decision threshold the subject often guesses, thus, the psychophysical assessment of the stimulus is greatly hampered. So the labels are only partly correct and very often random, which is a problematic scenario when applying supervised learning. We contribute by devising a novel supervised-unsupervised learning scheme, that aims to diferentiate true labels from random ones. This iterated combination of unsupervised one-class outlier detection and semi-supervised one-class learning yields neuroscientifically plausible correlates to behaviour that are more pronounced and meaningful than results found by the commonly used vanilla supervised learning approach that ignores the problematic label noise. While we discuss the experimental evidence for our audio signal quality application, it should be noted that this novelsupervised-unsupervised learning proceedure is applicable also beyond the neurosciences for general psychophysical experiments or generally for high label noise."
1240,Generalized quadratic models and moment-based neural dimensionality reduction,"A popular approach for investigating the neural code is to identify a low-dimensional subspace of stimuli that modulate a neuron's response. Here we describe a set of methods for neural characterization based on Generalized Quadratic Models (GQMs). These models contain a low-rank quadratic form Q that defines the neural subspace, a nonlinear transfer function f, and an exponential-family distribution function P. Special cases include the 2nd order Volterra model (with linear f and Gaussian P), the elliptical-LNP model (with arbitrary f and Poisson P), and the quadratic-logistic regression model (for logistic f and Bernoulli P).  Here we show that for ``canonical form'' GQMs, the first two response-weighted moments yield simplified maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes spike-triggered covariance analysis to analog and binary response data, and provides closed-form estimators under a variety of non-Gaussian stimulus distributions. In the linear-Gaussian case, we show that the corresponding estimator depends only on the first four moments of the raw stimulus distribution.  Finally, the GQM extends generalized linear models (GLMs) to allow multi-dimensional dependence on spike history.  We apply these methods to simulated and real neural data from retina and V1."
1241,A Bias-Variance Analysis of Model-Based Estimation in Reinforcement Learning ,"This paper provides the first bias and variance characterization of the model-based value function estimator in finite-horizon reinforcement learning (RL) problems with discrete state spaces. The closed-formed formulas we derive to estimate the bias and variance rely on an approximation that is exact if the estimates of the transition model, reward model, and value function are normally distributed. These results can be used to characterize performance of RL systems in a wide range of application domains. We are particularly interested in applications concerning resource management domains, and therefore we include experiments demonstrating the use of our estimators to evaluate strategies for population management of animal species. We find the bias/variance estimates produced by our method to be more accurate than those produced by the well-known bootstrap or jackknife estimators. We also compare our results to the bias-variance analysis of Mannor et al. [2007], and show that even in their setting (infinite-horizon, discounted problems), our approach may be preferable."
1242,Online Multi-Task Collaborative Filtering,"Traditional model based approaches for Collaborative Filtering are often based on batch learning algorithms, which assume all labeled data are given a priori before the learning tasks and the models often have to be re-trained when new data arrives in a recommendation task. Such techniques have several critical limitations, e.g., low efficiency and poor scalability for large-scale online applications. Recently, online collaborative filtering (OCF) has emerged as a promising technique to overcome the limitations, which sequentially learns the model over a sequence of data in an online learning fashion. Despite the advantage of high efficiency, the existing approach to OCF using a simple online gradient descent algorithm suffers from slow convergence. In this paper, we propose a new online collaborative filtering framework, that is, online multi-task collaborative filtering (OMTCF), which tackles the online collaborative filtering task by exploiting the idea of online multi-task learning. Unlike the existing OCF approach, OMTCF, by defining a user interaction matrix, effectively updates the models of multiple users simultaneously at each learning iteration, which is able to converge significantly faster. Encouraging results on real-world datasets show that the proposed technique is considerably more effective than the state-of-the-art OCF algorithm."
1243,Multilabel Classification using Bayesian Compressed Sensing,"In this paper, we present a Bayesian framework for multilabel classification using compressed sensing. The key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key benefits of the model are that a) it can naturally handle datasets that have  missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model."
1244,On Computational Feasibility of Mapping Kernels,"The mapping kernel framework has proven useful to design kernels for discrete structures.The resulting kernelsare positive definite, and can be efficiently computed. Recently, a certain class of string kernels,called partitionable kernels, was found to havetight relation to the computational feasibility of mapping kernels.Also, it turns outonly a small portion of the entire partitionable kernelshave been actually used in the literature, and most of them remain unused.In this paper, we shed light on the unexplored area of partitionable kernels, and show interesting and useful propertiesof a certain subclass of partitionable kernels,which is important from both the theoretical and practical points of view."
1245,Scaling Constrained Continuous Markov Random Fields with Consensus Optimization,"We study scaling a class of probabilistic graphical models well-suited to constrained, continuous domains. We show how to solve the most-probable-explanation problem for these models with a consensus-optimization framework. We derive closed-form solutions for consensus-optimization subproblems induced by several types of common dependencies. We improve the performance of consensus optimization by deriving an algorithm that can additionally find closed-form solutions to subproblems in certain cases, depending on the current optimization iterate, not just the subproblem itself. We demonstrate superior performance of our approach over commercial interior-point methods, the current state-of-the-art for the problems we study. In fact, in our evaluation our method scales linearly with the size of the problem."
1246,A Stochastic Gradient Method with an Exponential Convergence ?Rate  with Finite Training Sets,"We propose a new stochastic gradient method for optimizing the sum of? a finite set of smooth functions, where the sum is strongly convex.? While standard stochastic gradient methods? converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence ?rate.  In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard? algorithms, both in terms of optimizing the training error and reducing the test error quickly."
1247,Query Complexity of Derivative-Free Optimization,"Derivative Free Optimization (DFO) is attractive when the objective function's derivatives are not available and evaluations are costly.   Moreover, if the function evaluations are noisy, then approximating gradients by finite differences is difficult.  This paper gives quantitative lower bounds on the performance of DFO with noisy function evaluations, exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients. This challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient.  However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions.  A distinctive feature of the algorithm is that it only uses Boolean-valued function comparisons, rather than evaluations.  This makes the algorithm useful in an even wider range of applications, including optimization based on paired comparisons from human subjects, for example.  Remarkably, we show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same."
1248,Emergence of Object-Selective Features in Unsupervised Feature Learning," Recent work in unsupervised feature learning has focused on the goal  of discovering high-level features from unlabeled images.  Much  progress has been made in this direction, but in most cases it is  still standard to use a large amount of labeled data in order to  construct detectors sensitive to object classes or other complex  patterns in the data.  In this paper, we aim to test the hypothesis  that unsupervised feature learning methods, provided with only  unlabeled data, can learn high-level, invariant features that are  sensitive to commonly-occurring objects.  Though a handful of prior  results suggest that this is possible when each object class  accounts for a large fraction of the data (as in many labeled  datasets), it is unclear whether something similar can be  accomplished when dealing with completely unlabeled data.  A major  obstacle to this test, however, is scale: we cannot expect to  succeed with small datasets or with small numbers of learned  features.  Here, we propose a large-scale feature learning system  that enables us to carry out this experiment, learning 150,000  features from tens of millions of unlabeled images.  Based on two  scalable clustering algorithms (K-means and agglomerative  clustering), we find that our simple system can discover features  sensitive to a commonly occurring object class (human faces) and can  also combine these into detectors invariant to significant global  distortions like large translations and scale."
1249,Annotation on the cheap,"We consider the task of producing a high-quality labeling of a new data set, given access to a human annotator who is to be used sparingly. Our approach involves the active learning of a classifier that is allowed to abstain on difficult inputs."
1250,"Burn-in, bias, and the rationality of anchoring","Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as burn-in. Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks. Our theoretical and empirical results suggest that the anchoring bias is consistent with approximate Bayesian inference."
1251,Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes,"Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning.  In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics."
1252,Stochastic Optimization of the Variational Bound,"We present an algorithm for performing posterior inference by stochastically optimizing a variational lower bound.  Importantly, this algorithm circumvents symbolic evaluation of the variational lower bound and requires only an unnormalized likelihood function $p(x, y)$, making the benefits of variational inference accessible to casual practitioners.  We compare this algorithm with MCMC and demonstrate that it provides a fast, simple alternative.  We also demonstrate that this opens the door to a variety of variational posteriors which were previously unexplored."
1253,A Neural Autoregressive Topic Model,"We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm."
1254,Discovering Naive Representations and Problem Solving on Image Manifolds,"A number of generative algorithms capture the underlying patterns of variability indata, resulting in significant reduction of dimensionality. However, the abstractedpatterns discovered remain at a subsymbolic level, where they are not compactenough to be mapped to signs and become true symbols. In this work, we attemptto construct AI-style symbol systems based on non-linear dimensionalityreduction. For a large class of perceptual input relating to object motions, the embeddingspace itself can be used as a generative model of the problem domain, andwe show that it can be used directly to discover a set of attributes and the valuesthey acquire in different situations. We consider two domains, classical mechanicsand robot motion planning. In each case, we start with sensor data (sequences ofimages), and without any priors, we learn an embedding that captures the relationshipsin the problem domain. we show how various search and planning problemscan be conducted on the embedding space and mapped back into the task space,without invoking the state parameters normally used by models in human science.If needed, such mappings are also readily available from these embeddings"
1255,On the convergence and optimality of optimistic approximate policy iteration,"A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. While the former is often considered to possess practical advantages over the latter, there is, in the interactive case, currently little understanding on its behavior in the proximity of an optimum; after certain amount of guaranteed improvement, the learning process can become trapped in sustained oscillation or chattering, or it can converge to a solution with rather unknown properties. In this paper, we provide insight on the convergence behavior of the general, optimistic form of the greedy methodology by reflecting it against the policy gradient approach. First, we consider an important effect that approximations, either in state estimation or in value function representation, have on policy evaluation, and discuss how this effect defines the natural choice of methodology for policy improvement. Second, we use a recently proposed explanation to the policy oscillation phenomenon and extend it to cover also the optimistic hard-greedy case and the associated policy chattering phenomenon. Third, we show for a substantial subset of soft-greedy approaches that, while having potential for avoiding oscillation and chattering, this subset can never converge in any state to any optimal policy, except for certain pathological cases. We link this failure to an underlying incorrect interpretation of the value function and illustrate it with a minimal artificial example. Finally, we show that as softness and the step size are decreased together toward zero, the general form of Gibbs/Boltzmann soft-greedy optimistic policy iteration using an advantage function becomes, in the limit, equivalent with the natural actor-critic algorithm."
1256,Network floods reveal regulatory control flows and minimal networks in synthetic and bacterial datasets,"Biological networks tend to have high interconnectivity, complex topologies and multiple types of interactions. This renders difficult the identification of sub-networks that are involved in condition-specific responses. In addition, we generally lack scalable methods that can reveal the information flow in gene regulatory and biochemical pathways. Doing so will help us to identify key participants and paths under specific environmental and cellular context. This paper introduces the theory of network flooding, which aims to address the problem of network minimization and regulatory information flow in biological networks. Given a regulatory biological network, and a set of source (input) and sink (output) nodes, our task is to find (a) the minimal sub-network that encodes the regulatory program involving all input and output nodes and (b) the information flow from the source to the sink nodes of the network. To this direction, we describe a novel, scalable, network traversal algorithm, and we demonstrate its ability to achieve significant network size reduction in both synthetic and E. coli networks, without disrupting the core regulatory pathways. "
1258,Efficient Inference and Learning of switching Kalman filters and their Application to Gesture Recognition,"Computational models for high dimensional time series  such as video sequences, spectral trajectories of a speechsignal or the kinematic measurements of skilled human activity hold considerable interest,particularly models that capture the inherent stochastic variability in the signal.  The hidden Markov Model (HMM) is widely used  for modeling such data.  More complex models such as switching linear dynamical systems (S-LDS) account better for the continuity of the observations, which an HMM assumes to be conditionally independent, but they lack efficient learning and inference procedures.  This paper makes three advances to address these limitations\begin{enumerate}\item A previously known inference technique by Barber \cite{barber2006,mesot2007switching} is extended to S-LDS learning.  This extension provides computationally tractable EM-based estimation of S-LDS parameters.\item Under the diagonal assumption on the observation noise, a dynamic programming algorithm is proposed to speed up the per-frame inference-complexity from cubic to linear in the observation dimension.\item A system identification algorithm is provided for initializing the parameters of an S-LDS, leading to effective S-LDS learning.\end{enumerate}The effectiveness of the new algorithms is demonstrated in gesture recognition from kinematic measurements in a robot-assisted minimally invasive surgery (RMIS) task: S-LDS models show significant improvement in recognition accuracy over comparable factor analyzed HMMs.The ability to perform automatic gesture recognition in RMIS has several applications, such as assessing dexterity or manipulative skills during surgical training or providing guidance or assistance during tele-operated surgery."
1259,Learning-driven Exploration in Embodied Action-Perception Loops,"Extracting the structure underlying observed data points is a recurring problem in machine learning. When data can be actively collected in the context of a closed-action perception loop, behavior becomes a fundamental determinant of learning efficiency. Previous machine learning studies in closed action-perception loops, however, have largely focused on the control problem of maximizing acquisition of rewards and often treat the learning of structure as a secondary objective deriving from the search for rewards. Psychology, in contrast, has long argued that learning itself is a primary motivation in both human and animal behavior. Here, we study explorative behavioral control in the absence of external reward structure. Instead, we take the quality of an agent's internal model as the primary objective. In a simple probabilistic framework, we derive an estimate, predicted information gain (PIG), for the amount of information about an (unknown) environment that an agent can expect to receive by taking an action. We develop an explorative strategy through approximate maximization of information gain by backwards propagation of future PIG using a value-iteration algorithm. Across a range of environments, we demonstrate that the proposed behavioral policy learns significantly faster than previous reward-free explorative strategies. Finally, we address the possible evolutionary advantage of reward-free exploration by demonstrating that agents which explore efficiently when rewards are not available, are later better able to accomplish a range of goal-directed tasks."
1260,Anomaly Classification with the Anti-Profile Support Vector Machine,"We introduce the anti-profile Support Vector Machine (apSVM) as a novelalgorithm to address the anomaly classification problem, an extensionof anomaly detection where the goal is to distinguish data samplesfrom a number of anomalous and heterogeneous classes based on theirpattern of deviation from a normal stable class. We showthat under heterogeneity assumptions defined here that the apSVM canbe solved as the dual of a standard SVM with an indirect kernel that measures similarityof anomalous samples through similarity to the stable normalclass. We characterize this indirect kernel as theinner product in a Reproducing Kernel Hilbert Space betweenrepresenters that are projected to the subspace spanned by therepresenters of the normal samples. We show by simulation andapplication to cancer genomics datasets that the anti-profile SVMproduces classifiers that are more accurate and stablethan the standard SVM in the anomaly classification setting."
1261,A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes,"Parametric policy search algorithms are one of the methods of choice for the optimisation of Markov Decision Processes, with Expectation Maximisation and natural gradient ascent being considered the current state of the art in the field. In this article we provide a unifying perspective of these two algorithms by showing that their step-directions in the parameter space are closely related to the search direction of an approximate Newton method. This analysis leads naturally to the consideration of this approximate Newton method as an alternative gradient-based method for Markov Decision Processes. We are able show that the algorithm has numerous desirable properties, absent in the naive application of Newton's method, that make it a viable alternative to either Expectation Maximisation or natural gradient ascent. Empirical results suggest that the algorithm has excellent convergence and robustness properties, performing strongly in comparison to both Expectation Maximisation and natural gradient ascent."
1262,Entangled Monte Carlo,"We propose a novel method for scalable parallelization of SMC algorithms,Entangled Monte Carlo simulation (EMC).  EMC avoids the transmission ofparticles between  nodes, and instead reconstructs them from the particlegenealogy. In particular, we show that we can reduce the communication tothe particle weights for each machine while efficiently maintaining implicitglobal coherence of the parallel simulation. We explain methods toefficiently maintain a genealogy of particles from which any particle can bereconstructed. We demonstrate using examples from Bayesian phylogeneticthat the computational gain from parallelization using EMCsignificantly outweighs the cost of particle reconstruction. The timingexperiments show that reconstruction of particles is indeed much more efficientas compared to transmission of particles."
1263,On the Sample Complexity of Ranking,"Learning to rank is a core machine learning problem. When the truescoring functions are hard to learn or training data is scarce, thesample complexity for predicting a ranking with small error is ofconsiderable interest. We present a lower bound for such a samplecomplexity for any algorithm that estimates a broad class of scoringfunction based on randomly sampled binary comparisons. Additionally,we demonstrate two simple algorithms that achieve the bound inexpectation.  While one algorithm predicts rankings with roughlyuniform quality across the ranking, the other predicts more accuratelynear the top of the ranking than the bottom. Results are presented onsynthetic examples and on an application to epitope (peptide) ranking."
1264,Near-Optimal MAP Inference for Determinantal Point Processes,"  Determinantal point processes (DPPs) have recently been proposed as  computationally efficient probabilistic models of diverse sets for a  variety of applications, including document summarization, image  search, and pose estimation.  Many DPP inference operations,  including normalization and sampling, are tractable; however,  finding the most likely configuration (MAP), which is often required  in practice for decoding, is NP-hard, so we must resort to  approximate inference.  Because DPP probabilities are  log-submodular, greedy algorithms have been used in the past with  some empirical success; however, these methods only give  approximation guarantees in the special case of DPPs with monotone  kernels.  In this paper we propose a new algorithm for approximating  the MAP problem based on continuous techniques for submodular  function maximization.  Our method involves a novel continuous  relaxation of the log-probability function, which, in contrast to  the multilinear extension used for general submodular functions, can  be evaluated and differentiated exactly and efficiently.  We obtain  a practical algorithm with a 1/4-approximation guarantee for a  general class of non-monotone DPPs.  Our algorithm also extends to  MAP inference under complex polytope constraints, making it possible  to combine DPPs with Markov random fields, weighted matchings, and  other models.  We demonstrate that our approach outperforms greedy  methods on both synthetic and real-world data."
1265,SARSA Training of Deep Networks in Complex Games,"We SARSA-lambda trained a program based on deep neural nets to produce a program to compete in the 2011 AI Challenge Sponsored by Google. This was an extremely complicated video game, involving controlling the motions of hundreds of ants in real time as they played in novel game boards of up to 200 by 200 for 1000 steps with imperfect information. This may be the most complex domain ever attempted by reinforcement learning. A number of engineering methods are described that allowed us to finish near the top 10% of 9000 hand-coded human submitted entrants. We are extending our methods to produce a strong player of No Limit Texas Holdem."
1266,Active Batch Selection via Convex Relaxations with Guaranteed Performance Bounds,"Batch mode active learning (BMAL) effectively reduces human annotation effort in training a reliable classifier by selecting batches of promising and exemplar instances from large quantities of unlabeled data. In this paper, we propose two novel BMAL algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP hard optimization problem; we then propose two convex relaxations, one based on linear programming (LP) and the other based on semi-definite programming (SDP) to solve the batch selection problem. Finally, a deterministic performance bound is derived for the first relaxation and a probabilistic bound for the second. Our extensive empirical studies on the UCI datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques and also deliver high quality solutions."
1267,Probabilistic Low-Rank Subspace Clustering,"In this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we first develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in clustering and identifying outliers.  "
1268,How They Vote: Issue-Adjusted Models of Legislative Behavior,"We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues.  Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space."
1269,Density Propagation and Improved Bounds on the Partition Function,"Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds."
1270,Learning to rank for data-driven image segmentation,"We propose to learn a similarity between images to estimate how similar are their segmentation masks. Our similarity is expressed as a dot product in a linear subspace, and we learn the subspace which minimizes the number of triplets for which there is a mismatch between the orderings imposed by our similarity and the true segmentation.By adapting the supervised semantic indexing framework \cite{Bai:09}, we derive a stochastic gradient descent (SGD) algorithm to efficiently learn the similarity. We also explore different strategies to impose large margins. Experiments on two detection scenarios demonstrate that(i) the learned similarity presents an improved ranking-by-segmentation ability; (ii) the fraction of images which are correctly detected just by direct transfer of the nearest-neighbor bounding boxes also increases. This has implications for data-driven segmentation approaches where a set of good neighbors is key to the segmentation algorithm. While existing approaches use appearance-based similarities to select the neighbors, our similarity directly optimizes the selection of good neighbors for segmentation. "
1271,Towards Sparse Coding on Riemannian Manifolds via K-Geodesic Clustering,"In this paper we study the problem of sparse coding and dictionary learning on Riemannian manifolds. The extension of these concepts from Euclidean spaces is not straightforward. We first present a model comprised of geodesic submanifolds to represent manifold valued data. We present an algorithm called K-geodesic clustering to learn the model from the data. Then, we argue that this lends itself naturally to be considered as a dictionary model for sparse coding on manifolds. Then we propose an intrinsic and extrinsic approach for manifold sparse coding. Our experiments on human activity data show that our model fits the data more accurately compared to other classical approaches such as K-means clustering. We demonstrate the discriminatory power of the sparse codes in an activity recognition experiment and obtain accuracies that compare well with recent approaches proposed in the literature."
1272,A Probabilistic Model for Joint Active Learning and Model Selection,"In active learning the goal is to train an accurate model using as few activelylabeled samples as possible. Most active learning methods do not perform modelselection because only one model is trained on the actively labeled samples. Wepresent a framework for active learning where multiple models are trained withdifferent regularization parameters. In this framework the labeled data needed formodel selection from these models is part of the total budget of labeled samples.This framework exposes a natural trade-off between the focused active samplingthat usually is most effective for training models, and the unbiased sampling that isdesirable to reliably estimate model accuracy for model selection. We present analgorithm that adds actively labeled samples to either a training set or to a set usedfor model selection, with the goal being to increase the accuracy of the best modelwith as few total samples as possible. We demonstrate the algorithm on three datasets and show that actively sampling both the train and hold out sets yields moreaccurate models with fewer labels than actively sampling the train sets alone."
1273,Decoding Finger Flexion from Electrocorticographic Signals with Knowledge Based Prior Model,"Decoding by incorporating domain knowledge about the target variable (body movements in BCI) has been shown to be able to significantly improve the performance \cite{WangSJ11}. In the existing model, training a prior model to capture domain knowledge relies on training samples about the target variable. However, in most real BCI applications, brain signals are only collected under thoughts without actual body movements. Even though training sampels for the target variable are available, the model trained on which tends to be biased and has difficulty to generalize. In this paper, we train prior model by explicitly incorporating the domain knowledge without resorting to the training data. The experiment demonstrates its competitive performance."
1274,Stochastic gradient descent  confers resistance to label noise,"This paper explains why machine learning algorithms using thestochastic gradient descent (SGD) algorithm sometimes generalizebetter than algorithms using other optimization techniques.  Weillustrate our point with artificial data sources on which using SGDwith the SVM objective function generalizes much more accurately thanan algorithm which performs more intensive optimization, over a widevariety of choices of the regularization parameters.  We also reporton some similar effects on natural data."
1275,Distributed large-scale natural graph factorization,"More and more natural graphs arise in numerous Web applications, such as socialnetworks, email, and instant messaging. Such large-scalegraphs today easily contain hundreds of millions of nodes and billions of edges,and are expected to grow even larger in the future. While severaltheoretical models have been proposed for such networks, their analysisis still difficult due to their scale and nature. Inthis paper we propose a distributed framework for large-scale graph factorizationthat allows for easier analysis and more compact representation oflarge natural graphs. "
1276,Perceptron Learning of SAT,"Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science.  In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem.  In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space.  Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm.  Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates.  We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT.  Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task."
1277,Learning Networks of Heterogeneous Influence,"Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities. "
1278,Expectation maximization for average reward decentralized POMDPs,"Planning for multiple agents under uncertainty is an important task where proposed solutions are often based on decentralized partially observable Markov decision processes (DEC-POMDPs). In current DEC-POMDP approaches, long-term effects of actions need to be de-emphasized by a discount factor. However, in real-life problems such as wireless networking, the agents (wireless devices) will be evaluated by their average performance over time, hence both short-term and long-term effects of actions are important and solutions based on discounting can perform poorly. We introduce a new DEC-POMDP method that optimizes average reward, based on a modified expectation-maximization approach. The method yields improved performance in benchmark problems compared to a state of the art discounted-reward DEC-POMDP approach."
1279,Statistical Consistency of Finite-dimensional Unregularized Linear Classification,"This manuscript studies statistical properties of linear classifiers obtained through minimization of an unregularized convex risk over a finite sample. Although the results are explicitly finite-dimensional, inputs may be passed through feature maps; in this way, in addition to treating the consistency of logistic regression, this analysis also handles boosting over a finite weak learning class with, for instance, the exponential, logistic, and hinge losses.  In this finite-dimensional setting, it is still possible to fit arbitrary decision boundaries: scaling the complexity of the weak learning class with the sample size leads to the optimal classification risk almost surely."
1280,Bellman Error Based Feature Generation using Random Projections on Sparse Spaces,"We address the problem of automatic generation of features for value function approximation.Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error.  Empirical results demonstrate the strength of this method."
1281,Scalable Matrix-valued Kernel Learning and  High-dimensional Nonlinear Causal Inference,"We propose a general matrix-valued multiple kernel learning framework for high-dimensional nonlinear multivariate regression problems. This framework allows a broad class of mixed norm regularizers, including those that induce sparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel Hilbert Spaces~\cite{MichelliPontil05} induced by a collection of separable kernels. We develop a highly scalable and eigendecomposition-free Block coordinate descent procedure that orchestrates two inexact solvers: a Conjugate Gradient (CG) based Sylvester equation solver for solving vector-valued Regularized Least Squares (RLS) problems, and a specialized Sparse approximate SDP solver~\cite{HazanSDP} for learning output kernels. We show how high-dimensional causal inference tasks can be naturally cast as sparse function estimation problems within our framework, leading to novel nonlinear extensions of Grouped Graphical Granger Causality techniques. The algorithmic developments and extensive empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds."
1282,Multiclass Learning  with Simplex Coding,"In this paper we dicuss a novel  framework for multiclass learning, defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classesa relaxation approach commonly used in binary classification.In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized methods with training/tuning complexity which is {\em independent} to the number of classes.Tools from convex analysis are introduced that can be used beyond the scope of this paper. "
1283,FastEx: Fast Clustering with Exponential Families," Clustering is a key component in data analysis toolbox. Despite its  importance, scalable algorithms often eschew rich statistical models  in favor of simpler descriptions such as $k$-means clustering. In  this paper we present a sampler, capable of estimating  mixtures of exponential families. At its heart lies a novel proposal distribution using random  projections to achieve high throughput in generating proposals, which is crucial  for clustering models with large numbers of clusters. "
1284,A Latent Multi-Frame Image Model,"We consider the general problem of understanding the latent structures of videosequences which encode the motion patterns, spatial-temporal relationships andalso the appearance models of objects. Beyond previous models for video segmentation,we learn an appearance representation of the discovered segments acrossvideos. This is achieved by allowing video segments to share global appearancemodels but possibly having different positions and motion in the different videovolume. We propose an unsupervised framework to automatically infer the hierarchicalmodel of video segments and shared appearance models that also relate toobjects moving in the video. The model is a flexible extension of the HierarchicalDirichlet Process and it can be easily applied to different applications. We showa quantitative evaluation on a recent video segmentation task as well as exemplifyother use cases by demonstrating the application to video indexing. How videoscan be retrieved by a sketch that can be matched to the latent video structure.At time of publication we will release the code of our extendible, probabilisticframework for video analysis and segmentation."
1285,Using Both Supervised and Latent Shared Topics for Multitask Learning,"Since its introduction, Latent Dirichlet Allocation (LDA) has been extended to include two different types ofdocument-level supervision: topic labels and category labels. We introduce a new framework, Doubly SupervisedLatent Dirichlet Allocation (DSLDA), that integrates both types of supervision. We demonstrate thatthis approach is particularly useful for multitask learning, in which both supervised and latent (unsupervised)topics are shared between multiple categories. Experimental results on document classification show thatboth types of supervision improve the performance of DSLDA and that sharing both latent and supervisedtopics allows for better multitask learning."
1286,A tree-decomposed EM algorithm for covariance selection in noisy graphical models,"Gaussian graphical models (GGMs) are widely used in computer science, and have also enjoyed wide applicability in a number of scientific areas. We consider the problem of covariance selection, i.e. estimation of the (inverse) covariance matrix of the joint probability distribution of random variables on a high dimensional graph. To extend the applicability of GGMs, we consider the case where observations for variables are also subject to additional measurement noise. Unfortunately, the the estimation of model parameters in this setting becomes complicated by the fact that the structure of the underlying graph no longer provides direct information about the location of zeros in the inverse covariance matrix. We propose an efficient EM algorithm which uses the tree decomposition of the underlying graph in order to perform the parameter estimation through local operations. We also explore the effect of the treelike structure of the graph on computational performance of the algorithm as well as the accuracy of the estimates by applying it to a wide range of random graph models."
1287,Max-Product Particle Belief Propagation,"Belief Propagation (BP) is a popular message passing algorithm for inference in factored probabilistic graphical models. Sum-Product BP computes marginal distributions for node variables, while Max-Product BP outputs a solution that maximizes the joint distribution of all variables, and is used for MAP (maximum a posteriori) inference.BP is commonly applied to problems that assume a discrete (or discretized) state space. When the state space of a variables cannot be enumerated in practice, and the messages cannot be computed in closed form, methods based on sampling are considered. Several nonparametric Sum-Product approximations exist, however many inference problems of scientific interest require MAP inference for high-dimensional, continuous and multimodal distributed random variables, calling for nonparametric algorithms for Max-Product BP. In this paper we formulate a Max-Product version of the PBP algorithm, and analyze its behavior in performing inference for a model with continuous variables that do not easily admit a discrete representation. "
1288,Topic-Partitioned Multinetwork Embeddings,"We introduce a joint model of network content and context designed forexploratory analysis of email networks via visualization oftopic-specific communication patterns. Our model is an admixture modelfor text and network attributes which uses multinomial distributionsover words as mixture components for explaining text and latentEuclidean positions of actors as mixture components for explainingnetwork attributes.  We validate the appropriateness of our model byachieving state-of-the-art performance on a link prediction task andby achieving semantic coherence equivalent to that of latent Dirichletallocation. We demonstrate the capability of our model fordescriptive, explanatory, and exploratory analysis by investigatingthe inferred topic-specific communication patterns of a new governmentemail dataset, the New Hanover County email corpus."
1289,Learning Label Trees for Probabilistic Modelling of Implicit Feedback,"User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. In the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data."
1290,Learning with Recursive Perceptual Representations,"Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks."
1291,Link Prediction in Graphs with Autoregressive Features,"In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm."
1292,Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images,"We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity.To segment {\em biological} neuron membranes, we use a special type of deep {\em artificial} neural network as a pixel classifier.The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it.The input layer maps each window pixel to a neuron. It is followed by a succession ofconvolutional and max-pooling layers which preserve 2D information and extract features withincreasing levels of abstraction. The output layer produces a calibrated probability for each class.The classifier is trained by plain gradientdescent on a $512 \times 512 \times 30$ stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \emph{rand error}, \emph{warping error} and \emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer."
1293,Changepoint Detection over Graphs with the Spectral Scan Statistic,"We consider the change-point detection problem of deciding, based on noisy measurements, whether an unknown signal over a given graph is constant or is instead piecewise constant over two connected induced subgraphs of relatively low cut size. We analyze the corresponding generalized likelihood ratio (GLR) statistics and relate it to the problem of finding a sparsest cut in a graph. We develop a tractable relaxation of the GLR statistic based on the combinatorial Laplacian of the graph, which we call the spectral scan statistic, and analyze its properties. We show how its performance as a testing procedure depends directly on the spectrum of the graph, and use this  result to explicitly derive its asymptotic properties on few significant graph topologies. Finally, we demonstrate both theoretically and by simulations that the spectral scan statistic can outperform naive testing procedures based on edge thresholding and $\chi^2$ testing. "
1294,Scalable imputation of genetic data with a discrete fragmentation-coagulation process,"We present a Bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a Markov model of  partitions. The partitions at consecutive locations in the genome are related by their clusters first splitting and then merging.  Our model can be thought of as a discrete time analogue of continuous time fragmentation-coagulation processes [Teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable. We apply this model to the problem of genotype imputation, showing improved computational efficiency while maintaining the same accuracies as in [Teh et al 2011]."
1295,An MDL/Bayesian Approach without Assuming either Discrete or Continuous,"In the minimum description length (MDL) and Bayesian criteria, we construct description length  of data $z^n=z_1\cdots z_n$ of length $n$ such that the length divided by $n$ almost converges to its entropy rate as $n\rightarrow \infty$, assuming $z_i$ is in a finite set $A$. In model selection, if we knew the true probability $P$ of $z^n\in A^n$, we would choose a model $F$ such that the posterior probability of $F$ given $z^n$ is maximized. But, in many situations, we use $Q:A^n\rightarrow [0,1]$ such that $\sum_{z^n\in A^n}Q(z^n)\leq 1$ rather than $P$ because only data $z^n$ are available. In this paper, we consider an extension such that each of the attributes in data can be either discrete or continuous. The main issue is what $Q$ is qualified to be an alternative to $P$ in the generalized situations. We propose the condition in terms of the Radon-Nikodym derivative of $P$ with respect to $Q$, and give the procedure of constructing $Q$ in the general setting. As a result, we obtain the MDL/Bayesian criteria in a general sense. Numerical experiments demonstrate that the novel algorithm works efficiently enough to deal with many practical estimations."
1296,Noise Never Helps ? Revisited !,"Compensating changes between a subjects? training and feedback sessions in Brain Computer Interfacing is challenging but of great importance for a robust BCI operation. We contribute by noting that such individual changes can be reliably estimated using data from other subjects. Surprisingly it is the non-discriminative ?noise? signal subspace that can aid to construct features invariant to the change. This is in contrast to e.g. averaging the covariance matrices or construction of a common feature space between users. Notably, the prominent directions of change are very similar between subjects in the noise subspaces, whereas in the most discriminative directions they are not. Our noise harvesting method compares favourably to other state-of-the-art methods on toy data and EEG recordings from five subjects performing motor imagery. We show that not only a significant increase in performance can be achieved, but also that the changes observed in the non-discriminative noise subspace allow for a neurophysiologically meaningful interpretation."
1297,Gradient Weights help Nonparametric Regressors,"In regression problems over $\real^d$, the unknown function $f$ often varies more in some coordinates than in others.We show that weighting each coordinate $i$ with the estimated norm of the $i$th derivative of $f$ is an efficient way to significantly improve the performance of distance-based regressors, e.g. kernel and $k$-NN regressors. We propose a simple estimator of these derivative norms and prove its consistency. Moreover, the proposed estimator is efficiently learned online. "
1298,Monte Carlo Tree Search Using Goal-Directed and Constraint-Biased Action Abstractions,"Monte Carlo Tree Search (MCTS) is a family of methods for planning in large domains. It focuses on finding a good action for a particular state, making its complexity independent of the size of the state space. Effective application of MCTS requires both a good pruning heuristic and a roll-out policy. In this paper we leverage the human tendency to decompose tasks into 1) goal-directed action sequences and 2) sets of constraints to generate stochastic policies that facilitateefficient exploration in MCTS.We demonstrate the efficacy of our approach in the PacMan domain and highlight its advantages over traditional MCTS."
1299,Tree Learning Strategies for Large-Scale Taxonomies,"Standard linear models for multi-class categorization have a decision-time complexity which is linear in the number of categories, whereas approximating them by a tree-based sequence of decisions can reducethe prediction time to the logarithm of the number of categories. In this paper, we review several heuristics to build the best hierarchical taxonomyand propose a novel tree-learning approach by formulating the problem asa sequence of max-cut problems where the categories are split intosubcategories in a top-down fashion. We provide an empirical comparison on five different tree-building approaches on multiple datasets,showing that the previous approaches to learn the tree can significantly failif one is interested in the predictive log-likelihoods or on theaverage classification accuracies of the classifiers. "
1300,Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks,"Many social network datasets consist of a sequence of relational observations through time.  Latent variable models for these networks are popular Bayesian approaches to handle our uncertainty over the unobserved dynamics in the network and how these dynamics influence the observed data.  However, current approaches in this Bayesian modeling framework have not been able to capture the reciprocal influence of past observations on future latent representations.  In this paper, we introduce a new probabilistic model for dynamic social network data based on a semi-hidden Markov model, which assumes that the evolution of latent features are influenced by the local topology of the network in past observations.  We show that the particular form of our model is very interpretable in the context of social networks and describe how it captures a phenomenon which we call latent feature propagation.  We present a Markov-Chain Monte-Carlo inference procedure and experimentally show that our model improves upon current methods on link pediction performance using synthetic and real datasets of social networks."
1301,Behaviorally decoding search targets from gaze fixations,"Using a technique that we refer to as behavioral decoding, we demonstrate that the information available in the fixation behavior of subjects is often sufficient todecode the category of their search target?essentially reading a person?s mind by analyzing what they look at. One group of subjects searched for teddy bear targets among random category distractors, another group searched for butterflies among the same distractors. Two SVM-based classifiers trained to recognize teddy bears and butterflies were then used to classify the distractors that were preferentially fixated by subjects during search as either teddy bears or butterflies, based on their distance from the SVM decision boundary. Two methods of preferential fixationwere explored, the object first fixated during search and the object fixated the longest. Using the longest-fixation method, we found that the target of a person?s search could be decoded perfectly when one of the distractors were rated as being visually similar to the target category. Even with completely random distractors, the target category could still be decoded for 75-80% of the subjects. The much harder task of decoding the target on individual trials (from a single object fixation) resulted in much lower classification rates, although targets were stilldecoded above chance. These findings have implications for the visual similarity relationships underlying search guidance and distractor rejection, and demonstratethe feasibility in using these relationships to decode a person?s task or goal."
1302,Overlapping Decomposition for High-Order Directed Graphical Modeling,"We propose to estimate the dependence structure in high-order directed graph by decomposing it into subgraphswith overlaps. We first introduce a lasso type model to formulate this problem, and further transfer it into estimating a set of group variable selection problems.Specifically, we establish a generic hierarchical lasso method for the estimation, where scalable norms are employed for controlling the structure of subgraphs flexibly. The asymptotic properties of the proposed method are discussed with detailed analysis. We also develop an efficient algorithm to compute such model. Finally, we evaluate our model on both synthetic data and real traffic data."
1303,Memory-based Pipelined Hardware Architecture for Communication-free Neural Computation,"Communication has an important impact on the performance of neural simulation systems. In this paper, we propose a neurocomputing architecture in which synapses are computed in parallel, and communication between neurons is carried out simply by accessing memories. In the proposed architecture, a large set of memories produce a wide stream of data for which large-scale pipelining is can be obtained. We also describe a method for translating functional specifications of computations into fine-grained pipelined circuits. Furthermore, we present the design of a simulator for spiking neural networks (SNNs), in order to show that the proposed architecture can be used to build simulators supporting various neural models. Without using the low activation property of SNNs, the performance of our system is comparable to that of event-driven systems."
1304,Flexible Temporal Structure Learning,"This paper extends the recently introduced structure learning methods for Gaussian random fields and ``nonparanormal'' distributions to a multivariate non-Gaussian time series setting. Our approach is based on discriminative state-space models, and introduces sparsity constraints on dependence structures of multivariate outcomes, as well as other parameters of emission and transition distributions. This combines feature selection with time series modelling, giving rise to explainable representations of data and dependence structures at each latent state. In contrast to the recent literature on sparse graphical models, our approach allows for an easy integration of multi-modality and input variables. We apply our method to multivariate financial time series data. We show that it helps to uncover meaningful dependencies between stock prices and significantly outperforms common approaches for predicting share prices, offering potential for real applications."
1305,Dimensionality reduction for data visualisation using Taylor network,"It is well known that any continuous derivable function can be expanded to a Taylor series and an artificial neural network is used to approximate an unknown function, so a neural network is theoretically equivalent to a polynomial. In this paper we prose a structure of such a polynomial that can be trained quickly as an alternative to  traditional artificial neural network. We test it by applying on to data dimensionality reduction such as Sammon's mapping as well as its new extensions on both synthetic and real world data sets."
1306,Online Sum-Product Computation,"We consider the problem of performing efficient sum-product computations in an online setting over a tree.  A natural application of our methods is to compute the marginal distribution at a vertex in a tree-structured Markov random field.  Belief propagation can be used to solve this problem.  However, belief propagation requires time linear in the size of the tree.  This is too slow in an online setting where we are continuously receiving new data and computing individual marginals.  With our method we aim to update the data and compute marginals in time that is no more than logarithmic in the size of the tree, and is often significantly less.  We accomplish this via a hierarchical covering structure that caches previous local sum-product computations.  Our contribution is three-fold: we i) give a linear time algorithm to find an optimal hierarchical cover of a tree; ii) give a sum-product-like algorithm to efficiently compute marginals with respect to this cover; and iii) apply ``i'' and ``ii'' to find an efficient algorithm with a regret bound for the online {\em allocation} problem in a multi-task setting."
1307,Adaptive Methods for Online Learning with Kernels,"In online convex optimization, adaptive algorithms, which can utilize the second-order information of the lossfunction's (sub)gradient, have shown improvements over standard gradient methods. However, existing adaptive algorithms mainly consider linear models, and are often designed only for classification problems. In this paper,we first provide a general framework that unifies various existing adaptive algorithms. Based on this, new adaptive algorithms can be easily derived.  Next, we show that adaptive learning can be generalized to nonlinear models byusing the kernel trick in a computationally efficient manner. Regret guarantee of the proposed methods are analyzed, and experiments on various benchmark data sets demonstrate their outstanding performance."
1308,Rounding Methods for Discrete Linear Classification,"Learning discrete linear functions, whose weights represent indivisible properties, is a notoriously difficult challenge.In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space,the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set.Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem.Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods.These methods are compared on both synthetic and real-world data."
1309,Sparse Approximate Manifolds for Differential Geometric MCMC,"One of the enduring challenges in Markov chain Monte Carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost. The recent introduction of locally adaptive MCMC methods based on the natural underlying Riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration.In this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid MCMC algorithm that extends the applicability of Riemannian Manifold MCMC methods to statistical models that do not admit an analytically computable metric tensor. Secondly, we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made. We demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust student-t error model, for which the expected Fisher Information is analytically intractable."
1310,Randomized Proximal Point Algorithm for Large Scale Multiple Kernel Learning,"We consider the problem of learning weights to combine kernels to learn a good predictor. We study the computational problem that arises. We propose a randomized version of the proximal point algorithm to avoid a linear dependence on the number of predictors in the convergence rate. We derive finite-time performance bounds for the new algorithm that show that under mild conditions it finds the optimum of our penalized empirical risk criterion in an efficient manner. Experiments with simulated and real data are used to illustrate the new algorithm, which is found to be computationally more efficient with performance competitive to state-of-the-art alternatives."
1311,Decision under time constraints in spiking networks,A spiking network is proposed that implements decision/classification under time- constraints. The network is able to approximate the optimal decision making rule using first-to-fire neurons as well as estimate parameters from external reward signals. The parameter estimation rule is derived from a principled cost function and resembles Hebbian learning in the limit. The proposed architecture and the learned rule exhibit near-optimal tradeoffs of error rate and response time.
1312,Kernel-based Distance Metric Learning in the Output Space,"In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2- or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches."
1313,Learning with Multiple Models,"The standard approach to analyzing data in supervised or unsupervised learning is to assume that a certain specific model generated the data. The goal of the learning process is typically to recover the generating model, or a good approximation thereof. But in many cases, the data are generated by multiple models rather than a single one. In this paper we study the problem of learning when multiple models are considered, generalizing well known schemes such as clustering and multi-subspace approximation. The objective is to learn several models that explain the data best, and the loss for any given data point is the minimal loss among all considered models. We develop an efficient iterative optimization based procedure for the multiple model setup and provide sample complexity bounds."
1314,Fast Variational Inference in the Conjugate Exponential Family ,We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our methodunifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. Weexploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equationshave been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.
1315,Bayesian Pedigree Analysis using Measure Factorization,"Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease.  With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites.  Some pedigrees number in the thousands of individuals.  Meanwhile, analysis methods have remained limited to pedigrees of <100 individuals which limits analyses to many small independent pedigrees.Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited.  This is because linkage anlysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order.  LODs are difficult to interpret and nontrivial to extend to consider interactions among sites.  These developments and difficulties call for the creation of modern methods of pedigree analysis.Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models.   We show that these disease models can be turned into accurate and efficient estimators.  The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models.  This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction."
1316,Variance Inflation in High Dimensional Support Vector Machines,"Many important machine learning models, supervised and unsupervised, are based on simple Euclidean distances or projections in some high dimensional spaces. When estimating such models from small training sets we may face the problem that the span of the training data set input vectors is not the full input space. Hence, when applying the model to future data the model is effectively blind to the missed orthogonal subspace. This can lead to an inflated variance of hidden variables estimated within training set subspace and when applying the model to test data we may find that the hidden variables follow a different probability law with less variance. While the problem and simple means to reconstruct and deflate arewell understood in unsupervised learning, the case of supervised learning is less well understood. We here investigate the effect of variance inflation in supervised learning including the case of Support Vector Machines (SVM) and we propose non-parametric scheme to restore the generalizability. We illustrate the algorithm and its ability to restore performance on a wide range of data sets."
1317,Spatial Coarse-to-Fine Processing in a Neural Model with Cortical Feedback,"Methods utilizing coarse-to-fine processing, in which the general features of a stimulus are processed before more detailed structure, have found success in several applications, such as natural language processing, image processing, and computer vision. Such dynamics have also been observed in several neurological pathways, including the visual system. In this paper, we consider mechanisms of the spatial coarse-to-fine process in the central visual pathway. We present a model of the lateral geniculate  nucleus (LGN) and visual cortex (V1) incorporating both feedforward and feedback connections. We show that cortical feedback has a substantial effect on spatial dynamics in the LGN. Our results suggest that the LGN may use these recurrent connections to ?learn? the coarse-to-fine dynamic during development. We provide an ideal framework within which to explore this process through more computationally intensive simulations."
1318,Hypergraph Complexity from Directed Line Graphs,"In this paper, we aim to characterize hypergraphs in terms of structural complexities. Measuring the complexity of a hypergraph in straightforward way tends to be elusive since hypergraph may exhibit varying relational orders. We thus transform a hypergraph into a line graph which not only accurately reflects the multiple relationships exhibited by the hypergraph but is also easy to be manipulated for complexity analysis. To locate dominant substructure within a line graph, we identify a centroid vertex by computing the minimum variance of its shortest path lengths. A family of centroid expansion subgraphs of the line graph is derived from the centroid vertex in an attempt to capture dominant structural characteristics of a hypergraph. We then compute the complexity traces of a hypergraph by measuring entropies on the centroid expansion subgraphs. The Shannon or von Neumann entropy measured on the condensed subgraph family enables an efficient characterisation of the complexity trace. We perform hypergraph clustering in the principal components space of the complexity trace vectors.Experiments on (hyper)graph datasets abstracted from bioinformatic and image data demonstrate effectiveness and efficiency of the hypergraphs complexity traces."
1319,Accelerated Training for Matrix-norm Regularization: A Boosting Approach,"Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees $\epsilon$ accuracy within $O(1/\epsilon)$ iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle."
1320,Controlled Recognition Bounds for Visual Learning and Exploration,"We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of ?visual search? of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a ?passive? agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an ?omnipotent? agent, capable of infinite control authority, can achieve arbitrarily good performance(asymptotically)."
1321,The Raindrop Process:  Bayesian Nonparametric Latent Shape Models,"In difficult object segmentation tasks, utilizing image information alone is not sufficient; incorporation of object shape prior models has been shown to improve segmentation performance. Most formulations that incorporate both shape and image information are in the form of optimizing energy functionals. This paper introduces a nonparametric Bayesian model for segmenting multiple objects in an image taking both shape and image feature/appearance into account. The generative process starts by generating object locations from a spatial Poisson process, then shape parameters are generated from a shape prior model. This automatically partitions the image: pixels inside are assumed to be generated from an object observation/appearance model and pixels outside from a background model. We learn the model via Markov Chain Monte Carlo sampling and our experiments show that the model is capable of segmenting multiple objects."
1322,Efficient and Optimal Active Learning on Graphs,"We investigate the problem of active label prediction on a given graph. Towards this end, we propose an extremely efficient algorithm S^2 that adaptively selects a sequence of nodes that it would like to see the labels of. We present a theoretical analysis of the performance of S2 that is based on a novel refinement of standard measures of label complexity. We also show that this algorithm is information theoretically optimal in a certain regime and demonstrate its performance on real world data. "
1323,Sparse Probit Factor Analysis for Learning Analytics,"Providing personalized instructions requires a significant amount of effort spent organizing educational material, and analyzing each student's strength and weakness. This, in turn, places an enormous burden on course instructors. Intelligent tutoring systems (ITS) using machine-learning techniques are a novel way to reduce the instructor's efforts. Specifically, ITS consist of two parts, i.e., learning analytics (LA) and scheduling. LA corresponds to the analysis of student response data, whereas scheduling corresponds to the automatic suggestion of learning materials to the student based on the database retrieved through LA. In this work, we propose a statistical approach towards LA based on sparse probit binary factor analysis, and propose two novel algorithms to analyze student response data obtained in a course or test. The first algorithm utilizes convex optimization techniques, whereas the second utilizes a Bayesian latent feature framework. We demonstrate for synthetic and real-world student data that the proposed framework enables us to recover a question--concept association map, as well as a profile of the concept understanding for each student. This proposed framework represents a first step towards an ITS that alleviates the course instructor's workload and  improves the efficacy of student learning."
1324,Distributed Probabilistic Learning for Camera Networks with Missing Data,"Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points.  However, many problems in wide-area surveillance can benefit from distributed modeling, either because of physical or computational constraints.  Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data.  In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing.  In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors.  We demonstrate the utility of this approach on the problem of distributed affine structure from motion.  Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations."
1325,A Statistical Model for Recreational Trails in Aerial Images,"We present a statistical model of aerial images of recreational trails, and a method to infer trail routes in such images. We learn a set of textons describing the images, and use them to divide the image into super-pixels represented by their texton. We then learn, for each texton, the frequency of generating on-trail and off-trail pixels, and the direction of trail through on-trail pixels. From these, we derive an image likelihood function. We combine that with a prior model of trail length and smoothness, yielding a posterior distribution for trails, given an image. We search for good values of this posterior using a novel stochastic variation of Dijkstra?s algorithm. Our experiments on trail images and groundtruth collected in the western continental USA, show substantial improvement over those of the previous best trail-finding method"
1326,On the importance of initialization and momentum in deep learning,"In this work, we show that using carefully crafted (but fairly simple) random initializations, deep autoencoders and recurrent neural networks (RNNs) can be trained effectively using stochastic gradient descent (with suitably small learning rates), and that these results can be significantly improved through the use of aggressive momentum-based acceleration.   For deep autoencoders we show that using any one of a variety of recently proposed random initializations schemes, deep autoencoders can be trained to a level of performance exceeding that reported by Hinton and Salakhutdinov, and with the addition of Nesterov-type momentum, the results can be further improved to surpass those reported by Martens.  For RNNs we give a simple initialization scheme related to the one used for Echo State Networks and successfully train them on various datasets exhibiting pathological long range dependencies (spanning 50-200 timesteps, depending on the problem).Our results suggest that previous attempts to train deep and recurrent neural networks from random initializations failed mostly due to poor choices for such initializations, and that the curvature issues which are present in the training objectives of deep models can be addressed through the use of aggressive momentum-based acceleration, without the need for 2nd-order methods."
1327,Submodular Bregman Divergences with Applications,"We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular Bregman divergences. We  consider two kinds,  defined either from tight modular upper or tight modular lower  bounds of a submodular function. We show that the properties of  these divergences are analogous to the (standard continuous) Bregman  divergence. Further, we demonstrate how they generalize many useful  divergences, including the weighted Hamming distance, squared  weighted Hamming, weighted precision, recall, conditional mutual  information, and a generalized KL-divergence on sets. We also show  that the lower bound submodular Bregman is actually a special case  of the generalized Bregman divergence on the \lovasz{} extension of  a submodular function which we call the \lovasz{} Bregman divergence. We then  point out a number of applications of the submodular Bregman  divergences, and in particular show that a proximal  algorithm defined through the submodular Bregman divergences  provides a framework for many mirror-descent style algorithms related  to submodular function optimization. We also show that a  generalization of the k-means algorithm using the \lovasz{} Bregman divergence is natural in clustering scenarios where  the ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely efficient unlike the other order based distance measures. \extendedv{Finally we provide a    clustering framework for the submodular Bregman, and we derive    fast algorithms for clustering sets of binary vectors    (equivalently sets of sets)."
1328,Probability-One Homotopy Maps for Tracking Constrained Clustering Solutions,"Modern machine learning problems typically have multiple criteria, but there iscurrently no systematic mathematical theory to guide the design of formulationsand exploration of alternatives. Homotopy methods are a promising approach tocharacterize solution spaces by smoothly tracking solutions from one formulation(typically an ?easy? problem) to another (typically a ?hard? problem). We presentnew results in constructing homotopy maps for constrained clustering problems,which combine quadratic loss functions with discrete evaluations of constraintviolations. Our maps help balance requirements of locality in clusters as well asthose of discrete must-link and must-not-link constraints. Our experimental resultsdemonstrate significant advantages in tracking solutions compared to state-of-theartconstrained clustering algorithms."
1329,Risk-sensitive Reinforcement Learning for Applications in Human Decision Making,"This paper proposes a general framework for measuring risk in the context of Markov decision processes by introducing valuation maps, which can model both economically rational and irrational behaviors. Our framework covers most of existing literature in various fields as special cases. The induced risk-preferences are controlled by manipulating the forms of maps. To solve the derived infinite-stage discounted risk-sensitive optimization problems, we provide a dynamic programming algorithm for all maps within our framework and a generalized Q-learning algorithm for a sufficiently rich subfamily, called utility-based shortfall, by which the results in prospect theory can be well replicated. To test its applicability in real data, we apply the algorithm to analyze human behaviors in a sequential investment game. Our method outperforms standard models and the individual risk-preferences revealed by the model are consistent with behavioral data."
1330,Recklessly Approximate Sparse Coding ,"Introduction of the so called K-means features caused significant discussion in the deep learning community. Despite their simplicity, these features have achieved state of the art performance on several benchmark image classification tasks, beating out many more sophisticated learning methods. In this paper we demonstrate that a variant of these features arises as a one-step approximation to non-negative sparse coding with a fixed dictionary. This result connects these features to a broader theoretical framework and provides an explanation for their success."
1331,A Truncated Variational EM Approach for Spike-and-Slab Sparse Coding,"We study the recovery of sparse hidden dimensions based on sparse coding with `spike-and-slab' prior. As standard sparse coding, the used model assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse  prior, we study the application of a more flexible `spike-and-slab' prior which models the absence or presence of a source's contribution independently of its strenghts if it contributs. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: firstly, a novel truncated variational EM approach; and, secondly, a recently suggested approach based on standard factored variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of `spike-and-slab' priors for this domain. Furthermore, we find the truncated variational approach to improve on the standard factored approach in most of these tasks -- which may hint to biases introduced by assuming posterior independence in the factored variational approach. Likewise, we find the truncated variational approach to improve on the factored variational approach in applications to a standard denoising task.  While the performance of the factored approach saturates with increasing number of hidden dimensions, performance of the truncated approach improves. For higher noise levels, the truncated approach finally improves the state-of-the-art on this standard benchmark."
1332,Discovering Sparse Networks In Spiking Data,"  In order to reason about the interacting processes giving rise to a given set of data, we must understand the causal relationships between those latent processes. One way to uncover these relationships is to discover a directed network structure from the data. Often, these data have the form of discrete events --- for example, neural spikes --- which can be modeled effectively via interacting point processes; the Hawkes process is the classical example of such a joint process. Here we provide a fully-Bayesian treatment of the Hawkes process, introducing 1)~convenient conjugate priors for the model parameters and 2)~a sparsity-promoting  spike-and-slab prior on the elements of the interaction matrix.  We demonstrate how to perform efficient inference in this model with Markov chain Monte Carlo.  This enables us to both recover posterior samples of the network structure and infer the characteristics of the underlying temporal dynamics.  We validate our approach on a simulated dataset with known ground truth before examining two real-world data sets --- neural spike train data and financial tick streams.  In each case, we uncover sparse networks with meaningful parameters, suggesting that this method is widely applicable for network discovery."
1333,Querying Discriminative and Representative Samples for Batch Mode Active Learning,"Empirical risk minimization (ERM) provides a principal guideline for many machine learning algorithms. Under the ERM principle, we minimize an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, the training data should be i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where we select the most informative samples to label and these data may come from a distribution different with the source. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. Our active learning method queries the most informative samples which also preseve the source distribution as much as possible, thus identifying most uncertain and representative queries. Experiments on benchmark data sets and real world applications show our method outperforms the existing state-of-the-art batch mode active learning methods."
1334,Divide and prosper --- fault tolerant scalable sketches,"We describe a family of algorithms that can be used to extend  sketches such as the CountMin sketch and SpaceSaving, to settings  where fault tolerance and scalability are crucial. We show how tools  from systems research, namely consistent and proportional hashing  can be used to increase accuracy and throughput linearly in the  number of processors while simultaneously decreasing the failure  probability exponentially. We provide both tight theoretical  guarantees and experimental results that corroborate our findings."
1335,Minimizing Uncertainty in Pipelines,"In this paper, we consider the problem of debugging large pipelines by human labeling. We represent the execution of a pipeline using a directed acyclic graph of AND and OR nodes, where each node represents a data item produced by some operator in the pipeline. We assume that each operator assigns a confidence to each of its output data. We want to reduce the uncertainty in the output by issuing queries to a human expert, where a query consists of checking if a given data item is correct. In this paper, we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty. We perform a detailed evaluation of the complexity of the problem for various classes of graphs. We give efficient algorithms for the problem for trees, and show that, for a general dag, the problem is intractable."
1336,Sequential Inference with Compact Posterior Representations for the Indian Buffet Process,"Infinite latent feature models, such as those based on the Indian Buffet Process (IBP), provide a flexible way to learn latent features underlying observed data, without having to specify their number a priori. Scalable and accurate posterior inference in these models however remains a challenge. Moreover, in many settings, data arrives sequentially and batch methods such as Gibbs sampling are no longer an option. We present a scalable, sequential MCMC inference method for the IBP that can process one observation at a time. As opposed to the standard particle filter for the IBP, our method incorporates the current observation in the proposal distribution and in the computation of the particle weights. This leads to our method achieving better or comparable inference quality as compared to the standard particle filter for the IBP, while requiring far fewer number of particles (therefore yielding compact posterior representations) and being comparable in terms of inference speed.  Moreover, our method also yields competitive accuracies as compared to the state-of-the-art batch methods based on MCMC and variational inference, while being considerably faster."
1337,The Dual Regularization Path of Lasso,"The Lasso regularization path has been extensively studied in the literature. It is now well understood that the path is well defined, unique and continuous piecewise linear under certain conditions. However, when the covariates in the active set (covaraites with greatest absolute correlations with the residual) are dependent, the regularization path is not unique and existing path following algorithms such as LARS may break down. In this paper, we systematically analyze the dual regularization path of Lasso, i.e., the path of the dual optimal solution. In contrast to the primal regularization path, the dual regularization path is well defined, unique and continuous piecewise linear without the independence assumption. We then propose how to compute the sequence of breakpoints of the dual regularization path with or without dependent covariates in the active set. Under the independence assumption, when the sign restriction is violated, the covariate which violates the sign restriction should be dropped and a new direction of the current line segment is updated accordingly. However, when the independence assumption fails, we show it is not necessary to drop the covariate which violates the sign restriction and change the direction of the current line segment, overcoming one of the major issues in deriving the primal regularization Lasso path. We systematically study different possibilities and show how to find the correct solution. Once we get the dual regularization path, we obtain the primal regularization path by the KKT conditions. Our simulation studies validate the correctness of the path generated by the proposed algorithm."
1338,Practical Bayesian Optimization of Machine Learning Algorithms ,"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ?black art? requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm?s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieveexpert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including LatentDirichlet Allocation, Structured SVMs and convolutional neural networks."
1339,Sparse Bayesian unsupervised learning,"This paper is about variable selection, clustering and estimation in an unsupervised high-dimensional setting. Our approach is based on fitting constrained Gaussian mixture models, where we learn the number of clusters $K$ and the set of relevant variables $S$ using a generalized Bayesian posterior with a sparsity inducing prior. We prove a sparsity oracle inequality which shows that this procedure selects the optimal parameters $K$ and $S$. This result is the first of its kind for sparse model-based clustering. Our procedure is implemented using a Metropolis-Hastings algorithm, based on a clustering-oriented greedy proposal, which makes the convergence to the posterior very fast."
1340,Keyword-supervised topic models,"Supervised topic models are able to use document labels to find topics that are predictive of the labels. However, these models only predict labels through indirect topic allocations and do not account for the fact that different words can have different degrees of effect on the label of a document. In this paper, we present the keyword-supervised latent Dirichlet allocation (ksLDA) model as a supervised model that allows different words to have a more direct effect on the model of a document's label and for the effect of the words to be perturbed by their context. In this paper, we show that this new model performs better than supervised latent Dirichlet allocation (sLDA) on real-world classification and regression tasks and on limited-size training sets."
1341,Probabilistic Latent Component Analysis for Inputs with Real-Valued Dimensions,"We present a probabilistic latent component model which operates on inputs whose dimension indices are real-valued.  Unlike traditional probabilistic latent parameter models, that assume discrete-valued dimensions, the model we present generalizes that idea so that such decompositions can be applied on data with real-valued indices, i.e. data that cannot be represented as a matrix or as lying on a regular grid.  We derive a hierarchical Expectation-Maximization learning process for such a model and present its application on decomposing inputs which exhibit such a structure.  We demonstrate the utility of that approach with some results on latent audio component analysis, results that are otherwise unattainable using existing decompositions."
1343,First-Order Models for POMDPs,"Interest in relational and first-order languages for probabilitymodels has grown rapidly in recent years, and with it the possibilityof extending such languages to handle decision processes---both fullyand partially observable.  We examine the problem of extending afirst-order, open-universe language to describe POMDPs and identifynon-trivial representational issues in describing an agent'scapability for observation and action---issues that were avoided inprevious work only by making strong and restrictive assumptions. Wepresent a solution based on ideas from modal logic, and show how tohandle cases like being able to act upon an object thathas been detected through one's observations."
1345,Comparative Locally Linear Classifiers,"A common issue in recent successful locally classifiers is the relatively high computational complexity in testing because nearest neighbor search always be involved for localizing the data points using anchor points. In this paper, we introduce the idea of locally sensitive hashing (LSH) into the linear classifiers to speed up the localization process, and thus propose another large-margin based locally linear classifier. The features generated by the LSH process are called comparative features. We learn the comparative features by transforming the data points into a new feature space and performing threshold on them using the max-operator. The transformation matrix is actually the anchor point matrix, which is learned in an unsupervised manner by enforcing the corresponding coefficients are the elements on a hypercubic structure in the new feature space. The nearest neighbor search forlocalization is approximated by the max-operator. Compared to other local classifiers, the computational complexity of our classifier in testing is almost identical to linear support vector machines, and only 4-line simple MATLAB code is needed for the binary classification. Experimental results demonstrate that during testing our method is not only comparable or even better than other state-of-the-art local classifiers in terms of accuracy but also much faster in testing."
1346,Learnable Pooling Regions for Image Classification,"From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition. Grouping of local features and their codes is part of most recent recognition pipelines and equips these methods with a certain degree of robustness to translations and deformation yet preserving the same spatial layout of the local image features.Despite the predominance of this approach, we have seen little progress to fully adapt  the pooling strategy to the task at hand. This paper proposes a learning method that allows for learning a task dependent pooling scheme -- which includes previously proposed pooling schemes as a particular instantiation of our method.In contrast to previous work we allow different pooling strategies for each code, which shows in particular beneficial for small codes. We propose a batch-based optimization strategy that allows our approach to scale up to sizable dictionary. In this manner we discover new pooling regions that have not been previously used in computer vision."
1347,On the Difficulty of Learning Power Law Graphical Models,"A power-law graph is any graph $G=(V,E)$, whose degree distribution follows a power law \emph{i.e.} the number of vertices in the graph with degree $i, \;y_i$, is proportional to $i^{-\beta}$ : $y_i\propto i^{-\beta}$. In this paper, we provide information-theoretic lower bounds on the sample complexity of learning such power-law graphical models \emph{i.e.} graphical models whose Markov graph obeys the power law. In addition, we briefly revisit some existing state of the art estimators, and explicitly derive their sample complexity for power-law graphs."
1348,The Time-Marginal Coalescent Prior for Hierarchical Clustering,"We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clus-tering. The prior is constructed by marginalizing out the time information ofKingman?s coalescent, providing a prior over tree structures which we call theTime-Marginalized Coalescent (TMC). This allows for models which factorizethe tree structure and times, providing two benefits: more flexible priors may beconstructed and more efficient Gibbs type inference can be used. We demonstratethis on an example model and show we get competitive experimental results."
1349,Detecting Activations over Graphs using Spanning Tree Wavelet Bases,"We consider the detection activations over graphs under Gaussian noise, where signals are supposed to be peice-wise constant over the graph.Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with proveable theoretical guarantees for general graph topologies.We first provide a universal necessary condition for asymptotic distinguishability of the null and alternative hypotheses.We introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph.We prove that for any spanning tree, we can hope to correctly detect signals in a low signal-to-noise regime using spanning tree wavelets.We propose a randomized test, in which we use a uniform spanning tree in the basis construction.Using electrical network theory, we show that the uniform spanning tree provides strong theoretical guarantees for arbitrary graphs that in many cases match our necessary condition.We prove that for edge transitive graphs, $k$-nearest neighbor graphs, and $\epsilon$-graphs we obtain nearly optimal performance with the uniform spanning tree wavelet detector."
1351,Fusion with Diffusion for Robust Visual Tracking,"A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. The edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations.In this paper, a novel approach to integrate multiple similarity measures is pro-posed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the dif-fusion on the TPG is the same as the diffusion process on each of the original graphs, Moreover, it is not necessary to explicitly construct the TPG in our frame-work. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods."
1352,Top-down particle filtering for Bayesian decision trees,"Decision trees are a fundamental tool in machine learning and statistics, and Bayesian variants, which introduce a prior distribution on the decision tree itself, have demonstrated the utility of full posterior inference.  Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, existing Bayesian decision tree algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection) iteratively through local Monte Carlo modifications to the structure of the tree.  We present a Sequential Monte Carlo (SMC) algorithm that works in a top-down manner, mimicking the behavior and speed of classic algorithms. Through empirical comparisons with existing methods, we demonstrate the potential of this new approach and conclude that it represents a better computation-accuracy tradeoff."
1353,Automated Gaussian process modelling for many instances of small data sets,"We present techniques for automated Gaussian process (GP) modelling with many small data sets. These problems are common when applying GP models independently to each gene in a gene expression time series data set. Such sets typically contain very few time points. Naive application of common GP modelling techniques can lead to severe over-fitting or under-fitting in a significant fraction ofthe fitted models, depending on the details of the data set. We propose avoiding over-fitting by constraining the GP length-scale to values that focus most of the energy spectrum to frequencies above the Nyquist frequency corresponding to the sampling frequency in the data set. Under-fitting can be avoided by more informative priors on observation noise. Combining these methods allows applying GP methods reliably automatically to large numbers of independent instances of short time series. This is illustrated with experiments with both synthetic data and real gene expression data."
1354,A nonparametric variable clustering model,"Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a clustering, of observed variables so that variables in a cluster are highly correlated. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. "
1355,Robust Saliency Estimation using Visual Structures,"Emulation of human perception system can usually inspire computer vision and help understand the natural scenes that are usually recorded as raster images or videos. Saliency detection is one of such procedures that can extract useful cues about the key visual information in an observed scene. In this paper, we provide a robust saliency estimation method based on a geodesic analysis of visual structures in an image. In the proposed approach, manifold learning is first applied to compare local spatial variation against global contrast, and obtain a locally smoothed image projection. Geodesic analysis is then applied to find the plateaus in the hierarchical image structure that corresponds to the geodesic saliency map. With the estimated saliency map, visual structures are hierarchically cut out by simple adaptive thresholding. The experiment validated that our algorithm consistently outperformed a number of the state-of-art methods, yielding higher precision and better recall rates, when evaluated using one of the largest publicly available data sets. Further experiments on saliency cut also testified that the proposed method provide an efficient way for unsupervised object cut, which may have extensive application in image/video editing, object recognition and robotic vision."
1356,Nonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction,"We show how to incorporate information from labeled examples into nonnegative matrix factorization (NMF). In addition to mapping the data into a space of lower dimensionality, our approach aims to preserve the nonnegative components of the data that are important for classification. We identify these components from the support vectors of large-margin classifiers and derive iterative updates to preserve them in a semi-supervised version of NMF. These updates have a simple multiplicative form like their unsupervised counterparts. We evaluate these updates for dimensionality reduction when they are used as a precursor to linear classification.  We find that they yield much better performance than their unsupervised counterparts. We also find one unexpected benefit of the low dimensional representations discovered by our approach: often they yield more accurate classifiers than both ordinary and transductive SVMs trained in the original input space."
1357,Priors for Diversity in Generative Latent Variable Models,"Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization, for building density models of data, and for  providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions  on internal parameters.  For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the  underlying i.i.d.\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions, we are able to define a DPP on probability measures.  We show how to perform MAP inference  with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model."
1358,A Quantum Algorithm for PCA,"In this paper, we present a ?quantization? of classical dimensionality reduction techniques. ?Quantization? refers to the use of a quantum subroutine, or classical computer with access to a quantum computer, to enhance a classical algorithm. Here, we quantize a probabilistic Principal Component Analysis (PCA) algorithm by using a quantum subroutine based on a seminal quantum algorithm for numerical gradient estimation due to Jordan. We show that our quantized PCA algorithm requires only O(kn) queries to a black box on a quantum computer, while the classical approach requires O(knd) queries to a black box on a classical computer, where k is the lower dimension, n is the number of points, and d is the dimension. A significant result of our work is that it is independent of the numberof high dimensions d, and d is typically extremely large (the dominant factor)."
1359,Convex Subspace Representation Learning  for Multi-view Clustering,"Learning from multi-view data is important in many applications. In this paper, we propose a novel convex subspace representation learning method for unsupervised multi-view clustering. We first formulate the subspace learning in multiple views as one joint optimization problem with a common subspace representation matrix and a group sparsity inducing norm regularizer. By exploiting a dual matrix norm, we then show a convex min-max dual formulation with a sparsity inducing trace norm regularizer can be obtained. We develop a proximal bundle optimization algorithm to solve the min-max optimization problem for a global solution. Our empirical study shows the proposed approach can outperform the state of the art alternative methods across different multi-view learning scenarios."
1360,Different aproaches to feature selection in MDPs,"In problems modeled as Markov Decision Processes (MDP), knowledge transfer is related to the notion of generalization and state abstraction. Abstraction can be obtained through factored representation by describing states with a set of features. Thus, the definition of the best action to be taken in a state can be easily transferred to similar states, i.e., states with similar features. In this paper we present two approaches to find an appropriate compact set of features for such abstraction, thus facilitating the transfer of knowledge to new problems. We also present heuristic versions of both approaches and compare all of the approaches within a discrete simulated navigation problem. "
1361,24 Parallel Codes for Sparse PCA,"Given a multivariate data set, sparse principal component analysis aims to extract several linear combinations of the variables which together explain the variance in the data as much as possible, while controlling the number of nonzero loadings in these combinations. In this paper we consider 8 different optimization formulations for computing a single sparse loading vector; these are obtained by combining the following factors: we employ two norms for measuring  variance (L2, L1) and two sparsity-inducing norms (L0, L1), which are used in two different ways (constraint, penalty). Three of our formulations, notably the one with L0 constraint and L1 variance, have not been considered in the literature. We give a unifying reformulation which we propose to solve  via a natural alternating maximization method. Besides this, we provide one serial (single-core) and three parallel (multi-core, GPU, cluster) codes for each of the 8 problems. Parallelism in the methods is aimed at i) speeding up computations (our GPU code can be 100 times faster than an efficient serial code written in C), ii) obtaining solutions explaining more variance and iii) dealing with large-scale problems (our cluster code is able to solve a 357 GB problem in about a minute)."
1362,A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function,"We propose a novel Bayesian approach to solve stochastic optimization problems that involve ?nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ?rst, doing inference over the function space and second, ?nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf?cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function."
1363,Model Class Priors in Reinforcement Learning,"Many reinforcement learning problems have such a structure that estimation of the optimal policy may be significantly simpler than in the general case. While Bayesian approaches would naturally be suited to discovering and exploiting such a structure, previous work has concentrated priors which could model arbitrary environments. This paper proposes instead a hierarchical model where beliefs over different classes of priors are maintained. The model is fundamentally different from classical hierarchical Bayesian approaches, since the evidence depends on the policy followed so far. This obstacle can be removed by an appropriate decomposition of the posterior. Finally, we derive a number of online decision making algorithms, in conjunction with the class prior distribution, which maintain a distribution of value functions. Their performance is examined in a number of reinforcement learning problems and we show that it is at least as good as the performance of the model that knows the correct model class a priori."
1364,Bias-corrected Q-learning to Control Max-operator Bias in Q-learning,We identify a class of stochastic control problems with random rewards which induce high levels of statistical error in the estimate of Q-factors.  This produces significant levels of max-operator bias in Q-learning algorithms which can induce the algorithm to diverge for millions of iterations.  We present a bias-corrected Q-learning with asymptotically unbiased resistance against max-operator bias and show that the algorithm asymptotically converges to the optimal policy as Q-learning does.  We demonstrate that bias-corrected Q-learning shows practical resistance against max-operator bias and performs well in select problems with highly random rewards where Q-learning and other provably convergent algorithms rooted on Q-learning suffer max-operator bias.
1365,Modeling Laminar Recordings from Visual Cortex with Semi-Restricted Boltzmann Machines,"The proliferation of high density recording techniques presents us with new challenges for characterizing the statistics of neural activity over populations of many neurons. The Ising model, which is the maximum entropy model for pairwise correlations, has been used to model the instantaneous state of a population of neurons.  This model suffers from two major limitations: 1) Estimation for large models becomes computationally intractable, and 2) it cannot capture higher-order dependencies.  We propose applying a more general maximum entropy model, the semi-restricted Boltzmann machine (sRBM), which extends the Ising model to capture higher order dependencies using hidden units. Estimation of large models is made practical using minimum probability flow, a recently developed parameter estimation method for energy-based models. The partition functions of the models are estimated using annealed importance sampling, which allows for comparing models in terms of likelihood.  Applied to 32-channel polytrode data recorded from cat visual cortex, these higher order models significantly outperform Ising models. In addition, extending the model to spatiotemporal sequences of states allows us to predict spiking based on network history. Our results highlight the importance of modeling higher order interactions across space and time to characterize activity in cortical networks."
1366,Structure inference in cryo-electron microscopy using Gaussian mixture models,"The reconstruction problem in cryo-electron microscopy is to infer an unknown 3D electron density from a set of its 2D projections, where the projection directions are also unknown. Most existing algorithms need to be initialized with a 3D density, and therefore produce biased results. Futhermore, they typically use non-probabilistic approaches that depend on many user-specified hyperparameters. In this paper we introduce a new reconstruction algorithm that places the entire problem in a probabilistic framework with a very small number of hyperparameters. Our algorithm does not require an initial 3D model. Most importantly, it makes use of a novel representation of 3D densities using Gaussian mixture models. Model parameters are estimated using an expectation maximization type algorithm. The algorithm is applied to synthetic and real datasets."
1367,Shortest stochastic path with risk sensitive evaluation,"In an environment of uncertainty where decisions must be taken, how to make adecision considering the risk? The shortest stochastic path (SSP) problem modelsthe problem of reaching a goal with the least cost. However under uncertainty, abest decision may: minimize expected cost, minimize variance, minimize worstcase, maximize best case, etc. Markov Decision Processes (MDPs) defines optimaldecision in the shortest stochastic path problem as the decision that minimizesexpected cost, however MDPs does not care about the risk. An extension of MDPwhich has few works in Artificial Intelligence literature is Risk Sensitive MDP.RSMDPs considers the risk and integrates expected cost, variance, worst case andbest case in a simply way. We show theoretically the differences between MDPsand RSMDPs for modeling the SSP problem and show the results of each modelin an artificial scenario."
1369,The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification," Linear models are often much faster to learn and test than non-linear models. To enable non-linear (with  respect to the original feature space) learning with efficient linear learning algorithms, explicit  embeddings that approximate popular kernels have recently been proposed. However, the kernels are usually  designed so that their dot product in the high dimensional space is efficient, while we want the embedding  itself to be efficient (and rich enough). We propose a simple and effective pairwise piecewise-linear  embedding to approximate models under a factorization-like assumption. The method is based on discretization  and interpolation of individual features values and feature pairs.  The discretization allows us to model  different regimes of the feature space separately, while the interpolation preseves the original continuous  values.  Pairs allows us to approximate cross-feature relationships. Using this embedding within an SVM  strictly generalizes linear SVM. Additionally, some cross-features relations such as feature similarity can  be modeled exactly, while other cross-feature relations are approximated.  We conducted an extensive  experimental study and show results for a large number of datasets. We compared our method to linear,  polynomial, $\chi^2$-like and RBF kernels and embeddings. Our method consistently achieves good performance  significantly outperforming all other methods, including the RBF kernel on the majority of the  datasets. This is in contrast to other proposed embeddings that were faster than kernel methods, but with  lower accuracy. Additionally, our method is as efficient as the second polynomial explicit feature map. The  code will be made available if the paper is accepted."
1370,Calibration in Cost-sensitive Multiclass Classification: an Application to Reinforcement Learning,"In this paper we propose a computationally efficient version of classification based policy iteration. The key idea of these algorithms is to view the problem of coming up with the next policy in policy iteration as a classification problem, where a policy is viewed as a classifier.The main novelty is that we propose to replace the non-convex optimization problem of earlier algorithms with a convex one, where a new cost-sensitive surrogate loss is optimized in each iteration.The new loss is shown to be classification calibrated, which makes it a ``sound'' surrogate loss. As far as we know, this is the first calibration result in the context of multiclass classification. As a result, we are able to extend  theoretical guarantees that existed for the previous inefficient classification-based policy algorithms to our efficient method, thereby giving the first computationally efficient, theoretically sound version of classification-based policy iteration."
1372,Adaptive Radial Filtering for Multi-Oriented Character Recognition,"The recognition of fully multi-oriented handwritten characters is a difficult problem. Contrary to univarite signals where the shift invariance property in the Fourier transform can be used, multivariate signals like images require special care for extracting rotation invariant features. The proposed method considers first input features obtained by the Polar transform. A convolutional neural network is then used for extracting features of higher level. This classifier includes in addition the Fast Fourier Transform for extracting shift invariant features at the neural level. The convolutional layers process the image at the pixel level while the Fourier transform and the upper layers of the neural networks process rotation invariant features. The average recognition rate for multi-oriented characters is 91.68\% for the Latin digits."
1373,Near-Tight Bounds for Cross-Validation via Loss Stability,Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds.  In this work we introduce a new and weak measure of stability (\emph{loss stability}) and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal.  Our work thus quantitatively improves the currentbest bounds on cross-validation.
1374,A new perspective on convex relaxations of sparse SVM,This paper proposes a convex relaxation of a sparse support vector machine (SVM) based on the perspective relaxation of mixed-integer nonlinear programs. We seek to minimize the zero-norm of the hyperplane normal vector with a standard SVM hinge-loss penalty and extend our approach to a zero-one loss penalty. The relaxation that we propose is a second-order cone formulation that can be efficiently solved by standard conic optimization solvers. We compare the optimization properties and classification  performance of the second-order cone formulation with previous sparse SVM formulations suggested in the literature.
1375,Bayesian Inference Reveals Synapse-Specific Short-Term Plasticity in Neocortical Microcircuits,"Short-term synaptic plasticity is highly diverse and varies according to brain area, cortical layer, and developmental stage. Since this form of plasticity shapes neural dynamics, its diversity suggests a specific and essential role in neural information processing. Therefore, a correct identification of short-term plasticity is an important step towards understanding and modeling neural systems. Although accurate phenomenological models have been developed, they are usually fitted to experimental data using least-mean square methods. We demonstrate that, for typical synaptic dynamics, such fitting gives unreliable results. Instead, we introduce a Bayesian approach based on a Markov Chain Monte Carlo method, which provides the full posterior distribution over the parameters of the model. We test the approach on simulated data over different regimes and show that common short-term plasticity protocols yield broad distributions over some of the parameters. Finally, we infer the model parameters using experimental data from three different neocortical excitatory connection types, revealing novel synapse-specific distributions and synaptic transfer functions, while the approach yields more robust clustering results. We conclude that ? because short-term plasticity presumably provides key computational features ? our approach to demarcate synapse-specific synaptic dynamics is an important improvement on the state of the art."
1377,Kuhn meets Rosenblatt: Combinatorial Algorithms for Online Structured Prediction,"Online algorithms have been successful at a variety of prediction tasks.  In structured prediction settings, the model produced by an online learner is fed as input to some combinatorial algorithm for producing structured outputs.  This combinatorial algorithm is predominantly considered a black box, which severely limits the control available to the learner.  In this paper, we break open this black box.  For each example, it aims to change its model minimally subject to a margin-based optimality condition on the output.  We define a flexible linear framework that exploits the combinatorial properties of the desired structured output to achieve this in a convex optimization framework. We demonstrate the efficacy of this framework in two applications: dependency parsing via maximum spanning trees and word alignment via bipartite matching."
1378,"Causality Analysis in Time Series: Foundations, Consistency and New Development","Granger causality is the primary technique of causality analysis for time series data. While it has been well studied in the literature for two time series, its performance for multivariate time series in the presence of hidden variables as well as its connections to true causality has always been debated. In this work, we reexamine the theoretical foundations of Granger causality and strive to provide insights into three fundamental questions on Granger causality for time series data. Specifically, we resort to the Structural Equation Modeling (SEM),  a widely accepted framework for true causality analysis, and statistical consistency analysis, which reveals consistency behavior of statistical methods to uncover Granger causality, in order to answer the following questions: (1) what are the advantages of Granger causality in avoiding spurious causation;  (2) what are the consistency properties of two popular approaches to uncover Granger causality, including significance test and L1-penalized regression; (3) what are the consistency properties of advanced algorithms for nonlinear dependencies, i.e.,  semi-parametric approach, to uncover Granger causality for high-dimensional time series. Experiment results on synthetic datasets and social media application data are shown to support our theoretical analysis."
1379,Convergence Rate Analysis of MAP Coordinate Minimization Algorithms,"Finding maximum aposteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used.Solving these relaxations efficiently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However,these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence.Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima."
1380,Projection Retrieval for Classification,"In many applications classification systems often require in the loop human intervention. In such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution. To tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users."
1381,Dual-view Dirichlet Process Mixture Models for Cross-modal Data Analysis,"We propose Dual-view Dirichlet Process Mixture Models for analyzing cross-modal data. This model is a Bayesian nonparametric model incorporating a prior of infinite mixture distribution of data in any single modality, and it also captures the correspondences between mixture components from different modalities. We develop an efficient variational inference algorithm for learning the joint distribution of cross-modal data which can contribute to identifying latent structures. For prediction tasks, we provide fast approximated methods based on a latent subspace derived from this generative model and kernel regression. Comparisons of experimental results to other state of the art models on benchmark datasets demonstrate the superiority of our model in significantly improving performances on cross-modal information retrieval and image annotation."
1382,A nonparametric Bayesian approach to learning directed acylic graph structures,"The learning of graph structure is an important problem in machine learning. To this effect, we present a new stochastic process defining a probability distribution on infinite directed acyclic graph structures. This distribution can be used as a nonparametric Bayesian prior on the structure of graphical models having an unbounded number of hidden random variables. The proposed stochastic process is an extension of the cascading Indian buffet process that removes the limitation of purely layered structures. We evaluate the performance of both approaches in discovering the structure of belief networks and compare the structure complexity of the posterior distribution, showing that our approach can extract graphs with fewer units without scarifying predictive precision."
1383,Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
1384,Mixing-time Regularized Policy Gradient,"Policy gradient reinforcement learning (PGRL) methods have received substantial attention as a mean for seeking stochastic policies that maximize a cumulative reward. However, PRRL methods can often take a huge number of learning steps before it finds a reasonable stochastic policy. This learning speed depends on the mixing time of the Markov chains that are given by the policies that PGRL explores. In this paper, we give a new PGRL approach that regularizes the rule of updating the policy with the hitting time that bounds the mixing time.  In particular, hitting-time regressions based on temporal-difference learning are proposed. This will keep the Markov chain compact and can improve the learning efficiency. Numerical experiments show the proposed method outperforms the conventional PG methods."
1385,Robust Crowd Labeling using Little Expertise,"Crowd Labeling emerged from the need to label large-scale and complex data, atedious, expensive and time-consuming task. Each object to label is generally annotatedby multiple crowd labelers, and the collected labels are combined to inferone final estimated label. One open problem is the quality and integration of differentlabels, especially when the labelers participating to the task are of unknownexpertise. In order to address this challenge, we propose a new framework thatautomatically combines and boosts bulk crowd labels with a limited number ofground truth labels from experts. We show through extensive experiments that,unlike other state-of-the-art approaches, our method is robust to estimate true labelseven with the presence of a large proportion of not-so-good labelers in thecrowd."
1386,Bounded Gaussian Process Regression,"We extend the Gaussian process (GP) framework for regression by introducing two bounded likelihood functions and by considering a specific choice of link function in the previously suggested warped GP. In contrast to warped GP, the extension allows for an explicit specification of the noise distribution within the observational space. We approximate the intractable posterior distributions by the Laplace approximation and expectation propagation and show the properties of the models on a artificial example. We finally consider two real-world datasets originating from perceptual rating experiments which indicate a significant gain obtained with the proposed explicit noise model extension."
1387,Buy-in-Bulk Active Learning,"In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time.This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch.In this work, we study the label complexity of active learning algorithms thatrequest labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once.  In particluar, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increasethe total number of labels requested, it reduces the total cost requiredfor learning."
1388,Multi-Task Approximate Bayes-Optimal Dual Adaptive Control,"Optimally controlling a system with unknown properties requires trading off the dual requirements of exploration by generating probe signals and exploiting any obtained knowledge to minimize costs. Bayes-optimal approaches to dual adaptive control make this trade-off optimally, but in general have no closed form solution. Furthermore, they require an informative prior distribution to be maximally efficient. An agent with experience in multiple related tasks can exploit its knowledge about the distribution the tasks are drawn from to improve the quality of its control actions. In this paper, we propose extending an approximate scheme for Bayes-optimal dual-adaptive control to a multi-task setting. Evaluations of ourapproach on an object-pushing task show the approximate dual-adaptive controller is able to take advantage of its experience with similar tasks."
1389,Forward Model Extraction from Neural Population Activity,"Internal forward models are believed to explain the nervous system's ability to compensate for sensory feedback delays and adapt to changes in effector dynamics.  Single-neuron and behavioral studies have provided evidence of forward models, but to our knowledge it has not yet been possible to extract a full forward model of the effector directly from neural activity.  Here, we develop a novel probabilistic framework for forward model extraction that integrates neural commands with sensory feedback.  Using this framework, we can i) extract the subject's forward model, which is represented as parameters in the probabilistic model, and ii) infer the subject's timestep-by-timestep internal estimates of the motor effector position, which are latent variables in the model.  We leverage brain-computer interface (BCI) infrastructure, in which all neural commands driving the effector and sensory feedback are fully observed. We applied this framework to neural population activity recorded in macaque motor cortex during BCI control of a computer cursor to acquire visual targets. We found that recorded neural commands were more consistent with aiming straight toward targets from the subject's internal estimates of cursor position, as inferred by our probabilistic framework, than from the cursor positions displayed during online control. The extracted forward models explain about 75% of the subject's aiming errors. We believe that the probabilistic framework developed provides a critical link between sensory feedback and motor commands, and will likely facilitate the study of feedback motor control and motor learning."
1390,Novelty Detection in Multi-Instance Multi-Label Learning,"Novelty detection plays an important role in machine learning and signal processing. However, this problem has not been previously studied in multi-instance multi-label learning (MIML). In MIML, the training dataset consists of bags of instances associated with sets of labels. It is a common assumption that every instance in a bag is associated with one of the labels in the set. In this paper, we focus on the scenario where a bag may contain novel class instances, which are not associated with the bag-level label set. The goal is to determine for any given instance in a new bag whether it belongs to a known class or not. Detecting novelty in the MIML setting captures manyreal-world phenomena and has many potential applications. For example, in a collection of tagged images not all objects in a given image are represented in the image tag set. Discovering an object which has not been taggedbefore can be useful for the purpose of soliciting a label for the new object. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed methods, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in unseen bags."
1392,Online Learning of Hierarchical Balancing Strategies for Bipedal Humanoid Robots,"  Bipedal humanoid robots will fall under unforeseen perturbations without active stabilization.  Humans use dynamic full body behaviors in response to perturbations, and recent bipedal robot controllers for balancing are based upon human biomechanical responses.  These controllers assume simple physical models and require very accurate state information, making them less effective on physical robots in uncertain environments.  To address this issue, we propose a hierarchical control architecture that learns to switch between three low-level biomechanically-motivated strategies in response to perturbations.  The high level strategy is learned in an online fashion from state trajectory information gathered during experimental trials.  This learning approach is evaluated in physics-based simulations as well as on a small humanoid robot. Our results demonstrate how well this method stabilizes the robot during walking and whole body manipulation tasks."
1393,Hierarchical spike coding of sound,"We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task."
1394,Noisy Bayesian Active Learning,"We consider the problem of noisy Bayesian active learning, where we are given a finite set of functions $\mathcal{H}$, and a sample space $\mathcal{X}$. A function in $\mathcal{H}$ assigns a label to a sample in $\mathcal{X}$, and the result of a label query on a sample is corrupted by independent noise. The goal is to identify the function in $\mathcal{H}$ that generates the labels with high confidence using as few label queries as possible, by selecting the queries adaptively in a strategic manner. Previous work in Bayesian active learning considers Generalized Binary Search, and its variants for the noisy case, and analyzes the number of queries required by these sampling strategies. In this paper, we show that these schemes are, in general, suboptimal. Instead we propose and analyze an alternative strategy for sample collection. Our sampling strategy is motivated by a connection between Bayesian active learning and active hypothesis testing, and is based on querying the label of a sample which maximizes the Extrinsic Jensen--Shannon Divergence at each step. We provide upper and lower bounds on the performance of this sampling strategy, and show that these bounds are better than previous bounds."
1395,Human memory search as a random walk in a semantic network,"The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single, undirected process rather than one process for exploring a cluster and one process for switching between clusters."
1396,Infinite Tensor Factorization Priors,"There is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. We consider a general framework for nonparametric Bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure.  The mixing measure is assigned a novel infinite tensor factorization (ITF) prior that allows flexible dependence in cluster allocation across data types.  The ITF prior is formulated as a tensor product of stick-breaking processes.  Focusing on a convenient special case corresponding to a Parafac factorization, we provide basic theory justifying the flexibility of the proposed prior and resulting asymptotic properties.  Focusing on ITF mixtures of product kernels, we develop a  new Gibbs sampling algorithm for routine implementation relying on slice sampling. The methods are compared with alternative joint mixture models based on Dirichlet processes and related approaches through simulations and real data applications."
1397,Manifold Regularization and Embedding Through Laplacian Low-Rank Correction,"This paper introduces Laplacian low-rank correction (LLRC), a method for manifold regularization  that is generally applicable to arbitrary data sets and learning problems.  Unlike popular embedding algorithms such as Laplacian eigenmaps, locally linear embedding, and ISOMAP, our approach does not explicitly seek a low-dimensional representation of data.  Rather, LLRC leverages the graph Laplacian of a local similarity graph to construct a low-rank linear correction that maintains the original dimension of the feature space. This linear correction eliminates dominant off-manifold noise directions, such that the principal components of the corrected space represent the data manifold.LLRC can be kernelized to account for nonlinear structure. A primary advantage of such a linear approach is that the correction can be easily extended out-of-sample.A low-dimensional embedding may be obtained from LLRC by extracting the (kernel) principal components from the corrected feature representation, and can significantly outperform standard (kernel) PCA at recovering manifold structure. The modified representation may also be applied in a number of other learning problems, including classification, regression, and clustering. As an example, we apply LLRC to SVM classification in a semi-supervised setting. In particular, we develop a low-rank approximation to the standard Laplacian SVM. This approximation offers greatly reduced computation for large data sets and, in some cases, improved performance."
1398,Modeling Two Functionally Distinct Ventral Pathways Representing Static Form and Color/Texture,"Anatomical and physiological data suggests that the ventral visual pathway of the primate brain is subdivided into specialized processing modalities. We combine a model of color/texture processing with a separately developed model of shape/form processing to determine whether such pathways can be both functionally independent and complimentary. Our hypothesis was that the combination would yield better performance on an invariant object localization and classification task than either model alone. Functional independence is established if the optimal combination corresponds to the Boolean rules used for combining two statistically independent binary classifiers. To extract color/texture information, we learned a sparse dictionary of features from representative training data, pooled over the dictionary elements using a winner-take-all heuristic and then clustered the pooled data groups using a k-means algorithm. In tandem, we extracted shape information by pre-processing the image with a canny edge filter, then computed difference kernels based on co-occurrence statistics for edge combinations characteristic of the target category. We represented the two pathways as binary classifiers and combined them with optimal Boolean operators, as defined using a Neyman-Pearson theorem for Receiver Operating Characteristics (ROC) curves. Using ground-truth for a high-definition video-stream from a helicopter flying over Los Angeles, CA, we demonstrate that the two pathways are functionally independent and when combined perform substantially better than either pathway alone. Our results suggest that the separate processing modalities found in the primate ventral visual pathway represent functionally independent and complimentary approaches to viewpoint invariant object detection and localization."
1399,Lifted Parameter Learning for Markov Logic,"Statistical relational learning (SRL) augments probabilistic models with relational representations and facilitates reasoning over sets of objects. When learning the probabilistic parameters for SRL models, however, one often resorts to reasoning over individual objects. We propose to harness the full power of relational representations in the learning phase, by using lifted inference. For this we compile a Markov logic network into a compact and efficient first-order data structure and use weighted first-order model counting (WFOMC) to calculate the likelihood of the data in a lifted manner. By exploiting the relational structure in the model, it is possible to dramatically improve the run time of the likelihood calculation and learn parameters more accurately. This allows us to calculate the exact likelihood of the data for models where previously only approximate inference was feasible. Results on real-world data sets shows that this approach learns more accurate models."
1400,Coarse-to-fine video segmentation using supervoxel trees,"Image and video segmentation is a task of immense importance in the field of computer vision. Existing algorithms like graph cuts solve this problem by considering the possibility of every adjoining pixel (superpixel) or voxel (supervoxel) getting different labels. However, real images tend to have spatial continuity and videos have additional temporal continuity. In this paper, we consider a hierarchical tree of supervoxels.We propose a coarse-to-fine video segmentation scheme whereby larger supervoxels belonging to the same label, need not be refined into finer supervoxels. For videos with significant spatio-temporal continuity, such a scheme can lead to significant computational savings. By using admissible heuristic estimates of the unary and binary potentials, we can show that this scheme leads to the exact segmentation that would have been obtained by considering the finest layer of supervoxels."
1401,Local Support Vector Machines: Formulation and Analysis,"We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs."
1402,Bayesian n-Choose-k Models for Classification and Ranking,"In categorical data there is often structure in the number ofvariables that take on each label.  For example, the total number of objects in an image and the number of highlyrelevant documents per query in web search both tend to follow a structured distribution.In this paper, we study a probabilistic model that explicitly includes a priordistribution over such counts, along with a count-conditional likelihood thatdefines probabilities over all subsets of a given size.When labels are binary and the prior over counts is a Poisson-Binomialdistribution, a standard logistic regression model is recovered, but for othercount distributions, such priors induce global dependencies and combinatoricsthat appear to complicate learning and inference. However, we demonstrate that simple, efficient learning procedures can bederived for more general forms of this model.We show the utility of the formulation by exploring multi-object classification asmaximum likelihood learning, and ranking and top-K classification asloss-sensitive learning. "
1403,No More Pesky Learning Rates,"The performance of stochastic gradient descent (SGD) dependscritically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations accross samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained throughsystematic search, and effectively removes the need for learning rate tuning. "
1404,Consensus Ranking with Signed Permutations,Signed permutations (also known as the hyperoctahedral group) are used in modeling genome rearrangements. The algorithmic problems they raise are computationally demanding when not NP-hard. This paper presents an algorithm for learning consensus ranking between signed permutations under the inversion distance. This can be extended to estimate a natural class of exponential models over the group of signed permutations. We investigate experimentally the efficiency of our algorithm for modeling data generated by random reversals.
1405,Explanation with Causal Logic Models,"Despite their success in transferring the powerful human faculty of causal reasoning to a mathematical and computational form, causal models have not been widely used in the context of core AI applications such as robotics.  In this paper, we define Causal Logic Models (CLMs), a new discrete-time, probabilistic, first-order representation which uses causality as a fundamental building block. Rather than merely converting causal rules to first-order logic as various methods in Statistical Relational Learning have done, we treat the causal rules as basic primitives which cannot be altered without changing the system. We present an algorithm using CLMs for one type of causal reasoning known as causal explanation, i.e., understanding the causal links between events spaced out in time. Using CLMs rather than traditional fixed causal models allows causal explanation to be performed in dynamic situations where variables of interest are not necessarily known a priori. We show empirically that CLMs produce intuitive and succinct explanations given an evidence set, more in line with human causal reasoning. We also discuss how CLMs other types of causal reasoning such as prediction and counterfactuals can look qualitatively different from their counterparts with other representations."
1406,Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models,"Recent experiments have demonstrated that humans and animals typically reasonprobabilistically about their environment. This ability requires a neural codethat represents probability distributions and neural circuits that are capable ofimplementing the operations of probabilistic inference. The proposed probabilisticpopulation coding (PPC) framework provides a statistically efficient neuralrepresentation of probability distributions that is both broadly consistent withphysiological measurements and capable of implementing some of the basic operationsof probabilistic inference in a biologically plausible way. However, theseexperiments and the corresponding neural models have largely focused on simple(tractable) probabilistic computations such as cue combination, coordinate transformations,and decision making. As a result it remains unclear how to generalizethis framework to more complex probabilistic computations. Here we addressthis short coming by showing that a very general approximate inference algorithmknown as Variational Bayesian Expectation Maximization can be implementedwithin the linear PPC framework. We apply this approach to a generic problemfaced by any given layer of cortex, namely the identification of latent causes ofcomplex mixtures of spikes. We identify a formal equivalent between this spikepattern demixing problem and topic models used for document classification, inparticular Latent Dirichlet Allocation (LDA). We then construct a neural networkimplementation of variational inference and learning for LDA that utilizes a linearPPC. This network relies critically on two non-linear operations: divisive normalizationand super-linear facilitation, both of which are ubiquitously observed inneural circuits. We also demonstrate how online learning can be achieved using avariation of Hebb?s rule and describe an extesion of this work which allows us todeal with time varying and correlated latent causes."
1407,Hierarchical Estimation of Locomotion Mode and Gait Cycle using Switching Unscented Kalman Filters,"As we walk, the state of our limbs varies cyclically, while other variables such as walking speed vary along continuous axes and all are nonlinearly related to one another and to potentially observed aspects of gait. Moreover, our locomotion may switch between modes such as walking and standing. Here we present an efficient solution to nonlinear estimation problems with both cyclical and contin- uous state variables with dynamics that undergo switches. This solution is based on a Hidden Markov Model (HMM) over Unscented Kalman Filters (UKF). The resulting algorithm captures the total variability of the gait parameters with a to- tal variance accounted for (R-squared) of 0.99, outperforming existing linear regression estimators (R-squared = 0.92)"
1408,Cost-Sensitive Exploration in Bayesian Reinforcement Learning,"In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems."
1409,Modeling Expertise of Crowd by Normalized Gamma Decomposition,"We develop a flexible framework for modeling the expertise of a crowd, called normalized gamma decomposition of a confusion matrix.The proposed framework enables us to model the ability of workers, the labeling tendency (confusion) of workers, and the items' difficulties to correctly annotate.Moreover, we can apply our framework to a heterogynous labeling problem where we have to analyze a task that includes different types of labels."
1410,Learning with Partially Absorbing Random Walks,"We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$, and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\mathcal{S}$, while dropping sharply outside $\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning."
1411,Efficient MAP Inference in Binary Pairwise MRFs,"Markov random fields (MRFs) have broad application in many fields including computer vision. In general, however, finding the most likely (MAP) configuration of variables is NP-hard.  If we restrict attention to the class of associative (submodular) models then the task is tractable but further speed improvement would be welcome. Simplifications are revealed by reducing the MAP inference problem to maximum weight stable set (MWSS) on a compiled nand Markov random field (NMRF). This yields two results for general binary pairwise MRFs: (1) A rapid pre-processing algorithm that identifies an exact MAP configuration of a subset of the variables; the subset is larger for sparse MRFs with low associativity and random settings;local sensitivity parameters are also returned for every variable; and (2) We derive necessary and sufficient conditions for when such an MRF maps to a perfect NMRF, which guarantees efficient MAP inference."
1412,Instance Level Multiple Instance Learning Using Similarity Preserving Quasi Cliques,In this paper we introduce an instance-level approach to multiple instance learning. Our bottom-up approach learns a discriminative notion of similarity between instances in positive bags and use it to form a discriminative similarity graph. We then introduce the notion of similarity preserving quasi-cliques that aims at discovering large quasi-cliques with high scores of within-clique similarities. We argue that such large cliques provide clue to infer the underlying structure between positive instances. We use a ranking function that takes into account pairwise similarities coupled with prospectiveness of edges to score all positive instances. We show that these scores yield to positive instance discovery. Our experimental evaluations show that our method outperforms state-of-the-art MIL methods both at the bag-level and instance-level predictions in standard benchmarks and image and text datasets.
1414,Diagnose and Decide: An Optimal Bayesian Approach 004,"Many real-world scenarios require making informed choices after some sequence of actions that yield noisy information about a latent state.  Prior research has mostly focused on generic methods that struggle to scale to large domains. We focus on a subclass of such problems with two particular characteristics. First, though information gathering actions or tests only provide noisy information about the hidden state, once performed a test will always yield the same result. This means it is sufficient to perform each test once. Second, we assume that test costs can be expressed in the same units as costs of the final decisions made. We call such scenarios diagnose-and-decide problems. We prove diagnose-and-decide problems are a special subclass of POMDPs for which the optimal policy can be computed in time polynomial in the set of possible tests' outcomes. We develop a new simple algorithm which is able to take advantage of the unique structure in our problem while guaranteeing optimality. We demonstrate the advantages of our approach over greedy and traditional POMDP methods in two simulations based on real-world data (colon cancer screening and object recognition) as well as a large synthetic domain. "
1416,Robust Distance Metric Learning via Simultaneous $\ell_1$-Norm Minimization and Maximization,"Traditional distance metric learning with side information often formulates the learning objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Since covariance matrices are prone to outliers, it is desirable to develop a robust distance metric learning method. In this paper, motivated by existing studies that improve the robustness of machine learning models via the L1-norm, we propose a robust formulation of distance metric learning using the L1-norm distances. However, solving the formulated objective is very challenging because it simultaneously minimizes and maximizes (minmax) the non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is scarcely studied in literature. Extensive empirical evaluations on the proposed robust distance metric learning method are performed, in which our new method outperforms related state-of-the-art methods in a variety of experimental settings and demonstrate their effectiveness in the clustering tasks on both noiseless and noisy data.  "
1417,Relating Structural MRI and Behavioral Measure to Functional MRI Measures,"Structural magnetic resonance imaging (MRI) and behavioral measures in the elderly have been shown to be associated in someway or another with functional MRI measures in past studies. However, the goal of this study is to analyze the ability of structural MRI and behavioral measures to predict functional measures in relationship to one another. This study uses both linear regression and artificial neural networks to achieve this goal. The results show that linear regression performs better and the features that best relate to functional measures include age, mini-mental state examination scores, total regional volume, number of tracks connecting regions, and regional white matter hyperintensities volume."
1418,Subset Selection for Gaussian Processes,"Given a Gaussian Process on a graph (see, e.g., [RW06]), we consider the problem of selecting a subset of variables to observe which minimizes the total expected squared prediction error of the unobserved variables. We focus on Gaussian Processes on bounded tree-width graphs and on a restricted class of Gaussian Processes, called Gaussian Free Fields (GFF), which arise in semi-supervised learning and computer vision. We ?rst show that ?nding an exact solution is NP-hard even for GFFs. We give a simple greedy approximately optimal algorithm for GFFs on arbitrary graphs. We then give a message passing algorithm for Gaussian Processes on bounded tree-width graphs (bounded tree-width graphs have been widely studied in the context of inference, see, e.g., [Sud02])."
1419,Extending generalized delta rules for efficient Hessian calculations through backpropagation,"Recent extensions of first-order backpropagation (BP), also known asgeneralized delta rules, of Rumelhart et al. (1986) lead to the development of efficient Hessian calculations for second-orderoptimization (e.g., Levenberg-Marquardt methods).  Consider, for instance, the evaluation of  the so-called Gauss-Newton Hessian matrix J'*J of size n x n when optimizing a multi-layer neural network that has multipleZ outputs (Z > 1).  Fairbank & Alonso~(2012) described how to use first-order BPfor (Z+n) times per data pattern in forming Z rows of J and then J'*Jexplicitly column by column.  Their claim is thatthe proposed method works faster than the ``standard'' algebraic method bya factor of Z. Yet, their analysis totally ignores several key factors thatare already discussed individually in other computational techniques.Even under their assumption 1 << Z <= square root of n,the standard method can work faster in some situations.By combining the strengths of existing algorithms,we have derived an efficient algorithm that performs backward passes only forB times, followed by some algebraic manipulations, where B denotes the total number of hidden nodes.Since B is approximately equal to sqaure root of n, our improvement would be significant.We also show its further extensions and an efficient matrix-freealgorithm that combines BP with a forward mode of automatic differentiation."
1420,Learning mixture models with the hierarchical expectation maximization algorithm,"Driven by the need for computationally efficient parameter estimation from large, web-scale data sets, the hierarchical EM (HEM) algorithm has been proposed and proven effective for a variety of modeling tasks and applications. In this paper, we investigate the benefits of HEM as a general-purpose algorithm for parameter estimation in mixture models, compared to regular EM. First, we re-derive the algorithm in more generality, for generic exponential family distributions, with and without unobserved variables. Second, we discuss and experimentally verify its benefits across a broad spectrum of model classes and applications. Besides scalability, HEM's implicit regularization and adaptation for multiple instance learning make it an appealing alternative to standard EM, for practitioners."
1421,An Integrated Method for Causality Structure and Hidden Causes Discovery,"When we focus on causality research of real-life application, we may need to discover causal structure accurately and detecting hidden causes automatically without a priori restriction of hidden variable (non)existence in the underlying causal network. Unfortunately, most existing causal discovery algorithms may confuse direction between cause and effect and measure the hidden causes dif?cultly. Motivated by these facts, we present an integrated method for causal discovery with automatic detection of probable hidden causes, where the causal structure is induced by conditional independency testing and ?cliques? of initial variables with relationships between ?cliques?. Clues of hidden cause are obtained when we identify redundant edges.  In this paper, we distinguish causality of be-to-be and not be-to-not. As a sample application, we present our integrated method (ICIC)for LUCAS released in NIPS 2008 and show the results ?nally."
1422,Learning Latent Factor Models of Human Travel,"This paper describes probability models for human travel data, using learned latent factors.  	Latent factors represent interpretable properties including travel distance, desirability of destinations, and affinity between locations.  Individuals are clustered into distinct classes of travel models. The latent factors combine in a multiplicative manner, and are learned using maximum likelihood.  The resulting models exhibit significant improvements in predictive power over previous methods, while also using far fewer parameters than histogram-based methods.  The method is demonstrated from travel datasets collected from Flickr data and from taxi travel, and demonstrates improved predictive power over previous approaches."
1423,Sparse projections onto the simplex,"The past decade has seen the rise of $\ell_1$-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard $\ell_1$-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation."
1425,Coordinated collision avoidance of multiple agents with continuous stochastic plant dynamics,"We describe an approach to multi-agent planning under continuous stochastic dynamics. The approach yields collision-free state trajectories with adjustably high certainty while aiming for lowsocial cost.  To this end we describe a collision-detection module based on a distribution-independent probabilistic bound and compare fixed priority and auction-based coordination protocols to resolve collisions.While our experiments were conducted with agents governed by linear stochastic dynamics with state-independent noise, our methods extend to more general settings of state-dependent noise and with non-linear dynamics."
1426,Locating Changes in Highly-Dependent Data with Unknown Number of Change Points,"The problem of multiple change-point estimation is considered for time-series sequences with unknown number of change points. A consistency framework is suggested that is suitable for highly-dependent time series, and an asymptotically consistent algorithm is proposed.  The only assumption that we need to establish consistency is  that the data are generated by stationary ergodic time-series distributions. No modeling, independence or parametric assumptions are made; the data are allowed to be dependent and the dependence can be of arbitrary form. The theoretical result is complemented with an experimental evaluation."
1427,Modeling Salient Object-Object Interactions to Generate Textual Descriptions for Natural Images,"In this paper we propose a new method for automatically generating textual descriptions of images. Our method consists of two main steps. Using saliency maps, it detects the areas of interests in the image and then creates the description by recognizing the interactions between detected objects within those areas. These interactions are modeled using the pose(body parts configuration) of the objects. To create sentences a syntactic model is used that builds subtrees around the detected objects and then combines those subtrees using recognized interaction. Our Results show the improved accuracy of the descriptions generated by our algorithm."
1428,Probabilistic Event Cascades for Alzheimer's disease," Accurate and detailed models of the progression of neurodegenerative diseases such as  Alzheimer's (AD) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments. In this paper, we introduce the ALPACA (Alzheimer's disease Probabilistic Cascades) model, a generative model linking latent Alzheimer's progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a fixed ordering of events, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed disease progression models. We describe efficient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer's patients from the  Alzheimer's Disease Neuroimaging Initiative."
1429,Modelling the Lexicon in Unsupervised Part of Speech Induction,"Automatically inducing the syntactic part-of-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single  part-of-speech tag. This one-tag-per-type heuristic, which counters the tendency of Hidden Markov Model based taggers to over generate tags for a given word type, is clearly incompatible with basic syntactic theory. In this paper we extend the current state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model of the lexicon. In doing so we are able to incorporate a soft bias towards inducing few tags per type. We develop a novel particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with the state-of-the-art without making any unrealistic restrictions."
1430,Optimal integration of visual speed information across different spatiotemporal frequency channels,"How does the human visual system compute the speed of a coherent motion stimulus that contains motion energy in different spatiotemporal frequency bands? Here we argue that perceived speed is the result of optimal integration over different spatiotemporal frequency channels. We formalize this hypothesis with a Bayesian observer model that treats the channel responses as independent cues, and then optimally combines them together with a prior for slow speeds. We test the model against behavioral data from a 2AFC speed discrimination task with which we measured subjects' perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies, and of various combinations of these single gratings. We find that perceived speed of the combined stimuli is independent of the relative phase of the underlying grating components, and that the perceptual biases and discrimination thresholds are always smaller for the combined stimuli, supporting the cue combination hypothesis. The proposed Bayesian model fits the data well, accounting for perceptual biases and thresholds of both simple and combined stimuli. Fits are improved if we assume that the channel responses are subject to divisive normalization, which is in line with physiological evidence.  Our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for stimuli of arbitrary spatial structure.  "
1432,Efficient and direct estimation of a neural subunit model for sensory coding,"Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a `subunit' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (``convolutional??) copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the `subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency."
1433,Accurate Deterministic Parsing with Syntax-Directed Embeddings,"Distributional analysis of words by embedding in continuous vector spaces has been effective at supplying features for many NLP tasks.  In contrast to many applications that use fixed context windows, we present continuous embeddings based on words' contexts in syntactic dependency trees. We employ these embeddings within a deterministic dependency parser and achieve the state of the art performance for this class of model.To our knowledge, this work is the first application of continuous vector embeddings in dependency parsing."
1434,Spectral Learning of Latent-Variable HMMs ,We derive a spectral algorithm for learning the parameters of a latent-variable HMM. This method avoids the problem of local optima and provides a consistent estimate of the parameters. We demonstrate the method on a phoneme recognition task and show that it performs competitively with EM. 
1435,Richer Dynamic Textures: Optimized Hierarchical Bases and Sparse Updates,"Dynamic textures are used to create perceptually reasonable continuations of repetitive video sequences and as a tool for segmentation or classification of video sequences. We develop three extensions which have the goal of allowing Dynamic Textures to accurately represent richer, more complex natural video sequences. First, we jointly solve for the representation and the dynamics model, optimizing for the ability to best predict future appearance. Second, we derive the dynamic textures formulation using a hierarchical basis that captures both global correlations and local variations. Third, we derive an approach to drive the dynamic texture model to fit known data at a sparse set of known locations. Collectively, these improvements show quantitative and qualitative improvements on a variety of textures drawn from the dyntex database."
1436,One Permutation Hashing,"While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.In this paper, we develop a simple \textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing $k$ permutations to just one  would be much more \textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \& logistic regression also confirm the theoretical results."
1437,A template model for fine-grained object recognition,"Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms. "
1438,Multi-Task Learning in Low-Dimensional Manifolds for Successful Generalization from Few Samples,"We consider multi-task regression with parametric function approximators. A common approach to multi-task learningis to use Hierarchical Bayesian Models (HBMs) to learn the parameters of each task as well as the priordistribution over the parameter vectors of the tasks. For high dimensional parameter vectors as they occur for most interestingtasks the estimation of this prior distribution is challenging as it either requires a large amount of tasks-datasets or we need to resortto simplistic, less-expressive distributions. In this paper we propose to model the prior distribution not in the original high-dimensional space but in a low-dimensional manifold. This allows us to estimate the prior distribution accurately already from few datasets.In addition, our approach allows us to extract a latent variable for each task which can be used to characterize the task. We will test our method on a standard multi-task learningbenchmark problem as well as on a multi-task motor control problem. The experiments show that our method can exploit the low-dimensional structure of the tasks and outperforms other methods."
1439,Getting the First Page Right: Bayesian Active Retrieval under Uncertainty,"Triggered by the idea of an information retrieval system for objects with noisy and missing features, we investigate the general problem of actively learning a similarity function of complex objects when the inputs to this function are not known exactly. To reduce the uncertainty in the inputs, and in turn improve the similarity function, we are interested in acquiring more information about the input objects. As gathering clean and complete information is costly or even impossible, it is important to carefully select the information needed and to be able to deal with uncertainty in order to retrieve meaningful results fast and with low total cost. Hence, we propose a Bayesian active learning approach to efficiently learn the most similar objects to a given query object in the setting where only partial and noisy information about entities is available. In our information retrieval case this corresponds to the task of getting the first page (of retrieval results) right. We evaluate the proposed Bayesian decision theoretic framework to actively acquire information on several retrieval problems, including a real-world document retrieval task."
1441,A new edge selection method for preserving the topology of persistent brain network,"The functional brain connectivity at the macro-scale studies how the localized areas of brain, i.e., the regions of interest (ROIs), work together during the specific mental functions.Their inter-regional connections estimated by the dissimilarity measure between observations in ROIs are usually too dense to visualize and interpret.So, we need to select the important edges in the network.Thresholding the edge weight matrix is most popular way to select the edges because it is assumed that only the strongly connected edges are important.However, there is no widely accepted rule for determining the threshold as well as the weakly connected edges also have the information which discriminates networks.In this paper, we propose a new edge selection method which preserves the topological structures of brain network based on the persistent homology.The persistent homology scans the topological structures by increasing the threshold and transforms the topological invariants to the algebraic form, known as Betti numbers.We seek the edges which affect the changes of the zeroth and first Betti numbers, i.e., connected components and holes.We applied the proposed method to the functional brain network based on FDG-PET data consisting of 24 attention deficit hyperactivity disorder (ADHD), 26 autism spectrum disorder (ASD) children and 11 pediatric control (PedCon) subjects.We showed that our edge selection method finds the minimum number of edges preserving the origianl geodesic distance of network."
1442,Large-Scale Sparse PCA through Low-rank Approximations,We introduce a novel scheme for sparse PCA that has provable approximation guarantees.We first introduce an algorithm that can exactly solve sparse PCA for matrices of constant rank in polynomial time. Given a full-rank frequency matrix we obtain a constant rank matrix approximation and subsequently execute the exact sparse PCA solver on this low-rank approximation. We surprisingly see that this low-rank approximation step introduces very small errors. We theoretically explain this behavior by showing that data sets with few high-degree words must have data matrices that are close to low-rank. Our formalism allows us to show that our sparse PCA algorithm is asymptotically tight. Experimentally we evaluate our algorithm in a very large Twitter data set. A feature elimination step allows us to perform sparse PCA in millions of Tweets in a few minutes. Our scheme typically captures $80-90\%$ of the data variance by using rank-3 or rank-4 approximations. 
1443,On a Mechanical Doctor for Prognostication,"We study an adaptive strategy for a doctor to learn how to make a prognosis from processing event data and adjusting his skills based on past experience. This is an example of decision-making in the face of uncertainty, and fits well within the framework of online learning and the recently proposed technique of aggregation by mirror averaging. The particulars of this approach are threefold. (1) feedback (rewards or loss) comes in necessarily delayed (i.e., the size of the delay is the very subject of study) (2) Since making a prognosis is necessarily random, we introduce the notion of an individual Probability Density Function (iPDF). This is a curve for an individual patient which is expected to give a large value when evaluated on the value of its associated event.This approach is to be contrasted with the common statistical population-based approach which tries to uncover the probabilistic rules underlying all events. A final crucial ingredient (3) is the presence of censored observations, particular to such setting of survival analysis.This paper designs a strategy ('A mechanical Doctor, or MD') based on recent developments in the area  of machine and statistical learning, and illustrates the technique on a challenging problem in bioinformatics."
1444,Sequence Learning via Hypergraph Particle Filtering,"Variable-order Markov models create a tree of partitions to represent higher-order contexts of discrete sequences. Though efficient in lossless compression, the partitioning approaches are not very flexible, and difficult for sequential update. Here we propose a hypergraph-based sparse population coding method for learning the higher-order Markov models of sequence data. This representation facilitates fast and flexible update of the transition probability matrix of non-uniform variable-order Markov chains. We develop a particle filter-like sequential Bayesian estimation algorithm that learns and predicts sequences using online Monte Carlo sampling on the sparse hypergraph space. The parsimony vs. accuracy tradeoff in this ?hypergraph particle filter? can be made conveniently by Bayesian priors on model order and population size. The flexibility and sparseness of the Markov model structures learned is experimentally demonstrated in the domains of music, language, and robot motion generation. Our results are somewhat surprising since the Markov models of higher-order k with non-uniform, sparse connectivity often outperform the lower-order j < k Markov models with full parameterization both in reconstruction error and novelty of patterns discovered."
1445,Near Optimal Chernoff Bounds for Markov Decision Processes,"The expected return is a widely used objective in decision making under uncertainty.  Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize.  We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw connections to previously proposed objectives for risk-aware planing: minmax, exponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded.  Additionally, we present an efficient algorithm for optimizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale."
1446,Is Matching Pursuit Solving Convex Problems?,"Matching pursuit  (\texttt{MP}) algorithms have been successfullyapplied in signal processing and pattern recognition areas. However,as far as we know, it is still not clear whether any \texttt{MP}algorithm can solve a convex problem or not. In this paper, a novelconvex relaxation is proposed for a class of matching pursuitalgorithms, which includes the classical orthogonal matching pursuit(\texttt{OMP}) as a special case.  Based on the proposed scheme, ageneral matching pursuit (\texttt{GMP}) algorithm can be naturallyobtained. As it solves a convex problem, \texttt{GMP}  guarantees toconverge globally. In addition, a subspace exploratory search canfurther improve the performance. Finally, we show that \texttt{GMP}with an $\ell_1$ regularization term can recover the $k$-sparsesignals if the restricted isometry constant $\sigma_k\leq 0.307-\nu$,where $\nu$ can be arbitrarily close to 0. The proposed method can beeasily parallelized and the efficiency can be further improved.Simulations on an 8-core machine show that the proposed method cansuccessfully decode the problems of scale $2^{13} \times 2^{17}$within 10 seconds and scale $2^{10} \times 2^{20}$ within 2 seconds."
1447,Approximate Gaussian process inference for the drift function in stochastic differential equations,"We introduce a nonparametric approach for estimating drift functions in stochastic differential equations from incomplete observations of the state variable. Using a Gaussian process prior over the drift as a function of the state variable, we develop an approximate EM algorithm to deal with the unobserved, latent state dynamics between observations. The posterior dynamics over states is approximated by a piecewise linearized process and the MAP estimation of the drift is facilitated by a sparse Gaussian process approximation."
1448,Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression,"We present a new variational inference algorithm for Gaussianprocesses with non-conjugate likelihood functions.This includes binary and multi-class classification, as well asordinal regression.Our method constructs a convex lower bound, which can be optimizedby using an efficient fixed point update method.We then show empirically that our new approach is much faster than existing methods without any degradation in performance."
1449,Imitation Learning by Coaching,"Imitation Learning has been shown to be successful in solving many challenging real-world problems.Some recent approaches give strong performance guarantees by training the policy iteratively.However, it is important to note that these guarantees depend on  how well the policy we found can imitate the oracle on the training data. When there is a substantial difference between the oracle's ability and the learner's policy space,we may fail to find a policy that has low error on the training set.In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner and gradually approaches the oracle.By a reduction of learning by demonstration to online learning, we prove that coaching can yield a lower regret bound than using the oracle.We apply our algorithm to a novel cost-sensitive dynamic feature selection problem,a hard decision problem that considers a user-specified accuracy-cost trade-off. Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods."
1450,Stochastically Emerging Medoids: Application of Classical Problems in Probability Theory for Clustering Massive Data Sets,"K-medoid methods for clustering data have many desirable properties such as robustness and the ability to use non-numerical values, but their typically high computational complexity has made their application to large data sets difficult. In this paper, we present AGORAS, a novel stochastic algorithm for the k-medoids problem that is especially well-suited to clustering massive data sets. Our approach involves taking a sequence of uniform sample sets and a heuristic for determining the sample size and identifying cluster medoids from the sampled items. As a result, computing the final solution only involves solving k trivial sub-problems of centrality, which can be done much more efficiently on large data sets than searching a combinatorial space for the optimal value of an objective function. As a result, the complexity of AGORAS is effectively independent of the full data size, and it can scale to arbitrarily large data sets.  We evaluate AGORAS experimentally against PAM and CLARANS, the best-known existing algorithms for the k- medoids problem, across a variety of published and synthetic data sets.  We find that AGORAS outperforms PAM by up to four orders of magnitude for data sets with less than 10,000 points, and it outperforms CLARANS by two orders of magnitude on a data set of just 64,000 points.  Moreover, we find in some cases that AGORAS also outperforms these algorithms in terms of cluster quality. "
1452,An online learning rule for a recurrent neural network with hidden neurons,Recent experimental finding suggest that the brain can spontaneously generate spiking patterns with the same statistics as stimulus-evoked activity patterns. Here we propose an online learning rule for a network of stochastic visible and hidden neurons that is able to adapt its spontaneous firing statistics to the stimulus-evoked firing statistics. We show furthermore that learning synaptic weights towards hidden neurons enables the network to bridge a silency gap in the activity pattern of visible neurons.
1453,Modeling Scientific Impact with Citation Influence Regression,"When reviewing the scientific literature in a specific subject area, it would be usefulto have automatic tools that identify the most influential scientific articles aswell as how ideas propagate between articles. Bibliometric measures based on citationcounts, such as impact factors, provide some indication of the influence ofan article or the prestige of its publication venue. However, citations can occur fordifferent reasons and may not always indicate the transfer of ideas, so that citationcounts alone can be misleading. In this paper we develop latent variable probabilisticmodels for inferring influence in scientific corpora. The models operateon a collection of documents embedded in a citation graph with articles as nodesand citations as edges, where the latent topics of cited papers influence the priordistribution over topics in citing papers. We show how the proposed models canbe used to automatically determine the degree of influence of scientific articles,and their influence along the edges of the citation graph."
1454,Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models,"Links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics, i.e., letting the variance of particular distributions in a graphical model go to zero. For instance, in the context of clustering, such an approach yields precise connections between the k-means and EM algorithms.  In thispaper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models.  Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models.  We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis."
1455,A latent factor model for highly multi-relational data,"Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations."
1456,Entropy Estimations Using Correlated Symmetric Stable Random Projections,"Methods for efficiently estimating  the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of {\em Compressed Counting (CC)}~\cite{Proc:Li_Zhang_COLT11}  based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two \textbf{correlated} frequency moments estimated from  correlated samples of \textbf{symmetric} stable random variables. Interestingly, the  estimator for the moment we recommend for entropy estimation  barely has bounded variance itself, whereas the  geometric mean estimator (which has bounded higher-order moments) is not sufficient for entropy estimation. Our experiments confirm that this method is able to  well approximate the Shannon entropy using small storage (e.g., $100\sim 1000$ samples). A prior study~\cite{Proc:Zhao_IMC07}  approximated the Shannon entropy using symmetric stable random projections with (e.g.,) $10^5\sim1.6\times10^6$ \textbf{independent} samples."
1458,Simultaneously Leveraging Output and Task Structures  for Multiple-Output Regression,"Multiple-output regression models require estimating multiple functions, one for each output. To improve parameter estimation in such models, methods based on structural regularization of the model parameters are usually needed. In this paper, we present a multiple-output regression model that leverages the covariance structure of the functions (i.e., how the multiple functions are related with each other) as well as the conditional covariance structure of the outputs. This is in contrast with existing methods that usually take into account only one of these structures. More importantly, unlike most of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data. Several previously proposed structural regularization based  multiple-output regression models turn out to be special cases of our model. Moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs). Experimental results on both synthetic and real datasets demonstrate the effectiveness of our method."
1459,Learning Causal Relationships From Multivariate Time Series,"This paper considers a natural and widely prevalent setting where a collection of time series evolve in a causal manner, and one is interested in inferring the graph governing the causality between the variables therein. We consider this problem in the special case where variables are discrete and updates are Markov. We develop a new algorithm to learn causal graph structure based on the notion of {\em directed information}, and analytically and empirically demonstrate its performance. Our algorithm is an adaptation of a greedy heuristic for learning undirected graphical models, but with modifications to leverage causality. Analytically, the challenge lies in establishing sample complexity, because the samples are dependent."
1460,A Minimum Frame Error Criterion for Hidden Markov Model Training,"Abstract Hidden Markov models (HMM) have been widely studied and applied over decades. The standard supervised learning method for HMM is maximum likelihood estimation (MLE) which maximizes the joint probability of training data. However, the most natural way of training would be finding the parameters that directly minimize the error rate of a given training set. In this article, we propose a novel learning method that minimizes the number of incorrectly decoded labels frame-wise. To do this, we construct a smooth function that is arbitrarily close to the exact frame error rate and minimize it directly using a gradient-based optimization algorithm. The proposed approach is intuitive and simple. We applied our method to the task of chord recognition in music, and the results show that it performs better than Maximum Likelihood Estimation and Minimum Classification Error. "
1462,Herded Gibbs Sampling,"The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm that is entirely deterministic. We demonstrate with simple examples that herded Gibbs exhibits better convergence behavior for approximating the marginal distributions than Gibbs sampling. We use an image denoising example to demonstrate the effectiveness of herded Gibbs as an inference technique for Markov Random Field models. We also adopt herded Gibbs as the inference engine for Conditional Random Fields in Named Entity Recognition and show that it is competitive with the state of the art. The conclusion is that herded Gibbs, for graphical models with nodes of low degree, is very close to Gibbs sampling in terms of the complexity of the code and computation, but that it converges much faster. "
1463,Behavior segmentation and labeling using structured SVM,"We explore representing, segmenting and classifying behavior using bout-widestatistics, rather than local descriptors. To this effect we develop a structuredsupport vector machine (sSVM) where the loss function is computed at the boutlevelrather than at frame level, and bouts of different length are treated equally.We test our ideas on a set of videos of pairs of flies engaged in social behavior(exploration, aggression and courtship); four actions and a fifth grab-bag categoryin the videos are annotated by expert fly biologists. Our experiments show thatwith limited amount of data, we are able to obtain detection rate of 64%-93% foreach of the actions, while maintaining a precision of 64%-91%."
1464,Continuous Relaxations for Discrete Hamiltonian Monte Carlo,"Continuous relaxations play an important role in discreteoptimization, but have not seen much use in approximate probabilisticinference. Here we show that a general form of the GaussianIntegral Trick makes it possible to transform a wide class ofdiscrete variable undirected models into fully continuous systems. Thecontinuous representation allows the use of gradient-based HamiltonianMonte Carlo for inference,  results in new ways of estimatingnormalization constants (partition functions), and in general opens upa number of new avenues for inference in difficult discretesystems. We demonstrate some of these continuous relaxation inference algorithmson a number of illustrative problems."
1465,Mechanism Design for Machine Learning Problems,"While machine learning competitions like the Netflix Prize have had relative success on their own, they pave the way to think about procedural aspects of developing predictors. We believe that applying optimal structures designed using game theoretical thinking can make the process of development of machine learning solution much more efficient. However, there are some special features thatare specific to learning scenarios. In this paper, we make the initial steps towards achieving this. In particular, we address the issue that in a prediction problem the outcome of a mechanism must depend on a quantity unknown to all parties (i.e., how well the proposed algorithms will perform). We also propose a specific auction where the developers can submit multiple proposed predictors."
1466,Probabilistic Multi-label Classification with Sparse Feature Learning,"In this paper we propose a probabilistic multi-label classification model based on novel sparse feature learning. By employing an individual sparsity inducing $\ell_1$-norm and a group sparsity inducing $\ell_{2,1}$-norm, the proposed model has the capacity of capturing both label interdependencies and common predictive model structures.We formulate this sparse norm regularized learning problem as a non-smooth convex optimization problem, and develop a fast proximal gradient algorithm to solve it for an optimal solution. Our empirical study demonstrates the efficacy of theproposed method on a set of multi-label tasks given a limited number of labeled training instances."
1467,Deep Learning of invariant features via tracked video sequences,"We use video sequences produced by tracking as training data to learn invariant features. These features are spatial instead of temporal, and well suited to extract from still images. With a temporal coherence objective, a multi-layer neural network encodes invariance that grow increasingly complex with layer hierarchy. Without fine-tuning with labels, we achieve competitive performance on five non-temporal image datasets and state-of-the-art classification accuracy 61% on STL-10 object recognition dataset."
1468,Scaling Bayesian Optimization to High-Dimensions via Random Embedding,"Bayesian optimization is a powerful strategy for finding the extrema of objective functions. It is applicable in situations where one does not have a closed-form expression for the objective function, but where one can obtain noisy evaluations of this function at sampled values. It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex. Because of these properties, its popularity has increased in many domains, including robotics, planning, sensor placement, news recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension. Several NIPS workshops on this topic have identified the scaling of Bayesian optimization to high-dimensions as a core challenge. Despite this, little progress has been made in this direction. In this paper, we introduce random projection tools to scale Bayesian optimization to higher dimensions. Our proposed techniques enable us to treat continuous and categorical choices simultaneously. They perform well on several challenging domains, including a synthetic example of low intrinsic dimensionality but embedded in a million dimensions, automatic configuration of a practical linear programming solver and automatic configuration of a random forests classifier for the Kinect sensor."
1470,Visual Object Classification is Consistent with Bayesian Generative vs Discriminative Representations,"The ability to learn and distinguish categories is essential for human behavior, and the underlying neural computations are actively investigated. Taking a normative view, we can relate categorisation to the distinction between generative and discriminative classification in machine learning. Generative approaches solve the categorization problem by building a probabilistic model of how each cate- gory was formed and infer then category labels. In contrast, the discriminative approach learns a direct mapping between input and label. Recent work shows how human classification is consistent with discriminative and generative classifi- cation depending on conditions. We hypothesize that humans employ generative mechanisms for classification, when not encouraged otherwise. To test this we exploit a counterintuitive prediction for generative classification, namely how the discrimination boundary between two classes shifts if one category?s distribution is revealed to be broader during learning. We tested N=20 subjects to distinguish two classes, A and B in two tasks (two artificial-script, armadillo-horse stick- drawings). The classes in each task were parameterized by two scalars; objects for each class are drawn from Gaussian parameter distributions, with equal variance and different means (class ?prototypes?). Next, subjects classify unlabelled examples drawn between the classes, so we can infer their discrimination boundary. This process is then repeated but includes training data for class A, which lie far away from B. Counter-intuitively, generative classification predicts a shift of the discrimination boundary closer to B. Conversely, discriminative classifiers will show either no shift of the boundary or a shift of the boundary away from class B. Our results show that categorization in both tasks is consistent with generative and not discriminative classifiers, as classification boundaries shifted towards B for both tasks in all subjects."
1471,Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence,"We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to an algorithm called, unified gap-based exploration (UGapE), with common structure and theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms. "
1472,Forest Graph Estimation with Constraints,"We consider the problem of learning high dimensional forest graphical models.Unlike previous methods, we impose different structural constraints on the obtainedforest graphs: including (i) bounded tree constraint, (ii) bounded star constraint,and (iii) bounded path constraint. These constraints are well motivatedby empirical applications and theoretical considerations. We systematically studythese different constraints and illustrate their relationships. We develop hardnessresults and propose novel approximation algorithms with provable guarantees."
1473,Mapping Overlapping Functional Networks via Multiple Relational Embedding,"Studying the modular composition of the functional cerebral architecture is a challenging line of research. Of particular interest are network structures that are active during specific cognitive tasks. In this paper we address the question of identifying partially overlapping networks that are active across different fMRI experiment conditions. We propose to use Multiple Relational Embedding (MRE) on functional brain imaging data acquired during different cognitive tasks. Multiple functional relationships are embedded into a single joint latent embedding, that encodes both joint, and individual network structure. Experiments demonstrate that this approach can identify shared-, and individual functional connectivity structure, and that it recovers functional networks with higher stability compared to the embedding of individual fMRI sequences."
1474,On Finite Alphabet Compressive Sensing,"This paper considers the problem of compressive sensing over a finite alphabet, where the finite alphabet may be inherent to the data or a result of quantization. There are multiple examples of finite alphabet static as well as time-series data with inherent sparse structure; and quantizing real values is an essential step while handling real data in practice. This paper shows that there are significant benefits to analyzing the problem while incorporating its finite alphabet nature, versus ignoring this and employing a conventional real-alphabet compressing sensing toolbox to the problem. Specifically, when the alphabet is finite, our techniques a. have a lower sample complexity than over reals when the sparsity is below a threshold, b. facilitate constructive designs of sensing matrices based on coding theoretic principles; c. enable one to solve the exact $\ell_0$-minimization problem directly in polynomial time rather than a approach of relaxation followed by sufficient conditions for when the relaxation matches the original problem; and finally, d. allow for smaller data storage (in bits) compared to its real counterpart."
1475,Structure Learning for Weakly Dependent Observations,"We consider the problem of estimating the graph structure of a certain class of stochastic processes defined on an Ising model. This class contains as special cases the i.i.d. observation model, the geometric $\alpha$-mixing process, and the rapidly-mixing Glauber dynamics. We analyze thenode-based neighborhood estimation algorithm of \cite{RavWaiLaf09}, which reduces to $\ell_1$-regularized logistic regression. Our main result is to provide sufficient conditions on the triple $(\numobs, \pdim, \ddim)$ for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously."
1476,Adaptive Sparsity in Gaussian Graphical Models,"An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffrey's hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation."
1477,On the Sample Complexity of Robust PCA,"We estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix.This estimator is used in a convex algorithm for robust subspace recovery (i.e., robust PCA).Our model assumes a sub-Gaussian underlying distribution and an i.i.d.~sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $N$ is of order $O(N^{-0.5+\eps})$ for arbitrarily small $\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation, i.e., $O(N^{-0.5})$. Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is $O(D^{2+\delta})$ for arbitrarily small $\delta>0$ (whereas the sample complexity of direct covariance estimation with Frobenius norm is $O(D^{2})$).These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm, which are close to those of PCA. To the best of our knowledge, this is the only work analyzing the sample complexity of any robust PCA algorithm."
1478,View-invariance and mirror-symmetric tuning in a model of the macaque face-processing system,"Recent experimental results characterizing the face-processing network in macaque visual cortex pose a major puzzle. View-tuned units (found in patches ML/MF) are a natural step to a view-tolerant representation (found in patch AM), as predicted by several models. However, the observation that cells in patch AL are tuned to faces and their mirror reflections remains unexplained (cf. Freiwald and Tsao (2010)). We show that a model based on the hypothesis that the ventral stream implements a memory-based approach to transformation invariance predicts the main properties of ML/MF, AL and AM. In this view, a major computational goal of the ventral stream is to compute invariant ?signatures? that can be used to recognize novel objects under previously-seen transformations of arbitrary ``templates''.  These invariant signatures can be regarded as encodings of a novel object relative to a compressed memory of the transformation of familiar objects (PCA). "
1479,Inferring ground truth from multi-annotator ordinal data: a probabilistic approach,"A popular approach for large scale data annotation tasks is crowdsourcing, wherein each data point is labeled by multiple noisy annotators. We consider the problem of inferring ground truth based on noisy ordinal labels from multiple annotators of varying and unknown expertise levels. We propose a new model for crowd sourced ordinal data that accounts for instance difficulty as well as annotator expertise, and derive a variational inference algorithm for parameter estima- tion. We analyze the ordinal extensions of several state-of-the-art annotator models for binary/categorical labels and evaluate the performance of all the models on a large real world dataset containing query-url relevance scores, collected through Amazon?s Mechanical Turk. Our results indicate that the proposed model performs better or as well as existing state-of-the-art methods and is more resistant to ?spammy? ratings than popular baselines such as mean, median, and majority vote which do not account for annotator expertise."
1481,Constrained-based Human Body Shape Analysis,"We develop a framework for constrained dimensionality reduction that is highly flexible and we illustrate the approach by analyzing 3D human body shape. Previous approaches to body shape analysis reduced the dimensionality of the body shape using principal component analysis and then analyzed shape in a low-dimensional subspace, for example, by relating principle component directions to body measurements like height and weight. We show that the principal components of body shape are not always the best way to capture human shape variation. Instead we define a flexible optimization framework to describe variation in 3D human body shape using orthogonal shape subspaces related to different linear or non-linear constraints. We formulate several different objective functions in this framework and show how to learn models of body shape tailored to specific applications. In particular, we find directions of body shape variation that are more directly related to human measurements that enable improved measurement prediction when compared to previous methods. Additionally we define a {\em null space} of human body shape variation.  This null space captures how body shape varies in ways that are orthogonal to the measurement space for example. The null space reveals interesting properties of body shape that are not captured by standard tailoring measurements such as inter- and intra-person posture variation, ``bow legs'', and body shape asymmetries. Finally, our framework makes it easy to add sparsity constraints and we find that these improve measurement prediction from body scans."
1482,Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning,"We describe a latent variable model for supervised dimensionality reduction and distance metric learning. The model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones. The model?s continuous latent variables locate pairs of examples in a latent space of lower dimensionality. The model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate Gaussian. Nevertheless we show that inference is completely tractable and derive an Expectation-Maximization (EM) algorithm for parameter estimation. We also compare the model to other approaches in distance metric learning. The model?s main advantage is its simplicity: at each iteration of the EM algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem. Experiments show that these simple updates are highly effective."
1485,Active Inference for Brain-Computer Interfaces with Application to an SSVEP Speller,"We view a brain-computer interface as the means by which human users may communicate their intent to a computer. Based on this view, we cast the problem of brain-computer interface design as the problem of inferring the user?s intent by asking a sequence of queries to the human user. The approach is to use an active inference model to choose the next query to be asked based on the observed responses from the previous queries and expected response times for each query. We demonstrate this approach by developing a brain-computer interface for spelling English sentences based on steady-state visually evoked potentials (SSVEP). Results show that the interface allows subjects to spell more than 10 letters per minute, which provides an improvement of performance over previous SSVEP spellers."
1486,Learning Stable Non-linear Features in Contractive Auto-encoders,"Unsupervised learning of feature hierarchies is often a good initialization for supervised training of deep architectures.  In existing deep learning methods, these feature hierarchies are built layer by layer in a greedy fashion using auto-encoders or restricted Boltzmann machines.  Both yield encoders, which compute linear projections followed by a smooth thresholding function.  We point out that these encoders fail to find stable features when the required computation is in the exclusive-or class.  To overcome this limitation, we propose a two-layer encoder which is not restricted in the type of features it can learn.  The proposed encoder can be regularized by an extension of previous work on contractive regularization.  We demonstrate the advantages of two-layer encoders qualitatively, as well as on commonly used benchmark datasets.  "
1487,Out-of-Sample Extensions for Manifold Learning Using Sparse Kernel Ridge Regression,"Many manifold learning algorithms do not provide a mapping from the input space to the low-dimensional manifold, requiring an out-of-sample extension to project new points onto the low-dimensional manifold. We propose an out-of-sample extension approach based on a sparse approximation to kernel ridge regression to achieve significant computational savings. In particular, we present a convex program for approximating kernel ridge regression where given a set of points in $\mathbb{R}^d$ and corresponding points in $\mathbb{R}^p$, we find a mapping from $\mathbb{R}^d$ to $\mathbb{R}^p$ that depends only on a subset of the input data points acting as support vectors. Our construction uses group sparsity and guarantees an upper bound on the average squared Euclidean distance between the predicted points of the sparsified mapping and those of kernel ridge regression. We present two medical imaging applications that necessitate a fast projection to a low-dimensional space. The first is respiratory gating in ultrasound, where we assign the breathing state to each ultrasound frame during the acquisition in real-time. The second is the detection of the position of a patient in an MRI scanner while the bed the patient lies on is moving to a target location."
1488,Trajectory-Based Short-Sighted Probabilistic Planning,"Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [ref] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs.  We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately $10^{70}$ states."
1489,Conditional conjugate priors,"We extend the notion of conjugates in exponential families to  conditional distributions. This yields new sets of priors which are  computationally attractive. Moreover, we show that the commonly  used $\ell_1$ and $\ell_2$ priors are special cases of our  framework. We also point out the connections to the inference with Universum method. Experiments confirm the  efficiency of our approach."
1490,Jointly Learning and Selecting Features via Conditional Point-wise Mixture RBMs,"Feature selection is an important technique for ?nding relevant features from high-dimensional data. However, the performance of feature selection methods is often limited by the raw feature representation. On the other hand, unsupervised feature learning has recently emerged as promising tools for extracting useful features from data. Although supervised information can be exploited in the process of supervised ?ne-tuning (preceded by unsupervised pre-training), the training becomes challenging when the unlabeled data contain signi?cant amounts of irrelevant information. To address these issues, we propose a new unsupervised feature learning algorithm, the conditional point-wise mixture restricted Boltzmann machine, which attempts to perform feature grouping while learning the features. Our model represents each input coordinate as a mixture model when conditioned on the hidden units, where each group of hidden units can generate the corresponding mixture component. Furthermore, we present an extension of our method that combines bottom-up feature learning and top-down feature selection in a coherent way, which can effectively handle irrelevant input patterns by focusing on relevant signals and thus learn more informative features. Our experiments show that our model is effective in learning separate groups of hidden units (e.g., that correspond to informative signals vs. irrelevant patterns) from complex, noisy data."
1491,Semi-blind Source Separation via Sparse Representations and Online Dictionary Learning,"This work examines a semi-blind source separation problem having applications in audio, image, and video processing.  Our essential aim is to separate one source whose local structure is partially or approximately known from another a priori unspecified but structured source, given only a single linear combination of the two sources.  We propose a novel separation technique based on local sparse approximations; a key feature of our procedure is the online learning of dictionaries (using only the data itself) which sparsely model the unknown structured background source.  We demonstrate the performance of our proposed approach via simulation on two stylized applications -one entailing audio source separation, and another where the goal is to separate two spatially multipliexed images."
1493,Tight Bounds on Redundancy and Distinguishability of Label-Invariant Distributions,"The minimax KL-divergence of any distribution from alldistributions in a given collection has several practicalimplications. In compression, it is the least additional number of bitsover the entropy needed in the worst case to encode theoutput of a distribution in the collection. In onlineestimation and learning, it is the lowestexpected log-loss regret when guessing a sequence of randomvalues. In hypothesis testing, it upper bounds the largestnumber of distinguishable distributions in the collection.Motivated by problems ranging from population estimation totext classification and speech recognition, severalmachine-learning and information-theory researchers haverecently considered label-invariant distributions and propertiesof \iid-drawn samples.Using techniques that reveal and exploit the structure of these distributions,we improve on a sequence of previous works and show that theminimax KL-divergence of the collection of label-invariantdistributions over length-$n$ \iid sequencesis between $0.3\cdot n^{1/3}$ and $n^{1/3}\log^2n$."
1494,Conditional Likelihood Inference on Overlapping Figure-Ground Segment Hypotheses,"In this paper we present an inference procedure for the semantic segmentation of images, namely identifying the spatial layout and class labels of the objects present.Different from many CRF approaches that rely on dependencies modeled with unary and pairwisepixel or superpixel potentials, our method is entirely based on overlap estimates on diverse, potentially overlapping segments in the image. This enablesus to solve the statistical inference problem without using a random field and its associated dependencies. In the approach, posteriorsuperpixels are obtained by intersections of the overlapping regions.Then random variables are defined on such superpixels so that the overlap between each segment and the ground truth can be constructed from them. Inferenceis then performed using conditional maximum likelihood based on an EM formulation. In the PASCAL VOC challenge, the proposed approach is comparable with the 3-times winner SVRSEGM system, but in addition it successfully recognizes additional images with multiple interacting objects."
1495,Phylogenetic inference based on alignment of etymological data,"We apply models developed for population genetics to induce phylogeniesbased on linguistic data, specifically, on a large corpus of geneticallyrelated, or {\em cognate}, words from languages within a languagefamily.  This is achieved via a novel and natural projection of thelinguistic data into genetic primitives.  First, we process the cognatesets to obtain a globally-optimal alignment of the corpus.  Thealignments then serve as input to the model for phylogeneticreconstruction, which produces family tree structures that stronglymatch the ``true'' (or expected) structures.  We place our methods inthe context of those reported in the literature and illustrate themusing data from Uralic language family.  A suite of etymologicalsoftware is released for public use.  "
1496,Hierarchical Model-based Control of Non-Linear Dynamical Systems using Reinforcement Learning,"Non-adaptive methods are currently state of the art in approximating solutions to non-linear optimal control problems.These carry a large computational  cost associated with iterative calculations and have to be solved individually for different start and end points.In addition they may not scale well for real-world problems and require considerable tuning to converge.As an alternative, we present a novel hierarchical approach to non-Linear Control using Reinforcement Learning to choose between locally linear controllers. These are dynamically learnt and repositioned in state space using Linear Dynamic Systems. We illustrate our approach with a solution to a benchmark problem.We show that our approach, Reinforcement Learning Optimal Control (RLOC) competes in terms of solution quality with a state-of-the-art control algorithm iLQR, and offers a robust, flexible framework to address large scale non-linear control problems with unknown dynamics."
1497,Model-based Decision Strategy Recognition,"We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of decision agents on the basis of observation of their actions in solving sequential decision problems.  We model the problem faced by the decision agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of degrees of rationality with respect to optimal forward planning for the MDP.  To recognize the agents, we first use IRL to learn reward functions consistent with observed actions and then use these reward functions as the basis for clustering or classification models.   Experimental studies with GridWorld, a navigation problem,  and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for classifying automated decision rules (e.g., cutoff rule, successive first candidates), even in the presence of action noise and variations in the parameters of the rule.  We propose a new Bayesian IRL approach in which the likelihood function can be interpreted in terms of rationality models.  Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for recognition problems. "
1498,Causal Inference on Spillover Effects by Assignment Strategies and Linear Models,"Estimating causal effects of treatment on a network is challenging because the potential outcome of one unit is affected by the treatment on others (spillovers). To estimate the spillover effects, we introduce a novel estimand and propose two estimation approaches.In a randomization-based approach, we characterize a bias-manipulability tradeoff.Randomizations that reveal more causal information tend to have more bias and vice versa.We propose a novel randomization, namely $\inrx$, which fixes the treatment of $x\%$ of common neighbors in order to control these two competing factors.In model-based approach, assuming additivity of spillover effects results in a linear model, allowing Bayesian inference on the causal estimands. The model not only accounts for network uncertainty but also gives insight on optimal assignment through Fisher information analysis.Empirical results demonstrate the strength of the model-based approach under linear-additive spillover effects,and the strength of randomization under non-linear effects. "
1499,Toward improving the visual stimulus meaning for increasing the P300 detection,"The P300 speller is a well known Brain-Computer Interface paradigm that has been used for over two decades. A new P300 speller paradigm (XP300) is proposed. It includes several characteristics: (i) the items are not intensified by using rows and columns, (ii) the order of the visual stimuli is pseudo-random,(iii) a visual feedback is added on each item to increase the stimulus meaning, which is the main novelty. XP300 has been tested on ten healthy subjects on copy spelling mode, with only eight sensors. It has been compared with the classical P300 paradigm (CP300). With five repetitions, the average recognition rate across subjects is 85.25\% for XP300 and 77.25\% for CP300. Single-trial detection is significantly higher with XP300 by comparing the AUC (Area Under Curve) of the ROC (Receiver Operating Characteristic) curve. The mean AUC is 0.86 for XP300, 0.80 for CP300. More importantly, XP300 has also been judged as more convenient and user-friendly than CP300, hence being able to allow longer sessions."
1500,Direct Optimization of Ranking Measures for Learning to Rank Models,"We present a novel learning algorithm that directly optimizes the ranking measures without resorting to any upper bounds or approximations. Our appraoch is essentially an iterative greedy coordinate descent method in optimization. For each iteration, we only update one parameter along one coordinate with all others fixed. Since the ranking measure is a stepwise functionof a single parameter, we exploit an exhaustive line search algorithmto locate the interval with the best ranking measure along each coordinate.We pick the coordinates that lead to the largest improvement of ranking measures. In order to determine  the optimal value of the parameter for the selected coordinates, we construct a probabilistic framework for the permutation, and maximize the likelihood of top-$m$ ranked documents. This iterative procedure is continued until convergence.We conduct experiments on five datasets selected from Microsoft LETOR datasets, our experimental results show that the proposed DirectRank algorithm outperforms severalwell-known state-of-the-art ranking algorithms."
1501,Message passing with relaxed moment matching,"Bayesian learning is often hampered by large computational expense. As a powerful generalization of popular belief propagation,  expectation propagation (EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP can be sensitive to outliers and suffer from divergence for difficult cases.  To address this issue, we propose a new approximate inference approach, relaxed expectation propagation (REP). It relaxes the moment matching requirement of expectation propagation by adding a relaxation factor into the KL minimization. We penalize this relaxation with a $l_1$ penalty. With this penalty, when two distributions in the relaxed KL divergence are similar, we obtain the exact moment matching; in the presence of outliers, the relaxation factor will used to relax the moment matching constraint. Based on this penalized KL minimization, REP is robust to outliers and can greatly improve the posterior approximation quality over EP. To examine the effectiveness of REP,  we apply it to Gaussian process classification, a task known to be suitable to EP. Our classification results on synthetic and UCI benchmark datasets demonstrate significant improvement of REP over EP and Power EP---in terms of algorithmic stability, estimation accuracy and predictive performance."
1502,Bayesian Learning in Bayesian Networks of Moderate Size,"We study the problem of learning Bayesian network structures from data.Koivisto and Sood (2004)presented a DP algorithm that can computethe exact posterior probabilities of modular features in Bayesian networks of moderate size.In this paper, we propose a new algorithm which is able to efficiently sample network structuresby using the results of the DP algorithm.The network samples can then be used to efficiently estimatethe posteriors of any features.We empirically show that our algorithm considerably outperforms previous state-of-the-art methods."
1503,Convex Adversarial Collective Classification,"Many real-world domains, such as web spam, auctionfraud, and counter-terrorism, are both relational and adversarial.Existing work on adversarial machine learning assumes that theattributes of each instance can be manipulated independently.Collective classification violates this assumption, since objectlabels depend on the labels of related objects as well as their ownattributes.  In this paper, we present a novel method for robustlyperforming collective classification in the presence of a maliciousadversary that can modify up to a fixed number of binary-valuedattributes.  Our method is formulated as a convex quadratic programthat guarantees optimal weights against a worst-case adversary inpolynomial time.  In addition to increased robustness against activeadversaries, this kind of adversarial regularization can also lead toimproved generalization even when no adversary is present.  Inexperiments on real and simulated data, our method consistentlyoutperforms both non-adversarial and non-relational baselines."
1504,Hierarchical Classification with strutured SVMs,"Multi-label classification from hierarchical structure rises from  many real world applications, for instance documents can  be labeled with multiple categories or topics. It is  difficult to learn most probable label configuration efficiently, which involves searching over large combinatory label space, specially label set size is large, namely over 10000.Also  incorporating hierarchical structure has shown notable improvement on prediction accuracyand reduction of learning complexity. We present efficient multi-labellearning method utilizing hierarchical label structure using  linear structuralSVM. Learning is done over entire label space, which infers most probable labels from all possible label configurations. We show an efficient but simpleto implement learning  method using stochastic primal  optimization,PEGASOS, and dynamic programming. The number of variables is independentof the number of instances in the training dataset, which suits large size problem. Experiments show improved results benefiting fromhierarchy."
1505,The trace norm constrained matrix-variate Gaussian process for the prioritization of disease genes,"We propose the trace norm regularized matrix-variate Gaussian process model for low-rank matrix data.A variational constraint is enforced on the model inference; resulting in aposterior matrix-variate Gaussian process with a mean function of constrainedtrace norm. We show that the resulting inference is convex, and the meaninference may be interpreted as the matrix analogue of elastic netregularization; striking a balance between the Hilbert norm and the trace norm.The proposed approach is able to significantly improve the predictionquality when the matrix is partially observed , and all the observedentries have the same value. This is a known failure case for trace normconstrained matrix estimation with Dirac kernels.Our motivating application is the prioritization of candidate disease genes.This tasks seeks to identify new associations between human genes and humandiseases, using known associations as well as kernels induced by gene-geneinteraction networks and disease ontologies,"
1506,Minimax vs. UCT: A Comparative Study Using Synthetic Games,"Upper Confidence bounds for Trees (UCT) and Minimax are two of themost prominent tree-search based adversarial reasoning strategies fora variety of challenging domains, such as Chess and Go. Theircomplementary strengths in different domains have been the motivationfor several works attempting to achieve a better understanding oftheir behaviors. In this paper, rather than using complex games as atestbed for deriving indirect insights into UCT and Minimax, wepropose the study of relatively simple synthetic trees that permitanalysis and afford a greater degree of experimental freedom. Using anovel tree model that does not suffer from the shortcomings ofpreviously studied models, we provide a relatively straightforwardcharacterization of the kinds of games where UCT is superior toMinimax, and vice versa --- to the best of our knowledge, this is thefirst time such an effort has been successful. In particular, we showthat UCT shines in games where heuristics are accurately modeled usingadditive Gaussian noise, that contrary to previous work, earlyterminal states by themselves do not necessarily hurt UCT, and that intrees with heuristic dispersion lag, UCT is outperformed byMinimax."
1507,Accelerating model selection for classification using bandit-based methods,"This paper advances the state of the art on techniques to improve the computa-tional efficiency of cross-validation for model selection. Obtaining procedures formodel selection that are both computationally and statistically efficient is one ofthe core challenges of machine learning research. We demonstrate that significantadvances are made possible via Bayesian bandit techniques and Thompson sam-pling. In doing so, we also provide a much-needed comprehensive comparison ofthese methods."
1508,Laplacian and Distance Covariance Maps: Algorithms for Supervised and Unsupervised Manifold Learning,We propose algorithms for supervised and unsupervised manifold learning. In a supervised setting the dimensionality of the features is reduced while simultaneously preserving the neighborhood structure of the features and also maximizing a statistical measure of dependence known as distance covariance between the features and a response variable. This helps in solving the prediction problem with a low-dimensional set of features. In an unsupervised setting the manifold learning algorithm produces low-dimensional representations of a high-dimensional dataset while preserving the local geometric information. The algorithms are fomulated as majorization minimization and concave convex optimization problems and are iterative.
1510,Interpreting prediction markets: a stochastic approach,"We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution.This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved.In particular, we show that the stationary point of the stochastic process of prices generated by the market is equal to the market's Walrasian equilibrium of classic market analysis.Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour."
1511,MCMC algorithms for near deterministic systems,"Markov Chain Monte Carlo (MCMC) methods are used ubiquitously to generate samples from a probability distribution where exact sampling is not feasible. However, in the presence of near-deterministic components in a joint distribution, it is well-known that mixing is very slow and jumping from one mode to another can take very large amount of time.In this work, we explore one possible fix to this problem by visiting the deterministic problem corresponding to the near-deterministic components. If there exists an efficient algorithm to identify the solutions to this deterministic problem, then we show that it is often possible to design an MCMC algorithm where the proposal distribution is shaped by the efficient algorithm to the deterministic problem."
1512,Auto-WEKA: Automated Selection and Hyper-Parameter Optimization of Classification Algorithms,"There exist many different machine learning algorithms; as most of these can be configured via hyper-parameters, there is a staggeringly large number of possible alternatives overall. There has been a considerable amount of previous work on choosing among learning algorithms and, separately, on optimizing hyper-parameters (mostly when these are continuous and very few in number) in a given use context. However, we are aware of no work that addresses both problems together. Here, we demonstrate the feasibility of using a fully automated approach for choosing both a learning algorithm and its hyper-parameters, leveraging recent innovations in Bayesian optimization. Specifically, we apply this approach to the entire space of classifiers implemented in WEKA, spanning 3 ensemble methods, 14 meta-methods, 30 base classifiers, and a large range of hyper-parameter settings for each of these. On each of 10 data sets from the UCI repository, we show classification performance better than that of complete cross-validation over the default hyper-parameter settings of our 47 classification algorithms. We believe that our approach, which we dubbed Auto-WEKA, will enable typical users of machine learning algorithms to make better choices and thus to obtain better performance in a fully automated fashion."
1513,Copula Discriminant Analysis and Dynamic Time Wrapping for Isolated Acoustic Based Sketch Recognition,"Sketch recognition is an active research field, whose goalis to automatically recognize hand-drawn diagrams drawn on the digital device. However, one interesting finding is that the sound that is generated while people are sketching on the physical surface also provides rich information. Imagine a person sketching simple shapes on the table using fingernail or key, then it is possible for one to guess what that person has drawn by hearing the sound. This technique is cheap and only need one built-in microphone. In this paper, we will investigate this new area, which we called acoustic based sketch recognition, and proposed a novelrecognition algorithm to recognize the sketch through sound. The algorithm works by aggregating the results from copula discriminant analysis and dynamic time wrapping. The result shows that the aggregated decision has higher accuracy than using dynamic time wrapping alone.  "
1514,Risk-Aversion in Multi-armed Bandits,"In stochastic multi--armed bandits the objective is to solve the exploration--exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk--aversion where the objective is to compete against the arm with the best risk--return trade--off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, we investigate their theoretical guarantees, and we report preliminary empirical results."
1515,Confusion-Based Online Learning and a Passive-Aggressive Scheme,"This paper provides the first ---to the best of our knowledge---analysis of online learning algorithms for multiclass problems whenthe {\em confusion} matrix is taken as a performance measure. The workbuilds upon recent and elegant results on noncommutativeconcentration inequalities, i.e. concentration inequalities that applyto matrices, and more precisely to matrix martingales. We do establish generalization bounds for online learningalgorithm and show how the theoretical study motivate the propositionof a new confusion-friendly learning procedure. This learningalgorithm, called \copa (for COnfusion Passive-Aggressive) is apassive-aggressive learning algorithm; it is shown that the updateequations for \copa can be computed analytically, thus allowing theuser from having to recours to any optimization package to implement it. "
1516,Discounting Human Reward: Limitations of Episodicity,"Several studies have demonstrated that human-generated reward can be a powerful feedback signal for control learning algorithms. However, the algorithmic space for learning from human reward has hitherto not been explored systematically. Using model-based reinforcement learning from human reward in goal-based, episodic tasks, we investigate how anticipated future rewards should be discounted to create behavior that performs well on the task that the human trainer intends to teach. We identify a ``positive loops'' problem with low discounting (i.e., high discount factors) for episodic tasks that arises from an observed bias among humans towards giving positive reward. Empirical analysis verifies the existence of the positive loops problem and further indicates that high discounting (i.e., low discount factors) of human reward is necessary in goal-based, episodic tasks. Lastly, an alternate strategy for overcoming the positive loops problem --- converting the episodic task to a continuing one --- is shown to support a wide range of discounting and therefore provide greater algorithmic flexibility."
1517,Unfolding Latent Tree Structures using 4th Order Tensors,"Existing approaches for discovering latent structures often require the number ofhidden states as an input, a quantity usually unknown in practice. In this paper, wepropose a quartet based approachwhich is agnostic to this number. Our key contributionis a novel rank characterization of the tensor associated with the marginaldistribution of a quartet; and this characterization allows us to design a nuclearnorm based test for resolving the quartet relations. We then use this quartet testas a subroutine in a divide-and-conquer algorithm for recovering the latent treestructure. We show that, under certain conditions, the algorithm is consistent andits error probability decays exponentially with increasing sample size. In experiments,we demonstrate that our approach compares favorably to alternatives fordiscovering latent structures. In real world datasets, our approach also discoversmeaningful groupings of variables."
1518,A Hierarchical Motion Model for Short- and Long Range Motion,"Recent work [28] presented a unified model of motion perception inspired by theability of the human visual system to simultaneously perceive both short-rangeand long-range motion in dynamic scenes. Their model differed from conven-tional optical flow techniques because it performed inference compositionally bycombining local hypotheses for optical flow to build non-local hypotheses whichexploit non-local context and hence resolve local ambiguities. The model wasable to account for a range of psychophysical phenomena using random dot kine-matograms as stimuli. In this paper, we extend the model so that it can be appliedto natural image sequences. We test its performance on images from the KITTIoptical flow database [10], which include large ranges of motion. Our performanceis comparable to the performance of more standard algorithms on this database.To make the task even more challenging we introduce additional stimuli, mov-ing balls, into the KITTI dataset to give a richer motion field. We show that thehierarchical model still gives accurate results for these stimuli while more con-ventional algorithms degrade. The algorithm is naturally parallelizable and ourimplementation using a Graphics Processing Card (GPU) has a runtime of a fewminutes."
1519,MODELING THE DYNAMIC SYSTEM OF WIND IN MIDWEST USING MONTE CARLO HIDDEN MARKOV MODEL,"The paper presents a new method of modeling the dynamic of wind in Midwest using HMM clustering coupled with Monte Carlo. After testing the method with simulated data, the method is applied on wind data series in Midwest and get interesting results. The HMM model and cluster result can be used to help the wind forecast for wind mill production. One of the problem with the current method is the time for estimating the model"
1520,Generalized Least Squares for Principled Complex Backups in Temporal Difference Learning,"We derive the form of an estimator that uses generalized least squares to obtaina principled form of the eligibility trace. We show that both the $\lambda$-return and $\gamma$-return can be thought of as assuming that the inverse covariance matrix of $n$-step returns has specific values on its diagonal, and is zero elsewhere.  The new weighting scheme has a single parameter that can easily be set from data,closely matches the empirical covariance weights, and performs very well across several settings of $\gamma$ and $\epsilon$. "
1521,Branch-and-Bound Prediction for Large Data,"For the problems requiring instance-based learning, nearest neighbor is one of the most popular learning methods. While a linear search for the nearest neighbor can be improved upon by building an efficient structure such as a kd-tree, nearest neighbor method suffers from several shortcomings. These shortcomings include lack of generalization, issue of overfitting, sensitivity to outliers, and inability to triage inputs appropriately. Assuming we have probabilistic equivalence of kd-tree, this paper describes how efficiently we are able to find the most likely prediction through our branch-and-bound approach. The experiments show that branch-and-bound becomes more effective as tree size grows and works significantly faster in big trees. In order to argue this, we implicitly build a tree containing more than 30 billion training examples, and launch experiments to find the most likely example among those 30 billion subclasses. The results show that the branch-and-bound approach allows searches that are roughly 1.41 times deeper than possible with linear search for large binary trees. Considering that the branch-and-bound approach we employed here is parallelizable, these results open a possibility to develop a new class of machine learning algorithms for large data that delay some parts of training process to prediction time while efficiently solving critical issues such as overfitting."
1522,Focus of Attention for Linear Predictors,"We present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious. This trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets. We observe that some examples are easier to classify than others, a phenomenon which is characterized by the event when most of the features agree on the class of an example.By stopping the feature evaluation when encountering an easy-to-classify example, the predictor can achieve substantial gains in computation. Our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to include our attentive method we prove that the average number of features computed is $O(\sqrt{n \log \delta^{-0.5}})$ where $n$ is the original number of features, and $\delta$ is the error rate incurred due to early stopping. We demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim, Gisette, and synthetic datasets."
1523,A Convex Extension of MKL to Local Mixtures,"In the context of metric learning, localized multiple kernel learning algorithms have been proposed, featuring a richer model than MKL and improved accuracy. The optimization problem is not solved exactly because of non-convexities or approximations. We present a class of convex problems based on a generalized hinge loss, which can be solved accurately and whose solution is a sparse nonparametric local mixture of kernels. Consistency results of SVMs are generalized, and combined with convexity provide theoretical guarenties. We study the optimization of two examples, and adapt the SMO solver with colsed-form line-search to one of them, achieving fast and scalable learning. Additionally, we extend MKL to our approach, thus combining global and local kernel weights, which further improves performance and scales better with the number of kernels. Promising experiments are conducted on 3 algorithms."
1526,Using context and phonetic features for etymological alignment and reconstruction,"This paper presents methods for investigating etymological data. First, we introducealignment algorithms which explicitly utilize phonetic features and learnlong-range contextual rules that condition recurrent correspondences within a languagefamily. Second, we present an imputation procedure which allows us comparethe quality of alignment models, as well as the goodness of the data sets.We present evaluations to demonstrate that the new model yields improvements inperformance, compared to those previously reported in the literature. We releasethe suite of etymological software for public use."
1527,SMO based Optimization for Quadratic Programming Feature Selection,"Domains such as vision, bioinformatics, web search and web rankings invariably produce datasets where number of features is very large. To carry out classification/regression, feature selection is commonly employed to deal with the curse of dimensionality.  Recently, Quadratic Programming Feature Selection (QPFS) has been shown to outperform many of the existing feature selection methods. A quadratic program solver is used to solve the problem. This requires time complexity cubic in the number of features. Further, the algorithm needs to store the entire feature similarity matrix in memeory.In this paper, we propose an SMO based framework for QPFS (SMO-QPFS). We develop the formulation for working set selection for SMO-QPFS using second order approximations.  Our proposed approach has computaional time quadratic in the number of features in the worst case.In practice, it is shown to take linear time to converge as demonstarted by our experiments.Further, we only need to store feature similarities for the current working set (size $2\times 2$), in contrast to QPFS.This memory saving can be critical for doing feature selection in the datasets with tens of thousands of features.The performance of the SMO-QPFS is evaluated using  eight publicly available benchmark microarray datasets. From our experimental study, it is found that the SMO-QPFS is many times faster than the QPFS approach while retaining the same level of performance."
1528,Bayesian Inference from Non-Ignorable Network Sampling Designs,"Consider individuals interacting in a social network and a response that can be measured on each individual. We are interested in making inferences on a population quantity that is a function of both the response and the social interactions. In this paper, working within Rubin's inferential framework, we introduce a new notion of non-ignorable sampling design for the case of missing covariates. This notion is the key element for developing valid inferences in applications to epidemiology and healthcare in which hard-to-reach populations are sampled using link-tracing designs, including respondent-driven sampling, that carry information about the quantify of interest."
1529,Improved Graph Laplacian by Geometric Consistency,"We consider the problem of setting the parameters used to construct the graph Laplacian for Euclidean data, most notably ?, the kernel bandwidth. We exploit the connection between geometry and the Laplace-Beltrami operator to evaluate how closely the graph Laplacian recovers the geometry of the data. In doing so, we can consider a family of graph Laplacians indexed by a finite number of parameters and select the values that best recover the geometry of the data."
1530,Module propagation: probabilistic frequent subgraph discovery,"A central task in graph data analysis is to discover subgraphs recurring in a single or multiple graphs. Although many graph mining algorithms have been proposed to identify identical subgraphs recurring in multiple clean graphs, it remains an open problem to discover frequent subgraphs that appear in a {\em single} or multiple {\em noisy} graphs. Solving this problem is critical because most real-world graph data is noisy and many subgraphs are repeated only in a single graph. In this paper, we propose a new approach, Module Propagation, as a principled and practical solution to this open problem. Instead of relying on deterministic search as previous graph mining algorithms do, we reformulate the problem in a probabilistic framework.By maximizing the probability of a new Markov random field model via a fast message passing algorithm, we decompose a single or multiple noisy graphs into recurring {densely connected} and possibly overlapped subgraphs efficiently.The procedure is done efficiently  based on a message passing algorithm that explores the sparsity of our new model.We not only demonstrate the advantage of \mp on synthetic data over alternative graph mining algorithms, but also successfully apply it to a novel application of graph mining --- the discovery of modules in chemically similar crystal structures. This application can pave the way for predicting unknown crystal structures, an important yet challenging task in computational materials science."
1531,Cardinality Restricted Boltzmann Machines ,"The Restricted Boltzmann Machine (RBM) is a popular density model thatis also good for extracting features.A main source of tractability in RBM models is the model's assumptionthat given an input, hidden units activate independently from one another.Sparsity and competition in the hidden representation is believed tobe beneficial,and while an RBM with competition among its hidden units would acquire someof the attractive properties of sparse coding, such constraints are not added due to the widespread belief that the resulting model would become intractable.In this work, we show how a dynamic programming algorithm developed in 1981can be used to implement exact sparsity in the RBM's hidden units.We then expand on this and show how to pass derivatives through alayer of exact sparsity, which makes it possible to fine-tune adeep belief network (DBN) consisting of RBMs with sparse hiddenlayers.  We show that sparsity in the RBM's hidden layer improves theperformance of both the pre-trained representations and of thefine-tuned model."
1533,Learning Multi-Label Scene Classification using Asymmetric SIMPLS Classifier,"In this paper, we propose asymmetric SIMPLS classifier for automatic learning multi-label scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). We show that asymmetric SIMPLS is a sub-optimal solution of a joint learning framework in which we perform dimensionality reduction and multi-label classification simultaneously. Asymmetric SIMPLS and other five state-of-the-artlearning algorithms are evaluated and compared on a public dataset, which consists of a set of 2000 images with 5 clusters of basic scenes. Experimental results validate the effectiveness of our asymmetric SIMPLS compared toother methods. Furthermore, our work appears to generalize to other classification problems of the same nature."
1534,Online Multi-modal Similarity Learning for Large-scale Applications,"In many real-word scenarios, e.g., multimedia applications, data often originates from multiple heterogeneous sources or are given by diverse types of representation, which is referred to as multi-modal data. The definition of similarity between any two items/objects on multi-modal data is a key challenge encountered by many real-world applications, including multimedia information retrieval. In this paper, we present a novel online learning framework for learning similarity on multi-modal data through the combination of multiple kernels. We propose fast online multi-modal similarity learning (OMSL) algorithms which are significantly more efficient and scalable than the state-of-the-art techniques. Extensive experiments were conducted on large-scale multi-modal image retrieval applications. "
1535,Information Driven Exploration using Poisson Sampling over Ising Marginals,"We describe an information-gathering approach for exploring an unknown scene using a range sensor. It relies on an efficient approximation of the uncertainty in the map due to visibility (occlusions). The reduction in uncertainty due to a control action is represented on an Ising model using a Poisson covering. Our algorithm improves the performance of recent visibility-based planning approaches that come with guaranteed performance bounds on the expected path length to complete exploration, and extends them to allow exploration of an unbounded region, with an extension of the bounds to exploration rate rather than complete exploration."
1457,Learning Hierarchical Compositional Models in the Presence of Clutter,"Our goal is to identify hierarchical compositional models from highlycluttered data. The data to learn from are assumed to be imperfect intwo respects. Firstly, large portion of the data is coming frombackground clutter. Secondly, data generated by a recursivecompositional model are subject to random replacements of correctdescendants by randomly chosen ones at every level of the hierarchy.In this paper, we show the limits and capabilities of an approachwhich is based on likelihood maximization. The algorithm makesexplicit probabilistic assignments of individual data to compositionalmodel and background clutter. It uses these assignments to effectivelyfocus on the data coming from the compositional model and iterativelyestimate their compositional structure."
