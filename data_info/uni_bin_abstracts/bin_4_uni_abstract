title,abstract
High-Dimensional Feature Selection by Kernel-Based Feature-Wise Non-Linear Lasso,"The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features."
Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information,"The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high non-linearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets."
Can We Recognize Tiger by Bus Images? ?Robust and Discriminative Self-Taught Image Categorization,"The lack of training data is a common challenge in many real-world image categorization problems, which is often tackled by semi-supervised learning or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught image categorization approach to utilize any unlabeled images (e.g., those randomly downloaded from Internet) without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled images. Because our new objective involves non-smooth terms in both the loss function and the regularization, it is difficult to solve in general. Thus, we derive an efficient iterative algorithm to solve the optimization problem, and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.  "
"Robust Linear Discriminant Analysis Using Ratio Minimization of $l_{1,2}$-Norms","Traditional Linear Discriminant Analysis (LDA) minimizes the ratio of squared $l_2$-norms, which is sensitive to outliers. In recent research, many $l_1$-norm based robust learning models were proposed. However, so far there is no existing work to utilize $l_1$-norm based objective for LDA, due to the difficulty of $l_1$-norm ratio optimization. Meanwhile, trivially replacing $l_2$-norms by $l_1$-norms in LDA objective introduces the $l_1$-norm maximization problem and doesn't provide the robustness. In this paper, we propose a novel robust LDA formulation based on the $l_{1,2}$-norm ratio minimization. Minimizing the $l_{1,2}$-norm ratio is a much more challenging problem than the traditional methods, and existing optimization algorithms cannot solve such a non-smooth terms ratio problem. We derive a new efficient algorithm to solve this challenging problem, and provide the theoretical analysis on the algorithm convergence. Our algorithm is easy to be implemented, and converges fast in practice with the same computational complexity as the trace ratio LDA. Extensive experiments on both synthetic data and nine real benchmark data sets show the effectiveness of the proposed robust LDA method."
"Robust Linear Discriminant Analysis Using Ratio Minimization of $l_{1,2}$-Norms","Traditional Linear Discriminant Analysis (LDA) minimizes the ratio of squared $l_2$-norms, which is sensitive to outliers. In recent research, many $l_1$-norm based robust learning models were proposed. However, so far there is no existing work to utilize $l_1$-norm based objective for LDA, due to the difficulty of $l_1$-norm ratio optimization. Meanwhile, trivially replacing $l_2$-norms by $l_1$-norms in LDA objective introduces the $l_1$-norm maximization problem and doesn't provide the robustness. In this paper, we propose a novel robust LDA formulation based on the $l_{1,2}$-norm ratio minimization. Minimizing the $l_{1,2}$-norm ratio is a much more challenging problem than the traditional methods, and existing optimization algorithms cannot solve such a non-smooth terms ratio problem. We derive a new efficient algorithm to solve this challenging problem, and provide the theoretical analysis on the algorithm convergence. Our algorithm is easy to be implemented, and converges fast in practice with the same computational complexity as the trace ratio LDA. Extensive experiments on both synthetic data and nine real benchmark data sets show the effectiveness of the proposed robust LDA method."
Shaping for a Differential Game of Guarding a Territory,This paper applies fuzzy reinforcement learning to a three-player differential game of guarding a territory. A shaping reward function is designed to help the defenders learn their Nash equilibrium strategies. Simulation results illustrate the performance of fuzzy actor-critic learning with and without the shaping reward function in a three-player differential game of guarding a territory.
Shaping for a Differential Game of Guarding a Territory,This paper applies fuzzy reinforcement learning to a three-player differential game of guarding a territory. A shaping reward function is designed to help the defenders learn their Nash equilibrium strategies. Simulation results illustrate the performance of fuzzy actor-critic learning with and without the shaping reward function in a three-player differential game of guarding a territory.
Bayesian Hierarchical Reinforcement Learning,"We describe an approach to incorporating Bayesian priors in the maxq framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, given sensible priors, (ii) task hierarchies and Bayesian priors can be complementary sources of information, and using both sources is better than either alone, (iii) taking advantage of the structural decomposition induced by the task hierarchy significantly reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to automatic learning of hierarchically optimal rather than recursively optimal policies. This paper is a resubmission of ICML paper number 660, where we have addressed the reviewer requests for experiments with additional baselines."
Bayesian Hierarchical Reinforcement Learning,"We describe an approach to incorporating Bayesian priors in the maxq framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, given sensible priors, (ii) task hierarchies and Bayesian priors can be complementary sources of information, and using both sources is better than either alone, (iii) taking advantage of the structural decomposition induced by the task hierarchy significantly reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to automatic learning of hierarchically optimal rather than recursively optimal policies. This paper is a resubmission of ICML paper number 660, where we have addressed the reviewer requests for experiments with additional baselines."
Multi-View Clustering and Feature Learning via Structured Sparsity-Inducing Norms,"Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which make the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods."
A Schrodinger formalism for simultaneously computing the Euclidean distance transform and its gradient density,"In this paper, we leverage the well-known Hamilton-Jacobi to Schrodinger connection to present a unified framework for computing both the Euclidean distance function and its gradient density in two dimensions. We introduce a novel Schrodinger wave function for representing the Euclidean distance transform from a discrete set of points. An approximate distance transform is computed from themagnitude of the wave function while the gradient density is estimated from the Fourier transform of the phase of the wave function. In addition to its simplicity and efficient O(N log N)computation, we prove that the wave function-based density estimator increasingly, closely approximatesthe distance transform gradient density (as a free parameter approaches zero) without requiring the true distance function."
A Schrodinger formalism for simultaneously computing the Euclidean distance transform and its gradient density,"In this paper, we leverage the well-known Hamilton-Jacobi to Schrodinger connection to present a unified framework for computing both the Euclidean distance function and its gradient density in two dimensions. We introduce a novel Schrodinger wave function for representing the Euclidean distance transform from a discrete set of points. An approximate distance transform is computed from themagnitude of the wave function while the gradient density is estimated from the Fourier transform of the phase of the wave function. In addition to its simplicity and efficient O(N log N)computation, we prove that the wave function-based density estimator increasingly, closely approximatesthe distance transform gradient density (as a free parameter approaches zero) without requiring the true distance function."
Adaptive Sparseness for function learning using Parametric and Nonparametric Bayesian LASSO Methodology,"One of the most important problems in supervised learning is that of accurately inferring functional mappings based on (typically)  high dimensional training data.  Learning is accomplished by estimating parameters which weight the features used in the inference.  To achieve good generalization,  it is considered desirable, while retaining accuracy, to obtain sparse solutions to this problem (i.e. solutions which properly select the parameters which need to be estimated). The lasso methodology has been used extensively  to solve this problem;  it controls both complexity and accuracy by adding a regularization term to the least squares fit. EM methodology provides non-adaptive solutions which require manual control of complexity.  Parametric Bayesian approaches to the lasso adopt parametric priors to learn the function parameters;  these serve to adaptively control for both the complexity and accuracy of the learned function.   Typically, the function parameters have complicated relationships with one another and with the data.  Parametric priors frequently fail to accurately learn these relationships.Learning in the aforementioned settings can be handled using prior distributions which stipulate mixture models with a potentially infinite number of components;  these are known as nonparametric Bayesian priors.  Dirichlet process priors are a particular example of a nonparametric Bayesian prior.  We propose a Hierarchical Bayesian function learning algorithm which employs Dirichlet process priors to infer functional mappings. Model comparisons between algorithms employing Dirichlet process priors, models employing parametric priors, and models employing expectation-maximization algorithms demonstrate the superiority of the former.  An example illustrates the advantages and disadvantages which accrue from using these models for function learning. "
Continuous-weight neural networks for learning from sparse continuous dictionary representations,"Sparse feature representations from overdetermined dictionaries are commonly seen in digital signal processing fields, including computer vision and audio processing. Examples of such representations are sinusoidal models, wavelet representations, and interest point sets. Due to the high dimension of the dictionaries involved and the variable number of features per representation, current machine learning algorithms are not designed to train well on these feature sets. In this paper, we present continuous-weight neural networks, a novel machine learning paradigm with universal continuous approximation capabilities that can directly train on these sparse dictionary representations. The algorithm thus allows for sparser feature sets in numerous multimedia applications, and thus improved learning quality in many of these cases, as feature sparsity is strongly correlated with learning robustness. This paper additionally presents a class of training algorithms for the paradigm by demonstrating how to efficiently compute the gradient of the parametric model."
Squared-loss Mutual Information Regularization,"The information maximization principle, which prefers classifiers that maximize an information measure between data and labels, is a useful probabilistic alternative to the low-density separation principle. In this paper, we specify the squared-loss mutual information (SMI) as the information measure to be maximized and propose SMI regularization (SMIR) for semi-supervised classification. SMIR offers all of the following four abilities to semi-supervised algorithms: analytical solution, out-of-sample and multi-class classification, and probabilistic output. Furthermore, SMIR results in learning algorithms with data-dependent risk bounds that even incorporate the information of unlabeled data. Experiments demonstrate that SMIR compares favorably with state-of-the-art information-theoretic regularization approaches in terms of both accuracy and computational efficiency. "
Selective Labeling via Error Bound Minimization,"In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods."
Low Rank Tensor Completion with Spatio-Temporal Consistency,"Video completion is a computer vision technique to recover the missing values in video sequences by filling the unknown regions with the known information.  In recent research, tensor completion, a generalization of matrix completion for higher order data, emerges as a new solution to estimate the missing information in video with the assumption that the video frames are homogenous and correlated.  However, each video clip often stores the heterogeneous episodes and the correlations among all video frames are not high. Thus, the regular tenor completion methods are not suitable to recover the video missing values in practical applications.  To solve this problem, we propose a novel spatially-temporally consistent tensor completion method for recovering the video missing data. Instead of minimizing the average of the trace norms of all matrices unfolded along each mode in a tensor data, we introduce a new smoothness regularization along video time direction to utilize the temporal information between consecutive video frames. Meanwhile, we also minimize the trace norm of each individual video frame to employ the spatial correlations among pixels. Different to previous tensor completion approaches, our new method can keep the spatio-temporal consistency in video and do not assume the global correlation in video frames. Thus, the proposed method can be applied to the general and practical video completion applications. Our method shows promising results in all evaluations on 3D biomedical image sequence and video benchmark data sets.  "
Learning global properties of scene images from conditional correlational structure,"Scene images with similar spatial layout properties often display characteristic statistical regularities on a global scale. In order to develop an efficient code for these global properties that reflects their inherent regularities, we train a hierarchical probabilistic model to infer conditional correlational information from scene images. Fitting a model to a scene database yields a compact representation of global information that encodes salient visual structures with low dimensional latent variables. Using perceptual ratings and scene similarities based on spatial layouts of scene images, we demonstrate that the model representation is more consistent with perceptual similarities of scene images than the metrics based on the state-of-the-art visual features. "
Efficient Sample Reuse in Policy Gradients with Parameter-based Exploration,"The policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control.A common challenge in this scenario is how to stabilizepolicy gradient estimates for reliable policy updates.In this paper, we combine the following three ideas and givea highly stable and practical policy gradient method:(a) the policy gradients with parameter based exploration,which is a recently proposed policy search method with high stability,(b) an importance sampling technique, which allows us to reuse previously gathered data in a consistent way,and (c) an optimal baseline, which minimizes the variance of gradient estimates with their unbiasedness being maintained.For the proposed method, we give theoretical analysis of the variance of gradient estimates and show its usefulness through experiments."
Discriminatively Activated Sparselets,"As the number of object classes becomes large, redundancy amonglearned object models increases substantially and thus naturallymotivates the idea of compact intermediate representations that canbe shared across classes for efficient multiclass inference. Recently,a new universal intermediate representation for multiclass objectdetection, sparselets, was introduced yieldingone to two orders of magnitude reduction in inference time andenabling real-time multiclass object detection. However, as computationalefficiency is gained by making the sparselet activations increasingly sparse, the task performance of reconstructive sparseletmodels degrades unfavorably.  This paper provides a generalformalism where sparselet activations are learned discriminativelyin a structured output prediction framework. Our experimental resultson multiclass object detection and multiclass image classificationdemonstrate that the proposed discriminative sparselet activationsmaintain high task performance while achieving greater sparsity,which in turn significantly improves inference efficiency."
An Analytic and Empirical Evaluation of Return-on-Investment-Based Active Learning,"Return-on-Investment (ROI) is a cost-conscious approach to active learning (AL)that considers both estimates of cost and of benefit in active sample selection.In this paper, we investigate the conditions for successful cost-conscious ALusing ROI by proving the conditions under which ROI would be optimal. We thenempirically measure the degree to which optimality is jeopardized in practicewhen the conditions are violated. We find that the more linearly related abenefit estimator is to true benefit, the better it performs when paired with animperfect cost estimate in ROI.  Lastly, we use our analysis to explain themixed results of previous work. Our results show that ROI can indeedsuccessfully reduce total annotation costs."
An Analytic and Empirical Evaluation of Return-on-Investment-Based Active Learning,"Return-on-Investment (ROI) is a cost-conscious approach to active learning (AL)that considers both estimates of cost and of benefit in active sample selection.In this paper, we investigate the conditions for successful cost-conscious ALusing ROI by proving the conditions under which ROI would be optimal. We thenempirically measure the degree to which optimality is jeopardized in practicewhen the conditions are violated. We find that the more linearly related abenefit estimator is to true benefit, the better it performs when paired with animperfect cost estimate in ROI.  Lastly, we use our analysis to explain themixed results of previous work. Our results show that ROI can indeedsuccessfully reduce total annotation costs."
"Top-k Feature Selection via ?2,0-Norm Constraint","Real-world applications, such as bioinformatics, often need select top-$k$ features in the classification tasks. Previous sparse learning based feature selection methods impose the sparsity regularization to learn the features weights and rank them to select the top-$k$ features. Because the ranking weights were learned not for the exact top-$k$ features, such feature selection methods may not get the optimal results. We propose a novel and robust exact top-$k$ feature selection approach using an explicit $\ell_{2,0}$-norm constraint without any extra parameter. An efficient algorithm based on augmented Lagrangian method is derived to solve the $\ell_{2,0}$-norm constrained objective to find out the stable local solution. Extensive experiments on four biological datasets show that although our proposed model is not a convex problem, it outperforms the approximate convex counterparts and state-of-the-art feature selection methods in terms of classification accuracy on two popular classifiers. Because the regularization parameter of our method has explicit meaning (in bioinformatics applications $k$ is often fixed), \emph{i.e.} the number of selected feature, it avoids the burden of tuning the parameter and is a pragmatic feature selection method."
"Top-k Feature Selection via ?2,0-Norm Constraint","Real-world applications, such as bioinformatics, often need select top-$k$ features in the classification tasks. Previous sparse learning based feature selection methods impose the sparsity regularization to learn the features weights and rank them to select the top-$k$ features. Because the ranking weights were learned not for the exact top-$k$ features, such feature selection methods may not get the optimal results. We propose a novel and robust exact top-$k$ feature selection approach using an explicit $\ell_{2,0}$-norm constraint without any extra parameter. An efficient algorithm based on augmented Lagrangian method is derived to solve the $\ell_{2,0}$-norm constrained objective to find out the stable local solution. Extensive experiments on four biological datasets show that although our proposed model is not a convex problem, it outperforms the approximate convex counterparts and state-of-the-art feature selection methods in terms of classification accuracy on two popular classifiers. Because the regularization parameter of our method has explicit meaning (in bioinformatics applications $k$ is often fixed), \emph{i.e.} the number of selected feature, it avoids the burden of tuning the parameter and is a pragmatic feature selection method."
Statistical modeling of indoor scenes and objects,"We develop a Bayesian generative model for understanding indoor scenes.  While state-of-the-art methods approximate objects in these environments with gross 3D geometry (e.g., bounding boxes), we propose using geometric representations with a finer granularity. For example, we model a table as a set of four legs and a top.  Such models enhance recognition and reconstruction, and enable more refined use of appearance for scene understanding. In particular, we introduce a new likelihood function that rewards 3D object hypotheses whose 2D projection are more uniform in color distribution. Such a model would be confused if used on a bounding box for a concave object like a table. We present results showing the positive effect of each of these innovations.  The performance of our method is comparable to, and often exceeds, that of state-of-the-art methods on the scene surface orientation task, as well as object recognition, as evaluated on the two bench mark data sets used in this domain. "
Statistical modeling of indoor scenes and objects,"We develop a Bayesian generative model for understanding indoor scenes.  While state-of-the-art methods approximate objects in these environments with gross 3D geometry (e.g., bounding boxes), we propose using geometric representations with a finer granularity. For example, we model a table as a set of four legs and a top.  Such models enhance recognition and reconstruction, and enable more refined use of appearance for scene understanding. In particular, we introduce a new likelihood function that rewards 3D object hypotheses whose 2D projection are more uniform in color distribution. Such a model would be confused if used on a bounding box for a concave object like a table. We present results showing the positive effect of each of these innovations.  The performance of our method is comparable to, and often exceeds, that of state-of-the-art methods on the scene surface orientation task, as well as object recognition, as evaluated on the two bench mark data sets used in this domain. "
Statistical modeling of indoor scenes and objects,"We develop a Bayesian generative model for understanding indoor scenes.  While state-of-the-art methods approximate objects in these environments with gross 3D geometry (e.g., bounding boxes), we propose using geometric representations with a finer granularity. For example, we model a table as a set of four legs and a top.  Such models enhance recognition and reconstruction, and enable more refined use of appearance for scene understanding. In particular, we introduce a new likelihood function that rewards 3D object hypotheses whose 2D projection are more uniform in color distribution. Such a model would be confused if used on a bounding box for a concave object like a table. We present results showing the positive effect of each of these innovations.  The performance of our method is comparable to, and often exceeds, that of state-of-the-art methods on the scene surface orientation task, as well as object recognition, as evaluated on the two bench mark data sets used in this domain. "
Statistical modeling of indoor scenes and objects,"We develop a Bayesian generative model for understanding indoor scenes.  While state-of-the-art methods approximate objects in these environments with gross 3D geometry (e.g., bounding boxes), we propose using geometric representations with a finer granularity. For example, we model a table as a set of four legs and a top.  Such models enhance recognition and reconstruction, and enable more refined use of appearance for scene understanding. In particular, we introduce a new likelihood function that rewards 3D object hypotheses whose 2D projection are more uniform in color distribution. Such a model would be confused if used on a bounding box for a concave object like a table. We present results showing the positive effect of each of these innovations.  The performance of our method is comparable to, and often exceeds, that of state-of-the-art methods on the scene surface orientation task, as well as object recognition, as evaluated on the two bench mark data sets used in this domain. "
QuickBoost - Quickly Training Boosted Decision Trees,"Boosting is one of the most popular and effective learning techniques in use today. While exhibiting fast classification speed at test time, the training of an ensemble or cascade of boosted weak classifiers is slow, making it impractical for applications with real-time training requirements. In this paper, we propose a principled approach to overcome this drawback. We compute and prove a bound on the error of a weak classifier given its error on a subset of the training data;the bound may be used to prune unpromising weak classifiers early on. We propose a fast training algorithm that exploits this bound, yielding up to a 10-fold speedup at no cost in the final performance of the classifier."
QuickBoost - Quickly Training Boosted Decision Trees,"Boosting is one of the most popular and effective learning techniques in use today. While exhibiting fast classification speed at test time, the training of an ensemble or cascade of boosted weak classifiers is slow, making it impractical for applications with real-time training requirements. In this paper, we propose a principled approach to overcome this drawback. We compute and prove a bound on the error of a weak classifier given its error on a subset of the training data;the bound may be used to prune unpromising weak classifiers early on. We propose a fast training algorithm that exploits this bound, yielding up to a 10-fold speedup at no cost in the final performance of the classifier."
QuickBoost - Quickly Training Boosted Decision Trees,"Boosting is one of the most popular and effective learning techniques in use today. While exhibiting fast classification speed at test time, the training of an ensemble or cascade of boosted weak classifiers is slow, making it impractical for applications with real-time training requirements. In this paper, we propose a principled approach to overcome this drawback. We compute and prove a bound on the error of a weak classifier given its error on a subset of the training data;the bound may be used to prune unpromising weak classifiers early on. We propose a fast training algorithm that exploits this bound, yielding up to a 10-fold speedup at no cost in the final performance of the classifier."
QuickBoost - Quickly Training Boosted Decision Trees,"Boosting is one of the most popular and effective learning techniques in use today. While exhibiting fast classification speed at test time, the training of an ensemble or cascade of boosted weak classifiers is slow, making it impractical for applications with real-time training requirements. In this paper, we propose a principled approach to overcome this drawback. We compute and prove a bound on the error of a weak classifier given its error on a subset of the training data;the bound may be used to prune unpromising weak classifiers early on. We propose a fast training algorithm that exploits this bound, yielding up to a 10-fold speedup at no cost in the final performance of the classifier."
Semantic GIST: Probabilistic Modelling of Scenes using Scenelet,"In this paper, we propose a probabilistic modeling framework for scenes to encode semantic information of images into a compact Semantic Gist representation. The representation is based on a key concept called {\em scenelets}, which serves as building blocks for scenes. We learn these scenelets using a topic model to group correlated objects such that the learned set of scenelets maximally retain the semantic saliency of images in terms of KL-divergence. Our model also integrates information from individual discriminative object detectors and global image features by coding them as priors. Empirical results demonstrate the power of our model. We first show that using a small set of scenelet classifiers, we can predict the existence of a large set of objects without running individual object detectors.Furthermore, we can even predict the presence of objects without running large sets of object detectors by MAP estimation using our model.We also show that the framework can improve the performance of individual detectors by incorporating the contextual object and scenelet information. Experiments on challenging datasets including PASCAL and SUN09  demonstrate that our model outperforms other state-of-the-art ones."
Density-Difference Estimation,"We address the problem of estimating the difference betweentwo probability densities.A naive approach is a two-step procedure of first estimating two densities separatelyand then computing their difference.However, such a two-step procedure does not necessarily work wellbecause the first step is performed without regard to the second stepand thus a small error incurred in the first stage can cause a big error in the second stage.In this paper, we propose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities.We derive a non-parametric finite-sample error boundfor the proposed single-shot density-difference estimatorand show that it achieves the optimal convergence rate.We experimentally demonstrate the usefulness of the proposed methodin class-prior estimation and change-point detection."
Clustering Aggregation as Maximum-Weight Independent Set,"We formulate clustering aggregation as a special instance of Maximum-Weight Independent Set (MWIS) problem. For a given data-set, an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters. The vertices, which represent the distinct clusters, are weighted by an internal index measuring both cohesion and separation. The edges connect the vertices whose corresponding clusters overlap. Intuitively, an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together. We formalize this intuition as the MWIS problem on the attributed graph, i.e., finding the heaviest subset of mutually non-adjacent vertices.This MWIS problem exhibits a special structure. Since the clusters of each input clustering form a partition of the dataset, the vertices corresponding to each clustering form a maximal independent set (MIS) in the attributed graph. We propose a variant of simulated annealing method that takes advantage of this special structure. Our algorithm starts from each MIS, which is close to a distinct local optimum of the MWIS problem, and utilizes a local search heuristic to explore its neighborhood in order to find the MWIS. Extensive experiments on many challenging datasets show that: 1. our approach to clustering aggregation automatically decides the optimal number of clusters; 2. it does not require any parameter tuning for the underlying clustering algorithms; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance; 4. it is robust against moderate or even bad input clusterings."
Clustering Aggregation as Maximum-Weight Independent Set,"We formulate clustering aggregation as a special instance of Maximum-Weight Independent Set (MWIS) problem. For a given data-set, an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters. The vertices, which represent the distinct clusters, are weighted by an internal index measuring both cohesion and separation. The edges connect the vertices whose corresponding clusters overlap. Intuitively, an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together. We formalize this intuition as the MWIS problem on the attributed graph, i.e., finding the heaviest subset of mutually non-adjacent vertices.This MWIS problem exhibits a special structure. Since the clusters of each input clustering form a partition of the dataset, the vertices corresponding to each clustering form a maximal independent set (MIS) in the attributed graph. We propose a variant of simulated annealing method that takes advantage of this special structure. Our algorithm starts from each MIS, which is close to a distinct local optimum of the MWIS problem, and utilizes a local search heuristic to explore its neighborhood in order to find the MWIS. Extensive experiments on many challenging datasets show that: 1. our approach to clustering aggregation automatically decides the optimal number of clusters; 2. it does not require any parameter tuning for the underlying clustering algorithms; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance; 4. it is robust against moderate or even bad input clusterings."
Robust L1 Normalized Cut and Symmetric Nonnegative Matrix Factorization,"Spectral clustering is widely used in practice. However existing formulation is prone to large errors. In this paper, we first transform it into a matrix decompositionproblem and then propose a robust formulation using L1-norm. This leads to L1 normalized cut and L1 symmetric nonnegative matrix factorization (SNMF) models. We derive computational algorithms for L1 normalized cut. We also providevery efficient updating rules for L1 SNMF with rigorous convergence analysis. Extensive experiments on 6 datasets with significant data corruption/occlusionshow that L1 normalized cut and L1 SNMF provide consistently better clustering results as compared to standard methods and other L1-type functionals."
A System for Predicting Action Content On-Line and in Real Time before Action Onset in Humans ? an Intracranial Study,"The ability to predict action content from neural signals in real time before action onset has been long sought in the neuroscientific study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a ?matching-pennies? game against either the experimenter or a computer. In each trial, subjects were given a 5s countdown, after which they had to raise their left or right hand immediately as the ?go? signal appeared on a computer screen. They won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The working hypothesis of this experiment was that neural precursors of the subjects? decisions precede action onset and potentially also the awareness of the decision to move, and that these signals could be detected in intracranial local field potentials (LFP).We found that low-frequency LFP signals from a combination of 10 channels, especially bilateral anterior cingulate cortex and supplementary motor area, were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5s before the go signal with 68?3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 channels simultaneously, and tested it on retrospective data from 6 patients. On average, we could predict the correct hand choice in 80% of the trials, which rose to 90% correct if we let the system drop about 1/3 of the trials on which it was less confident. Our system demonstrates ? for the first time ? the feasibility of accurately predicting a binary action in real time for patients with intracranial recordings, well before the action occurs."
Phase vs. amplitude ? learning from subjective image quality assessment,"In frequency-based representation of images, phase and amplitude convey complementary information. Phase has been regarded as dominating the image appearance, however power (amplitude) spectra have recently been found useful for image classification. In the primary visual cortex (V1), simple cells are sensitive to and thereby encode the phase of visual stimuli, while complex cells, being majority in V1, show phase-invariance and encode the energy (magnitude) of simple cells? spikes. In this paper, we attempt to quantitatively exploit the relative importance of phase and amplitude to visual perception by learning from subjective image quality assessment. We designed an image quality metric based on the weighted combination of the amplitude and phase errors, and determined the weights so as to maximize the prediction accuracy of the metric over subjectively- rated databases, where a joint optimization over multiple databases strengthened the reliability of weights. The results confirm that: 1) both the phase and the amplitude are necessary for image quality assessment; 2) the amplitude becomes more important at the finest image scale while the phase dominates at the coarser scale. Moreover, the multiplicative combination of the amplitude and phase errors plausibly interprets the visual perception on negative images."
Perfect Dimensionality Recovery by Variational Bayesian PCA,"The variational Bayesian (VB) approach isone of the best tractable approximations to the Bayesian estimation,and it was demonstrated to perform well in many applications.However, its good performance was not fully understood theoretically.For example, VB sometimes produces a sparse solution,which is regarded as a practical advantage of VB,but such sparsity is hardly observed in the rigorous Bayesian estimation.In this paper, we focus on probabilistic PCA andgive more theoretical insight into the empirical success of VB.More specifically, for the situation where the noise variance is unknown,we derive a sufficient condition for perfect recovery of the true PCAdimensionalityin the large-scale limitwhen the size of an observed matrixgoes to infinity with its column-row ratio fixed.In our analysis, we obtain bounds for a noise variance estimatorand simple closed-form solutions for other parameters,which themselves are actually very useful for better implementation of VB-PCA."
Sparse Additive Matrix Factorization for Robust PCA,"Principal component analysis (PCA) can be regarded as approximating adata matrix witha low-rank one by imposing sparsity on its singular values,and its robust variant further captures sparse noise.In this paper, we extend such sparse matrix learning methods,and propose a novel framework called sparse additive matrix factorization(SAMF).SAMF systematically inducesvarious types of sparsityby the so-called model-induced regularization in the Bayesian framework.We  propose an iterative algorithm calledthe mean update (MU) for the variational Bayesian approximation to SAMF, which gives the global optimal solution for a large subset of parameters in each step.We demonstrate the usefulness of our  methodon artificial dataand the foreground/background video separation."
A Linearly Convergent First-order Algorithm for Total Variation Minimization in Image Processing,We introduce a new formulation for total variation minimization in image denoising. We present a linearly convergent first-order method for solving this reformulated problem and show that it possesses a nearly dimension-independent iteration complexity bound.
A Linearly Convergent First-order Algorithm for Total Variation Minimization in Image Processing,We introduce a new formulation for total variation minimization in image denoising. We present a linearly convergent first-order method for solving this reformulated problem and show that it possesses a nearly dimension-independent iteration complexity bound.
Random Projections for Support Vector Machines,"Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be a data matrix of rank $\rho$, representing $n$ points in $\mathbb{R}^d$. The linear support vector machine constructs a hyperplane separator that maximizes the  1-norm soft margin. We develop a new \emph{oblivious} dimension reduction technique which is precomputed and can be applied to any input matrix \math{\mathbf{X}}. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \math{\epsilon}-\emph{relative error}, ensuring comparable generalization as in the original space. We present extensive experiments in support of the theory."
Random Projections for Support Vector Machines,"Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be a data matrix of rank $\rho$, representing $n$ points in $\mathbb{R}^d$. The linear support vector machine constructs a hyperplane separator that maximizes the  1-norm soft margin. We develop a new \emph{oblivious} dimension reduction technique which is precomputed and can be applied to any input matrix \math{\mathbf{X}}. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \math{\epsilon}-\emph{relative error}, ensuring comparable generalization as in the original space. We present extensive experiments in support of the theory."
Random Projections for Support Vector Machines,"Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be a data matrix of rank $\rho$, representing $n$ points in $\mathbb{R}^d$. The linear support vector machine constructs a hyperplane separator that maximizes the  1-norm soft margin. We develop a new \emph{oblivious} dimension reduction technique which is precomputed and can be applied to any input matrix \math{\mathbf{X}}. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \math{\epsilon}-\emph{relative error}, ensuring comparable generalization as in the original space. We present extensive experiments in support of the theory."
Compressive Sensing MRI with Wavelet Tree Sparsity,"In Compressive Sensing Magnetic Resonance Imaging (CS-MRI), one canreconstruct a MR image with good quality from only a small number ofmeasurements. This can significantly reduce MR scanning time.According to structured sparsity theory, the measurements can be further reduced to$\mathcal{O}(K+\log n)$ for tree-sparse data instead of$\mathcal{O}(K+K\log n)$ for standard $K$-sparse data with length $n$.However, few of existing algorithms has utilized thisfor CS-MRI, while most of them use Total Variation andwavelet sparse regularization. On the other side, some algorithmshave been proposed for tree sparsity regularization, but few of them has validated  the benefit of tree structure in CS-MRI. In this paper, we propose a fastconvex optimization algorithm to improve CS-MRI.  Waveletsparsity, gradient sparsity and tree sparsity are all considered in our model for real MR images.The original complex problem is decomposed to three simpler subproblemsthen each of the subproblems can be efficiently solved with an iterative scheme.Numerous experiments have been conducted and show that the proposedalgorithm outperforms the state-of-the-art CS-MRIalgorithms, and gain better reconstructions results on real MR images than generaltree based solvers or algorithms."
Compressive Sensing MRI with Wavelet Tree Sparsity,"In Compressive Sensing Magnetic Resonance Imaging (CS-MRI), one canreconstruct a MR image with good quality from only a small number ofmeasurements. This can significantly reduce MR scanning time.According to structured sparsity theory, the measurements can be further reduced to$\mathcal{O}(K+\log n)$ for tree-sparse data instead of$\mathcal{O}(K+K\log n)$ for standard $K$-sparse data with length $n$.However, few of existing algorithms has utilized thisfor CS-MRI, while most of them use Total Variation andwavelet sparse regularization. On the other side, some algorithmshave been proposed for tree sparsity regularization, but few of them has validated  the benefit of tree structure in CS-MRI. In this paper, we propose a fastconvex optimization algorithm to improve CS-MRI.  Waveletsparsity, gradient sparsity and tree sparsity are all considered in our model for real MR images.The original complex problem is decomposed to three simpler subproblemsthen each of the subproblems can be efficiently solved with an iterative scheme.Numerous experiments have been conducted and show that the proposedalgorithm outperforms the state-of-the-art CS-MRIalgorithms, and gain better reconstructions results on real MR images than generaltree based solvers or algorithms."
Online Learning of Rotations Using Geometric Structures,"This paper provides a solution to online learning for rotations by employing exponentials of sparse antisymmetric matrices. The method performs similarly to Riemannian gradient based methods but is derived using simple matrix algebraic techniques. We first show that a general optimization problem with a rotation constraint can be transformed into an equivalent problem in the space of antisymmetric matrices. An efficient approach is then introduced to iteratively solve the problem using antisymmetric matrices with one or more nonzero columns and an equal number of nonzero rows. Specially, we show that it is sufficient to employ antisymmetric matrices with only one nonzero column and row. Fast implementation is also presented to simplify the computation of sparse antisymmetric matrix exponentials involved in the algorithm. Experimental results obtained by using a variety of loss functions indicate that the algorithm converges quickly and estimates unknown rotations accurately."
Online Learning of Rotations Using Geometric Structures,"This paper provides a solution to online learning for rotations by employing exponentials of sparse antisymmetric matrices. The method performs similarly to Riemannian gradient based methods but is derived using simple matrix algebraic techniques. We first show that a general optimization problem with a rotation constraint can be transformed into an equivalent problem in the space of antisymmetric matrices. An efficient approach is then introduced to iteratively solve the problem using antisymmetric matrices with one or more nonzero columns and an equal number of nonzero rows. Specially, we show that it is sufficient to employ antisymmetric matrices with only one nonzero column and row. Fast implementation is also presented to simplify the computation of sparse antisymmetric matrix exponentials involved in the algorithm. Experimental results obtained by using a variety of loss functions indicate that the algorithm converges quickly and estimates unknown rotations accurately."
Flexible Shift-Invariant Locality and Globality Preserving Projections,"In machine learning, the dimension reduction methods have commonly been used as a principled way to understand the high-dimensional data. To solve the out-of-sample problem, local preserving projection (LPP) was proposed and applied to many applications. However, LPP suffers two crucial deficiencies: 1) the LPP loses shift invariance property which is an important property of embedding methods; 2) the rigid linear embedding is used as constraint, which often inhibits the optimal manifold structures finding. To overcome these two important problems, we propose a novel flexible shift invariant locality and globality preserving projection method, which utilizes a newly defined graph Laplacian to make the projection shift invariant. Meanwhile, the relaxed embedding is introduced to allow the finding of more optimal manifold structures. We derive the new optimization algorithm to solve the proposed objective with rigorously proved global convergence. Extensive experiments have been performed on the machine learning benchmark data sets. In all empirical results, our method shows promising results."
Flexible Shift-Invariant Locality and Globality Preserving Projections,"In machine learning, the dimension reduction methods have commonly been used as a principled way to understand the high-dimensional data. To solve the out-of-sample problem, local preserving projection (LPP) was proposed and applied to many applications. However, LPP suffers two crucial deficiencies: 1) the LPP loses shift invariance property which is an important property of embedding methods; 2) the rigid linear embedding is used as constraint, which often inhibits the optimal manifold structures finding. To overcome these two important problems, we propose a novel flexible shift invariant locality and globality preserving projection method, which utilizes a newly defined graph Laplacian to make the projection shift invariant. Meanwhile, the relaxed embedding is introduced to allow the finding of more optimal manifold structures. We derive the new optimization algorithm to solve the proposed objective with rigorously proved global convergence. Extensive experiments have been performed on the machine learning benchmark data sets. In all empirical results, our method shows promising results."
Dictionary Training with Side Information,"Recently, learning with side information, which incorporates information contained in the training data but not available in the testing phase to guide the learning process, attracts great attention in machine learning field. In this work, we propose a discriminative dictionary learning method that involves side information. In particular, we introduce a new soft constraint derived from side information and combine it with the reconstruction error and the classification error to form a unified objective function. The optimal solution to the objective function is efficiently obtained using the K-SVD algorithm. Our algorithm learns the dictionary and an optimal linear classifier jointly. We apply the proposed method to two pattern recognition problems, namely low resolution expression recognition and face recognition, where it demonstrates the effectiveness of the proposed method in classification performance."
Early Active Learning via Robust Representation and Structured Sparsity,"Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. The robust sparse representation loss function is utilized to reduce the effect of outliers and the structural sparsity regularization is adopted to find the most representative samples during the sparse representations. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost. We rigorously prove the global convergence of our solutions. Empirical results on multiple benchmark data sets show the promising results of our method."
Early Active Learning via Robust Representation and Structured Sparsity,"Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. The robust sparse representation loss function is utilized to reduce the effect of outliers and the structural sparsity regularization is adopted to find the most representative samples during the sparse representations. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost. We rigorously prove the global convergence of our solutions. Empirical results on multiple benchmark data sets show the promising results of our method."
Robust Rank-$k$ Matrix Completion,"Many applications can be formulated as reconstructing a matrix $M$ from noisy observations of a small, random subset of its entries. Much recent work has focused on the assumption that the data matrix has low rank and used its nuclear norm to approximate the rank. However, recent research casts doubts about this category of approach since such yielded solution could be indeed not low rank and unstable for practical applications. In this paper, we explicitly seek a matrix of \emph{exact} rank. Moreover, our method is robust to outlying or corrupted observations. We optimize the objective function in an alternative and asymptotic convergent manner, based on a combination of ancillary variables and augmented Lagrangian methods (ALM). We perform extensive experiments on three real world data sets and all empirical results demonstrate the effectiveness of our method."
New Relaxations of Graph Cuts for Clustering,"In recent clustering research, the graph cut methods, such as normalized cut and ratio cut, have been well studied and applied to solve many unsupervised learning applications. The original graph cut is an NP-hard problem. Traditional approaches used spectral relaxation to solve the graph cut problem. The main disadvantage of this approach is that the obtained spectral solutions %are not the final clustering results and the post-processing step has to be applied. Thus, the final results could severely deviate from the true solution.To solve this problem, in this paper, we propose a new relaxation mechanism for graph cut methods. Instead of minimizing the squared distances of clustering results, we use the $\ell_1$-norm distance. Meanwhile, considering the normalized consistency, we also use the $\ell_1$-norm for the normalized terms in the new graph cut relaxations. Due to the sparse result from the $\ell_1$-norm minimization, the solution of our new relaxed graph cut methods get discrete values with many zeros, which is close to the ideal solution. However, the new objectives are difficult to be optimized, because the minimization problem involves the ratio of non-smooth terms. The existing sparse learning optimization algorithms cannot be applied to solve this problem. In this paper, we propose a new optimization algorithm to solve this difficult non-smooth ratio minimization problem. The extensive experiments have been performed on three two-way clustering and eight multi-way clustering data sets. All empirical results show that our new relaxation methods consistently enhance the normalized cut and ratio cut clustering results."
New Relaxations of Graph Cuts for Clustering,"In recent clustering research, the graph cut methods, such as normalized cut and ratio cut, have been well studied and applied to solve many unsupervised learning applications. The original graph cut is an NP-hard problem. Traditional approaches used spectral relaxation to solve the graph cut problem. The main disadvantage of this approach is that the obtained spectral solutions %are not the final clustering results and the post-processing step has to be applied. Thus, the final results could severely deviate from the true solution.To solve this problem, in this paper, we propose a new relaxation mechanism for graph cut methods. Instead of minimizing the squared distances of clustering results, we use the $\ell_1$-norm distance. Meanwhile, considering the normalized consistency, we also use the $\ell_1$-norm for the normalized terms in the new graph cut relaxations. Due to the sparse result from the $\ell_1$-norm minimization, the solution of our new relaxed graph cut methods get discrete values with many zeros, which is close to the ideal solution. However, the new objectives are difficult to be optimized, because the minimization problem involves the ratio of non-smooth terms. The existing sparse learning optimization algorithms cannot be applied to solve this problem. In this paper, we propose a new optimization algorithm to solve this difficult non-smooth ratio minimization problem. The extensive experiments have been performed on three two-way clustering and eight multi-way clustering data sets. All empirical results show that our new relaxation methods consistently enhance the normalized cut and ratio cut clustering results."
Boosting for Multiclass Semi-Supervised Learning,"We focus on multiclass Semi-Supervised learning which islearning from a limited amount of labeled data and plenty of unlabeled data. Most of the existing semi-supervised algorithms use approaches such as one-versus-all to convert the multiclass problem to several binary classification problems which can have various problems, likeunbalanced classification problems. We propose amulticlass semi-supervised boosting algorithm that solves multiclass classification problems directly. Our algorithmworks based on two main semi-supervised assumptions, themanifold and the cluster assumption, to exploit the novel information from the unlabeled examples to improve the classification performance. It uses a novel loss functionconsisting of the margin cost on labeled data and two regularization terms on labeled and unlabeled data. Experimental results on a number of UCI datasets show that the proposed algorithm performs better than the state-of-the-art boosting algorithms for multiclass semi-supervised learning."
Boosting for Multiclass Semi-Supervised Learning,"We focus on multiclass Semi-Supervised learning which islearning from a limited amount of labeled data and plenty of unlabeled data. Most of the existing semi-supervised algorithms use approaches such as one-versus-all to convert the multiclass problem to several binary classification problems which can have various problems, likeunbalanced classification problems. We propose amulticlass semi-supervised boosting algorithm that solves multiclass classification problems directly. Our algorithmworks based on two main semi-supervised assumptions, themanifold and the cluster assumption, to exploit the novel information from the unlabeled examples to improve the classification performance. It uses a novel loss functionconsisting of the margin cost on labeled data and two regularization terms on labeled and unlabeled data. Experimental results on a number of UCI datasets show that the proposed algorithm performs better than the state-of-the-art boosting algorithms for multiclass semi-supervised learning."
Boosting for Multiclass Semi-Supervised Learning,"We focus on multiclass Semi-Supervised learning which islearning from a limited amount of labeled data and plenty of unlabeled data. Most of the existing semi-supervised algorithms use approaches such as one-versus-all to convert the multiclass problem to several binary classification problems which can have various problems, likeunbalanced classification problems. We propose amulticlass semi-supervised boosting algorithm that solves multiclass classification problems directly. Our algorithmworks based on two main semi-supervised assumptions, themanifold and the cluster assumption, to exploit the novel information from the unlabeled examples to improve the classification performance. It uses a novel loss functionconsisting of the margin cost on labeled data and two regularization terms on labeled and unlabeled data. Experimental results on a number of UCI datasets show that the proposed algorithm performs better than the state-of-the-art boosting algorithms for multiclass semi-supervised learning."
Incorporating Hidden Knowledge through a Latent Variable,"A teacher provides a student prior knowledge such that the student learns more efficiently and makes fewer mistakes. Although prior knowledge plays significantly important role in human learning, it has not yet been thoroughly investigated in machine learning. Among different sources of knowledge, hidden knowledge, the information that is only available during training but not testing, has a wide variety of applications and great potential to bring huge benefits. But it remains challenging to capture the hidden knowledge and to incorporate them into the learning process. In this work, we take the first step to exploit and encode the useful information within the hidden knowledge into the learning task through a discrete latent variable. Experiments on gesture recognition demonstrated that hidden knowledge could be successfully captured and incorporated to learn more discriminative and generalizable hidden state models for classification."
Incorporating Hidden Knowledge through a Latent Variable,"A teacher provides a student prior knowledge such that the student learns more efficiently and makes fewer mistakes. Although prior knowledge plays significantly important role in human learning, it has not yet been thoroughly investigated in machine learning. Among different sources of knowledge, hidden knowledge, the information that is only available during training but not testing, has a wide variety of applications and great potential to bring huge benefits. But it remains challenging to capture the hidden knowledge and to incorporate them into the learning process. In this work, we take the first step to exploit and encode the useful information within the hidden knowledge into the learning task through a discrete latent variable. Experiments on gesture recognition demonstrated that hidden knowledge could be successfully captured and incorporated to learn more discriminative and generalizable hidden state models for classification."
Robustness and Generalization for Metric Learning,"Metric learning has attracted a lot of interest over the last decade, but little work has been done about the generalization ability of such methods. In this paper, we address this issue by proposing  an adaptation of the notion of algorithmic robustness,  previously introduced by Xu and Mannor in  classic supervised learning, to derive generalization bounds for metric learning.  We also show that a weak notion of robustness is a necessary and  sufficient condition to generalize well, justifying that it  is fundamental for  metric learning. We provide some illustrative examples of our approach on a large class of existing algorithms."
Robustness and Generalization for Metric Learning,"Metric learning has attracted a lot of interest over the last decade, but little work has been done about the generalization ability of such methods. In this paper, we address this issue by proposing  an adaptation of the notion of algorithmic robustness,  previously introduced by Xu and Mannor in  classic supervised learning, to derive generalization bounds for metric learning.  We also show that a weak notion of robustness is a necessary and  sufficient condition to generalize well, justifying that it  is fundamental for  metric learning. We provide some illustrative examples of our approach on a large class of existing algorithms."
Learning the Architecture of Sum-Product Networks Using Clustering on Variables,"The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture significantly improves its performance compared to using a previously-proposed static architecture."
Learning the Architecture of Sum-Product Networks Using Clustering on Variables,"The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture significantly improves its performance compared to using a previously-proposed static architecture."
Feature Selection at the Discrete Limit,"In this paper, we propose to use L2,p for feature selection with emphasis on small p. As p ? 0, feature selection becomes discrete feature selection problem. We provide two algorithms, proximal gradient algorithm and rank-one-update algorithm which is more efficient at large regularization \lambda. Experiments on real life datasets show that features selected at small p consistently out-perform features selected at p = 1, the standard L2,1 approach."
Feature Selection at the Discrete Limit,"In this paper, we propose to use L2,p for feature selection with emphasis on small p. As p ? 0, feature selection becomes discrete feature selection problem. We provide two algorithms, proximal gradient algorithm and rank-one-update algorithm which is more efficient at large regularization \lambda. Experiments on real life datasets show that features selected at small p consistently out-perform features selected at p = 1, the standard L2,1 approach."
"Feature Selection using L_{p,\infty } norm","In this paper, we present a feature selection method using  L_{p,\infty} norm as a regularization term. Compared with standard  L_{p,\infty} feature selection,  L_{p,\infty } feature selection gives more flexible to approach the number of non-zero features/variables, which is the desired goal in feature/variable selection tasks.   We use proximal gradient method to solve  L_{p,\infty } norm problem. An efficient algorithm is proposed to solve the associated proximal operator with rigorous analysis.    Extensive experiments on both multi-class and multi-label feature selection tasks demonstrate the effectiveness of our methods. The experiment results also suggest smaller $p$ values(say p=0.25) give relatively better results. "
Local Features for Robust Manifold Modeling,"Despite the promise of low-dimensional manifold models for various vision and machine learning tasks, their utility has been hamstrung in practice by two fundamental challenges: practical image manifolds are plagued by a large number of nuisance variables and are non-isometric to the parameter space. In this paper, we develop a new manifold modeling, learning, and processing framework that directly addresses these challenges. At the heart of the framework are two key ideas: the use of the Earth Mover's Distance (EMD) to enable isometry of image manifolds to the underlying parameter space, and the use of local image features (such as SIFT features) to enable robust learning even to nuisance articulations.We analytically describe the performance of our approach and propose a fast kernel-based method for approximate EMD calculation to ensure computational efficiency. A powerful application of our approach is the automatic organization of large, unstructured collections of photographs gathered from the internet. "
Local Features for Robust Manifold Modeling,"Despite the promise of low-dimensional manifold models for various vision and machine learning tasks, their utility has been hamstrung in practice by two fundamental challenges: practical image manifolds are plagued by a large number of nuisance variables and are non-isometric to the parameter space. In this paper, we develop a new manifold modeling, learning, and processing framework that directly addresses these challenges. At the heart of the framework are two key ideas: the use of the Earth Mover's Distance (EMD) to enable isometry of image manifolds to the underlying parameter space, and the use of local image features (such as SIFT features) to enable robust learning even to nuisance articulations.We analytically describe the performance of our approach and propose a fast kernel-based method for approximate EMD calculation to ensure computational efficiency. A powerful application of our approach is the automatic organization of large, unstructured collections of photographs gathered from the internet. "
Low Rank Data Recovery Using Schatten p-Norm,"In this paper, we present Schatten $p$-Norm model for low-rank data recovery. Besides playing the role of data recovery, Schatten p-Norm model is more attractive due to its suppression on the shrinkage of singular values at smaller p.The proposed Schatten p-Norm model can be transformed into solving the proximal operator, and an efficient algorithm based on ALM method is proposed.  Another iterative algorithm is also presented to solve this problem with rigorous convergence analysis.Extensive experiment results on 6 occluded datasets indicate the relatively better data recovery results at smaller p values."
Complex Activity Recognition using Granger Constrained DBN (GCDBN) in Sports and Surveillance Video,"Common approaches for modeling interactions of multiple interacting and co-occurring objects in complex activities can theoretically model any number of co-occurring agents or events. However, these can be intractable for complex representations, require manual structure definitions, and/or generatively learn their structures. Our approach involves automatically constraining the nodes and links of a Dynamic Bayesian Network (DBN) in an informative and discriminative manner, resulting in sparse models that are both tractable and improve classification. This is accomplished by explicitly constraining the temporal links based on a direct measure of temporal dependence using Granger Cause statistics, resulting in the Granger Constrained DBN (GCDBN). The experiments show how the GCDBN outperforms other state-of-the-art models in complex activity classification on both handball and surveillance video data."
Linear Time Solver for Primal SVM,"Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, and designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with proved linear convergence for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easy parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state of the art solvers such as SVMperf, Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms."
Compressible Motion Fields,"Traditional video compression methods obtain a compactrepresentation for image frames by computing coarse motion fieldsdefined on patches of pixels called blocks. This piecewiseconstant flow approximation reduces the size of the motion fieldbut introduces block artifacts in the decoded (warped) imageframe. In this paper, we address the problem of estimating densemotion fields that, while accurately predicting one frame from agiven reference frame by warping it with the field, are also\emph{compressible}. We introduce a representation for motionfields based on wavelet bases, and approximate thecompressibility of their coefficients with a piecewise smoothsurrogate function that is relatively easy to optimize. We thenshow how to quantize and encode such coefficients with adaptiveprecision. We demonstrate the effectiveness of our approach bycomparing its performance with a state-of-the-art videocompression algorithm. Experimental results reveal that ourmethod significantly outperforms classical block-based motioncompensation adopted in most modern video compression algorithms."
Smooth Convolutional Stacked Autoencoder for Feature Learning in ECoG Based Brain Computer Interface,"Recent years have seen great interest in ECoG based Brain Computer Interface (BCI) systems. The performance of the BCI systems largely depends on the low-level features used in the decoding algorithm. The currently widely used features are based on frequency decompositions and designed manually, which are only justified empirically. In this work, we describe an automatic feature learning framework for ECoG based BCI decoding algorithms, using a multi-layer deep learning structure we termed as {\em smooth convolutional stacked auto-encoders (SCSA)}. One advantage of SCSA is that it is an open architecture that facilities incorporating various domain-specific constraints, e.g., smoothness of the extracted features over time.  Based on SCSA and data sets of ECoG signals, we demonstrate significant improvement in performance over the current state of the art methods based on manual features. Furthermore, the automatically extracted features from SCSA also shed light on the optimality of those features obtained empirically."
Simulation of Database-Valued Markov Chains Using SimSQL,"This paper describes the SimSQL system, which allows for SQL-based specification,simulation, and querying of database-valued Markov chains, i.e., chainswhose value at any time step comprises the contents of an entire database. This isof particular interest in statistical machine learning, because SimSQL can easilybe used to declaratively specify Markov Chain Monte Carlo simulations that areautomatically parallelized to run on a large compute cluster."
Simulation of Database-Valued Markov Chains Using SimSQL,"This paper describes the SimSQL system, which allows for SQL-based specification,simulation, and querying of database-valued Markov chains, i.e., chainswhose value at any time step comprises the contents of an entire database. This isof particular interest in statistical machine learning, because SimSQL can easilybe used to declaratively specify Markov Chain Monte Carlo simulations that areautomatically parallelized to run on a large compute cluster."
Simulation of Database-Valued Markov Chains Using SimSQL,"This paper describes the SimSQL system, which allows for SQL-based specification,simulation, and querying of database-valued Markov chains, i.e., chainswhose value at any time step comprises the contents of an entire database. This isof particular interest in statistical machine learning, because SimSQL can easilybe used to declaratively specify Markov Chain Monte Carlo simulations that areautomatically parallelized to run on a large compute cluster."
Matrix Completion with Ordering Relation Constraints,"We relax the equality constraints in the very general and well-known affine Schatten p-norm minimization problem into one-side inequality constraints. Owing to the imposed equality con-straints, existing methods can only achieve some degree of denoising, via the optimization of the objective energy function. By our proposed re-laxation, the decision variables in the objective function possess flexible nonlinearity while maintaining their ordering relation constraints. We show that, our new objective function is convex, and its global minimum can be obtained by a more general form of the Fixed-Point Con-tinuation framework with almost the same com-putational cost. Experiments show that, our algo-rithm has good performance over various widely used datasets."
Discovering Common Functional Connectomics Signatures,"Based on the structural connectomes constructed from diffusion tensor imaging (DTI) data, we present a novel framework to discover functional connectomics signatures from resting-state fMRI (R-fMRI) data for the characterization of brain conditions. First, by applying a sliding time window approach, the brain states represented by functional connectomes were automatically divided into temporal quasi-stable segments. These quasi-stable functional connectome segments were then integrated and pooled from populations as input to an effective dictionary learning and sparse coding algorithm, in order to identify common functional connectomes (CFC) and signature patterns, as well as their dynamic transition patterns. The computational framework was validated by benchmark stimulation data, and highly accurate results were obtained. By applying the framework on the datasets of 44 post-traumatic stress disorder (PTSD) patients and 51 healthy controls, it was found that there are 16 CFC patterns reproducible across healthy controls/PTSD patients, and two additional CFCs with altered connectivity patterns exist solely in PTSD subjects. These two signature CFCs can successfully differentiate 85% of PTSD patients, suggesting their potential use as biomarkers."
Laplacian Consistency,"Computing a faithful similarity/affinity metric is essential to many graph-based learning algorithms.In this paper, we propose a graph-based affinity learning method in an unsupervised scenario and show its application to shape retrieval, face clustering and web categorization.Our method, Laplacian Consistency  (LC), performs a dynamic diffusion process by propagating the similarity mass along the intrinsic manifold of data points.Convergence analysis is given and a closed-form solution is provided, making the LC process fast to calculate and easy to understand. Theoretical analysis shows our LC process only changes the eigenvalues gradually while keeping the eigenvector in the Laplacian spectral space. We also prove the superiority of our method from different points of views. Our method has nearly no parameter tuning and leads to significantly improved affinity maps, which help to greatly enhance the quality of various graph-based learning algorithms."
Factoring nonnegative matrices with linear programs,"This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes."
Maximum Weight Subgraphs with Mutex Constraints,"In this paper, we propose a novel algorithm for computing maximum weight subgraphs(MWSs) that satisfy mutex constraints on a weighted graph. As opposedto commonly used linear equality constraints, the mutex constraints expressedin a quadratic equality form allow for a greater modeling flexibility, which canbe beneficial in many applications. Although the proposed algorithm solves arelaxed formulation of MWS problem, it obtains a discrete solution in all ourexperiments on real data, which in turn guarantees that the solution satisfies themutex constraints. We evaluated our algorithm on two hard combinatorial problems:matching of salient points under perspective and nonrigid distortion andsolving image jigsaw puzzles. It significantly outperforms known state-of-the-artalgorithms, including loopy believe propagation and Integer Projected Fixed PointMethod (IPFP), even if it is restricted to using only constraints equivalent to linearequality constraints."
High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer Disease Progression Prediction,"Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.  "
The Interplay between Stability and Regret in Online Learning,"This paper considers the stability of online learning algorithms and its implications for learnability (bounded regret).  We introduce a novel quantity called {\em forward regret} that intuitively measures how good an online learning algorithm is if it is allowed a one-step look-ahead into the future.  We show that given stability, bounded forward regret is equivalent to bounded regret. We also show that the existence of an algorithm with bounded regret implies the existence of a stable algorithm with bounded regret and bounded forward regret. The equivalence results apply to general, possibly non-convex problems. To the best of our knowledge, our analysis provides the first general connection between stability and regret in the online setting that is not restricted to a particular class of algorithms. Our stability-regret connection provides a simple recipe for analysing  regret incurred by any online learning algorithm.  We illustrate our recipe by providing a novel dimension independent regret bound for the follow-the-perturbed-leader (FTPL) algorithm for online linear programming (OLP) over a hypersphere, a non-convex set. Using our framework, we analyse several existing online learning algorithms as well as the ``approximate'' versions  of algorithms like RDA that solve an optimization problem at each iteration. Our proofs are simpler than existing analysis for the respective algorithms, show a clear trade-off between stability and forward regret, and provide tighter regret bounds in some cases."
Graphical Models via Generalized Linear Models,"Undirected graphical models, or Markov networks, such as Gaussian graphical models and Ising models enjoy popularity in a variety of applications.  In many settings, however, data may not follow a Gaussian or binomial distribution assumed by these models. We introduce a new class of graphical models based on generalized linear models (GLM) by assuming that node-wise conditional distributions arise from exponential families.  Our models allow one to estimate networks for a wide class of exponential distributions, such as the Poisson, negative binomial, and exponential, by fitting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We provide examples of high-throughput genomic networks learned via our GLM graphical models for multinomial and Poisson distributed data."
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
Near optimal policy decomposition for tasks with multiple goals explains human behavior,"The natural environment presents people with multiple potential goals that compete for action selection. Recent studies suggest that the brain generates several concurrent and partially prepared actions associated with alternative goals and use perceptual information to drive the goal competition, until a single action is selected. In the current study, we propose a near-optimal policy decomposition that the brain may use in visuomotor tasks with multiple competing goals. We show how human and animal strategies in the presence of competing goals can be expressed as a weighted mixture of multiple control policies, each of which produces a sequence of actions associated with a specific goal. We evaluate the performance of the proposed framework in a series of simulated reaching and saccade tasks with multiple targets in environments with and without presence of obstacles. The results show that the proposed model can qualitatively predict many aspects of human/animal strategies in goal-directed movements."
An Indexing Method for Efficient Model-Based Search,"Large databases of patterns, such as faces, body poses, fingerprints, or gestures, are becoming increasingly widespread, thanks to advances in computer technology. In this paper we focus on the problem of efficient search in such databases, when using model-based search. In model-based search, the user submits as a query a classifier, that has been trained to recognize the type of patterns that the user wants to retrieve. While model-based search can lead to good retrieval accuracy, the efficiency of model-based search can be inadequate if we need to apply the query classifier to every single database pattern. We propose a method for improving the efficiency of model-based search. The proposed method assumes that classifiers have been trained using JointBoost, and operates by defining an embedding, which maps both classifiers and database patterns into a common vector space. Using this embedding, the problem of finding the database patterns maximizing the response of the query classifier is reduced to a nearest neighbor search problem in a vector space. This reduction allows the use of standard vector indexing method to speed up the search. In our experiments, we show that the proposed embedding, together with a simple PCA-based indexing scheme, significantly improve the efficiency of model-based search, as measured on a database of face images constructed from the public FRGC-2 dataset."
An Indexing Method for Efficient Model-Based Search,"Large databases of patterns, such as faces, body poses, fingerprints, or gestures, are becoming increasingly widespread, thanks to advances in computer technology. In this paper we focus on the problem of efficient search in such databases, when using model-based search. In model-based search, the user submits as a query a classifier, that has been trained to recognize the type of patterns that the user wants to retrieve. While model-based search can lead to good retrieval accuracy, the efficiency of model-based search can be inadequate if we need to apply the query classifier to every single database pattern. We propose a method for improving the efficiency of model-based search. The proposed method assumes that classifiers have been trained using JointBoost, and operates by defining an embedding, which maps both classifiers and database patterns into a common vector space. Using this embedding, the problem of finding the database patterns maximizing the response of the query classifier is reduced to a nearest neighbor search problem in a vector space. This reduction allows the use of standard vector indexing method to speed up the search. In our experiments, we show that the proposed embedding, together with a simple PCA-based indexing scheme, significantly improve the efficiency of model-based search, as measured on a database of face images constructed from the public FRGC-2 dataset."
Nonparametric Reduced Rank Regression,"We propose an approach to multivariate nonparametric regression thatgeneralizes reduced rank regression for linear models.  An additivemodel is estimated for each dimension of a $q$-dimensional response,with a shared $p$-dimensional predictor variable.  To control thecomplexity of the model, we employ a functional form of the Ky-Fanor nuclear norm, resulting in a set of function estimates that havelow rank.  Backfitting algorithms are derived and justified using anonparametric form of the nuclear norm subdifferential.  Oracleinequalities on excess risk are derived that exhibit the scalingbehavior of the procedure in the high dimensional setting.  Themethods are illustrated on gene expression data."
Nonparametric Reduced Rank Regression,"We propose an approach to multivariate nonparametric regression thatgeneralizes reduced rank regression for linear models.  An additivemodel is estimated for each dimension of a $q$-dimensional response,with a shared $p$-dimensional predictor variable.  To control thecomplexity of the model, we employ a functional form of the Ky-Fanor nuclear norm, resulting in a set of function estimates that havelow rank.  Backfitting algorithms are derived and justified using anonparametric form of the nuclear norm subdifferential.  Oracleinequalities on excess risk are derived that exhibit the scalingbehavior of the procedure in the high dimensional setting.  Themethods are illustrated on gene expression data."
Nonparametric Reduced Rank Regression,"We propose an approach to multivariate nonparametric regression thatgeneralizes reduced rank regression for linear models.  An additivemodel is estimated for each dimension of a $q$-dimensional response,with a shared $p$-dimensional predictor variable.  To control thecomplexity of the model, we employ a functional form of the Ky-Fanor nuclear norm, resulting in a set of function estimates that havelow rank.  Backfitting algorithms are derived and justified using anonparametric form of the nuclear norm subdifferential.  Oracleinequalities on excess risk are derived that exhibit the scalingbehavior of the procedure in the high dimensional setting.  Themethods are illustrated on gene expression data."
Multiresolution analysis on the symmetric group,"There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of O(n^p) complexity with small p for sparse signals (in contrast to the O(n^q n!) complexity typical of FFTs). We discuss potential applications in ranking, sparse approximation, and multi-object tracking. "
Multiresolution analysis on the symmetric group,"There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of O(n^p) complexity with small p for sparse signals (in contrast to the O(n^q n!) complexity typical of FFTs). We discuss potential applications in ranking, sparse approximation, and multi-object tracking. "
Infinitesimal Annealing for Training Semi-Supervised Support Vector Machine,"The semi-supervised support vector machine(S^3VM) is a popular classification algorithm for finding the maximum-margin separating hyper-plane for both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good(local) solution of S^3VM is to gradually increase the effect of unlabeled data, `a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches. "
Data Representation with Rank Regularized PCA,"Trace-norm is often used in low-rank data representation models. In this paper, we point out some drawbacks of the trace norm based approach and propose a rank regularized formulation which can be solved very efficiently. We did extensive experiments on six datasets. Experiments show the advantage of the proposed approach."
Learning with Target Prior,"In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\y$ can be modeled with a prior model $p(\y)$ and the relations between data and target variables are estimated through $p(\y)$ and a set of uncorresponded data $\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\t$ that maximizes the log likelihood of $f_\t(\x)$ on a uncorresponded training set with regards to $p(\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video."
Learning from Data and Constraints: an Unified Probabilistic View of Clustering,"In this paper we introduce an unified view of clustering, i.e. learning from unlabeled data and constraints. Clustering is considered as a prediction problem for the states of latent variables, i.e. unknown labels. Unlabeled data and constraints are seen as two main sources to provide related information of latent variables. We present a probabilistic clustering model based on Hidden Markov Random Fields (HMRFs), which can embed different related information together to guide theclustering process. We also present a novel constrained clustering method, i.e. a simplified version of HMRF-based clustering model. Unlabeled data and a few pairwise constraints are combined to generate the neighborhood system between latent variables. One of the main limitations of existing constrained clusterings, which is the requirement of a large amount of constraints, can be significantly alleviated. Further, connections between HMRF-based clustering model and manyexisting clusterings are established to demonstrate the inclusiveness and flexibility of the proposed model. Experiments on synthetic and real data are also performed to demonstrate the benefits of the proposed constrained clustering method."
Recognizing Human Activities from Incompletely Observed Videos,"In this paper, we present a novel method for handling the problem of recognizing human activities from incompletely observed videos. Compared with the similar problem of human activity prediction from unfinished activities [12], in an incompletely observed video an un-observed subsequence of frames may occur any time with any duration and yield a temporal gap in the video. In practice, incompletely observed videos may occur when the video signal drops off, when camera or objects of interest are occluded, or when videos are composited from multiple sources. In this paper, we formulate the problem of human activity recognition from incompletely observed videos in a probabilistic framework. In this framework, we take a set of training video samples (completely observed) of each activity class as the basis, and then use sparse coding to derive the likelihood that an incompletely observed test video belongs to a certain activity class. Furthermore, we propose to divide each activity into multiple temporal stages, apply sparse coding to derive the activity likelihood at each stage, and finally combine the likelihoods at each stage to achieve a global posterior for the activity. We evaluate the proposed method on both the widely used UT-Interaction human activity dataset and a new human activity dataset selected from the Year-1 corpus of the DARPA Mind's Eye program [4]. For the new DARPA dataset, both the activities and the videos show very large within-class temporal, spatial, and background variation. Our results demonstrate that the proposed method performs substantially better than several competing methods on both datasets."
Recognizing Human Activities from Incompletely Observed Videos,"In this paper, we present a novel method for handling the problem of recognizing human activities from incompletely observed videos. Compared with the similar problem of human activity prediction from unfinished activities [12], in an incompletely observed video an un-observed subsequence of frames may occur any time with any duration and yield a temporal gap in the video. In practice, incompletely observed videos may occur when the video signal drops off, when camera or objects of interest are occluded, or when videos are composited from multiple sources. In this paper, we formulate the problem of human activity recognition from incompletely observed videos in a probabilistic framework. In this framework, we take a set of training video samples (completely observed) of each activity class as the basis, and then use sparse coding to derive the likelihood that an incompletely observed test video belongs to a certain activity class. Furthermore, we propose to divide each activity into multiple temporal stages, apply sparse coding to derive the activity likelihood at each stage, and finally combine the likelihoods at each stage to achieve a global posterior for the activity. We evaluate the proposed method on both the widely used UT-Interaction human activity dataset and a new human activity dataset selected from the Year-1 corpus of the DARPA Mind's Eye program [4]. For the new DARPA dataset, both the activities and the videos show very large within-class temporal, spatial, and background variation. Our results demonstrate that the proposed method performs substantially better than several competing methods on both datasets."
Sensitivity analysis of neuronal dynamics under additive STDP rule,"Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learning rule. The basis of STDP has strong experimental evidences and it depends on precise input and output spike timings.  In this paper we show that under biologically plausible spiking regime, slight variability in the spike timing leads to drastically different evolution of synaptic weights when its dynamics are governed by the additive STDP rule."
Sensitivity analysis of neuronal dynamics under additive STDP rule,"Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learning rule. The basis of STDP has strong experimental evidences and it depends on precise input and output spike timings.  In this paper we show that under biologically plausible spiking regime, slight variability in the spike timing leads to drastically different evolution of synaptic weights when its dynamics are governed by the additive STDP rule."
Sensitivity analysis of neuronal dynamics under additive STDP rule,"Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learning rule. The basis of STDP has strong experimental evidences and it depends on precise input and output spike timings.  In this paper we show that under biologically plausible spiking regime, slight variability in the spike timing leads to drastically different evolution of synaptic weights when its dynamics are governed by the additive STDP rule."
A Simple and Practical Algorithm for Differentially Private Data Release,"We present a new algorithm for differentially private data release, based on a simple combination of the Exponential Mechanism with the Multiplicative Weights update rule.  Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques."
When Block Meets Group: Structured Sparse Modeling For Learning,"This paper proposes a novel framework, the {\it block/group sparse coding} ($\bg$), for dictionary learning.  Two important features distinguish $\bg$ from other existing methods in that all dictionary blocks are trained simultaneously with respect to each data group and instead of the inter-block coherence, the intra-block coherence is explicitly minimized as an important objective.  We provide both empirical and heuristic evident for this latter novel feature that can be regarded as the consequence of using the group structure for the data and the block structure for the dictionary.  The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-coordinates descent, and the details of the optimization algorithms are presented.  We evaluate the proposed method on several classification (supervised) and clustering (unsupervised) problems using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed $\bg$ algorithm."
When Block Meets Group: Structured Sparse Modeling For Learning,"This paper proposes a novel framework, the {\it block/group sparse coding} ($\bg$), for dictionary learning.  Two important features distinguish $\bg$ from other existing methods in that all dictionary blocks are trained simultaneously with respect to each data group and instead of the inter-block coherence, the intra-block coherence is explicitly minimized as an important objective.  We provide both empirical and heuristic evident for this latter novel feature that can be regarded as the consequence of using the group structure for the data and the block structure for the dictionary.  The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-coordinates descent, and the details of the optimization algorithms are presented.  We evaluate the proposed method on several classification (supervised) and clustering (unsupervised) problems using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed $\bg$ algorithm."
When Block Meets Group: Structured Sparse Modeling For Learning,"This paper proposes a novel framework, the {\it block/group sparse coding} ($\bg$), for dictionary learning.  Two important features distinguish $\bg$ from other existing methods in that all dictionary blocks are trained simultaneously with respect to each data group and instead of the inter-block coherence, the intra-block coherence is explicitly minimized as an important objective.  We provide both empirical and heuristic evident for this latter novel feature that can be regarded as the consequence of using the group structure for the data and the block structure for the dictionary.  The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-coordinates descent, and the details of the optimization algorithms are presented.  We evaluate the proposed method on several classification (supervised) and clustering (unsupervised) problems using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed $\bg$ algorithm."
When Block Meets Group: Structured Sparse Modeling For Learning,"This paper proposes a novel framework, the {\it block/group sparse coding} ($\bg$), for dictionary learning.  Two important features distinguish $\bg$ from other existing methods in that all dictionary blocks are trained simultaneously with respect to each data group and instead of the inter-block coherence, the intra-block coherence is explicitly minimized as an important objective.  We provide both empirical and heuristic evident for this latter novel feature that can be regarded as the consequence of using the group structure for the data and the block structure for the dictionary.  The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-coordinates descent, and the details of the optimization algorithms are presented.  We evaluate the proposed method on several classification (supervised) and clustering (unsupervised) problems using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed $\bg$ algorithm."
Convex Collective Matrix Factorization,"In many realistic applications, multiple interlinked sources of data are available and they cannot be easily represented in the form of a single matrix. Collective matrix factorization has recently been introduced to improve generalization performances by jointly factorizing multiple relations or matrices. In this paper, we extend the trace norm for matrix factorization to the collective matrix factorization case. This norm defined on the space of relations is used to regularize the empirical loss, leading to a convex formulation of the problem. Similarly to the trace norm on matrices, we show that the collective-matrix completion problem admits afast iterative singular-value thresholding algorithm.The collective trace norm is also characterized as a decomposition norm, usefulto find an optimal solution thanks to an unconstrained minimization procedure. Empirically we show that stochastic gradient descent suits well for solving theconvex collective factorization even for large scale problems. We also show thatthe proposed algorithm directly solving the convex problem is muchfaster than unconstrained gradient minimization optimizing in the space of low-rankmatrices."
Exponential Concentration for Mutual Information Estimation with Application to Forests,"We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph."
Synergy is the whole minus the union of the parts,"Quantifying cooperation among random variables in predicting a single target random variable is an important problem in biological systems with 10s to 1000s of co-dependent variables.  A common definition of synergy within these system is based on the intuition that synergy is the difference between the whole and the sum of the parts. We introduce *synergistic mutual information* as the sum minus the union of the parts. We compare both measures against a set of pedagogical examples. We conclude that in the presence of redundant information, subtracting the sum underestimates true synergy."
Synergy is the whole minus the union of the parts,"Quantifying cooperation among random variables in predicting a single target random variable is an important problem in biological systems with 10s to 1000s of co-dependent variables.  A common definition of synergy within these system is based on the intuition that synergy is the difference between the whole and the sum of the parts. We introduce *synergistic mutual information* as the sum minus the union of the parts. We compare both measures against a set of pedagogical examples. We conclude that in the presence of redundant information, subtracting the sum underestimates true synergy."
Object Classification with Attributes as Side Information,"This paper proposes a new approach to incorporate attributes as side information for object classification. Attributes are a list of semantically meaningful properties that are available only during training. Instead of predicting these attributes during testing with attribute classifiers, we utilize attributes as side information to improve learning the category classifier on the primary features in a learning with side information paradigm. We propose a novel approach for learning with side information based on the assumption that the posteriors of the label using side information and primary features are close. With the proposed approach, we develop the corresponding learning methods for logistic regress model with L2 regularization. Experiments demonstrate the effectiveness of our approach in classification performance."
Object Classification with Attributes as Side Information,"This paper proposes a new approach to incorporate attributes as side information for object classification. Attributes are a list of semantically meaningful properties that are available only during training. Instead of predicting these attributes during testing with attribute classifiers, we utilize attributes as side information to improve learning the category classifier on the primary features in a learning with side information paradigm. We propose a novel approach for learning with side information based on the assumption that the posteriors of the label using side information and primary features are close. With the proposed approach, we develop the corresponding learning methods for logistic regress model with L2 regularization. Experiments demonstrate the effectiveness of our approach in classification performance."
Semisupervised Classifier Evaluation and Recalibration,"How many labeled examples are needed to estimate a classifier's performance on a new dataset? We study the case where data is plentiful, but labels are expensive.  We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier's confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by re-estimating the class-conditional confidence distributions."
Semisupervised Classifier Evaluation and Recalibration,"How many labeled examples are needed to estimate a classifier's performance on a new dataset? We study the case where data is plentiful, but labels are expensive.  We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier's confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by re-estimating the class-conditional confidence distributions."
Decoding Finger Flexion from Electrocorticographic Signals with Knowledge Based Prior Model,"Decoding by incorporating domain knowledge about the target variable (body movements in BCI) has been shown to be able to significantly improve the performance \cite{WangSJ11}. In the existing model, training a prior model to capture domain knowledge relies on training samples about the target variable. However, in most real BCI applications, brain signals are only collected under thoughts without actual body movements. Even though training sampels for the target variable are available, the model trained on which tends to be biased and has difficulty to generalize. In this paper, we train prior model by explicitly incorporating the domain knowledge without resorting to the training data. The experiment demonstrates its competitive performance."
Decision under time constraints in spiking networks,A spiking network is proposed that implements decision/classification under time- constraints. The network is able to approximate the optimal decision making rule using first-to-fire neurons as well as estimate parameters from external reward signals. The parameter estimation rule is derived from a principled cost function and resembles Hebbian learning in the limit. The proposed architecture and the learned rule exhibit near-optimal tradeoffs of error rate and response time.
Decision under time constraints in spiking networks,A spiking network is proposed that implements decision/classification under time- constraints. The network is able to approximate the optimal decision making rule using first-to-fire neurons as well as estimate parameters from external reward signals. The parameter estimation rule is derived from a principled cost function and resembles Hebbian learning in the limit. The proposed architecture and the learned rule exhibit near-optimal tradeoffs of error rate and response time.
Sparse Probit Factor Analysis for Learning Analytics,"Providing personalized instructions requires a significant amount of effort spent organizing educational material, and analyzing each student's strength and weakness. This, in turn, places an enormous burden on course instructors. Intelligent tutoring systems (ITS) using machine-learning techniques are a novel way to reduce the instructor's efforts. Specifically, ITS consist of two parts, i.e., learning analytics (LA) and scheduling. LA corresponds to the analysis of student response data, whereas scheduling corresponds to the automatic suggestion of learning materials to the student based on the database retrieved through LA. In this work, we propose a statistical approach towards LA based on sparse probit binary factor analysis, and propose two novel algorithms to analyze student response data obtained in a course or test. The first algorithm utilizes convex optimization techniques, whereas the second utilizes a Bayesian latent feature framework. We demonstrate for synthetic and real-world student data that the proposed framework enables us to recover a question--concept association map, as well as a profile of the concept understanding for each student. This proposed framework represents a first step towards an ITS that alleviates the course instructor's workload and  improves the efficacy of student learning."
Sparse Probit Factor Analysis for Learning Analytics,"Providing personalized instructions requires a significant amount of effort spent organizing educational material, and analyzing each student's strength and weakness. This, in turn, places an enormous burden on course instructors. Intelligent tutoring systems (ITS) using machine-learning techniques are a novel way to reduce the instructor's efforts. Specifically, ITS consist of two parts, i.e., learning analytics (LA) and scheduling. LA corresponds to the analysis of student response data, whereas scheduling corresponds to the automatic suggestion of learning materials to the student based on the database retrieved through LA. In this work, we propose a statistical approach towards LA based on sparse probit binary factor analysis, and propose two novel algorithms to analyze student response data obtained in a course or test. The first algorithm utilizes convex optimization techniques, whereas the second utilizes a Bayesian latent feature framework. We demonstrate for synthetic and real-world student data that the proposed framework enables us to recover a question--concept association map, as well as a profile of the concept understanding for each student. This proposed framework represents a first step towards an ITS that alleviates the course instructor's workload and  improves the efficacy of student learning."
Sparse Probit Factor Analysis for Learning Analytics,"Providing personalized instructions requires a significant amount of effort spent organizing educational material, and analyzing each student's strength and weakness. This, in turn, places an enormous burden on course instructors. Intelligent tutoring systems (ITS) using machine-learning techniques are a novel way to reduce the instructor's efforts. Specifically, ITS consist of two parts, i.e., learning analytics (LA) and scheduling. LA corresponds to the analysis of student response data, whereas scheduling corresponds to the automatic suggestion of learning materials to the student based on the database retrieved through LA. In this work, we propose a statistical approach towards LA based on sparse probit binary factor analysis, and propose two novel algorithms to analyze student response data obtained in a course or test. The first algorithm utilizes convex optimization techniques, whereas the second utilizes a Bayesian latent feature framework. We demonstrate for synthetic and real-world student data that the proposed framework enables us to recover a question--concept association map, as well as a profile of the concept understanding for each student. This proposed framework represents a first step towards an ITS that alleviates the course instructor's workload and  improves the efficacy of student learning."
A Statistical Model for Recreational Trails in Aerial Images,"We present a statistical model of aerial images of recreational trails, and a method to infer trail routes in such images. We learn a set of textons describing the images, and use them to divide the image into super-pixels represented by their texton. We then learn, for each texton, the frequency of generating on-trail and off-trail pixels, and the direction of trail through on-trail pixels. From these, we derive an image likelihood function. We combine that with a prior model of trail length and smoothness, yielding a posterior distribution for trails, given an image. We search for good values of this posterior using a novel stochastic variation of Dijkstra?s algorithm. Our experiments on trail images and groundtruth collected in the western continental USA, show substantial improvement over those of the previous best trail-finding method"
A Statistical Model for Recreational Trails in Aerial Images,"We present a statistical model of aerial images of recreational trails, and a method to infer trail routes in such images. We learn a set of textons describing the images, and use them to divide the image into super-pixels represented by their texton. We then learn, for each texton, the frequency of generating on-trail and off-trail pixels, and the direction of trail through on-trail pixels. From these, we derive an image likelihood function. We combine that with a prior model of trail length and smoothness, yielding a posterior distribution for trails, given an image. We search for good values of this posterior using a novel stochastic variation of Dijkstra?s algorithm. Our experiments on trail images and groundtruth collected in the western continental USA, show substantial improvement over those of the previous best trail-finding method"
Fusion with Diffusion for Robust Visual Tracking,"A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. The edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations.In this paper, a novel approach to integrate multiple similarity measures is pro-posed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the dif-fusion on the TPG is the same as the diffusion process on each of the original graphs, Moreover, it is not necessary to explicitly construct the TPG in our frame-work. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods."
Robust Saliency Estimation using Visual Structures,"Emulation of human perception system can usually inspire computer vision and help understand the natural scenes that are usually recorded as raster images or videos. Saliency detection is one of such procedures that can extract useful cues about the key visual information in an observed scene. In this paper, we provide a robust saliency estimation method based on a geodesic analysis of visual structures in an image. In the proposed approach, manifold learning is first applied to compare local spatial variation against global contrast, and obtain a locally smoothed image projection. Geodesic analysis is then applied to find the plateaus in the hierarchical image structure that corresponds to the geodesic saliency map. With the estimated saliency map, visual structures are hierarchically cut out by simple adaptive thresholding. The experiment validated that our algorithm consistently outperformed a number of the state-of-art methods, yielding higher precision and better recall rates, when evaluated using one of the largest publicly available data sets. Further experiments on saliency cut also testified that the proposed method provide an efficient way for unsupervised object cut, which may have extensive application in image/video editing, object recognition and robotic vision."
Convex Subspace Representation Learning  for Multi-view Clustering,"Learning from multi-view data is important in many applications. In this paper, we propose a novel convex subspace representation learning method for unsupervised multi-view clustering. We first formulate the subspace learning in multiple views as one joint optimization problem with a common subspace representation matrix and a group sparsity inducing norm regularizer. By exploiting a dual matrix norm, we then show a convex min-max dual formulation with a sparsity inducing trace norm regularizer can be obtained. We develop a proximal bundle optimization algorithm to solve the min-max optimization problem for a global solution. Our empirical study shows the proposed approach can outperform the state of the art alternative methods across different multi-view learning scenarios."
Robust Distance Metric Learning via Simultaneous $\ell_1$-Norm Minimization and Maximization,"Traditional distance metric learning with side information often formulates the learning objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Since covariance matrices are prone to outliers, it is desirable to develop a robust distance metric learning method. In this paper, motivated by existing studies that improve the robustness of machine learning models via the L1-norm, we propose a robust formulation of distance metric learning using the L1-norm distances. However, solving the formulated objective is very challenging because it simultaneously minimizes and maximizes (minmax) the non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is scarcely studied in literature. Extensive empirical evaluations on the proposed robust distance metric learning method are performed, in which our new method outperforms related state-of-the-art methods in a variety of experimental settings and demonstrate their effectiveness in the clustering tasks on both noiseless and noisy data.  "
Behavior segmentation and labeling using structured SVM,"We explore representing, segmenting and classifying behavior using bout-widestatistics, rather than local descriptors. To this effect we develop a structuredsupport vector machine (sSVM) where the loss function is computed at the boutlevelrather than at frame level, and bouts of different length are treated equally.We test our ideas on a set of videos of pairs of flies engaged in social behavior(exploration, aggression and courtship); four actions and a fifth grab-bag categoryin the videos are annotated by expert fly biologists. Our experiments show thatwith limited amount of data, we are able to obtain detection rate of 64%-93% foreach of the actions, while maintaining a precision of 64%-91%."
Behavior segmentation and labeling using structured SVM,"We explore representing, segmenting and classifying behavior using bout-widestatistics, rather than local descriptors. To this effect we develop a structuredsupport vector machine (sSVM) where the loss function is computed at the boutlevelrather than at frame level, and bouts of different length are treated equally.We test our ideas on a set of videos of pairs of flies engaged in social behavior(exploration, aggression and courtship); four actions and a fifth grab-bag categoryin the videos are annotated by expert fly biologists. Our experiments show thatwith limited amount of data, we are able to obtain detection rate of 64%-93% foreach of the actions, while maintaining a precision of 64%-91%."
Probabilistic Multi-label Classification with Sparse Feature Learning,"In this paper we propose a probabilistic multi-label classification model based on novel sparse feature learning. By employing an individual sparsity inducing $\ell_1$-norm and a group sparsity inducing $\ell_{2,1}$-norm, the proposed model has the capacity of capturing both label interdependencies and common predictive model structures.We formulate this sparse norm regularized learning problem as a non-smooth convex optimization problem, and develop a fast proximal gradient algorithm to solve it for an optimal solution. Our empirical study demonstrates the efficacy of theproposed method on a set of multi-label tasks given a limited number of labeled training instances."
Probabilistic Multi-label Classification with Sparse Feature Learning,"In this paper we propose a probabilistic multi-label classification model based on novel sparse feature learning. By employing an individual sparsity inducing $\ell_1$-norm and a group sparsity inducing $\ell_{2,1}$-norm, the proposed model has the capacity of capturing both label interdependencies and common predictive model structures.We formulate this sparse norm regularized learning problem as a non-smooth convex optimization problem, and develop a fast proximal gradient algorithm to solve it for an optimal solution. Our empirical study demonstrates the efficacy of theproposed method on a set of multi-label tasks given a limited number of labeled training instances."
