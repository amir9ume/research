title,abstract
Multiscale Hidden Conditional Neural Fields with Adaptive-Rate Latent Variable Grouping,"We hypothesize that considering multiple levels of abstraction with adaptive-rate time quantization improves temporal sequence learning. To prove this empirically, we developed multiscale hidden conditional neural fields with adaptive-rate latent variable grouping. Our model is comprised of multiple layers, where each layer is recursively built from the preceding layer by aggregating observations that are similar in the latent space, representing the sequence at a coarser scale. We extract a nonlinear combination of features from grouped variables using a set of gate functions, learning the optimal abstraction of the sequence at each scale. This allows our model to learn higher level abstractions at ever more coarse-grained time scale as the layer gets higher. Optimization is performed layer-wise, making the complexity grow linearly with the number of layers. We evaluate our approach on three human activity datasets: ArmGesture, NATOPS, and Canal9. Our method achieves a near perfect recognition accuracy on the ArmGesture dataset, and outperforms all baseline models on the NATOPS and Canal9 dataset."
Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery,"Given pairwise dissimilarities between data points, we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection. We formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming. The solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives. We obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives. When data points are distributed around multiple clusters according to the dissimilarities, we show that the data in each cluster select only representatives from that cluster. Unlike metric-based methods, our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality. We demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text."
Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery,"Given pairwise dissimilarities between data points, we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection. We formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming. The solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives. We obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives. When data points are distributed around multiple clusters according to the dissimilarities, we show that the data in each cluster select only representatives from that cluster. Unlike metric-based methods, our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality. We demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text."
Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA,"Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules in correspondence to components of a monosynaptic spinal loop. Results of emulated activities are shown. The paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions. "
Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition,We present data-driven techniques to augment Bag of Word (BoW) models within a Vector Space Model framework. This approach allows for more robust modeling and recognition of complex activities for instances where the structure and topology of the activities are not known a priori. Our approach addresses the limitation of standard BoW approaches that fail to represent the temporal information inherent in activity streams. We also introduce the use of randomly sampled Regular Expressions to capture the global structure of activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in three complex data-sets. 
Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition,We present data-driven techniques to augment Bag of Word (BoW) models within a Vector Space Model framework. This approach allows for more robust modeling and recognition of complex activities for instances where the structure and topology of the activities are not known a priori. Our approach addresses the limitation of standard BoW approaches that fail to represent the temporal information inherent in activity streams. We also introduce the use of randomly sampled Regular Expressions to capture the global structure of activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in three complex data-sets. 
Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition,We present data-driven techniques to augment Bag of Word (BoW) models within a Vector Space Model framework. This approach allows for more robust modeling and recognition of complex activities for instances where the structure and topology of the activities are not known a priori. Our approach addresses the limitation of standard BoW approaches that fail to represent the temporal information inherent in activity streams. We also introduce the use of randomly sampled Regular Expressions to capture the global structure of activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in three complex data-sets. 
Convergence Analysis and Ef?cient Algorithms for Hyper-Graph Matching,"This paper focuses on the theoretical and algorithmic design for hyper graph matching where the affinity is represented by a tensor. We start with the simple Gradient Assignment (HGA) that works in the discrete domain iteratively, and ?nd its m-point loop convergence property under rather weak conditions, where m is the tensor order. Then Hyper Constrained Gradient Assignment (HCGA) is proposed to avoid such unwanted iteration track, and we show this algorithm will ensure to converge to a unique discrete point. Then we further extend HCGA to the continuous domain: HSCGA which plays the role to the classical Graduate Assignment (HGAGM) as HCGA to HGA. Then we explore the underlying connection between HCGA and its continuous counterpart both theoretically and empirically: under weak conditions, we first prove HGAGM will converge to m-discrete point like HGA, then illustrate HSCGA have the same convergence property with HCGA. These findings build the theoretical connection between the proposed two algorithms. Experimental results on both synthetic and real data consent our theoretical analysis: both algorithms perform competitively to state-of-the-arts. While HCGA outstands due to its working in the discrete space, able to handle ill cases when Hungarian method return multiple solutions, and being an ef?cient and anytime algorithm."
Efficient Robust Recovery of Low-rank Tensors,"Robust tensor recovery in the presence of outliers, gross corruptions and missing values plays an instrumental role in robustifying tensor decompositions for multilinear data analysis and has a diverse array of applications.  In this paper, we study the problem of robust low-rank tensor recovery in a convex optimization framework, drawing upon the recent advance in robust PCA and tensor completion.  We propose tailored optimization algorithms with global convergence guarantees for solving both the constrained and the Lagrangian formulations of the problem.  These algorithms are based on the highly efficient alternating direction augmented Lagrangian and accelerated proximal gradient methods.  We investigate the empirical recoverability properties of the convex formulation and compare the recovery and computational performance of the algorithms on both simulated and real data."
Efficient Robust Recovery of Low-rank Tensors,"Robust tensor recovery in the presence of outliers, gross corruptions and missing values plays an instrumental role in robustifying tensor decompositions for multilinear data analysis and has a diverse array of applications.  In this paper, we study the problem of robust low-rank tensor recovery in a convex optimization framework, drawing upon the recent advance in robust PCA and tensor completion.  We propose tailored optimization algorithms with global convergence guarantees for solving both the constrained and the Lagrangian formulations of the problem.  These algorithms are based on the highly efficient alternating direction augmented Lagrangian and accelerated proximal gradient methods.  We investigate the empirical recoverability properties of the convex formulation and compare the recovery and computational performance of the algorithms on both simulated and real data."
Venue Discovery,Didn't finish writing the abstract yet...
Learning Multiple Concepts with Incremental Diverse Density,"We present a novel method of learning multiple disjunct concepts with diverse density using an incremental approach.  We demonstrate that by maximizing the diverse density over individual target concept points and minimizing the probability of their intersection, concepts can be learned incrementally.  This method reduces the complexity of the algorithm from factorial, with respect to the number of targets, to exponential order.  We demonstrate that this greedy approach successfully learns disjunctive target concepts with competitive classification accuracy on a benchmark multiple instance learning dataset in comparison to other common diverse density approaches.  We also introduce a novel application of the multiple instance learning framework to an emotion recognition task using prosodic and spectral speech features."
Learning Multiple Concepts with Incremental Diverse Density,"We present a novel method of learning multiple disjunct concepts with diverse density using an incremental approach.  We demonstrate that by maximizing the diverse density over individual target concept points and minimizing the probability of their intersection, concepts can be learned incrementally.  This method reduces the complexity of the algorithm from factorial, with respect to the number of targets, to exponential order.  We demonstrate that this greedy approach successfully learns disjunctive target concepts with competitive classification accuracy on a benchmark multiple instance learning dataset in comparison to other common diverse density approaches.  We also introduce a novel application of the multiple instance learning framework to an emotion recognition task using prosodic and spectral speech features."
Learning Multiple Concepts with Incremental Diverse Density,"We present a novel method of learning multiple disjunct concepts with diverse density using an incremental approach.  We demonstrate that by maximizing the diverse density over individual target concept points and minimizing the probability of their intersection, concepts can be learned incrementally.  This method reduces the complexity of the algorithm from factorial, with respect to the number of targets, to exponential order.  We demonstrate that this greedy approach successfully learns disjunctive target concepts with competitive classification accuracy on a benchmark multiple instance learning dataset in comparison to other common diverse density approaches.  We also introduce a novel application of the multiple instance learning framework to an emotion recognition task using prosodic and spectral speech features."
An Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks,"We present an axiomatic construction of hierarchical clustering in asymmetric networks where the dissimilarity from node $a$ to node $b$ is not necessarily equal to the dissimilarity from node $b$ to node $a$. The theory is built on the axioms of value, influence, and transformation. The Axiom of Value says that in a two-node network the nodes cluster at resolution equal to the maximum dissimilarity between them. The Axiom of Influence says that no clusters are formed at resolutions that do not allow bidirectional paths to be formed. The Axiom of Transformation states that if we consider a network and do not increase any pairwise dissimilarity, the level at which two nodes become part of the same cluster is not larger than the level at which they were clustered together in the original network. Two asymmetric hierarchical clustering methods that abide to these axioms are derived. Reciprocal clustering requires clusters to form through arcs that are similar in both directions. Nonreciprocal clustering allows clusters to form through cycles of small dissimilarity. We further show that any clustering method that satisfies the axioms of value, influence, and transformation lies between reciprocal and nonreciprocal clustering in the sense that all other methods cluster two points together at resolutions larger than nonreciprocal clustering and smaller than reciprocal clustering. To conclude, we apply this theory to the formation of circles of trust in social networks."
An Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks,"We present an axiomatic construction of hierarchical clustering in asymmetric networks where the dissimilarity from node $a$ to node $b$ is not necessarily equal to the dissimilarity from node $b$ to node $a$. The theory is built on the axioms of value, influence, and transformation. The Axiom of Value says that in a two-node network the nodes cluster at resolution equal to the maximum dissimilarity between them. The Axiom of Influence says that no clusters are formed at resolutions that do not allow bidirectional paths to be formed. The Axiom of Transformation states that if we consider a network and do not increase any pairwise dissimilarity, the level at which two nodes become part of the same cluster is not larger than the level at which they were clustered together in the original network. Two asymmetric hierarchical clustering methods that abide to these axioms are derived. Reciprocal clustering requires clusters to form through arcs that are similar in both directions. Nonreciprocal clustering allows clusters to form through cycles of small dissimilarity. We further show that any clustering method that satisfies the axioms of value, influence, and transformation lies between reciprocal and nonreciprocal clustering in the sense that all other methods cluster two points together at resolutions larger than nonreciprocal clustering and smaller than reciprocal clustering. To conclude, we apply this theory to the formation of circles of trust in social networks."
CODA: High Dimensional Copula Discriminant Analysis,"We propose a high dimensional classification method, named Copula Discriminant Analysis(CODA), which generalizes the normal-based linear discriminant analysis to the larger nonparanormal as proposed by Han Liu (2009). To simultaneously achieve estimation efficiency and robustness, the nonparametric rank-based methods including the Spearman?s rho and Kendall?s tau are exploited in estimating the covariance matrix. In high dimensional settings, we prove that the sparsity pattern of the discriminant features can be consistently recovered with the parametric rate, and the expected misclassification error is consistent to the Bayes risk. Our theory is backed up by careful numerical experiments, which show that the extra flexibility gained by the CODA method incurs little efficiency loss even when the data are truly Gaussian. These results suggest that the CODA method can be a safe replacement of the normal-based high dimensional linear discriminant analysis."
Spectral Graph Cut from a Filtering Point of View,"We analyze spectral graph theory based image segmentation algorithms and show there is a natural connection with edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated application of bilateral filtering. Then, using this interpretation we present and implement a fast normalized cut algorithm. Experiments show that our implementation can solve the original optimization problem with a  10x-100x speedup. In addition to these practical advantages, our work shows a deep connection between two currently separate approaches to segmentation, which suggests further directions for improvements."
Spectral Graph Cut from a Filtering Point of View,"We analyze spectral graph theory based image segmentation algorithms and show there is a natural connection with edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated application of bilateral filtering. Then, using this interpretation we present and implement a fast normalized cut algorithm. Experiments show that our implementation can solve the original optimization problem with a  10x-100x speedup. In addition to these practical advantages, our work shows a deep connection between two currently separate approaches to segmentation, which suggests further directions for improvements."
Bilinear Low-Rank Matrix Hashing with Rank-Sensitive Block Permutation,"Conventional locality-sensitive hashing only handles the inputs in the forms of vectors or sets. This paper explores a new topic of matrix hashing. Search of nearest neighbor data in the matrix form can be found in many applications such as detection of image duplicates and geographical distributions. Reducing the task to 1D vector search may incur significant information loss. Our contributions are two-folds: first, under mild assumptions on the matrix, we investigate the relationship between the matrix rank and the difficulty of nearest matrix search. Based on the notation of \emph{random matrix similarity}, we show that low-rank matrices are often more favorable in such a task. Second, we compare different schemes on matrix hashing. Among them, the linear hashing scheme is a natural extension from the conventional vector field to matrices. However, for matrices of size $n \times m$, its complexity is $O(n m)$ for both computation and storage, which is unaffordable for large matrices.To solve this problem, this paper proposes a bilinear matrix hashing scheme which greatly reduces the complexity by exploiting the matrix singular structures. We present very interesting observations on bilinear matrix hashing: although the efficacy of the scheme is mainly determined by the matrix low-rankness, its performance will be enhanced when matrix blocks are permutated to increase the rank. Therefore the final bilinear scheme involves a very interesting and practical tradeoff between block-level high-rankness and inner-block low-rankness. We conduct an in-depth study of this issue based on kurtosis analysis and provide a rank-sensitive algorithm for learning a universal block permutation. Extensive experiments are presented to corroborate the effectiveness of the proposed matrix hashing scheme."
What is foreground: From the view of photographers,"In this paper we focus on foreground extraction in digital images. We redefine foreground as an object close to the camera in real distance, and at the same time clear in the image. Based on this observation, we propose a depth of field measure which represents the distance of object from camera, and cooperate with saliency based approach. Specifically, for a given image, we firstly extract a Saliency Map (SM) and a Defocus Map (DM), with their values corresponding to saliency value and depth of field value. Then we segment images and extract segment feature based on these two extracted maps. A classifier is trained to separate foreground from background. On test images, segments are considered to be foreground if they have high enough confidence on the trained classification model. The proposed method is tested on MSRA Salient Object Detection image set and Flickr image set. Experimental result demonstrates that our method can obtain better result on foreground extraction over related methods."
What is foreground: From the view of photographers,"In this paper we focus on foreground extraction in digital images. We redefine foreground as an object close to the camera in real distance, and at the same time clear in the image. Based on this observation, we propose a depth of field measure which represents the distance of object from camera, and cooperate with saliency based approach. Specifically, for a given image, we firstly extract a Saliency Map (SM) and a Defocus Map (DM), with their values corresponding to saliency value and depth of field value. Then we segment images and extract segment feature based on these two extracted maps. A classifier is trained to separate foreground from background. On test images, segments are considered to be foreground if they have high enough confidence on the trained classification model. The proposed method is tested on MSRA Salient Object Detection image set and Flickr image set. Experimental result demonstrates that our method can obtain better result on foreground extraction over related methods."
What is foreground: From the view of photographers,"In this paper we focus on foreground extraction in digital images. We redefine foreground as an object close to the camera in real distance, and at the same time clear in the image. Based on this observation, we propose a depth of field measure which represents the distance of object from camera, and cooperate with saliency based approach. Specifically, for a given image, we firstly extract a Saliency Map (SM) and a Defocus Map (DM), with their values corresponding to saliency value and depth of field value. Then we segment images and extract segment feature based on these two extracted maps. A classifier is trained to separate foreground from background. On test images, segments are considered to be foreground if they have high enough confidence on the trained classification model. The proposed method is tested on MSRA Salient Object Detection image set and Flickr image set. Experimental result demonstrates that our method can obtain better result on foreground extraction over related methods."
What is foreground: From the view of photographers,"In this paper we focus on foreground extraction in digital images. We redefine foreground as an object close to the camera in real distance, and at the same time clear in the image. Based on this observation, we propose a depth of field measure which represents the distance of object from camera, and cooperate with saliency based approach. Specifically, for a given image, we firstly extract a Saliency Map (SM) and a Defocus Map (DM), with their values corresponding to saliency value and depth of field value. Then we segment images and extract segment feature based on these two extracted maps. A classifier is trained to separate foreground from background. On test images, segments are considered to be foreground if they have high enough confidence on the trained classification model. The proposed method is tested on MSRA Salient Object Detection image set and Flickr image set. Experimental result demonstrates that our method can obtain better result on foreground extraction over related methods."
What is foreground: From the view of photographers,"In this paper we focus on foreground extraction in digital images. We redefine foreground as an object close to the camera in real distance, and at the same time clear in the image. Based on this observation, we propose a depth of field measure which represents the distance of object from camera, and cooperate with saliency based approach. Specifically, for a given image, we firstly extract a Saliency Map (SM) and a Defocus Map (DM), with their values corresponding to saliency value and depth of field value. Then we segment images and extract segment feature based on these two extracted maps. A classifier is trained to separate foreground from background. On test images, segments are considered to be foreground if they have high enough confidence on the trained classification model. The proposed method is tested on MSRA Salient Object Detection image set and Flickr image set. Experimental result demonstrates that our method can obtain better result on foreground extraction over related methods."
Multi-task Vector Field Learning,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach."
Object Focused Q-learning for Autonomous Agents ,"We present Object Focused Q-learning (OF-Q), a novel reinforcement learning algorithm that can offer exponential speedups over classic Q-learning on domains composed of nearly-independent objects. OF-Q treats the state space as a collection of different objects organized into different object classes. Our key contribution is to estimate the risk of different objects by learning non-optimal Q-functions that we incorporate into a control policy. We compare our algorithm to traditional Q-learning and previous arbitration algorithms in two domains, including a version of Space Invaders."
Object Focused Q-learning for Autonomous Agents ,"We present Object Focused Q-learning (OF-Q), a novel reinforcement learning algorithm that can offer exponential speedups over classic Q-learning on domains composed of nearly-independent objects. OF-Q treats the state space as a collection of different objects organized into different object classes. Our key contribution is to estimate the risk of different objects by learning non-optimal Q-functions that we incorporate into a control policy. We compare our algorithm to traditional Q-learning and previous arbitration algorithms in two domains, including a version of Space Invaders."
Object Focused Q-learning for Autonomous Agents ,"We present Object Focused Q-learning (OF-Q), a novel reinforcement learning algorithm that can offer exponential speedups over classic Q-learning on domains composed of nearly-independent objects. OF-Q treats the state space as a collection of different objects organized into different object classes. Our key contribution is to estimate the risk of different objects by learning non-optimal Q-functions that we incorporate into a control policy. We compare our algorithm to traditional Q-learning and previous arbitration algorithms in two domains, including a version of Space Invaders."
A Generalized Theory of PAC-Learning,"In this article we prove that probably approximately correct (PAC) learning is guaranteed under a more general assumption that the loss function has finite variance under the sample distribution. We establish our theory based on the existence of a metric between hypothesis, to which the loss function is Lipschitz continuous. These results will give hope for learning using unbouded loss functions and real-valued hypotheses, such as energy-based models, unbouded loss regression and neural networks. More importantly, even with such generalization, asymptotic bounds as good as previous formulations of PAC-learning have been achieved."
An Optimal Policy for Target Localization with Application to Electron Microscopy,"This paper considers the task of finding a target location by making a limited number of sequential observation. Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position. Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked. In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon. Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies."
Data split strategies for evolving predictive models,"A conventional textbook prescription for building good predictive models is to split the data into three parts: training set (for model fitting), validation set (for model selection), and test set (for final model assessment).  Predictive models can potentially evolve over time as developers improve their performance either by acquiring new data or improving the existing model. The main contribution of this paper is to discuss problems encountered and propose various workflows to manage the allocation of newly acquired data into different sets in such dynamic model building and updating scenarios. We propose three different workflows (parallel dump, serial waterfall, and hybrid) for allocating new data into the existing training, validation, and test splits. Particular emphasis is laid on avoiding the bias due to the repeated use of the existing validation or the test set. "
Active sampling as a curriculum for model selection,Conventionally active learning has been used to query the best example to label in order to reduce the labeling cost. In this paper we show how active learning (uncertainty sampling in particular) can be used as a curriculum strategy for nested model selection problems. We exploit the phase transition like phenomenon observed in active learning for misspecified models to select the number of components in mixture models.
Automatic Feature Induction for Stagewise Collaborative Filtering,"Recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms."
Automatic Feature Induction for Stagewise Collaborative Filtering,"Recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms."
Automatic Feature Induction for Stagewise Collaborative Filtering,"Recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms."
Selective Labeling via Error Bound Minimization,"In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods."
Max-Margin Structured Output Regression for Spatio-Temporal Action Localization,"Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because one needs to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efficient Max-Path search method, thus makes it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method significantly outperforms the state-of-the-art methods."
Max-Margin Structured Output Regression for Spatio-Temporal Action Localization,"Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because one needs to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efficient Max-Path search method, thus makes it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method significantly outperforms the state-of-the-art methods."
Non-parametric Approximate Dynamic Programming via the Kernel	Method,"This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful, dimension-independent approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our non-parametric procedure is competitive with parametric ADP approaches."
Passivity-based Monitoring of POMDPs,"Maintaining exact belief states in POMDPs can be a difficult task since the size of the belief state grows exponentially with the number of state variables. Boyen and Koller described an approximation method which exploits locality in the process by maintaining smaller belief states over clusters of correlated variables. While this is a useful method, it does not fully account for the causal relations between the variables. We study a particular type of causal relation, called passivity, which captures the notion that a variable changes only if any of the variables that directly influence it change or if it is the target of an action. We show that passivity can be exploited in conjunction with locality to accelerate the monitoring task. The idea is to maintain separate beliefs over subsets of correlated variables, and to update only those beliefs whose variables we suspect to have changed. We present an algorithm, called Passivity-based Parallel Monitoring (PPM), that implements this idea. We show empirically that PPM outperforms two state-of-the-art solutions, while maintaining competitive accuracy. Our experiments indicate that the relative computational gains grow significantly with the size of the process."
A Stochastic Spiking Network Model of Sensorimotor Control,"Despite decades of research on sensorimotor control, there are no models thatprovide a link between the stochastic activity of neuron populations and humanbehavior when faced with uncertain, redundant, or novel environments. Here wepropose a new computational model that provides a direct link between neuronalactivity and behavior. Our model is based on a recent mathematical frameworkthat works with state probability densities rather than explicit state variables. Itcomprises a spiking neural network including sensory receptors, sensory cortex,control operators, and motoneurons. Sensory cortex computes internal state probabilityestimates by Bayesian filtering of measurements from sensory receptorsand efference information from the motor neurons. Motoneuron commands arecalculated by optimizing a cost/value function, using internal estimates and controloperators stored as synaptic weights. We simulated the model in Matlab andimplemented it on a Phantom robot arm. Simulations and robotic demonstrationspredicted a wide variety of behavior, such as reaching and tracking, reflexes intask-relevant directions, and reward/penalty trade-off responses. Our new computationalmodel can provide a neurophysiological explanation of specific humansensorimotor functions under uncertainty."
Balancing a Pole with a Cerebellum,"Much is yet to be learned about the detailed biological functioning of the human brain. Of all the parts of the brain, one of the best understood at the level of synaptic organization is the cerebellum. This article describes a biologically constrained cellular-level model of the mammalian cerebellum which is capable of learning to balance an inverted pendulum. Results indicate that the sample complexity of this novel learning agent surpasses baseline reinforcement learning and control agents. Experiments with this model predict that regular error signals play an essential part in avoiding extinction of learned responses. These predictions indicate a direction for future studies involving the acquisition of long-lasting behaviors in the absence of repeated error signals."
Balancing a Pole with a Cerebellum,"Much is yet to be learned about the detailed biological functioning of the human brain. Of all the parts of the brain, one of the best understood at the level of synaptic organization is the cerebellum. This article describes a biologically constrained cellular-level model of the mammalian cerebellum which is capable of learning to balance an inverted pendulum. Results indicate that the sample complexity of this novel learning agent surpasses baseline reinforcement learning and control agents. Experiments with this model predict that regular error signals play an essential part in avoiding extinction of learned responses. These predictions indicate a direction for future studies involving the acquisition of long-lasting behaviors in the absence of repeated error signals."
Balancing a Pole with a Cerebellum,"Much is yet to be learned about the detailed biological functioning of the human brain. Of all the parts of the brain, one of the best understood at the level of synaptic organization is the cerebellum. This article describes a biologically constrained cellular-level model of the mammalian cerebellum which is capable of learning to balance an inverted pendulum. Results indicate that the sample complexity of this novel learning agent surpasses baseline reinforcement learning and control agents. Experiments with this model predict that regular error signals play an essential part in avoiding extinction of learned responses. These predictions indicate a direction for future studies involving the acquisition of long-lasting behaviors in the absence of repeated error signals."
Balancing a Pole with a Cerebellum,"Much is yet to be learned about the detailed biological functioning of the human brain. Of all the parts of the brain, one of the best understood at the level of synaptic organization is the cerebellum. This article describes a biologically constrained cellular-level model of the mammalian cerebellum which is capable of learning to balance an inverted pendulum. Results indicate that the sample complexity of this novel learning agent surpasses baseline reinforcement learning and control agents. Experiments with this model predict that regular error signals play an essential part in avoiding extinction of learned responses. These predictions indicate a direction for future studies involving the acquisition of long-lasting behaviors in the absence of repeated error signals."
Fast Max-kernel Search,"The wide applicability of kernels make the problem of max-kernel searchubiquitous and more general than the usual similarity search. We focus onsolving this problem efficiently. We begin by characterizing the inherenthardness of the max-kernel search problem with a novel notion of {\emdirectional concentration}. Following that, we present a method to use an $O(n\log n)$ scheme to index any set of objects (points in $\Real^\dims$ or abstractobjects) \textit{directly in the kernel space} without any explicitrepresentation of the points in this kernel space. A provably $O(\log n)$branch-and-bound algorithm is presented using this index for exact max-kernelsearch. Empirical results for a variety of data sets as well as abstract objectsdemonstrate up to 4 orders of magnitude speedup in some cases. Extensions forapproximate max-kernel search are also presented."
Fast Max-kernel Search,"The wide applicability of kernels make the problem of max-kernel searchubiquitous and more general than the usual similarity search. We focus onsolving this problem efficiently. We begin by characterizing the inherenthardness of the max-kernel search problem with a novel notion of {\emdirectional concentration}. Following that, we present a method to use an $O(n\log n)$ scheme to index any set of objects (points in $\Real^\dims$ or abstractobjects) \textit{directly in the kernel space} without any explicitrepresentation of the points in this kernel space. A provably $O(\log n)$branch-and-bound algorithm is presented using this index for exact max-kernelsearch. Empirical results for a variety of data sets as well as abstract objectsdemonstrate up to 4 orders of magnitude speedup in some cases. Extensions forapproximate max-kernel search are also presented."
Fast Max-kernel Search,"The wide applicability of kernels make the problem of max-kernel searchubiquitous and more general than the usual similarity search. We focus onsolving this problem efficiently. We begin by characterizing the inherenthardness of the max-kernel search problem with a novel notion of {\emdirectional concentration}. Following that, we present a method to use an $O(n\log n)$ scheme to index any set of objects (points in $\Real^\dims$ or abstractobjects) \textit{directly in the kernel space} without any explicitrepresentation of the points in this kernel space. A provably $O(\log n)$branch-and-bound algorithm is presented using this index for exact max-kernelsearch. Empirical results for a variety of data sets as well as abstract objectsdemonstrate up to 4 orders of magnitude speedup in some cases. Extensions forapproximate max-kernel search are also presented."
Sparse Approximation via Penalty Decomposition Methods,"In this paper we consider general sparse approximation problems, that is, general $l_0$ minimization problems with the $l_0$-``norm'' of a vector being a part of constraints or objective function. We assume that the $l_0$ part is the only nonconvex part in these problems. We then propose penalty decomposition (PD) methods for solving them in which a sequence of penalty subproblems are solved by a block coordinate descent (BCD) method. Under some suitable assumptions, we show that any accumulation point of the sequence generated by the PD method is a local minimizer of the problems. Moreover, we establish that any accumulation point of the sequence generated by the BCD method is a local minimizer of the penalty subproblem. Finally, we test the performance of our PD methods by applying them to sparse logistic regression and compressed sensing problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality.    "
Sparse Approximation via Penalty Decomposition Methods,"In this paper we consider general sparse approximation problems, that is, general $l_0$ minimization problems with the $l_0$-``norm'' of a vector being a part of constraints or objective function. We assume that the $l_0$ part is the only nonconvex part in these problems. We then propose penalty decomposition (PD) methods for solving them in which a sequence of penalty subproblems are solved by a block coordinate descent (BCD) method. Under some suitable assumptions, we show that any accumulation point of the sequence generated by the PD method is a local minimizer of the problems. Moreover, we establish that any accumulation point of the sequence generated by the BCD method is a local minimizer of the penalty subproblem. Finally, we test the performance of our PD methods by applying them to sparse logistic regression and compressed sensing problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality.    "
Sparse Principal Components Analysis via Decomposition Methods,"In this paper we first propose new formulations for sequentially finding sparse PCs by using $l_0$-``norm'' of the loading vector as part of the constraints. By using these formulations, a sparse PC can be obtained with the desired cardinality of the loading vectors. Then we propose formulations that also consider the orthogonality of the loading vectors. We further generalize the proposed formulations to simultaneously find the first $r$ sparse PCs. To solve the proposed formulations, we apply decomposition methods. Under some suitable assumptions,  we show that any accumulation point of the sequence generated by the proposed decomposition methods satisfies the first-order optimality conditions of the corresponding problems. Finally, we compare the sparse PCA approaches proposed in this paper with several existing methods using gene expression data. The computational results demonstrate that the sparse PCs obtained by our approaches substantially outperform those obtained by the other methods in terms of solution quality. "
Sparse Principal Components Analysis via Decomposition Methods,"In this paper we first propose new formulations for sequentially finding sparse PCs by using $l_0$-``norm'' of the loading vector as part of the constraints. By using these formulations, a sparse PC can be obtained with the desired cardinality of the loading vectors. Then we propose formulations that also consider the orthogonality of the loading vectors. We further generalize the proposed formulations to simultaneously find the first $r$ sparse PCs. To solve the proposed formulations, we apply decomposition methods. Under some suitable assumptions,  we show that any accumulation point of the sequence generated by the proposed decomposition methods satisfies the first-order optimality conditions of the corresponding problems. Finally, we compare the sparse PCA approaches proposed in this paper with several existing methods using gene expression data. The computational results demonstrate that the sparse PCs obtained by our approaches substantially outperform those obtained by the other methods in terms of solution quality. "
Tree-structured Kernel Dimension Reduction,"Tree-structured approaches are to iteratively split data until some termination criterion is satisfied.  Most of the approaches can deal with either supervised learning, or unsupervised learning, but not both.  In this paper, we propose (residual) tree-structured Kernel Dimension Reduction (rtKDR/tKDR) approaches to address the issue. Specifically, we alternatively use kernel dimension reduction (KDR) to estimate a linear projection direction of (residual) response variables in each non-terminal node for maximizing conditional independence between explanatory variables and (residual) response variables under Reproducing Kernel Hilbert Spaces, and split the (residual) data based on the median of projection indices that the (residual) data project into the direction. When the explanatory variables are continuous, discrete and response ones, rtKDR/tKDR can deal with not only supervised learning but also unsupervised learning.  Furthermore, rtKDR has the potential of discovering the intrinsic dimensions from high-dimensional nonlinear data sets. Experiments in several benchmark datasets indicate that rtKDR/tKDR attain better space partition and prediction performances compared with several recently published space partition algorithms. "
A Recalibration Procedure which maximizes the AUC: A Use-Case for Bi-Normal Assumptions,"Area under the receiver operating characteristic curve (AUC) is a popular measure for evaluating the quality of binary classification rules. Commonly used score-based classifiers label an outcome as a positive if the score is greater than a certain threshold. We show that this may not be optimal in terms of maximizing the AUC. Under certain assumptions the optimal thresholding rule is derived using the Neyman-Pearson lemma. Specifically, we show that a thresholding rule that isquadratic in the score dominates the commonly used linear thresholding rule. Our work provides a real data use-case where the recalibration significantly improvesthe AUC."
A Recalibration Procedure which maximizes the AUC: A Use-Case for Bi-Normal Assumptions,"Area under the receiver operating characteristic curve (AUC) is a popular measure for evaluating the quality of binary classification rules. Commonly used score-based classifiers label an outcome as a positive if the score is greater than a certain threshold. We show that this may not be optimal in terms of maximizing the AUC. Under certain assumptions the optimal thresholding rule is derived using the Neyman-Pearson lemma. Specifically, we show that a thresholding rule that isquadratic in the score dominates the commonly used linear thresholding rule. Our work provides a real data use-case where the recalibration significantly improvesthe AUC."
Human Boosting,"Humans are exceptional learners, but like machine learning algorithms, they have inductive biases and on some learning tasks they perform poorly. In this paper, we try to work around these biases using boosting. Boosting allows us to combine several weak learners and construct a strong learner. We consider classification (category learning) tasks and adapt the FilterBoost framework for use with human learners. Boosting is sequential in nature and cannot be easily verified in a laboratory setting. We use Mechanical Turk to design and conduct experiments on synthetic and real world datasets which test certain properties of human learners and discuss potential applications for this framework."
Human Boosting,"Humans are exceptional learners, but like machine learning algorithms, they have inductive biases and on some learning tasks they perform poorly. In this paper, we try to work around these biases using boosting. Boosting allows us to combine several weak learners and construct a strong learner. We consider classification (category learning) tasks and adapt the FilterBoost framework for use with human learners. Boosting is sequential in nature and cannot be easily verified in a laboratory setting. We use Mechanical Turk to design and conduct experiments on synthetic and real world datasets which test certain properties of human learners and discuss potential applications for this framework."
Understanding Trees via Margins,"Building off recent work such as [Fruend, et.al.,2007], we seek to further understanding of  the behavior of multidimensional trees in high dimensional data. These trees are used in nearest-neighbor search, vector quantization, classification and other tasks. Usual analysis of trees investigate the capability of trees to adapt to some notion of intrinsic dimensions. In this paper, we provide an alternate avenue to mitigate high dimension effects -- margins. To this end, (1) we quantify the contribution of margins to the performance of tree, and (2) we formalize an intuitive notion of robustness and present its dependence on the margins.  We also provide empirical evidence showing that large margin splits can result in good quality indexing in high dimensions while being fairly robust to perturbations. "
Understanding Trees via Margins,"Building off recent work such as [Fruend, et.al.,2007], we seek to further understanding of  the behavior of multidimensional trees in high dimensional data. These trees are used in nearest-neighbor search, vector quantization, classification and other tasks. Usual analysis of trees investigate the capability of trees to adapt to some notion of intrinsic dimensions. In this paper, we provide an alternate avenue to mitigate high dimension effects -- margins. To this end, (1) we quantify the contribution of margins to the performance of tree, and (2) we formalize an intuitive notion of robustness and present its dependence on the margins.  We also provide empirical evidence showing that large margin splits can result in good quality indexing in high dimensions while being fairly robust to perturbations. "
Majorization for CRFs and Latent Likelihoods,"The partition function plays a key role in probabilistic modeling including conditional random fields, graphical models, and maximum likelihood estimation. To optimize partition functions, this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Suchbounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems, low-rank versions of the bound are provided and outperformLBFGS as well as first-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods."
Majorization for CRFs and Latent Likelihoods,"The partition function plays a key role in probabilistic modeling including conditional random fields, graphical models, and maximum likelihood estimation. To optimize partition functions, this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Suchbounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems, low-rank versions of the bound are provided and outperformLBFGS as well as first-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods."
Structured  Sparse Learning of Multiple Gaussian Graphical Models,"We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data."
Structured  Sparse Learning of Multiple Gaussian Graphical Models,"We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data."
Structured  Sparse Learning of Multiple Gaussian Graphical Models,"We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data."
Structured  Sparse Learning of Multiple Gaussian Graphical Models,"We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data."
Structured  Sparse Learning of Multiple Gaussian Graphical Models,"We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data."
Structured  Sparse Learning of Multiple Gaussian Graphical Models,"We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data."
Nonconvex Relaxation Approaches to Robust Matrix Recovery,"Motivated by the recent developments of nonconvex penalties in sparsity modeling, we propose a nonconvex optimization model for handing the low-rank matrix recovery problem.Different from the famous robust principal component analysis (RPCA), we suggest recovering low-rank and sparse matrices via a nonconvex loss function and a nonconvex penalty instead of the convex ones.The advantages of the nonconvex approach lie in its stronger robustness.To solve the model, we devise a majorization-minimization augmented Lagrange multiplier (MM-ALM) algorithm which finds the local optimal solutions of the proposed nonconvex model.We also provide two efficient strategies to speedup MM-ALM, which make the running time comparable with the state-of-the-art algorithms solving RPCA.Finally, the experimental results demonstrate the superiority of our nonconvex approach over RPCA in terms of matrix recovery accuracy."
On Detecting Multiple Simultaneous Change-points in High Dimensional Time Series,"This paper studies the detection of multiple simultaneous (systematic) change points for high-dimensional nonstantionary time series data. The analytic framework used is based on the standard and adaptive fused group lasso method, where the mixed $L_{2,1}$ norms in the penalty are either uniform or re-weighted by data-dependent weights. This paper shows that, under appropriate conditions, this approach is $L_2$ consistent and, by adopting the data-dependent weights, could correctly select the change points with probability approaching unity. It quantifies the conditions on the interplay among the averaged (over different dimensions) minimum magnitude of structural changes, the number of change points and the number of observations for consistently discovering the change points. The performance of this approach is illustrated via an analysis of a large panel of U.S. economic and financial time series data over the past $50$ years. "
Imbalanced Random Subspace Ensemble Methods for computer-aided nodule detection,"Many lung computer-aided detection (CAD) methods have been proposed for pulmonary nodules. Because high sensitivity is essential in the candidate identification stage, there are countless false positives produced by the initial suspect nodule generation process, giving more work to radiologists. Moreover, there are more false positives than real nodules detected. In order to eliminate or reduce the false positives, we address this issue as a binary classification problem. However, given the imbalance between false positives and true positives and the different misclassification costs, we propose two new strategies of random subspace ensemble methods. Experimental results show the effectiveness of the proposed methods in terms of sensitivity and specificity."
Imbalanced Random Subspace Ensemble Methods for computer-aided nodule detection,"Many lung computer-aided detection (CAD) methods have been proposed for pulmonary nodules. Because high sensitivity is essential in the candidate identification stage, there are countless false positives produced by the initial suspect nodule generation process, giving more work to radiologists. Moreover, there are more false positives than real nodules detected. In order to eliminate or reduce the false positives, we address this issue as a binary classification problem. However, given the imbalance between false positives and true positives and the different misclassification costs, we propose two new strategies of random subspace ensemble methods. Experimental results show the effectiveness of the proposed methods in terms of sensitivity and specificity."
Linking Heterogeneous Input Spaces with Pivots for Multi-Task Learning,"Most existing works on multi-task learning (MTL) assume the same input space for different tasks. In this paper, we address a general setting where different tasks have heterogeneous input spaces. This setting has a lot of potential applications, yet it poses new algorithmic challenges - how can we link seemingly uncorrelatedtasks to mutually boost their learning performance?Our key observation is that in many real applications, there might exist some correspondence among the inputs of different tasks, which is referred to as pivots. Forsuch applications, we first propose a learning scheme for multiple tasks and analyze its generalization performance. Then we focus on the problems where only a limited number of the pivots are available, and propose a general frameworkto leverage the pivot information. The idea is to map the heterogeneous input spaces to a common space, and construct a single prediction model in this space for all the tasks. We further propose an effective optimization algorithm to find both the mappings and the prediction model. Experimental results demonstrate its effectiveness, especially with very limited number of pivots."
A Nonparametric Bayesian Classifier under a Mixture Loss Function,"Many classification problems can be conveniently formulated in terms of Bayesian mixture prior models. The mixture prior structure lends itself especially well for adapting to varying degrees of sparsity. Typically, parametric assumptions are made about the components of the mixture priors. In the following, we propose a parametric and a nonparametric classification procedures using a mixture prior Bayesian approach for a risk function that combines misclassification loss and an $L_2$ penalty. While the parametric procedure is closer to traditional approaches,in simulations, we show that the nonparametric classifier typically outperforms it when the parametric prior is misspecified; the two procedures have comparable performance even when the shape of the parametric prior is specified correctly.  We illustrate the properties of the two classifiers on a publicly available gene expression dataset."
A Nonparametric Bayesian Classifier under a Mixture Loss Function,"Many classification problems can be conveniently formulated in terms of Bayesian mixture prior models. The mixture prior structure lends itself especially well for adapting to varying degrees of sparsity. Typically, parametric assumptions are made about the components of the mixture priors. In the following, we propose a parametric and a nonparametric classification procedures using a mixture prior Bayesian approach for a risk function that combines misclassification loss and an $L_2$ penalty. While the parametric procedure is closer to traditional approaches,in simulations, we show that the nonparametric classifier typically outperforms it when the parametric prior is misspecified; the two procedures have comparable performance even when the shape of the parametric prior is specified correctly.  We illustrate the properties of the two classifiers on a publicly available gene expression dataset."
Multiresolution Gaussian Processes,"We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity."
Localizing 3D cuboids in single-view images,"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners."
Transaction-Based Link Strength Prediction Using Matrix Factorizations,"The revolution of online social networks and methods of analyzing them have attracted interest in many research fields. Predicting whether a friendship holds in a social network between two individuals or not, link prediction, has been a heavily researched topic in the last decade. In this paper we research a related problem, link strength prediction, which aims to assign ratings or strengths to friendship links. A basic approach would be matrix factorization applied to only friendship ratings. However, the existence of extensive transactions among users may be used for better predictions. We propose a new type of multiple-matrix factorization model for incorporating a transaction matrix. We derive gradient descent update equations for learning latent factors that predict values in the target rating matrix. To evaluate the model, we introduce data from Cloob which is a popular Iranian social network as well as synthetic data."
Transaction-Based Link Strength Prediction Using Matrix Factorizations,"The revolution of online social networks and methods of analyzing them have attracted interest in many research fields. Predicting whether a friendship holds in a social network between two individuals or not, link prediction, has been a heavily researched topic in the last decade. In this paper we research a related problem, link strength prediction, which aims to assign ratings or strengths to friendship links. A basic approach would be matrix factorization applied to only friendship ratings. However, the existence of extensive transactions among users may be used for better predictions. We propose a new type of multiple-matrix factorization model for incorporating a transaction matrix. We derive gradient descent update equations for learning latent factors that predict values in the target rating matrix. To evaluate the model, we introduce data from Cloob which is a popular Iranian social network as well as synthetic data."
Transaction-Based Link Strength Prediction Using Matrix Factorizations,"The revolution of online social networks and methods of analyzing them have attracted interest in many research fields. Predicting whether a friendship holds in a social network between two individuals or not, link prediction, has been a heavily researched topic in the last decade. In this paper we research a related problem, link strength prediction, which aims to assign ratings or strengths to friendship links. A basic approach would be matrix factorization applied to only friendship ratings. However, the existence of extensive transactions among users may be used for better predictions. We propose a new type of multiple-matrix factorization model for incorporating a transaction matrix. We derive gradient descent update equations for learning latent factors that predict values in the target rating matrix. To evaluate the model, we introduce data from Cloob which is a popular Iranian social network as well as synthetic data."
Workflows for Computer Vision: Open Publication and Reproducibility of Experiments,"The inability to reproduce computational research is a rapidly growing area of concern in computer vision. In this paper, we incorporate a structured, end-to-end analysis methodology, based on workflows, to easily and automatically allow for standardized replication and testing of state-of-the-art models, inter-operability of heterogeneous codebases, and incorporation of novel algorithms. We demonstrate the utility of our approach by introducing a novel computer vision dataset and conducting an in-depth, comparative analysis of state-of-the-art methods on the new Atomic Pair Actions dataset using workflows. This allows us to re-use pre-existing workflows as well as incorporating new algorithms developed in heterogeneous codebases. The entire framework, including the workflows and the dataset, is then exported as web objects which can be executed via the web, or downloaded and imported into a compatible workflow system, by any user to re-create the full analysis or to change/extend the workflows as desired. In addition, we make the full dataset (the videos, their associated tracks with ground truth, and metadata) and all exported workflows widely available to the research community both as openly accessible web objects."
Hierarchical Identification of Leaves,"There is massive diversity among deformable biological objects such as cells and plants, including a large number of genetic categories and a large variation in appearance within categories. We present a novel method for determining the species of botanical objects from scanned images. We focus on leaves but the strategy is generic, based on a hierarchical representation of latent variables called identification keys which embody domain knowledge about taxonomy and landmarks. Classification proceeds systematically from coarse-grained to fine-grained characterizations. First, keys are estimated, one at a time, starting with landmarks and proceeding to the genus, and finally the individual species is identified. Each step is conditional on previous estimates. Two other main ingredients are multiple key-based local coordinate systems and likelihood ratios of discriminant scores. We obtain the best performance to date on several databases of scanned simple leaves."
Adaptive Compressive Network Modeling,"Network data is ubiquitous nowadays, such as social networks, bio-networks, computer networks, and complex information network. However, the research of network data is still mostly heuristic and lacks rigorous theoretical underpinnings. Until very recently, a new line of work named compressive network modeling sheds light on this problem.  Specifically, [13] proposes a general framework to use compressed sensing techniques to analyze network data. This method explores a large latent dictionary which is able to recover the hidden structure within the network. However, one limitation of this framework is that the dictionary used to model the network must be pre-given, e.g. clique spaces. Such a pre-determined dictionary is not adaptive to unknown datasets. In this paper, instead of assuming the dictionary is prefixed, we propose a method to automatically learn a dictionary based on the observed networks, which better adapts to the empirical data. Our approach, named adaptive compressive network modeling, is amenable to theoretical analysis, computationally tractable and can be successfully applied to many areas. "
Scalable Heterogeneous Transfer Ranking,"Learning to rank aims to automatically learn a ranking function with point-wise, pair-wise, or list-wise cost functions and has been demonstrated to achieve superior performance in many information retrieval applications. It is known that the ranking problem usually requires significantly more labeling efforts per task. However, in practical application we can often obtain abundant labeled documents in popular languages but very few or even no labeled documents in less popular languages.  In this paper, we propose to study the problem of heterogeneous transfer ranking, a transfer learning problem with heterogeneous features in order to utilize the rich labeled data in popular languages to help the ranking task in less popular languages. We develop a large-margin algorithm, namely LM-HTR, to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We analyze the theoretical bound of the prediction loss and develop fast algorithms via stochastic gradient descent so that our model can be scalable to large-scale applications. Experiment results on four application datasets demonstrate the advantages of our algorithms over other state-of-the-art methods."
Scalable Heterogeneous Transfer Ranking,"Learning to rank aims to automatically learn a ranking function with point-wise, pair-wise, or list-wise cost functions and has been demonstrated to achieve superior performance in many information retrieval applications. It is known that the ranking problem usually requires significantly more labeling efforts per task. However, in practical application we can often obtain abundant labeled documents in popular languages but very few or even no labeled documents in less popular languages.  In this paper, we propose to study the problem of heterogeneous transfer ranking, a transfer learning problem with heterogeneous features in order to utilize the rich labeled data in popular languages to help the ranking task in less popular languages. We develop a large-margin algorithm, namely LM-HTR, to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We analyze the theoretical bound of the prediction loss and develop fast algorithms via stochastic gradient descent so that our model can be scalable to large-scale applications. Experiment results on four application datasets demonstrate the advantages of our algorithms over other state-of-the-art methods."
Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation,"Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available.Existing approaches strive to make the distribution of instances in one domain match that of the other domain.  Often, they are oblivious to individual differences of these samples and only tune to their statistical properties collectively as a population. Instead of this macroscopic view, we aim to be microscopic. In particular, we believe not all instances are created equally in terms of adaptability. Thus, it is beneficial to identify the most helpful instances for  adaptation. Mindful of this thought, we propose a landmark-based approach that broadens the notion of matching distributions  by examining it at the instance level. We define landmarks as a subset of labeled data instances in the source that are distributed most similarly to the target, thus presumably more amenable to being adapted. We then leverage the discovered landmarks to bridge the source to the target.  Specifically, we incorporate the landmarks in a cohort of auxiliary adaptation tasks that are provably easier to solve. The key intuition is that domain-invariant feature spaces for those auxiliary tasks form the basis to compose invariant features for the original problem. We show how this composition can be optimized discriminatively with the aid of landmarks,  requiring no labeled data from the target.  The proposed method is validated on standard benchmark datasets for object recognition. Empirical results show the proposed method outperforms the state-of-the-art significantly."
Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation,"Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available.Existing approaches strive to make the distribution of instances in one domain match that of the other domain.  Often, they are oblivious to individual differences of these samples and only tune to their statistical properties collectively as a population. Instead of this macroscopic view, we aim to be microscopic. In particular, we believe not all instances are created equally in terms of adaptability. Thus, it is beneficial to identify the most helpful instances for  adaptation. Mindful of this thought, we propose a landmark-based approach that broadens the notion of matching distributions  by examining it at the instance level. We define landmarks as a subset of labeled data instances in the source that are distributed most similarly to the target, thus presumably more amenable to being adapted. We then leverage the discovered landmarks to bridge the source to the target.  Specifically, we incorporate the landmarks in a cohort of auxiliary adaptation tasks that are provably easier to solve. The key intuition is that domain-invariant feature spaces for those auxiliary tasks form the basis to compose invariant features for the original problem. We show how this composition can be optimized discriminatively with the aid of landmarks,  requiring no labeled data from the target.  The proposed method is validated on standard benchmark datasets for object recognition. Empirical results show the proposed method outperforms the state-of-the-art significantly."
Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation,"Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available.Existing approaches strive to make the distribution of instances in one domain match that of the other domain.  Often, they are oblivious to individual differences of these samples and only tune to their statistical properties collectively as a population. Instead of this macroscopic view, we aim to be microscopic. In particular, we believe not all instances are created equally in terms of adaptability. Thus, it is beneficial to identify the most helpful instances for  adaptation. Mindful of this thought, we propose a landmark-based approach that broadens the notion of matching distributions  by examining it at the instance level. We define landmarks as a subset of labeled data instances in the source that are distributed most similarly to the target, thus presumably more amenable to being adapted. We then leverage the discovered landmarks to bridge the source to the target.  Specifically, we incorporate the landmarks in a cohort of auxiliary adaptation tasks that are provably easier to solve. The key intuition is that domain-invariant feature spaces for those auxiliary tasks form the basis to compose invariant features for the original problem. We show how this composition can be optimized discriminatively with the aid of landmarks,  requiring no labeled data from the target.  The proposed method is validated on standard benchmark datasets for object recognition. Empirical results show the proposed method outperforms the state-of-the-art significantly."
Stratified Tree Search: A Novel Suboptimal Heuristic Search Algorithm,"Traditional heuristic search algorithms use the ranking of states a heuristic function provides to guide the search. In this paper---with the object of improving suboptimality and runtime of search algorithms when only weak heuristics are available---, we present Stratified Tree Search (STS), a novel suboptimal heuristic search algorithm that uses a heuristic function to make a partition of the state space to guide search. We call the partition used by STS a type system. STS assumes that nodes of the same type will lead to solutions of the same cost. Thus, STS expands only one node of each type in every level of search. We empirically evaluated STS in heuristic search domains. Our results show that STS can find solutions of lower suboptimality in less time than standard heuristic search algorithms for finding suboptimal solutions."
Stratified Tree Search: A Novel Suboptimal Heuristic Search Algorithm,"Traditional heuristic search algorithms use the ranking of states a heuristic function provides to guide the search. In this paper---with the object of improving suboptimality and runtime of search algorithms when only weak heuristics are available---, we present Stratified Tree Search (STS), a novel suboptimal heuristic search algorithm that uses a heuristic function to make a partition of the state space to guide search. We call the partition used by STS a type system. STS assumes that nodes of the same type will lead to solutions of the same cost. Thus, STS expands only one node of each type in every level of search. We empirically evaluated STS in heuristic search domains. Our results show that STS can find solutions of lower suboptimality in less time than standard heuristic search algorithms for finding suboptimal solutions."
L$^{\natural}$-CCCP : L$^{\natural}$-Concave Convex Procedure,"L$^{\natural}$-Convexity is a discrete counterpart of convexity in a continuous function. In this paper, we propose L$^{\natural}$-CCCP (L$^{\natural}$-ConCave Convex Procedure); an approximation algorithm for minimizing the difference of two L$^{\natural}$-convex functions, which can formulate any discrete function optimization. L$^{\natural}$-CCCP is basically a discrete analog of CCCP (ConCave Convex Procedure) for D.C.\@ programing problems. L$^{\natural}$-CCCP is performed as a terminating iterative procedure, each of whose iterations can be computed in the same order as submodular minimization. We~describe the implementation of L$^{\natural}$-CCCP and prove termination at a stationary point. Moreover, we describe an application of L$^{\natural}$-CCCP to multi-label energy minimization with empirical examples."
Kernel Latent SVM for Visual Recognition,"Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning."
Kernel Latent SVM for Visual Recognition,"Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning."
Sparse Principal Component Analysis with missing observations,"In this paper, we study the problem of sparse Principal Component Analysis(PCA) in the high-dimensional setting with missing observations. Our goal isto estimate the first principal component when we only have access to partial ob-servations. Existing estimation techniques are usually derived for fully observeddata sets and require a prior knowledge of the sparsity of the first principal compo-nent in order to achieve good statistical guarantees. Our contributions is threefold.First, we establish the first information-theoretic lower bound for the sparse PCAproblem with missing observations. Second, we propose a simple procedure thatdoes not require any prior knowledge on the sparsity of the unknown first principalcomponent or any imputation of the missing observations, adapts to the unknownsparsity of the first principal component and achieves the optimal rate of estima-tion up to a logarithmic factor. Third, if the covariance matrix of interest admits asparse first principal component and is in addition approximately low-rank, thenwe can derive a completely data-driven procedure computationally tractable inhigh-dimension, adaptive to the unknown sparsity of the first principal componentand statistically optimal (up to a logarithmic factor)."
Calibrated Elastic Regularization in Matrix Completion,"This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a unified analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. "
Calibrated Elastic Regularization in Matrix Completion,"This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a unified analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. "
Deep Representations and Codes for Image Auto-Annotation,"The task of assigning a set of relevant tags to an image is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and fix an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of full sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we outperform or compete with existing annotation approaches that use over a dozen distinct image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efficient storage and fast comparisons. In our experiments, using deeper architectures always outperform shallow ones."
Deep Representations and Codes for Image Auto-Annotation,"The task of assigning a set of relevant tags to an image is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and fix an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of full sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we outperform or compete with existing annotation approaches that use over a dozen distinct image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efficient storage and fast comparisons. In our experiments, using deeper architectures always outperform shallow ones."
On High Dimensional Positive-definite Covariance Matrix Estimation,"We propose a novel approach called PLaCE (Positive-definite Large Covariance Estimation) for estimating high dimensional positive-definite covariance matrices. Our method can be viewed as an extension of the generalized thresholded operator (GTO, Rothman et. al 2010) with a positive-definite guarantee. Computationally, we derive an efficient algorithm named ISP (Iterative Soft-thresholding and Projection) based on the augmented Lagrangian method. Theoretically, we analyze the oracle properties of the PLaCE in the sparse covariance estimation under the different matrix norms. Empirically, we conduct the numerical experiments on both simulated and real data sets to illustrate the usefulness of the proposed method."
On High Dimensional Positive-definite Covariance Matrix Estimation,"We propose a novel approach called PLaCE (Positive-definite Large Covariance Estimation) for estimating high dimensional positive-definite covariance matrices. Our method can be viewed as an extension of the generalized thresholded operator (GTO, Rothman et. al 2010) with a positive-definite guarantee. Computationally, we derive an efficient algorithm named ISP (Iterative Soft-thresholding and Projection) based on the augmented Lagrangian method. Theoretically, we analyze the oracle properties of the PLaCE in the sparse covariance estimation under the different matrix norms. Empirically, we conduct the numerical experiments on both simulated and real data sets to illustrate the usefulness of the proposed method."
Controlling Transfer For Reinforcement Learning,"Recently, algorithms for transfer learning in reinforcement learning have been proposed that utilize a map between state-action pairs in the target and source tasks. This map is used to suggest possible values for states in the target task based on values of states in the source task. In this paper, we describe a generic transfer algorithm that, given any such map, determines online if the the map is correct and if not limits its use and minimizes {\em negative transfer}. We give bounds on the expected negative transfer and perform experiments to illustrate the usefulness of the algorithm."
Sample Bias Correction for Regression,"This paper presents a theoretical and empirical study of a discrepancy minimization (DM) sample bias correction algorithm in regression.  We give a theoretical analysis of sample bias correction for kernel ridge regression and prove new and more informative learning guarantees in terms of the \emph{discrepancy} of the empirical distributions. These results provide a strong theoretical support for the use and application of the DM algorithm. We have carried out an extensive empirical analysis of this algorithm both with artificial and real-world data sets and compared it with three other state-of-the-art sample bias correction algorithms applicable in regression. Till now, this algorithm had only been applied to the problem of domain adaptation and it was primarily evaluated for computational efficiency. Here we carry out a thorough comparative study of the algorithm used for the problem of sample bias correction. We report in detail the results of these empirical results which demonstrate the benefits of this algorithm."
Accuracy at the Top,"We introduce a new notion of classification accuracy based on the top $\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines. We define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems. We also present margin-based guarantees for this algorithm based on the $\tau$-quantile of the functions in the hypothesis set. Finally, we report the results of several experiments evaluating the performance of our algorithm. In a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top."
Extending Multi-Atlas Label Fusion to Groupwise Segmentation,"Groupwise segmentation that simultaneously segments a set of images and ensures the segmentations for all images are consistent with each other usually work better than segmenting each image independently. However, existing groupwise segmentation techniques are all developed based on simple and less powerful segmentation techniques, which limits the performance of groupwise segmentation when compared with other leading non-groupwise segmentation techniques. To address this problem, we develop a novel statistical model to extend the multi-atlas label fusion technique, which has shown to be very competitive for challenging biomedical image segmentation problems, for groupwise segmentation. Experiments on hippocampus segmentation in magnetic resonance images show the effectiveness of the new technique."
Multi-Class Classification with Maximum Margin Multiple Kernel,"We present a new algorithm for multi-class classification withmultiple kernels. Our algorithm is based on a natural notion of themulti-class margin of a kernel. We show that larger values ofthis quantity guarantee the existence of an accurate multi-classpredictor and also define a family of multiple kernel algorithms based onthe maximization of the multi-class margin of a kernel.  Wepresent an extensive theoretical analysis in support of our algorithm,including novel multi-class Rademacher complexity margin bounds.Finally, we also report the results of a series of experiments withseveral data sets, including comparisons where we improve upon theperformance of state-of-the-art algorithms both in binary andmulti-class classification with multiple kernels."
Graph Estimation From Multi-attribute Data,"Many real world network problems often concern multivariate nodal  attributes such as image, textual, and multi-view feature vectors on  nodes, rather than simple univariate nodal attributes. The existing  graph estimation methods built on Gaussian graphical models and  covariance selection algorithms can not handle such data, neither can  the theories developed around such methods be directly  applied. In this paper, we propose a new principled framework for  estimating multi-attribute networks. Instead of estimating the  partial correlation as in current literature, our method estimates  the {\it partial canonical correlations} that naturally accommodate  complex nodal features.  Computationally, we provide an efficient  algorithm which utilizes the multi-attribue  structure. Theoretically, we provide sufficient conditions which  guarantee consistent graph recovery. Empirically, we apply our  method on a genomic dataset to illustrate its usefulness. "
Fast Exact Search in Hamming Space Using Anchors,"Recently there are a surge of approaches to learn a compact binary code representation of the data for large-scale nearest neighbor search. Once the binary codes are obtained, these methods usually employ the linear scan or hash table lookup to search in Hamming space, which are not efficient. There are some efforts trying to speed up the search in Hamming space, however, these techniques require specific data structure which consumes a lot of memory. In this paper, we propose a novel method for sub-linear exact search in Hamming space with only linear memory cost. The key idea of our method is reducing the search range using the triangle inequality induced by a small number of anchors, which are the representative samples of the database. An effective data structure is proposed to efficiently reject samples that are unnecessarily calculated. To further improve the performance, a MaxMin algorithm is presented to select the anchors. Both the theoretical analysis and empirical evaluation demonstrate that our approach is not only more efficient than the state-of-the-art methods but also consumes much less memory, which is essential to the large-scale problems."
Fast Exact Search in Hamming Space Using Anchors,"Recently there are a surge of approaches to learn a compact binary code representation of the data for large-scale nearest neighbor search. Once the binary codes are obtained, these methods usually employ the linear scan or hash table lookup to search in Hamming space, which are not efficient. There are some efforts trying to speed up the search in Hamming space, however, these techniques require specific data structure which consumes a lot of memory. In this paper, we propose a novel method for sub-linear exact search in Hamming space with only linear memory cost. The key idea of our method is reducing the search range using the triangle inequality induced by a small number of anchors, which are the representative samples of the database. An effective data structure is proposed to efficiently reject samples that are unnecessarily calculated. To further improve the performance, a MaxMin algorithm is presented to select the anchors. Both the theoretical analysis and empirical evaluation demonstrate that our approach is not only more efficient than the state-of-the-art methods but also consumes much less memory, which is essential to the large-scale problems."
A Linearly Convergent First-order Algorithm for Total Variation Minimization in Image Processing,We introduce a new formulation for total variation minimization in image denoising. We present a linearly convergent first-order method for solving this reformulated problem and show that it possesses a nearly dimension-independent iteration complexity bound.
Error Correction in Learning Using SVMs,"This paper is concerned with learning binary classifiers under adversarial label-noise. We introduce the problem of error correction in learning where the goal is to recover the original clean data from a label-manipulated version of it, given (i)~ no constraints on the adversary other than an upper-bound on the number of errors, and (ii)~some regularity properties for the original data.  We present a simple and practical error-correction algorithm called SubSVMs that essentially learns separate SVMs on several class-balanced, small-size (often log-size), random subsets of the data and then reclassifies the training points using a majority vote. Our analysis reveals the need for the two main ingredients of SubSVMs, namely class-balanced sampling and subsampled bagging. We present experimental results on synthetic as well as benchmark data to demonstrate the effectiveness of our approach. In addition to the primary goal of noise-tolerance, log-size subsampled bagging also yields significant run-time advantages over standard SVMs."
Multi-Label Classification with Relevance Ordering,"In many real multi-label tasks, it is often the case that the ordering of relevant labels for an example is important, while that of irrelevant labels is meaningless. Such a problem, however, could not be addressed by existing multi-label learning or label ranking approaches, because the former usually ignores the ordering among relevant labels while the latter often fails to explicitly distinguish relevance from irrelevance and involves more consideration on ordering irrelevant labels. Considering that there is no adequate criterion available, in this paper, we propose PRO Loss, a new criterion which concerns the ordering of only relevant labels and classification of all the labels. We then propose ProSVM that optimizes PRO Loss efficiently with the use of alternating direction method of multipliers. We further improve efficiency with an upper approximation that reduces the number of constraints from O(T^2) to O(T), where T is the number of labels. Experiments show that our proposals not only perform superior to state-of-the-art approaches on PRO Loss, but also achieve highly competitive performance on existing evaluation criteria."
Graph Estimation with Joint Additive Models,"In recent years, there has been considerable interest in estimating conditional independence graphs in the high-dimensional setting in which the number of features exceeds the number of observations.  Most prior work in this area has assumed that the observations are drawn from a multivariate Gaussian distribution, or that conditional dependence relations among variables are linear, which as we will see are roughly equivalent. Unfortunately, if this assumption is violated, then the resulting conditional independence estimates can be inaccurate. Here we present a semi-parametric method, Sparse Conditional Estimation with Joint Additive Models (SpaCE JAM),  which allows for arbitrary additive conditional relationships among the features.  We present an efficient algorithm for its computation, and prove that our estimator is consistent.  We also  extend our method to estimation of  directed graphs.  SpaCE JAM enjoys superior performance to existing methods when there are non-linear relationships among the features, and is comparable to methods that assume multivariate normality when the features are linearly related."
Graph Estimation with Joint Additive Models,"In recent years, there has been considerable interest in estimating conditional independence graphs in the high-dimensional setting in which the number of features exceeds the number of observations.  Most prior work in this area has assumed that the observations are drawn from a multivariate Gaussian distribution, or that conditional dependence relations among variables are linear, which as we will see are roughly equivalent. Unfortunately, if this assumption is violated, then the resulting conditional independence estimates can be inaccurate. Here we present a semi-parametric method, Sparse Conditional Estimation with Joint Additive Models (SpaCE JAM),  which allows for arbitrary additive conditional relationships among the features.  We present an efficient algorithm for its computation, and prove that our estimator is consistent.  We also  extend our method to estimation of  directed graphs.  SpaCE JAM enjoys superior performance to existing methods when there are non-linear relationships among the features, and is comparable to methods that assume multivariate normality when the features are linearly related."
Graph Estimation with Joint Additive Models,"In recent years, there has been considerable interest in estimating conditional independence graphs in the high-dimensional setting in which the number of features exceeds the number of observations.  Most prior work in this area has assumed that the observations are drawn from a multivariate Gaussian distribution, or that conditional dependence relations among variables are linear, which as we will see are roughly equivalent. Unfortunately, if this assumption is violated, then the resulting conditional independence estimates can be inaccurate. Here we present a semi-parametric method, Sparse Conditional Estimation with Joint Additive Models (SpaCE JAM),  which allows for arbitrary additive conditional relationships among the features.  We present an efficient algorithm for its computation, and prove that our estimator is consistent.  We also  extend our method to estimation of  directed graphs.  SpaCE JAM enjoys superior performance to existing methods when there are non-linear relationships among the features, and is comparable to methods that assume multivariate normality when the features are linearly related."
High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning,"Joint sparsity regularization in multi-task learning has attracted much attentionin recent years. The traditional convex formulation employs the group Lasso relaxationto achieve joint sparsity across tasks. Although this approach leads to asimple convex formulation, we argue in this paper that the quadratic regularizerinduced by the group Lasso formulation is suboptimal. To remedy this problem,we view jointly sparse multi-task learning as a specialized random effects model,and derive a convex relaxation approach that involves two steps. The first steplearns the covariance matrix of the coefficients using a convex formulation whichwe refer to as sparse covariance coding; the second step solves a ridge regressionproblem with a sparse quadratic regularizer based on the covariance matrix obtainedin the first step. It is shown that this approach produces an asymptoticallyoptimal quadratic regularizer in the multitask learning setting if the number oftasks approaches infinity. Experimental results demonstrate that the convex formulationobtained via the proposed model significantly outperforms group Lasso."
High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning,"Joint sparsity regularization in multi-task learning has attracted much attentionin recent years. The traditional convex formulation employs the group Lasso relaxationto achieve joint sparsity across tasks. Although this approach leads to asimple convex formulation, we argue in this paper that the quadratic regularizerinduced by the group Lasso formulation is suboptimal. To remedy this problem,we view jointly sparse multi-task learning as a specialized random effects model,and derive a convex relaxation approach that involves two steps. The first steplearns the covariance matrix of the coefficients using a convex formulation whichwe refer to as sparse covariance coding; the second step solves a ridge regressionproblem with a sparse quadratic regularizer based on the covariance matrix obtainedin the first step. It is shown that this approach produces an asymptoticallyoptimal quadratic regularizer in the multitask learning setting if the number oftasks approaches infinity. Experimental results demonstrate that the convex formulationobtained via the proposed model significantly outperforms group Lasso."
Partition Tree Weighting,"This paper introduces Partition Tree Weighting, a  low-complexity probabilistic sequence prediction algorithm for piecewise stationary sources. It works by performing Bayesian model averaging over a large class of possible partitions of the data into stationary segments. Our prior is designed to be both efficiently computable and well suited towards data compression applications. We provide a competitive analysis of the redundancy (i.e. cumulative log-loss regret) of ourmethod, and show empirically that PTW consistently performs well relative to competing methods. We conclude by showing how to use PTW to improve the performance of a recently introduced universal data compression algorithm."
Partition Tree Weighting,"This paper introduces Partition Tree Weighting, a  low-complexity probabilistic sequence prediction algorithm for piecewise stationary sources. It works by performing Bayesian model averaging over a large class of possible partitions of the data into stationary segments. Our prior is designed to be both efficiently computable and well suited towards data compression applications. We provide a competitive analysis of the redundancy (i.e. cumulative log-loss regret) of ourmethod, and show empirically that PTW consistently performs well relative to competing methods. We conclude by showing how to use PTW to improve the performance of a recently introduced universal data compression algorithm."
Partition Tree Weighting,"This paper introduces Partition Tree Weighting, a  low-complexity probabilistic sequence prediction algorithm for piecewise stationary sources. It works by performing Bayesian model averaging over a large class of possible partitions of the data into stationary segments. Our prior is designed to be both efficiently computable and well suited towards data compression applications. We provide a competitive analysis of the redundancy (i.e. cumulative log-loss regret) of ourmethod, and show empirically that PTW consistently performs well relative to competing methods. We conclude by showing how to use PTW to improve the performance of a recently introduced universal data compression algorithm."
Low-Rank Affine Subspace Clustering,"We consider the problem of clustering high-dimensional data lying approximately in a union of affine subspaces. State-of-the-art methods solve this problem in two stages: learning an affinity matrix and applying spectral clustering. However, each stage is solved using a different optimization criteria. In this paper, we propose a unified approach in which we solve jointly for an affinity matrix and the segmentation of the data. We pose this problem as a rank minimization problem, which we solve using an alternating direction method, where the optimal solution at each iteration can be computed in closed form. Experiments on synthetic and real data demonstrate the superiority of our approach over state-of-the-art methods."
On the Consistency of AUC Optimization,"AUC has been widely used as an evaluation criterion in diverse learning tasks. Many learning approaches have been developed to optimize AUC; however, owing to the non-convexity and discontinuousness of AUC, almost all approaches work with surrogate loss functions. Thus, the consistency of AUC is a crucial issue. In this paper, we theoretically study the asymptotic consistency of learning approaches based on surrogate loss functions and provide a sufficient condition. Based on this result, we prove that the exponential loss and logistic loss are consistent with AUC, whereas the hinge loss is inconsistent. We then derive the {$q$-norm hinge loss} and {general hinge loss} that are consistent with AUC. We also derive the consistent bounds for the exponential loss and logistic loss. Additionally, we obtain the consistent bounds for many surrogate loss functions under non-noisy setting. Finally, we find an equivalence between the exponential surrogate loss of AUC and the exponential surrogate loss of accuracy, leading to a direct consequence that AdaBoost and RankBoost are asymptotically equivalent."
Privacy-preserving spectral analysis of large incoherent matrices,"In this paper we provide a highly efficient method capable of performingaccurate---yet privacy-preserving---spectral analyses of large matrices.Previously this was feasible only for small or moderately sized matrices. Incontrast, our algorithm benefits from the ``blessing of dimensionality''.Indeed, we prove that the error of our algorithm is bounded in terms of thecoherence of the input matrix. High-dimensional data tends to have lowcoherence resulting in greater accuracy of our algorithm. We evaluate ouralgorithm on both real and synthetic data sets where we obtain encouragingresults."
Deep Online Learning,"Learning an effective feature representation from data plays an important role for many machine learning tasks. Classical online learning methods typically operate directly on the input feature space. In this paper, we propose a novel framework of Deep Online Learning (DOL) which aims to improve online learning tasks by learning hierarchical feature representations from labeled and unlabeled data. We conducted extensive experiments by comparing the proposed online learning algorithms with several state-of-the-art online learning algorithms. The encouraging empirical results show that the DOL algorithms can effectively learn structure information on high-dimensional data to significantly boost online learning performance on real application with complex data including image and text data. These results indicate that exploring hierarchical representations learned from data is a promising direction for improving many existing online learning tasks."
Deep Online Learning,"Learning an effective feature representation from data plays an important role for many machine learning tasks. Classical online learning methods typically operate directly on the input feature space. In this paper, we propose a novel framework of Deep Online Learning (DOL) which aims to improve online learning tasks by learning hierarchical feature representations from labeled and unlabeled data. We conducted extensive experiments by comparing the proposed online learning algorithms with several state-of-the-art online learning algorithms. The encouraging empirical results show that the DOL algorithms can effectively learn structure information on high-dimensional data to significantly boost online learning performance on real application with complex data including image and text data. These results indicate that exploring hierarchical representations learned from data is a promising direction for improving many existing online learning tasks."
Learning to Rank: from Batch to Online Learning,"Learning to rank represents a family of important machine learning algorithms for information retrieval applications. Traditional methods typically learn a ranking model from training data in a {\it batch} learning mode, which suffer from two major limitations: (i) they can {\it hardly} track fast-changing search intention of online users in a timely fashion; and (ii) their {\it scalability} is usually poor, especially for web-scale applications. To overcome these limitations, in this paper, we propose a novel framework of online learning to rank, which sequentially updates ranking models in an online learning fashion which thus can adapt the fast-changing search intention of online users, which is of particular importance for many online applications. Specifically, we propose two algorithms for online learning to rank, and theoretically analyze their IR measure bounds. We conduct extensive empirical studies on the LETOR testbed, in which our results show that our algorithms significantly outperform some existing online ranking technique, and achieve fairly comparable results with the state-of-the-art batch learning to rank algorithms, but enjoy significant advantage in scalability. Our framework enjoys a wide range of applications for many web-scale online services, such as personalized search, recommendation, and online advertising, etc."
Learning to Rank: from Batch to Online Learning,"Learning to rank represents a family of important machine learning algorithms for information retrieval applications. Traditional methods typically learn a ranking model from training data in a {\it batch} learning mode, which suffer from two major limitations: (i) they can {\it hardly} track fast-changing search intention of online users in a timely fashion; and (ii) their {\it scalability} is usually poor, especially for web-scale applications. To overcome these limitations, in this paper, we propose a novel framework of online learning to rank, which sequentially updates ranking models in an online learning fashion which thus can adapt the fast-changing search intention of online users, which is of particular importance for many online applications. Specifically, we propose two algorithms for online learning to rank, and theoretically analyze their IR measure bounds. We conduct extensive empirical studies on the LETOR testbed, in which our results show that our algorithms significantly outperform some existing online ranking technique, and achieve fairly comparable results with the state-of-the-art batch learning to rank algorithms, but enjoy significant advantage in scalability. Our framework enjoys a wide range of applications for many web-scale online services, such as personalized search, recommendation, and online advertising, etc."
Multi-Modal Image Annotation and Retrieval with Multi-Instance Multi-Label LDA,"In image annotation tasks, one image is usually associated with multiple labels, and different image regions may provide different hints. By representing each image region as an instance, multi-instance multi-label (MIML) learning provides a natural formulation for such problems. It is worth noting that, in many real tasks such as web image applications, in addition to the image visual information, there is extra information that can be helpful, such as the surrounding texts or user tags for images. In this paper, we study a multi-modal setting where both visual and tag information are available for each image. We propose Multi-modal Multi-instance Multi-label Latent Dirichlet Allocation (M3LDA), where the model consists of a visual-label part, a tag-label part and a label-topic part. The basic idea is that the topic decided by the visual information and the topic decided by the tag information should be consistent, leading to the correct label assignment. Experiments show that M3LDA outperforms state-of-the-art approaches. Moreover, in contrast to existing approaches that can only give annotations to whole images, M3LDA is able to give annotations to image regions, providing a promising way to understand the relation between input patterns and output semantics."
Multi-Modal Image Annotation and Retrieval with Multi-Instance Multi-Label LDA,"In image annotation tasks, one image is usually associated with multiple labels, and different image regions may provide different hints. By representing each image region as an instance, multi-instance multi-label (MIML) learning provides a natural formulation for such problems. It is worth noting that, in many real tasks such as web image applications, in addition to the image visual information, there is extra information that can be helpful, such as the surrounding texts or user tags for images. In this paper, we study a multi-modal setting where both visual and tag information are available for each image. We propose Multi-modal Multi-instance Multi-label Latent Dirichlet Allocation (M3LDA), where the model consists of a visual-label part, a tag-label part and a label-topic part. The basic idea is that the topic decided by the visual information and the topic decided by the tag information should be consistent, leading to the correct label assignment. Experiments show that M3LDA outperforms state-of-the-art approaches. Moreover, in contrast to existing approaches that can only give annotations to whole images, M3LDA is able to give annotations to image regions, providing a promising way to understand the relation between input patterns and output semantics."
Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ) for Multi-voxel Pattern Analysis in fMRI,"We present an efficient algorithm for simultaneously training elastic-net-regularized generalized linear models across many related problems, which may arise from bootstrapping, cross-validation and nonparametric permutation testing.  Our approach leverages the redundancies across problems to obtain approximately 10x computational improvements relative to solving the problems sequentially by the glmnet algorithm. We demonstrate our fast simultaneous training of generalized linear models (FaSTGLZ) algorithm,  for multivariate analysis of fMRI and run otherwise computationally intensive bootstrapping and permutation test analyses that are typically necessary for obtaining statistically rigorous classification results and  meaningful interpretation.  We also use our algorithmic framework to propose a new algorithm for stabilizing feature selection in sparse regression models."
Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ) for Multi-voxel Pattern Analysis in fMRI,"We present an efficient algorithm for simultaneously training elastic-net-regularized generalized linear models across many related problems, which may arise from bootstrapping, cross-validation and nonparametric permutation testing.  Our approach leverages the redundancies across problems to obtain approximately 10x computational improvements relative to solving the problems sequentially by the glmnet algorithm. We demonstrate our fast simultaneous training of generalized linear models (FaSTGLZ) algorithm,  for multivariate analysis of fMRI and run otherwise computationally intensive bootstrapping and permutation test analyses that are typically necessary for obtaining statistically rigorous classification results and  meaningful interpretation.  We also use our algorithmic framework to propose a new algorithm for stabilizing feature selection in sparse regression models."
Bayesian Nonparametric Modeling of Suicide Attempts,"The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the U.S. population. In the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts."
Using Support Vector Machines for Solid State Materials Characterization,"Spectroscopic techniques like X-Ray diffraction and Nuclear Magnetic Resonance (NMR) are used in conjunction with empirical and ab-initio calculations to perform structural characterization for materials. Overall, these experimental and computational methods are generally expensive and time consuming, requiring much human input and interpretation, particularly with regards to novel materials. In this work, NMR spectral data for Titanium Oxide polymorphs are simulated and first principle calculations of NMR measurable quantities performed for these materials via batch processes. An array of machine learning kernels in the form of support vector machines (SVM) are then used to learn the complex mapping between structural details and simulated input NMR spectra. The SVM array when presented with input spectra for new but related polymorphs outputs structural details rapidly and accurately. This approach has the potential to reduce the time to discover structural details for new materials, by providing a viable method for solving the inverse problem, with minimal human intervention. A similar approach may be applied to other experimental characterization techniques and ab initio calculations, for example scanning tunneling microscopy images and density of states calculations, in order to build an expert system for solid state materials characterization."
Using Support Vector Machines for Solid State Materials Characterization,"Spectroscopic techniques like X-Ray diffraction and Nuclear Magnetic Resonance (NMR) are used in conjunction with empirical and ab-initio calculations to perform structural characterization for materials. Overall, these experimental and computational methods are generally expensive and time consuming, requiring much human input and interpretation, particularly with regards to novel materials. In this work, NMR spectral data for Titanium Oxide polymorphs are simulated and first principle calculations of NMR measurable quantities performed for these materials via batch processes. An array of machine learning kernels in the form of support vector machines (SVM) are then used to learn the complex mapping between structural details and simulated input NMR spectra. The SVM array when presented with input spectra for new but related polymorphs outputs structural details rapidly and accurately. This approach has the potential to reduce the time to discover structural details for new materials, by providing a viable method for solving the inverse problem, with minimal human intervention. A similar approach may be applied to other experimental characterization techniques and ab initio calculations, for example scanning tunneling microscopy images and density of states calculations, in order to build an expert system for solid state materials characterization."
Using Support Vector Machines for Solid State Materials Characterization,"Spectroscopic techniques like X-Ray diffraction and Nuclear Magnetic Resonance (NMR) are used in conjunction with empirical and ab-initio calculations to perform structural characterization for materials. Overall, these experimental and computational methods are generally expensive and time consuming, requiring much human input and interpretation, particularly with regards to novel materials. In this work, NMR spectral data for Titanium Oxide polymorphs are simulated and first principle calculations of NMR measurable quantities performed for these materials via batch processes. An array of machine learning kernels in the form of support vector machines (SVM) are then used to learn the complex mapping between structural details and simulated input NMR spectra. The SVM array when presented with input spectra for new but related polymorphs outputs structural details rapidly and accurately. This approach has the potential to reduce the time to discover structural details for new materials, by providing a viable method for solving the inverse problem, with minimal human intervention. A similar approach may be applied to other experimental characterization techniques and ab initio calculations, for example scanning tunneling microscopy images and density of states calculations, in order to build an expert system for solid state materials characterization."
Analysis on Co-Training with Insufficient Views,"Co-training is a famous semi-supervised learning paradigm which exploits unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sufficient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sufficient to correctly predict the label. First, we show that co-training might suffer from two limitations, i.e., label noise and sampling bias, when neither view is sufficient. We then define the diversity between the two views with respect to the confidence of prediction and prove that if the two views have large diversity, co-training suffers little from above two limitations and could succeed in improving the learning performance by exploiting unlabeled data even with insufficient views."
Improving Academic Homepage Crawling with Unlabeled Data,"We investigate the design of a focused crawler for obtaining academic homepages: Given a set ofseed URLs that comprise hubs for academic homepages, how easy is it to filter out unwanted webpagesfrom the content being crawled? Academic homepages or professional homepages ofresearchers are rich sources of metadata from the perspective of digital libraries. Content-gatheringprocesses in digital libraries therefore need to track these resources on a frequent-basis necessitatingtechniques for obtaining accurate lists of academic homepages.We present results indicating that publicly-available datasets for building content-based focusedcrawlers for academic homepagessuffer from a high-degree of false positives. We trace this problem to the disparityin the training and deployment environments of the crawler. We propose URL-based hints ascomplementary evidence to address this issue and propose the use of content features and URL n-gramsas independent views for identifying homepages. Then, we illustrate that these independentviews can be effectively used with unlabeled examples in a co-training framework to adapt thecontent-based classifier to the changed environment. Experiments on our validation setsshow that this process remarkably decreases the false-positive rate of our content-basedclassifier from 34% to 15%."
Improving Academic Homepage Crawling with Unlabeled Data,"We investigate the design of a focused crawler for obtaining academic homepages: Given a set ofseed URLs that comprise hubs for academic homepages, how easy is it to filter out unwanted webpagesfrom the content being crawled? Academic homepages or professional homepages ofresearchers are rich sources of metadata from the perspective of digital libraries. Content-gatheringprocesses in digital libraries therefore need to track these resources on a frequent-basis necessitatingtechniques for obtaining accurate lists of academic homepages.We present results indicating that publicly-available datasets for building content-based focusedcrawlers for academic homepagessuffer from a high-degree of false positives. We trace this problem to the disparityin the training and deployment environments of the crawler. We propose URL-based hints ascomplementary evidence to address this issue and propose the use of content features and URL n-gramsas independent views for identifying homepages. Then, we illustrate that these independentviews can be effectively used with unlabeled examples in a co-training framework to adapt thecontent-based classifier to the changed environment. Experiments on our validation setsshow that this process remarkably decreases the false-positive rate of our content-basedclassifier from 34% to 15%."
Improving Academic Homepage Crawling with Unlabeled Data,"We investigate the design of a focused crawler for obtaining academic homepages: Given a set ofseed URLs that comprise hubs for academic homepages, how easy is it to filter out unwanted webpagesfrom the content being crawled? Academic homepages or professional homepages ofresearchers are rich sources of metadata from the perspective of digital libraries. Content-gatheringprocesses in digital libraries therefore need to track these resources on a frequent-basis necessitatingtechniques for obtaining accurate lists of academic homepages.We present results indicating that publicly-available datasets for building content-based focusedcrawlers for academic homepagessuffer from a high-degree of false positives. We trace this problem to the disparityin the training and deployment environments of the crawler. We propose URL-based hints ascomplementary evidence to address this issue and propose the use of content features and URL n-gramsas independent views for identifying homepages. Then, we illustrate that these independentviews can be effectively used with unlabeled examples in a co-training framework to adapt thecontent-based classifier to the changed environment. Experiments on our validation setsshow that this process remarkably decreases the false-positive rate of our content-basedclassifier from 34% to 15%."
Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions,"Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff."
Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions,"Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff."
Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions,"Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff."
Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions,"Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff."
Non-Linear Dimensionality Reduction by Isometric Patch Alignment,"We propose a novel dimensionality reduction method which has low computational cost. This method is inspired by two key observations: (i) the structure of reasonably large patches of high-dimensional data can be preserved as a whole, rather than divided into small neighborhoods; and (ii) attaching two neighboring patches will align them such that the overall rank does not increase. In the proposed approach, first the data is clustered, so that it is conceptually reduced to a set of overlapping low-rank clusters. Each cluster is embedded into a low-dimensional patch and then all of the patches are rearranged such that their border points are matched. We show that the rearrangement can be computed by solving a relatively small semi-definite program. The embedding computed by this optimization is provably low-rank. The proposed method is stable, fast, and scalable; experimental results demonstrate its capability for manifold learning, data visualization, and even complex tasks such as protein structure determination."
Fully Bayesian inference for neural models with negative-binomial spiking,"  Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses.  The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability.  Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals.  This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efficient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models.  We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains."
Fully Bayesian inference for neural models with negative-binomial spiking,"  Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses.  The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability.  Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals.  This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efficient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models.  We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains."
Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification,"This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification.  We show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation.  Applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification.  Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose five different criteria for adaptively acquiring nearest neighbors.  Experiments with both synthetic and real datasets demonstrate the effectivness of our classification criteria."
Learning with Feature Concatenation,"We propose a flexible hierarchical Bayesian framework for integrating multiple views based on feature concatenation, which can seamlessly incorporate any priors from generative model and any convex losses used in discriminative model. By virtue of the probabilistic interpretation of this framework, we uncover the connection from feature concatenation to multiple kernel learning (MKL) and weighted voting of view classifiers. This connection inspires a novel feature concatenation formula with the exponential distribution as the prior in the fully Bayesian inference, and also provides an elegant extension of many existing MKL formulations via the generalized maximum likelihood method. Moreover, under certain mild condition, the resulting optimization problems are convex. Experiments on image classification and UCI datasets illustrate the benefits of our proposed framework."
Learning with Feature Concatenation,"We propose a flexible hierarchical Bayesian framework for integrating multiple views based on feature concatenation, which can seamlessly incorporate any priors from generative model and any convex losses used in discriminative model. By virtue of the probabilistic interpretation of this framework, we uncover the connection from feature concatenation to multiple kernel learning (MKL) and weighted voting of view classifiers. This connection inspires a novel feature concatenation formula with the exponential distribution as the prior in the fully Bayesian inference, and also provides an elegant extension of many existing MKL formulations via the generalized maximum likelihood method. Moreover, under certain mild condition, the resulting optimization problems are convex. Experiments on image classification and UCI datasets illustrate the benefits of our proposed framework."
Learning with Feature Concatenation,"We propose a flexible hierarchical Bayesian framework for integrating multiple views based on feature concatenation, which can seamlessly incorporate any priors from generative model and any convex losses used in discriminative model. By virtue of the probabilistic interpretation of this framework, we uncover the connection from feature concatenation to multiple kernel learning (MKL) and weighted voting of view classifiers. This connection inspires a novel feature concatenation formula with the exponential distribution as the prior in the fully Bayesian inference, and also provides an elegant extension of many existing MKL formulations via the generalized maximum likelihood method. Moreover, under certain mild condition, the resulting optimization problems are convex. Experiments on image classification and UCI datasets illustrate the benefits of our proposed framework."
The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes,"Stochastic differential equations (SDE) are a natural tool for modelling systemsthat are inherently noisy or contain uncertainties that can be modelled as stochasticprocesses. Crucial to the process of using SDE to build mathematical modelsis the ability to estimate parameters of those models from observed data. Overthe past few decades, significant progress has been made on this problem, butwe are still far from having a definitive solution. We describe a novel methodof approximating a diffusion process that we show to be useful in Markov chainMonte-Carlo (MCMC) inference algorithms. We take the ?white? noise that drivesa diffusion process and decompose it into two terms. The first is a ?colourednoise? term that can be deterministically controlled by a set of auxilliary variables.The second term is small and enables us to form a linear Gaussian ?small noise?approximation. The decomposition allows us to take a diffusion process of interestand cast it in a form that is amenable to sampling by MCMC methods. We explainwhy many state-of-the-art inference methods fail on highly nonlinear inferenceproblems. We demonstrate experimentally that our method performs well in suchsituations. Our results show that this method is a promising new tool for use ininference and parameter estimation problems."
Removing Localized Corruption from Natural Images,"  Traditional approaches to removing image corruption such as blur or  noise combine a natural image prior with a reconstruction term. The  latter relies on a good generative model of the corruption --- which  may not exist for many distortions encountered in the real world. In  this paper we explore approaches for learning a direct mapping from  the corrupt input image to the clean image, obviating the need for  any kind of generative model. We evaluate the approaches on several  types of synthetic corruption, finding that neural-network based  models perform the best.  Our techniques can be used for many types of localized corruption. We  demonstrate this using photographs of real-world scenes taken behind a pane  of glass with water droplets, akin to a rainy window.  Our model removes most  of the raindrops without significant blur, the first such demonstration of  this application."
Removing Localized Corruption from Natural Images,"  Traditional approaches to removing image corruption such as blur or  noise combine a natural image prior with a reconstruction term. The  latter relies on a good generative model of the corruption --- which  may not exist for many distortions encountered in the real world. In  this paper we explore approaches for learning a direct mapping from  the corrupt input image to the clean image, obviating the need for  any kind of generative model. We evaluate the approaches on several  types of synthetic corruption, finding that neural-network based  models perform the best.  Our techniques can be used for many types of localized corruption. We  demonstrate this using photographs of real-world scenes taken behind a pane  of glass with water droplets, akin to a rainy window.  Our model removes most  of the raindrops without significant blur, the first such demonstration of  this application."
Removing Localized Corruption from Natural Images,"  Traditional approaches to removing image corruption such as blur or  noise combine a natural image prior with a reconstruction term. The  latter relies on a good generative model of the corruption --- which  may not exist for many distortions encountered in the real world. In  this paper we explore approaches for learning a direct mapping from  the corrupt input image to the clean image, obviating the need for  any kind of generative model. We evaluate the approaches on several  types of synthetic corruption, finding that neural-network based  models perform the best.  Our techniques can be used for many types of localized corruption. We  demonstrate this using photographs of real-world scenes taken behind a pane  of glass with water droplets, akin to a rainy window.  Our model removes most  of the raindrops without significant blur, the first such demonstration of  this application."
Bayesian estimation of discrete entropy with mixtures of Pitman-Yor process priors,"  We consider the problem of estimating Shannon's entropy H in the  under-sampled regime, where the number of possible symbols may be  unknown or countably infinite.  Pitman-Yor processes (a  generalization of Dirichlet processes) provide tractable prior  distributions over the space of countably infinite discrete  distributions, and have found major applications in Bayesian  non-parametric statistics and machine learning. Here we show that  they also provide natural priors for Bayesian entropy estimation,  due to the remarkable fact that the moments of the induced posterior  distribution over H can be computed analytically. We derive  formulas for the posterior mean (Bayes' least squares estimate) and  variance under such priors.  Moreover, we show that a fixed  Dirichlet or Pitman-Yor process prior implies a narrow prior on H,  meaning the prior strongly determines the entropy estimate in the  under-sampled regime. We derive a family of continuous mixing  measures such that the resulting mixture of Pitman-Yor processes  produces an approximately flat (improper) prior over H.  We  explore the theoretical properties of the resulting estimator, and  show that it performs well on data sampled from both exponential and  power-law tailed distributions."
Bayesian estimation of discrete entropy with mixtures of Pitman-Yor process priors,"  We consider the problem of estimating Shannon's entropy H in the  under-sampled regime, where the number of possible symbols may be  unknown or countably infinite.  Pitman-Yor processes (a  generalization of Dirichlet processes) provide tractable prior  distributions over the space of countably infinite discrete  distributions, and have found major applications in Bayesian  non-parametric statistics and machine learning. Here we show that  they also provide natural priors for Bayesian entropy estimation,  due to the remarkable fact that the moments of the induced posterior  distribution over H can be computed analytically. We derive  formulas for the posterior mean (Bayes' least squares estimate) and  variance under such priors.  Moreover, we show that a fixed  Dirichlet or Pitman-Yor process prior implies a narrow prior on H,  meaning the prior strongly determines the entropy estimate in the  under-sampled regime. We derive a family of continuous mixing  measures such that the resulting mixture of Pitman-Yor processes  produces an approximately flat (improper) prior over H.  We  explore the theoretical properties of the resulting estimator, and  show that it performs well on data sampled from both exponential and  power-law tailed distributions."
Bayesian estimation of discrete entropy with mixtures of Pitman-Yor process priors,"  We consider the problem of estimating Shannon's entropy H in the  under-sampled regime, where the number of possible symbols may be  unknown or countably infinite.  Pitman-Yor processes (a  generalization of Dirichlet processes) provide tractable prior  distributions over the space of countably infinite discrete  distributions, and have found major applications in Bayesian  non-parametric statistics and machine learning. Here we show that  they also provide natural priors for Bayesian entropy estimation,  due to the remarkable fact that the moments of the induced posterior  distribution over H can be computed analytically. We derive  formulas for the posterior mean (Bayes' least squares estimate) and  variance under such priors.  Moreover, we show that a fixed  Dirichlet or Pitman-Yor process prior implies a narrow prior on H,  meaning the prior strongly determines the entropy estimate in the  under-sampled regime. We derive a family of continuous mixing  measures such that the resulting mixture of Pitman-Yor processes  produces an approximately flat (improper) prior over H.  We  explore the theoretical properties of the resulting estimator, and  show that it performs well on data sampled from both exponential and  power-law tailed distributions."
Cost-Sensitive Online Active Learning for Online anomaly detection,"Online anomaly detection is an important problem in data mining, which enjoys many real-world applications in a variety of domains. In literature, most of existing studies attempt to solve anomaly detection by formulating it as either a classical batch supervised classification task or an online unsupervised learning task. Both of approaches have their limitations in solving a real-world anomaly detection task. In this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL) for online anomaly detection, which goes beyond conventional approaches to solve anomaly detection in a natural, effective and scalable way. We propose two CSOAL algorithms under the proposed framework, and theoretically analyze their performance in terms of cost-sensitive measures, including (i) the lower bound for the weighted sum of sensitivity and specificity achieved by the first algorithm, and (ii) the upper bound for the weighted cost made by the second algorithm. We extensively examine the empirical performance of the proposed algorithms on several challenging anomaly detection tasks, in which encouraging results validate the efficacy of our proposed technique in solving anomaly detection with online active learning."
Cost-Sensitive Online Active Learning for Online anomaly detection,"Online anomaly detection is an important problem in data mining, which enjoys many real-world applications in a variety of domains. In literature, most of existing studies attempt to solve anomaly detection by formulating it as either a classical batch supervised classification task or an online unsupervised learning task. Both of approaches have their limitations in solving a real-world anomaly detection task. In this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL) for online anomaly detection, which goes beyond conventional approaches to solve anomaly detection in a natural, effective and scalable way. We propose two CSOAL algorithms under the proposed framework, and theoretically analyze their performance in terms of cost-sensitive measures, including (i) the lower bound for the weighted sum of sensitivity and specificity achieved by the first algorithm, and (ii) the upper bound for the weighted cost made by the second algorithm. We extensively examine the empirical performance of the proposed algorithms on several challenging anomaly detection tasks, in which encouraging results validate the efficacy of our proposed technique in solving anomaly detection with online active learning."
Fast Hierarchical Topic Modeling via Nonnegative Matrix Factorization,"Hierarchical clustering is one of the cluster analysis tasks which aims at building a hierarchy of topics. Most state-of-the-art algorithms proposed in literature for hierarchical clustering are based on sampling. Their computational costs largely depend on the number of words in the given corpus, which is usually extremely. In this paper, we come up with an efficient hierarchical clustering method using non-negative matrix factorization (NMF), which is a dimension reduction technique that approximates a given matrix by a product of two low rank matrices. To maintain the hierarchy of topics, we design special constraints for low rank matrices, resulting in a novel optimization problem which we propose to solve using coordinate descent algorithms. Experiments on both artificial and real-world data sets demonstrate the effectiveness of our approach."
Fast Hierarchical Topic Modeling via Nonnegative Matrix Factorization,"Hierarchical clustering is one of the cluster analysis tasks which aims at building a hierarchy of topics. Most state-of-the-art algorithms proposed in literature for hierarchical clustering are based on sampling. Their computational costs largely depend on the number of words in the given corpus, which is usually extremely. In this paper, we come up with an efficient hierarchical clustering method using non-negative matrix factorization (NMF), which is a dimension reduction technique that approximates a given matrix by a product of two low rank matrices. To maintain the hierarchy of topics, we design special constraints for low rank matrices, resulting in a novel optimization problem which we propose to solve using coordinate descent algorithms. Experiments on both artificial and real-world data sets demonstrate the effectiveness of our approach."
Fast Hierarchical Topic Modeling via Nonnegative Matrix Factorization,"Hierarchical clustering is one of the cluster analysis tasks which aims at building a hierarchy of topics. Most state-of-the-art algorithms proposed in literature for hierarchical clustering are based on sampling. Their computational costs largely depend on the number of words in the given corpus, which is usually extremely. In this paper, we come up with an efficient hierarchical clustering method using non-negative matrix factorization (NMF), which is a dimension reduction technique that approximates a given matrix by a product of two low rank matrices. To maintain the hierarchy of topics, we design special constraints for low rank matrices, resulting in a novel optimization problem which we propose to solve using coordinate descent algorithms. Experiments on both artificial and real-world data sets demonstrate the effectiveness of our approach."
Low-Rank Modeling via Capped-Trace Norm,"The problem of low-rank modeling has recently received increasing attentions in machine learning. Most problems that directly tackle the rank function are known to be NP-hard and thus many heuristics such as those based on the trace norm have been proposed for low-rank modeling. The convex relaxation based on the trace norm admits a global solution and has theoretical guarantees under certain assumptions. However, the trace norm may not be a good approximation of the rank function in practice, resulting in estimation bias. In this paper, we consider low-rank modeling via the capped trace norm (CTRN) which provides a better approximation of the rank function than the trace norm. The basic idea of the CTRN is to perform thresholding on singular values and reduce the dominating impact of the top singular values. Although the capped $\ell_1$ norm has been well studied in the vector case, to our best knowledge the extension to the matrix case has not been studied in the literature. The practical challenge lies in the efficient optimization associated with the CTRN which is non-convex. We employ the difference-of-convex (DC) programming by decomposing the non-convex CTRN into the difference of two convex functions. By applying the concave-convex procedure, the problem can be iteratively computed via solving convex optimization problems. We present a block coordinate descend algorithm for general problems where the objective and/or the constraint can be a difference-of-convex function, and we present the convergence property of the algorithm. Our convergence proof is much simpler and requires weaker assumptions than existing work. We perform extensive experiments using both synthetic and real data. Our results show that the proposed algorithms outperform many popular heuristics for low-rank modeling, including the trace norm and the Schatten-$p$ norm."
Large-Scale Bandit Problems and KWIK Learning,"We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model  known as ``Knows What It Knows'' or KWIK learning. We give matching impossibility results showing that the KWIK-learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time."
Large-Scale Bandit Problems and KWIK Learning,"We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model  known as ``Knows What It Knows'' or KWIK learning. We give matching impossibility results showing that the KWIK-learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time."
Large-Scale Bandit Problems and KWIK Learning,"We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model  known as ``Knows What It Knows'' or KWIK learning. We give matching impossibility results showing that the KWIK-learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time."
Cooperating with a Markovian Ad Hoc Teammate,"This paper focuses on learning in the presence of a Markovian teammate in Ad hoc teams. A Markovian teammate's policy is a function of a set of discrete feature values derived from the joint history of interaction, where the feature values transition in a Markovian fashion on each time step. We introduce a novel algorithm ``Learning to Cooperate with a Markovian teammate'', or LCM, that converges to optimal cooperation with any Markovian teammate that satisfies certain assumptions, and achieves safety with any arbitrary teammate, in tractable sample complexity. The novel aspect of LCM is the manner in which it satisfies the above two goals via efficient exploration and exploitation. The main contribution of this paper is a full specification and a detailed analysis of LCM's theoretical properties. "
Tractable Objectives for Robust Policy Optimization,"Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance.  One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations.   In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty.  Instead we focus on identifying optimization objectives for which solutions can be efficiently approximated.  We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efficiently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP.  "
Tractable Objectives for Robust Policy Optimization,"Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance.  One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations.   In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty.  Instead we focus on identifying optimization objectives for which solutions can be efficiently approximated.  We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efficiently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP.  "
Conditional gradient algorithms for regularized learning,"We consider the problem of optimizing learning objectives with a regularization penalty in high-dimensional settings. For several important learning problems, state-of-the-art optimization approaches such as proximal gradient algorithms are difficult to apply and do not scale up to large datasets. We propose new conditional-type algorithms, with theoretical guarantees, resp. for norm-minimization and penalized learning problems. Promising experimental results are presented on two large-scale real-world datasets. "
Lasso Screening Rules via Dual Polytope Projection,"Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. Bytransforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose a fast and efficient screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. We have evaluated our screening rule using both synthetic and real data sets. Results show that our rule is more effective to identifyinactive predictors than existing state-of-art screening rules. We also extend our screening rule to identify inactive groups in group Lasso."
Shadow Densities for Speeding Up Kernel Methods ,"This paper presents an approach to improve the training and evaluation of kernel manifold learning algorithms relying on spectral decomposition. The approach, called the shadow method, exploits research regarding the spectral decomposi-tion of kernel operators applied to probability distributions. It is used to define the shadow of a kernel density estimate. The shadow density estimate (ShDE)in turn defines shadow KPCA (ShKPCA). For large, redundant datasets ShKPCA improves training and evaluation time of KPCA by an order of magnitude or more, each. A single parameter $\ell$ controls the computational gains. The shadow method is justified through bounds on the density estimate error, on the spectral decomposition error, and on the spectral operator error, all in terms of $\ell$. For low $\ell$ there are large improvements but the method is lossy. Increasing approaches baseline performance and leads to lower speed improvements. Experimentally $\ell$ =  4 works well across a broad spectrum of datasets. Modifications to  improve $\ell$ low performance are given, but with reduced computational gains during training."
Shadow Densities for Speeding Up Kernel Methods ,"This paper presents an approach to improve the training and evaluation of kernel manifold learning algorithms relying on spectral decomposition. The approach, called the shadow method, exploits research regarding the spectral decomposi-tion of kernel operators applied to probability distributions. It is used to define the shadow of a kernel density estimate. The shadow density estimate (ShDE)in turn defines shadow KPCA (ShKPCA). For large, redundant datasets ShKPCA improves training and evaluation time of KPCA by an order of magnitude or more, each. A single parameter $\ell$ controls the computational gains. The shadow method is justified through bounds on the density estimate error, on the spectral decomposition error, and on the spectral operator error, all in terms of $\ell$. For low $\ell$ there are large improvements but the method is lossy. Increasing approaches baseline performance and leads to lower speed improvements. Experimentally $\ell$ =  4 works well across a broad spectrum of datasets. Modifications to  improve $\ell$ low performance are given, but with reduced computational gains during training."
Shadow Densities for Speeding Up Kernel Methods ,"This paper presents an approach to improve the training and evaluation of kernel manifold learning algorithms relying on spectral decomposition. The approach, called the shadow method, exploits research regarding the spectral decomposi-tion of kernel operators applied to probability distributions. It is used to define the shadow of a kernel density estimate. The shadow density estimate (ShDE)in turn defines shadow KPCA (ShKPCA). For large, redundant datasets ShKPCA improves training and evaluation time of KPCA by an order of magnitude or more, each. A single parameter $\ell$ controls the computational gains. The shadow method is justified through bounds on the density estimate error, on the spectral decomposition error, and on the spectral operator error, all in terms of $\ell$. For low $\ell$ there are large improvements but the method is lossy. Increasing approaches baseline performance and leads to lower speed improvements. Experimentally $\ell$ =  4 works well across a broad spectrum of datasets. Modifications to  improve $\ell$ low performance are given, but with reduced computational gains during training."
Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL,"Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.  We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging."
Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL,"Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.  We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging."
Spectral Learning of General Weighted Automata via Constrained Matrix Completion,"Many tasks in text and speech processing and computational biology involve functions from variable-length strings to real numbers. A wide class of such functions can be computed by weighted automata. Spectral methods based on singular value decompositions of Hankel matrices have been recently proposed for learning probability distributions over strings that can be computed by weighted automata. In this paper we show how this method can be applied to the problem of learning a general weighted automata from a sample of string-label pairs generated by an arbitrary distribution. The main obstruction to this approach is that in general some entries of the Hankel matrix that needs to be decomposed may be missing. We propose a solution based on solving a constrained matrix completion problem. Combining these two ingredients, a whole new family of algorithms for learning general weighted automata is obtained. Generalization bounds for a particular algorithm in this class are given. The proofs rely on a stability analysis of matrix completion and spectrallearning."
"Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L_p$ Loss","In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general $L_p$ norm.  We generalize the Cramer-Rao lower bound and show how the $L_p$ loss can be written as a functional of the Fisher Information in the asymptotic limit, by proving the moment convergence of certain functions of Poisson random variables.  In this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing $L_p$ loss in the limit as $p$ goes to zero.  "
"Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L_p$ Loss","In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general $L_p$ norm.  We generalize the Cramer-Rao lower bound and show how the $L_p$ loss can be written as a functional of the Fisher Information in the asymptotic limit, by proving the moment convergence of certain functions of Poisson random variables.  In this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing $L_p$ loss in the limit as $p$ goes to zero.  "
"Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L_p$ Loss","In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general $L_p$ norm.  We generalize the Cramer-Rao lower bound and show how the $L_p$ loss can be written as a functional of the Fisher Information in the asymptotic limit, by proving the moment convergence of certain functions of Poisson random variables.  In this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing $L_p$ loss in the limit as $p$ goes to zero.  "
When Does a Mixture of Products Contain a Product of Mixtures?,"We prove results on the relative representational power of mixtures of products and products of mixtures (restricted Boltzmann machines). Tools of independent interest are mode-based polyhedral approximations sensitive enough to compare even full-dimensional models, and characterizations of possible mode and support sets of both model classes. The title question is intimately related to questions in coding theory and the theory of hyperplane arrangements. In particular we find that an exponentially larger mixture model, requiring an exponentially larger number of parameters, is required to represent the distributions that can be represented by the restricted Boltzmann machine. "
When Does a Mixture of Products Contain a Product of Mixtures?,"We prove results on the relative representational power of mixtures of products and products of mixtures (restricted Boltzmann machines). Tools of independent interest are mode-based polyhedral approximations sensitive enough to compare even full-dimensional models, and characterizations of possible mode and support sets of both model classes. The title question is intimately related to questions in coding theory and the theory of hyperplane arrangements. In particular we find that an exponentially larger mixture model, requiring an exponentially larger number of parameters, is required to represent the distributions that can be represented by the restricted Boltzmann machine. "
Multi-Task Averaging,"We present a multi-task learning approach to jointly estimate the means of multipleindependent data sets. We prove that the proposed multi-task averaging (MTA) algorithmresults in a convex combination of the single-task maximum likelihood estimates.We derive the optimal amount of regularization, and show that it can be effectivelyestimated. Simulations and real data experiments demonstrate that MTAoutperforms both maximum likelihood and James-Stein estimators, and that ourapproach to estimating the amount of regularization rivals cross-validation in performancebut is more computationally efficient."
Geodesic Distance Function Learning: Theory and an Algorithm,"Learning a distance function is of great importance in machine learning and pattern recognition. Geodesic distance, which has been widely used, is one of the most important intrinsic distances on the manifold. In this paper, we study the geodesic distance function $d(p, x)$ for a fixed point $p$. We provide two theorems to exactly characterize such a distance function. Our theoretical analysis shows if a function $r_p(x)$ is a Euclidean distance function in a neighborhood of $p$ in exponential coordinates and the gradient field of $r_p(x)$ has unit norm almost everywhere, then $r_p(x)$ must be the unique geodesic distance function $d(p,x)$. Based on our theoretical analysis, a novel approach from vector field perspective is proposed to learn the geodesic distance function. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm."
Measuring Reproducibility of High-throughput Deep-sequencing Experiments based on Self-adaptive Mixture Copula,"Measurement of the statistical reproducibility between biological experiment replicates is vital first step of the entire series of bioinformatics analyses for meaningful biology finding in mega-data. To distinguish the real biological relevant signals from artificial signals, irreproducible discovery rate (IDR) employing Copula, which can separate dependence structure and marginal distribution from data, has been put forth. However, the main disadvantage of IDR is that it assumes the data subject to normal distribution, which does not match the real data's feature. To address the issue, we propose a Self-adaptive Mixture Copula (SaMiC) to measure the reproducibility of experiment replicates from high-throughput deep-sequencing data. Simple and easy to implement, the proposed SaMiC method can self-adaptively tune its coefficients so that the measurement of reproducibility is more effective for general distributions.  Experiments in simulated and real data indicate that compared with IDR, the SaMiC method can better estimate  reproducibility between replicate samples."
Identification of Spike-Processing Neural Circuits,"Reverse engineering of neural circuits requires the development of sound experimental and theoretical methods for determining the circuit connectivity and for estimating the processing of both spiking and continuous sensory signals. Here we present a new approach for identification of receptive fields in spiking neuron models that admit both continuous  signals and multidimensional spike trains as input stimuli. We consider circuit models of the sensory periphery in olfaction, audition and  vision as well as models of spike processing in higher brain centers, including models with lateral connectivity and feedback. We present algorithms for identifying temporal, spectrotemporal and spatiotemporal receptive fields directly from spike times produced by a neuron. The algorithms obviate the need to repeat experiments in order to compute the neuron's rate of response, rendering our methodology of interest to both experimental and theoretical neuroscientists."
Identification of Spike-Processing Neural Circuits,"Reverse engineering of neural circuits requires the development of sound experimental and theoretical methods for determining the circuit connectivity and for estimating the processing of both spiking and continuous sensory signals. Here we present a new approach for identification of receptive fields in spiking neuron models that admit both continuous  signals and multidimensional spike trains as input stimuli. We consider circuit models of the sensory periphery in olfaction, audition and  vision as well as models of spike processing in higher brain centers, including models with lateral connectivity and feedback. We present algorithms for identifying temporal, spectrotemporal and spatiotemporal receptive fields directly from spike times produced by a neuron. The algorithms obviate the need to repeat experiments in order to compute the neuron's rate of response, rendering our methodology of interest to both experimental and theoretical neuroscientists."
Sparse Correlation Estimation for Elliptical distributions,"We propose a semiparametric procedure---named RTK (Rank-based Thresholding via Kendall's tau)---to estimate the correlation matrix of high dimensional elliptical distributions. Unlike most existing methods that are based on Pearson's correlation, our approach exploits the nonparametric rank-based correlation estimator. Theoretically, our procedure achieves the optimal parametric rates of convergence for both parameter estimation and sparsity recovery in high dimensional settings; Empirically, our method always deliver a positive definite solution. Numerical results on both simulated and real datasets are also provided to support our theory."
Sparse Correlation Estimation for Elliptical distributions,"We propose a semiparametric procedure---named RTK (Rank-based Thresholding via Kendall's tau)---to estimate the correlation matrix of high dimensional elliptical distributions. Unlike most existing methods that are based on Pearson's correlation, our approach exploits the nonparametric rank-based correlation estimator. Theoretically, our procedure achieves the optimal parametric rates of convergence for both parameter estimation and sparsity recovery in high dimensional settings; Empirically, our method always deliver a positive definite solution. Numerical results on both simulated and real datasets are also provided to support our theory."
Teaching Classification Tasks to Humans,"Given a classification task, what is the best way to teach the resulting boundary to a human? While machine learning techniques can provide excellent techniques for finding the boundary, they tell us little about how we would teach a human the same task. We propose to investigate the problem of example selection and presentation in the context of teaching humans, and explore a variety of mechanisms in the interests of finding what may work best. In particular, we begin with the baseline of random presentation and then examine combinations of several mechanisms: the indication of an example?s relative difficulty, the use of the shaping heuristic from the psychology literature (moving from easier examples to harder ones), and a novel kernel-based ?coverage model? of the subject?s mastery of the task. From our experiments on 53 human subjects learning classification tasks via our teaching system, we found that we can achieve the greatest gains with a combination of shaping and the coverage model."
Generalized Ambiguity Decomposition for Convex Ensembles of Experts and Arbitrary Differentiable Loss Functions,"The squared error of a convex ensemble of regressors is related to the squared error of the individual regressors and the diversity of the ensemble as measured by the weighted sum of squared errors of each regressor's prediction from the ensemble's prediction. This relationship, also known as ambiguity decomposition, highlights the impact of diversity on ensemble's performance for least squares regression. In this paper, we present a generalization of ambiguity decomposition that can be applied to any convex ensemble of experts under a differentiable loss function. The proposed decomposition is applicable to both classification and regression, and provides a task-driven notion of diversity. It is shown that the diversity term in this decomposition is scaled by a factor dependent on the instance and the loss function in a classical supervised learning setting. This lends support to the intuition that not all instances are equally important from a diversity perspective. We then derive the decomposition for some common regression and classification loss functions, and demonstrate its accuracy on different UCI datasets."
Generalized Ambiguity Decomposition for Convex Ensembles of Experts and Arbitrary Differentiable Loss Functions,"The squared error of a convex ensemble of regressors is related to the squared error of the individual regressors and the diversity of the ensemble as measured by the weighted sum of squared errors of each regressor's prediction from the ensemble's prediction. This relationship, also known as ambiguity decomposition, highlights the impact of diversity on ensemble's performance for least squares regression. In this paper, we present a generalization of ambiguity decomposition that can be applied to any convex ensemble of experts under a differentiable loss function. The proposed decomposition is applicable to both classification and regression, and provides a task-driven notion of diversity. It is shown that the diversity term in this decomposition is scaled by a factor dependent on the instance and the loss function in a classical supervised learning setting. This lends support to the intuition that not all instances are equally important from a diversity perspective. We then derive the decomposition for some common regression and classification loss functions, and demonstrate its accuracy on different UCI datasets."
How Prior Probability Influences Decision Making: A Unifying Probabilistic Model,"How does the brain combine prior knowledge with sensory evidence whenmaking decisions under uncertainty? Two competing descriptive modelshave been proposed based on experimental data.  The first posits anadditive offset to a decision variable, implying a static effect ofthe prior. However, this model is inconsistent with recent data from amotion discrimination task involving temporal integration of uncertainsensory evidence. To explain this data, a second model has beenproposed which assumes a time-varying influence of the prior. Here wepresent a normative model of decision making that incorporates priorknowledge in a principled way.  We show that the additive offset modeland the time-varying prior model emerge naturally when decision makingis viewed within the framework of partially observable Markov decisionprocesses (POMDPs).  Decision making in the model reduces to (1)computing beliefs given observations and prior information in a Bayesianmanner, and (2) selecting actions based on these beliefs to maximize the expected sum of future rewards. We show that the model can explain both data previously explained using the additive offset model as well as more recent data on the time-varying influence of prior knowledge on decision making."
How Prior Probability Influences Decision Making: A Unifying Probabilistic Model,"How does the brain combine prior knowledge with sensory evidence whenmaking decisions under uncertainty? Two competing descriptive modelshave been proposed based on experimental data.  The first posits anadditive offset to a decision variable, implying a static effect ofthe prior. However, this model is inconsistent with recent data from amotion discrimination task involving temporal integration of uncertainsensory evidence. To explain this data, a second model has beenproposed which assumes a time-varying influence of the prior. Here wepresent a normative model of decision making that incorporates priorknowledge in a principled way.  We show that the additive offset modeland the time-varying prior model emerge naturally when decision makingis viewed within the framework of partially observable Markov decisionprocesses (POMDPs).  Decision making in the model reduces to (1)computing beliefs given observations and prior information in a Bayesianmanner, and (2) selecting actions based on these beliefs to maximize the expected sum of future rewards. We show that the model can explain both data previously explained using the additive offset model as well as more recent data on the time-varying influence of prior knowledge on decision making."
How Prior Probability Influences Decision Making: A Unifying Probabilistic Model,"How does the brain combine prior knowledge with sensory evidence whenmaking decisions under uncertainty? Two competing descriptive modelshave been proposed based on experimental data.  The first posits anadditive offset to a decision variable, implying a static effect ofthe prior. However, this model is inconsistent with recent data from amotion discrimination task involving temporal integration of uncertainsensory evidence. To explain this data, a second model has beenproposed which assumes a time-varying influence of the prior. Here wepresent a normative model of decision making that incorporates priorknowledge in a principled way.  We show that the additive offset modeland the time-varying prior model emerge naturally when decision makingis viewed within the framework of partially observable Markov decisionprocesses (POMDPs).  Decision making in the model reduces to (1)computing beliefs given observations and prior information in a Bayesianmanner, and (2) selecting actions based on these beliefs to maximize the expected sum of future rewards. We show that the model can explain both data previously explained using the additive offset model as well as more recent data on the time-varying influence of prior knowledge on decision making."
How Prior Probability Influences Decision Making: A Unifying Probabilistic Model,"How does the brain combine prior knowledge with sensory evidence whenmaking decisions under uncertainty? Two competing descriptive modelshave been proposed based on experimental data.  The first posits anadditive offset to a decision variable, implying a static effect ofthe prior. However, this model is inconsistent with recent data from amotion discrimination task involving temporal integration of uncertainsensory evidence. To explain this data, a second model has beenproposed which assumes a time-varying influence of the prior. Here wepresent a normative model of decision making that incorporates priorknowledge in a principled way.  We show that the additive offset modeland the time-varying prior model emerge naturally when decision makingis viewed within the framework of partially observable Markov decisionprocesses (POMDPs).  Decision making in the model reduces to (1)computing beliefs given observations and prior information in a Bayesianmanner, and (2) selecting actions based on these beliefs to maximize the expected sum of future rewards. We show that the model can explain both data previously explained using the additive offset model as well as more recent data on the time-varying influence of prior knowledge on decision making."
Biased perception leads to biased action: Validating a Bayesian model of interception," We tested whether and how biases in visual perception might influence motor actions. To do so, we designed an  interception task where subjects had to indicate the time when a moving object, whose trajectory was occluded from the  subjects, would reach a target-area. Subjects made their judgements based on a brief display of the objects initial  motion at a starting point. Based on the known illusion that slow contrast stimuli appear to move slower than high  contrast ones, we predict that if perception directly influences motion actions subjects would show delayed  interception times for low contrast objects. In order to provide a more quantitative prediction, we developed a Bayesian  model for the complete sensory-motor interception task. Using fit parameters for the prior and likelihood on visual  speed from a previous study we were able to predict not only the expected interception times but also the precise  characteristics of response variability. Psychophysical experiments confirm the model's predictions. Individual  differences in subjects timing response can be accounted for by individual differences in the perceptual priors on  visual speed. Taken together, our behavioral and model results show that biases in perception percolate downstream to  bias action response in a predictable manner. Furthermore, our work emphasizes that the Bayesian model of speed  perception is generalizable to new domains."
Hippocampal CA3 Cells As Hidden Units of A Recurrent Neural Network,"Abstract Hippocampal cells are known for their spatial and temporal selectivity. However, it is unclear how such selectivity arises in different regions of the hippocampus and how it contributes to episodic memory. We simulate learning in a recurrent neural network (RNN) structurally similar to the neural circuit in area CA3. Our methods based on general temporal sequences can be extended to more specific inputs such as spatial and temporal correlated signals. Our simulation results provide a novel explanation of how multi-modal episodic memory is learned and suggest that the experimentally observed, sparse and selective tuning of CA3 cells facilitates the learning of temporal sequences as memory episodes."
Hippocampal CA3 Cells As Hidden Units of A Recurrent Neural Network,"Abstract Hippocampal cells are known for their spatial and temporal selectivity. However, it is unclear how such selectivity arises in different regions of the hippocampus and how it contributes to episodic memory. We simulate learning in a recurrent neural network (RNN) structurally similar to the neural circuit in area CA3. Our methods based on general temporal sequences can be extended to more specific inputs such as spatial and temporal correlated signals. Our simulation results provide a novel explanation of how multi-modal episodic memory is learned and suggest that the experimentally observed, sparse and selective tuning of CA3 cells facilitates the learning of temporal sequences as memory episodes."
Hippocampal CA3 Cells As Hidden Units of A Recurrent Neural Network,"Abstract Hippocampal cells are known for their spatial and temporal selectivity. However, it is unclear how such selectivity arises in different regions of the hippocampus and how it contributes to episodic memory. We simulate learning in a recurrent neural network (RNN) structurally similar to the neural circuit in area CA3. Our methods based on general temporal sequences can be extended to more specific inputs such as spatial and temporal correlated signals. Our simulation results provide a novel explanation of how multi-modal episodic memory is learned and suggest that the experimentally observed, sparse and selective tuning of CA3 cells facilitates the learning of temporal sequences as memory episodes."
Infinite Structured Hidden Markov Model,"We present the infinite structured hidden Markov model (ISHMM). An ISHMM is an HMM that possesses an unbounded number of states, parameterizes state dwell-time distributions explicitly, and can constrain what kinds of state transitions are possible. We present two parameterizations of the ISHMM. The first is a novel construction for an infinite explicit duration HMM. The second is an entirely novel infinite left-to-right HMM. We provide inference algorithms for the ISHMM and show results from using the ISHMM to analyze both real and synthetic data."
Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data,"Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences."
Efficient coding connects prior and likelihood function in perceptual Bayesian inference,"  A common challenge for Bayesian approaches in modeling perceptual behavior is the fact that the two fundamental  components of a Bayesian model, the prior distribution and the likelihood function, are formally unconstrained. Here  we argue that a neural system that emulates Bayesian inference naturally imposes constraints by way of how it  represents sensory information in populations of neurons. More specifically, we propose an efficient encoding  principle that constrains both the likelihood and the prior based on low-level environmental statistics. The resulting  Bayesian estimates can show biases away from the peaks of a prior distribution, a behavior seemingly at odds  with the traditional view of Bayesian estimates yet one that has indeed been reported in human perception of visual orientation. We demonstrate that our framework correctly predicts these biases, and show  that the efficient encoding characteristics of the model neural population matches the reported orientation tuning  characteristics of neurons in primary visual cortex. Our results suggest that efficient coding can be a promising  hypothesis in constraining neural implementations of Bayesian inference."
The Interplay between Stability and Regret in Online Learning,"This paper considers the stability of online learning algorithms and its implications for learnability (bounded regret).  We introduce a novel quantity called {\em forward regret} that intuitively measures how good an online learning algorithm is if it is allowed a one-step look-ahead into the future.  We show that given stability, bounded forward regret is equivalent to bounded regret. We also show that the existence of an algorithm with bounded regret implies the existence of a stable algorithm with bounded regret and bounded forward regret. The equivalence results apply to general, possibly non-convex problems. To the best of our knowledge, our analysis provides the first general connection between stability and regret in the online setting that is not restricted to a particular class of algorithms. Our stability-regret connection provides a simple recipe for analysing  regret incurred by any online learning algorithm.  We illustrate our recipe by providing a novel dimension independent regret bound for the follow-the-perturbed-leader (FTPL) algorithm for online linear programming (OLP) over a hypersphere, a non-convex set. Using our framework, we analyse several existing online learning algorithms as well as the ``approximate'' versions  of algorithms like RDA that solve an optimization problem at each iteration. Our proofs are simpler than existing analysis for the respective algorithms, show a clear trade-off between stability and forward regret, and provide tighter regret bounds in some cases."
Learned Prioritization for Trading Off Accuracy and Speed,"Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines."
Learned Prioritization for Trading Off Accuracy and Speed,"Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines."
Generalized Classification-based Approximate Policy Iteration,"Classification-based approximate policy iteration allows us to benefit from the regularities of the optimal policy by explicitly controlling the complexity of the policy space. This leads to considerable improvements whenever the optimal policy is easy to represent. The conventional classification-based methods, however, do not benefit from the regularities of the value function as they often use a rollout-based estimate of the action-value function, which is rather data-inefficient and cannot generalize the estimate of the action-value function over states. In this paper, we introduce a general framework for classification-based approximate policy iteration, CAPI, that lets us benefit from the present regularities of both the policy and the value.Our theoretical analysis extends existing work by allowing the policy evaluation to be performed by any reinforcement learning algorithm, by handling nonparametric representations of policies, and by providing tighter convergence bounds on the estimation error of policy learning.A small illustration shows that this approach can be faster than purely value-based methods."
Another Nonparametric Functional Estimator that Achieves Asymptotic Optimality and Adapts to Irregular Domains,"We propose a new nonparametric functional estimation method. Existingstate-of-the-art methods are designed for regular domains (such as $\mathbb{R}^{d}$ or$[0,1]^d$) -- when the domain is irregular, they run intoimplementation difficulties; More specifically, one does not know the formulaof the corresponding reproducing kernels. Our newly designed method adapts to anyirregular domain. When some boundary conditionsare satisfied, it preserves the asymptotic optimality, which includesthe optimal convergence rate. The new methodalso achieves the asymptotic efficiency, however we did not include here. A significant advantage of the new approach is that it adapts to any domain, while traditional methodsrequire restrictive conditions on the domains."
Value Pursuit Iteration,"Value Pursuit Iteration (VPI) is an approximate value iteration algorithm that finds a close to optimal policy for  reinforcement learning and planning problems with large state spaces.VPI has two main features: First, it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features. The algorithm is almost insensitive to the number of irrelevant features. Second, after each iteration of VPI, the algorithm adds a set of functions based on the currently learned value function to the dictionary. This increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function.We theoretically study VPI and provide a finite-sample error upper bound for it."
Link Prediction in Biological Networks using Penalized Multi-Attribute ERGMs,"Reconstruction of genetic networks is an important, yet challenging problem in systems biology. Gene networks often include different interaction mechanisms, such as transcriptional regulatory and protein-protein interactions. Further, different data sources provide valuable information about the relationships among genes, which motivate methods that allow for data integration.We propose a novel multi-attribute exponential random graph model for supervised prediction of gene networks, coupled with a penalized estimation framework for improved prediction performance. The proposed framework facilitates the analysis of gene networks with multiple edge types, and provides a systematic method for incorporating multiple sources of biological data, as well as diverse attributes regarding the function and location of genes, and structure of observed networks. Results of numerical experiments indicate that the method enjoys superior performance compared to state-of-the-art reconstruction methods."
Graphical Models via Generalized Linear Models,"Undirected graphical models, or Markov networks, such as Gaussian graphical models and Ising models enjoy popularity in a variety of applications.  In many settings, however, data may not follow a Gaussian or binomial distribution assumed by these models. We introduce a new class of graphical models based on generalized linear models (GLM) by assuming that node-wise conditional distributions arise from exponential families.  Our models allow one to estimate networks for a wide class of exponential distributions, such as the Poisson, negative binomial, and exponential, by fitting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We provide examples of high-throughput genomic networks learned via our GLM graphical models for multinomial and Poisson distributed data."
Graphical Models via Generalized Linear Models,"Undirected graphical models, or Markov networks, such as Gaussian graphical models and Ising models enjoy popularity in a variety of applications.  In many settings, however, data may not follow a Gaussian or binomial distribution assumed by these models. We introduce a new class of graphical models based on generalized linear models (GLM) by assuming that node-wise conditional distributions arise from exponential families.  Our models allow one to estimate networks for a wide class of exponential distributions, such as the Poisson, negative binomial, and exponential, by fitting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We provide examples of high-throughput genomic networks learned via our GLM graphical models for multinomial and Poisson distributed data."
Symbolic Dynamic Programming for Continuous State and Observation POMDPs,"Partially-observable Markov decision processes (POMDPs) provide a powerfulmodel for real-world sequential decision-making problems. In recent years, point-based value iteration methods have proven to be extremely effective techniquesfor ?nding (approximately) optimal dynamic programming solutions to POMDPswhen an initial set of belief states is known. However, no point-based work hasprovided exact point-based backups for both continuous state and observationspaces, which we tackle in this paper. Our key insight is that while there maybe an in?nite number of possible observations, there are only a ?nite number ofobservation partitionings that are relevant for optimal decision-making when a?nite, ?xed set of reachable belief states is known. To this end, we make twoimportant contributions: (1) we show how previous exact symbolic dynamic pro-gramming solutions for continuous state MDPs can be generalized to continu-ous state POMDPs with discrete observations, and (2) we show how this solutioncan be further extended via recently developed symbolic methods to continuousstate and observations to derive the minimal relevant observation partitioning forpotentially correlated, multivariate observation spaces. We demonstrate proof-of-concept results on uni- and multi-variate state and observation steam plant control."
Robust Sparse Regression and Matching Pursuit,"In this paper we consider support recovery in sparse regression, when some number $n_1$ out of $n+n_1$ total covariate/response pairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interested in understanding how many outliers, $n_1$, we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables) provide guarantees on support recovery. Perhaps surprisingly, we also show that the natural brute force algorithm that searches over all subsets of $n$ covariate/response pairs, and all subsets of possible support coordinates in order to minimize regression error, is remarkably poor, unable to correctly identify the support with even $n_1 = O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in the basic setting we consider, where all authentic measurements and noise are independent and Gaussian. In this setting, we provide a simple algorithm that gives stronger performance guarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$ corrupted points, where $p$ is the dimension of the signal to be recovered."
Robust Sparse Regression and Matching Pursuit,"In this paper we consider support recovery in sparse regression, when some number $n_1$ out of $n+n_1$ total covariate/response pairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interested in understanding how many outliers, $n_1$, we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables) provide guarantees on support recovery. Perhaps surprisingly, we also show that the natural brute force algorithm that searches over all subsets of $n$ covariate/response pairs, and all subsets of possible support coordinates in order to minimize regression error, is remarkably poor, unable to correctly identify the support with even $n_1 = O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in the basic setting we consider, where all authentic measurements and noise are independent and Gaussian. In this setting, we provide a simple algorithm that gives stronger performance guarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$ corrupted points, where $p$ is the dimension of the signal to be recovered."
Differentially Private Learning with Kernels,"In this paper, we consider the problem of differentially private learning using kernel empirical risk minimization (ERM) where access to the training features is through a kernel function only. Existing work for this problem is either for the linear kernel or for translation invariant kernel, where (approximate) training features are available explicitly and furthermore their generalization error guarantees are dependent on the data dimensionality. Restricting access to data through kernel functions eliminates possibility of explicitly releasing the optima w^* to the kernel ERM. To alleviate this problem, we define three different models for differential private learning using kernel ERM. Our first model is an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. In the second model, learner sends back a differentially private version of the optimal parameter vector w^* but requires to see a small subset of unlabeled test set beforehand. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of w^*. For each of the model, we derive algorithms inspired by the technique for online database release by Gupta et al. 2011 and provide privacy as well as ``goodness'' guarantees. Furthermore, we show that our method can be applied to the setting of Rubinstein et al. 2009, Chaudhuri et al. 2011 also and obtain similar generalization error bounds with two distinctions: a) our bounds are independent of the data dimensionality, b) our sample complexity bounds have worse dependence on the required generalization error."
Semi-supervised Attribute Pattern Learning,"We focus on combining multiple views of object features to learn attribute patterns for semi-supervised classification. By formulating the problem as a semi-supervised classification with constrains: objects in the same class are similar to labeled data of the class upon each view of features and the quantized multi-modality attributes generated from the multiview features, an iterative attribute pattern learning method are proposed. In our approach, feature attributes are initialized by Euclidean Weighted Constrained-KMeans in each view, followed by attribute pattern discovery via Hamming Weighted Constrained-KMeans on the multi-modality attributes. These attribute patterns can further help to adjust the results of feature attributes in individual views by the feature co-occurrence. We therefore adopt a self-learning strategy to reconcile the disagreements between the classifications of the individual views of features and the multi-modality attributes with guaranteed convergence. On the other hand, we also achieve a variant of our method, where the supervision information is only imposed upon the multi-modality attributes for classification. The classifications on both synthetic and real-world datasets demonstrate that our attribute pattern learning method achieve better performance than the most informative view of features and concatenated features of all views by utilizing the complementary information from multiple views by feature co-occurrence."
Semi-supervised Attribute Pattern Learning,"We focus on combining multiple views of object features to learn attribute patterns for semi-supervised classification. By formulating the problem as a semi-supervised classification with constrains: objects in the same class are similar to labeled data of the class upon each view of features and the quantized multi-modality attributes generated from the multiview features, an iterative attribute pattern learning method are proposed. In our approach, feature attributes are initialized by Euclidean Weighted Constrained-KMeans in each view, followed by attribute pattern discovery via Hamming Weighted Constrained-KMeans on the multi-modality attributes. These attribute patterns can further help to adjust the results of feature attributes in individual views by the feature co-occurrence. We therefore adopt a self-learning strategy to reconcile the disagreements between the classifications of the individual views of features and the multi-modality attributes with guaranteed convergence. On the other hand, we also achieve a variant of our method, where the supervision information is only imposed upon the multi-modality attributes for classification. The classifications on both synthetic and real-world datasets demonstrate that our attribute pattern learning method achieve better performance than the most informative view of features and concatenated features of all views by utilizing the complementary information from multiple views by feature co-occurrence."
Semiparametric Bigraphical Models,"In multivariate analysis, a Gaussian bigraphical model is commonly used for modeling matrix-valued data. In this paper we propose a semiparametric extension of the Gaussian bigraphical model, called the nonparanormal bigraphical model. Nonparametric rank-based regularization estimators are exploited to  estimate the sparse precision matrices and graphs under a  penalized profile likelihood framework. Theoretically,  our semiparametric procedure achieves the  parametric rate of convergence for both parameter estimation and graph recovery. Empirically, our semiparametric approach significantly outperforms its parametric counterpart for non-Gaussian data and behaves competitive even for Gaussian data. "
Convex Loss Minimization with Noisy Labels,"We study supervised binary classification in the presence of random classification noise. This setting can be thought of as a particular instance of learning from partial information: the learner, instead of seeing the actual labels, sees labels that have been flipped with some small probability. Using a simple unbiased estimator of the gradient of the loss, we derive online regret bounds for convex loss functions. These bounds immediately lead to efficient algorithms for learning from iid data with noisy labels via a simple online-to-batch conversion. We point out an interesting situation for hinge loss: a batch method using unbiased estimates leads to a non-convex problem whereas online learning using unbiased estimates is still efficient and comes with theoretical guarantees. We show that convexity of the batch problem can be retained if the loss function satisfies a simple symmetry condition. We illustrate the usefulness of our techniques on synthetic and real data."
Convex Loss Minimization with Noisy Labels,"We study supervised binary classification in the presence of random classification noise. This setting can be thought of as a particular instance of learning from partial information: the learner, instead of seeing the actual labels, sees labels that have been flipped with some small probability. Using a simple unbiased estimator of the gradient of the loss, we derive online regret bounds for convex loss functions. These bounds immediately lead to efficient algorithms for learning from iid data with noisy labels via a simple online-to-batch conversion. We point out an interesting situation for hinge loss: a batch method using unbiased estimates leads to a non-convex problem whereas online learning using unbiased estimates is still efficient and comes with theoretical guarantees. We show that convexity of the batch problem can be retained if the loss function satisfies a simple symmetry condition. We illustrate the usefulness of our techniques on synthetic and real data."
Convex Loss Minimization with Noisy Labels,"We study supervised binary classification in the presence of random classification noise. This setting can be thought of as a particular instance of learning from partial information: the learner, instead of seeing the actual labels, sees labels that have been flipped with some small probability. Using a simple unbiased estimator of the gradient of the loss, we derive online regret bounds for convex loss functions. These bounds immediately lead to efficient algorithms for learning from iid data with noisy labels via a simple online-to-batch conversion. We point out an interesting situation for hinge loss: a batch method using unbiased estimates leads to a non-convex problem whereas online learning using unbiased estimates is still efficient and comes with theoretical guarantees. We show that convexity of the batch problem can be retained if the loss function satisfies a simple symmetry condition. We illustrate the usefulness of our techniques on synthetic and real data."
Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation,"Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented."
Partial Gaussian Graphical Model Estimation,"This paper studies the partial estimation of Gaussian graphical models from high-dimensional empirical observations. We derive a convex formulation for this problem using l1-regularized maximum-likelihood estimation, which can be solved via a block coordinate descent algorithm. Statistical estimation performance can be established for our method. The proposed approach has competitive empirical performance compared to existing methods, as demonstrated by various experiments on synthetic and real datasets."
Path Integral Control by Reproducing Kernel Hilbert Space Embedding,"We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided."
Path Integral Control by Reproducing Kernel Hilbert Space Embedding,"We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided."
On Lifting the Gibbs Sampling Algorithm,"Statistical relational learning models combine the power of first-order logic, the de facto tool for handling relational structure, with that of probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the speed, accuracy and scalability of existing graphical models' inference algorithms by exploiting symmetry in the first-order representation. In this paper, we consider blocked Gibbs sampling, an advanced variation of the classic Gibbs sampling algorithm and lift it to the first-order level. We propose to achieve this by partitioning the first-order atoms in the relational model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing such clusters and determining their complexity and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy and convergence."
On Lifting the Gibbs Sampling Algorithm,"Statistical relational learning models combine the power of first-order logic, the de facto tool for handling relational structure, with that of probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the speed, accuracy and scalability of existing graphical models' inference algorithms by exploiting symmetry in the first-order representation. In this paper, we consider blocked Gibbs sampling, an advanced variation of the classic Gibbs sampling algorithm and lift it to the first-order level. We propose to achieve this by partitioning the first-order atoms in the relational model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing such clusters and determining their complexity and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy and convergence."
Structured Message Passing,"Almost all message-passing based approximate inference approaches proposed to date, e.g., belief propagation (BP), its generalizations and expectation propagation (EP), use tabular representation of messages and potentials. In this paper, we argue that this limits their accuracy in practice. To remedy this, we propose structured message passing (SMP), a unifying framework for taking advantage of structured representations. Within this framework, we investigate two structured approaches: sparse hash tables and algebraic decision diagrams for representing and manipulating messages, and propose a new message-passing algorithm that is an instance of SMP. The key idea in our new SMP algorithm is to artificially introduce determinism and context-specific independence in the messages which enables us to exploit the power and compactness of structured representations. We investigate our new algorithm both theoretically and experimentally. Our experimental results show the power and superiority of SMP over tabular message passing algorithms."
Structured Message Passing,"Almost all message-passing based approximate inference approaches proposed to date, e.g., belief propagation (BP), its generalizations and expectation propagation (EP), use tabular representation of messages and potentials. In this paper, we argue that this limits their accuracy in practice. To remedy this, we propose structured message passing (SMP), a unifying framework for taking advantage of structured representations. Within this framework, we investigate two structured approaches: sparse hash tables and algebraic decision diagrams for representing and manipulating messages, and propose a new message-passing algorithm that is an instance of SMP. The key idea in our new SMP algorithm is to artificially introduce determinism and context-specific independence in the messages which enables us to exploit the power and compactness of structured representations. We investigate our new algorithm both theoretically and experimentally. Our experimental results show the power and superiority of SMP over tabular message passing algorithms."
Global Multi-view Subspace Learning,"Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results, particularly for high dimensional data."
Global Multi-view Subspace Learning,"Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results, particularly for high dimensional data."
Global Multi-view Subspace Learning,"Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results, particularly for high dimensional data."
Reliability weighted belief consensus of experts,"The belief consensus algorithm for a network of interacting experts (such as sensors or classifiers) is based on the result that for a given hypothesis of interest, the belief of each expert converges to a sum of the log-likelihoods for that hypotheses from all experts. However, the algorithm does not address the realistic possibility of certain experts being less accurate than the others. We propose a reliability-weighted version of the belief consensus algorithm and prove that the individual expert beliefs still converge to a stable solution given by a reliablity-weighted sum of the log-likelihoods from all experts. We also propose a scheme for learning the reliabilities of individual experts using two smooth approximations (softmax and exponential) of the 0-1 reward function with the empirical risk minimization principle. Experiments on four UCI classification datasets suggest that the proposed reliability-weighted belief consensus algorithm with reliability estimation performs better than the standard unweighted version."
Reliability weighted belief consensus of experts,"The belief consensus algorithm for a network of interacting experts (such as sensors or classifiers) is based on the result that for a given hypothesis of interest, the belief of each expert converges to a sum of the log-likelihoods for that hypotheses from all experts. However, the algorithm does not address the realistic possibility of certain experts being less accurate than the others. We propose a reliability-weighted version of the belief consensus algorithm and prove that the individual expert beliefs still converge to a stable solution given by a reliablity-weighted sum of the log-likelihoods from all experts. We also propose a scheme for learning the reliabilities of individual experts using two smooth approximations (softmax and exponential) of the 0-1 reward function with the empirical risk minimization principle. Experiments on four UCI classification datasets suggest that the proposed reliability-weighted belief consensus algorithm with reliability estimation performs better than the standard unweighted version."
Reliability weighted belief consensus of experts,"The belief consensus algorithm for a network of interacting experts (such as sensors or classifiers) is based on the result that for a given hypothesis of interest, the belief of each expert converges to a sum of the log-likelihoods for that hypotheses from all experts. However, the algorithm does not address the realistic possibility of certain experts being less accurate than the others. We propose a reliability-weighted version of the belief consensus algorithm and prove that the individual expert beliefs still converge to a stable solution given by a reliablity-weighted sum of the log-likelihoods from all experts. We also propose a scheme for learning the reliabilities of individual experts using two smooth approximations (softmax and exponential) of the 0-1 reward function with the empirical risk minimization principle. Experiments on four UCI classification datasets suggest that the proposed reliability-weighted belief consensus algorithm with reliability estimation performs better than the standard unweighted version."
Semantic Kernel Forests from Multiple Taxonomies,"When learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient.  While an \emph{object taxonomy} specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that \emph{are} relevant.  In light of these issues, we propose a discriminative feature learning approach that leverages \emph{multiple} hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenic ties, while another could reflect their habitats).  For each taxonomy, we first learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes.  Then, using the resulting \emph{semantic kernel forest}, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class.  To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure.  We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements."
Semantic Kernel Forests from Multiple Taxonomies,"When learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient.  While an \emph{object taxonomy} specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that \emph{are} relevant.  In light of these issues, we propose a discriminative feature learning approach that leverages \emph{multiple} hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenic ties, while another could reflect their habitats).  For each taxonomy, we first learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes.  Then, using the resulting \emph{semantic kernel forest}, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class.  To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure.  We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements."
Semantic Kernel Forests from Multiple Taxonomies,"When learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient.  While an \emph{object taxonomy} specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that \emph{are} relevant.  In light of these issues, we propose a discriminative feature learning approach that leverages \emph{multiple} hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenic ties, while another could reflect their habitats).  For each taxonomy, we first learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes.  Then, using the resulting \emph{semantic kernel forest}, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class.  To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure.  We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements."
Diversity and Capacity Control in Boosting,"Various explanations have been proposed for understanding the great success of boosting algorithms, particularly AdaBoost, mainly including the statistical view and the margin theory. However, there are still observations out of the explanations. In this paper, we investigate the learning capacity (also known as the hypothesis space complexity and structure risk) of AdaBoost, which has not been well investigated in existing explanations. Previously, the learning capacity of boosting was canonically measured by the VC-dimension of the convex hull of the linear combination of hypotheses, which shows that the capacity grows exponentially to the number of base hypotheses. This paper proves the connection between the learning capacity and the diversity among base hypotheses, which discloses that the learning capacity can be small if the diversity is large. We then reveal that AdaBoost can automatically maximize diversity while optimizing its loss function, and therefore, implicitly controls its learning capacity. The investigation on diversity of AdaBoost may provide a clue to complement the understanding of AdaBoost and boosting algorithms."
Sketch-Based Linear Value Function Approximation,"Hashing is a common method to reduce large, potentially infinite feature vectors to a fixed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and fifty-five Atari 2600 games to highlight the superior learning performance of tug-of-war hashing."
Sketch-Based Linear Value Function Approximation,"Hashing is a common method to reduce large, potentially infinite feature vectors to a fixed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and fifty-five Atari 2600 games to highlight the superior learning performance of tug-of-war hashing."
Sketch-Based Linear Value Function Approximation,"Hashing is a common method to reduce large, potentially infinite feature vectors to a fixed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and fifty-five Atari 2600 games to highlight the superior learning performance of tug-of-war hashing."
Clustered Approximation for Gaussian Kernel Support Vector Machines,"The Gaussian kernel support vector machine (SVM) is one of the most widely used classification methods. However, scalability of this method is a big issue when facing millions of samples. Recently, many papers have suggested tackling this problem by using a low-rank approximation for the Gaussian kernel matrix. In this paper, we first show that the structure of the Gaussian kernel matrix changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter $\gamma$ in the Gaussian kernel. Based on this observation, we propose a Clustered Approximation (CA) framework for Gaussian kernel matrix approximation. For many non-linearly separable datasets, we find that the best parameter $\gamma$ for classification usually lies in the region where CA achieves lower approximation error than low-rank approximation when they both use the same amount of memory.Moreover, when we apply CA methods to scale and speed up the Gaussian kernel SVM training, the resulting algorithms outperform state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on a non-linearly separable dataset \covtype with half-a-million samples, our proposed method Clustered Block Approximation SVM (CBA-SVM) achieves 96.03\% testing accuracy with training time of less than 3 minutes on a single workstation with 4G RAM. In comparison, on this problem, \LIBSVM takes more than 13 hours to get 96.08\% accuracy, while the fast low-rank approximation based method, low-rank linearized SVM (LLSVM), takes 1 hour but only gets 85.05\% testing accuracy. "
Clustered Approximation for Gaussian Kernel Support Vector Machines,"The Gaussian kernel support vector machine (SVM) is one of the most widely used classification methods. However, scalability of this method is a big issue when facing millions of samples. Recently, many papers have suggested tackling this problem by using a low-rank approximation for the Gaussian kernel matrix. In this paper, we first show that the structure of the Gaussian kernel matrix changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter $\gamma$ in the Gaussian kernel. Based on this observation, we propose a Clustered Approximation (CA) framework for Gaussian kernel matrix approximation. For many non-linearly separable datasets, we find that the best parameter $\gamma$ for classification usually lies in the region where CA achieves lower approximation error than low-rank approximation when they both use the same amount of memory.Moreover, when we apply CA methods to scale and speed up the Gaussian kernel SVM training, the resulting algorithms outperform state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on a non-linearly separable dataset \covtype with half-a-million samples, our proposed method Clustered Block Approximation SVM (CBA-SVM) achieves 96.03\% testing accuracy with training time of less than 3 minutes on a single workstation with 4G RAM. In comparison, on this problem, \LIBSVM takes more than 13 hours to get 96.08\% accuracy, while the fast low-rank approximation based method, low-rank linearized SVM (LLSVM), takes 1 hour but only gets 85.05\% testing accuracy. "
Clustered Approximation for Gaussian Kernel Support Vector Machines,"The Gaussian kernel support vector machine (SVM) is one of the most widely used classification methods. However, scalability of this method is a big issue when facing millions of samples. Recently, many papers have suggested tackling this problem by using a low-rank approximation for the Gaussian kernel matrix. In this paper, we first show that the structure of the Gaussian kernel matrix changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter $\gamma$ in the Gaussian kernel. Based on this observation, we propose a Clustered Approximation (CA) framework for Gaussian kernel matrix approximation. For many non-linearly separable datasets, we find that the best parameter $\gamma$ for classification usually lies in the region where CA achieves lower approximation error than low-rank approximation when they both use the same amount of memory.Moreover, when we apply CA methods to scale and speed up the Gaussian kernel SVM training, the resulting algorithms outperform state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on a non-linearly separable dataset \covtype with half-a-million samples, our proposed method Clustered Block Approximation SVM (CBA-SVM) achieves 96.03\% testing accuracy with training time of less than 3 minutes on a single workstation with 4G RAM. In comparison, on this problem, \LIBSVM takes more than 13 hours to get 96.08\% accuracy, while the fast low-rank approximation based method, low-rank linearized SVM (LLSVM), takes 1 hour but only gets 85.05\% testing accuracy. "
A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation,"In this paper, we consider the $\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables. We further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure. As an example, a recent state-of-the-art method, QUICrequires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem."
A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation,"In this paper, we consider the $\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables. We further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure. As an example, a recent state-of-the-art method, QUICrequires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem."
A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation,"In this paper, we consider the $\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables. We further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure. As an example, a recent state-of-the-art method, QUICrequires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem."
Importance Sampling Active Learning Algorithm,"This paper presents the importance sampling active learning (ISAL) algorithm, which can be very sample efficient by drawing instances to label from some appropriate distribution.  In particular,  ISAL  begins with a set of user-specified instance distributions; on each iteration, it identifies  a distribution that puts large weight on instances whose labels are  uncertain, then requests the label of an instance drawn from that distribution. We prove that ISAL can be more sample-efficient than passive learning, and that it  can achieve an exponential convergence rate to the Bayes classifier on noise-free data.  We also provide empirical studies that show  ISAL   is more efficient than many other active learning algorithms. "
Importance Sampling Active Learning Algorithm,"This paper presents the importance sampling active learning (ISAL) algorithm, which can be very sample efficient by drawing instances to label from some appropriate distribution.  In particular,  ISAL  begins with a set of user-specified instance distributions; on each iteration, it identifies  a distribution that puts large weight on instances whose labels are  uncertain, then requests the label of an instance drawn from that distribution. We prove that ISAL can be more sample-efficient than passive learning, and that it  can achieve an exponential convergence rate to the Bayes classifier on noise-free data.  We also provide empirical studies that show  ISAL   is more efficient than many other active learning algorithms. "
Importance Sampling Active Learning Algorithm,"This paper presents the importance sampling active learning (ISAL) algorithm, which can be very sample efficient by drawing instances to label from some appropriate distribution.  In particular,  ISAL  begins with a set of user-specified instance distributions; on each iteration, it identifies  a distribution that puts large weight on instances whose labels are  uncertain, then requests the label of an instance drawn from that distribution. We prove that ISAL can be more sample-efficient than passive learning, and that it  can achieve an exponential convergence rate to the Bayes classifier on noise-free data.  We also provide empirical studies that show  ISAL   is more efficient than many other active learning algorithms. "
Learning the Dependency Structure of Latent Factors,"In this paper, we study latent factor models with the dependency structure in the latent space.  We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance."
Learning the Dependency Structure of Latent Factors,"In this paper, we study latent factor models with the dependency structure in the latent space.  We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance."
A Polynomial-time Form of Robust Regression,"Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach compared to standard methods."
A Polynomial-time Form of Robust Regression,"Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach compared to standard methods."
A Polynomial-time Form of Robust Regression,"Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach compared to standard methods."
Robust elastic-net nonnegative matrix factorization with box constraints,"In this paper, we propose an elastic-net nonnegative matrix factorization (NMF) with box constraints to remove grouped outliers and recover the inherent nonnegative low-rank structure of the given high dimensional noisy image data. Based on the augmented Lagrangian framework, we solve the linearly constrained minimization reformulation of the elastic-net NMF with the successive overrelaxed outer product iteration (SOOPI). We evaluate the performance of the proposed method for the background modeling of video image sequence and removal of varying illumination and grossly corrupted artifacts in face images. The numerical results show that our proposed elastic-net NMF model does better recover low-rank structure than the state-of-the-art nuclear norm based robust principal component analysis (PCA) and other robust NMF models."
Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High Dimensional Results,"Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. This paper develops efficient OMP-like algorithms to deal with precisely this setting. Our algorithms are as efficient as OMP, and improve on the best-known results for missing and noisy data in regression, both in the high-dimensional setting where we seek to recover a sparse vector from only a few measurements, and in the classical low-dimensional setting where we recover an unstructured regressor. In the high-dimensional setting, our support-recovery algorithm {\it requires no knowledge} of even the statistics of the noise. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise."
Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High Dimensional Results,"Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. This paper develops efficient OMP-like algorithms to deal with precisely this setting. Our algorithms are as efficient as OMP, and improve on the best-known results for missing and noisy data in regression, both in the high-dimensional setting where we seek to recover a sparse vector from only a few measurements, and in the classical low-dimensional setting where we recover an unstructured regressor. In the high-dimensional setting, our support-recovery algorithm {\it requires no knowledge} of even the statistics of the noise. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise."
Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs,"We describe an approach to speed-up inference with latent variable PCFGs, which have been shown to  be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature.  We also describe an error bound for this approximation, which bounds the difference between the probabilities calculated by the algorithm and the true probabilities that the approximated model gives. Empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance."
Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs,"We describe an approach to speed-up inference with latent variable PCFGs, which have been shown to  be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature.  We also describe an error bound for this approximation, which bounds the difference between the probabilities calculated by the algorithm and the true probabilities that the approximated model gives. Empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance."
Exponential Concentration for Mutual Information Estimation with Application to Forests,"We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph."
Minor Surfaces for Clustering and Manifold Learning,"We show that mode-based cluster boundaries exhibit themselves as minor surfaces of the data distribution. Following this observation, we propose a connectivity metric based on the minor surface search between samples. This extends the mode-seeking procedure into a connectivity graph representation, which could be used in many machine learning applications. The use of the graph construction is particularly demonstrated in clustering and manifold learning problems. The experiments are carried out on synthetic and real datasets using Gaussian mixture models and kernel density estimates. Instead of climbing the mode for each sample, we perform connected component analysis in the proposed graph and achieve the same performace as mean-shift algorithm. The minor surface search discards the connections that does not pass through the data cloud, which makes it possible to use in manifold learning problems. We employ a distance matrix masked by our connectivity graph as an input to the Isomap algorithm, and show that the dependence of the performance on the $knn$ and $\epsilon$-ball generalizations decreases with our approach."
Minor Surfaces for Clustering and Manifold Learning,"We show that mode-based cluster boundaries exhibit themselves as minor surfaces of the data distribution. Following this observation, we propose a connectivity metric based on the minor surface search between samples. This extends the mode-seeking procedure into a connectivity graph representation, which could be used in many machine learning applications. The use of the graph construction is particularly demonstrated in clustering and manifold learning problems. The experiments are carried out on synthetic and real datasets using Gaussian mixture models and kernel density estimates. Instead of climbing the mode for each sample, we perform connected component analysis in the proposed graph and achieve the same performace as mean-shift algorithm. The minor surface search discards the connections that does not pass through the data cloud, which makes it possible to use in manifold learning problems. We employ a distance matrix masked by our connectivity graph as an input to the Isomap algorithm, and show that the dependence of the performance on the $knn$ and $\epsilon$-ball generalizations decreases with our approach."
Non-linear Metric Learning,"In this paper, we introduce two novel metric learning algorithms, ?2-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: ?2-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear ?2-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of ?2-LMNN, obtain best results in 19 out of 20 learning settings. "
Factorial LDA: Sparse Multi-Dimensional Text Models,"Multi-dimensional latent variable models can capture the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional latent variable model in which a document is influenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientific discipline, and focus (e.g. methods vs. applications.) Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors."
Automating Collusion Detection in Sequential Games,"Collusion is the practice of two parties deliberately cooperating to the detriment of others.  While such behavior may be desirable in certain circumstances, in many it is considered dishonest and unfair.  If agents otherwise hold strictly to the established rules, though, collusion can be challenging to police.  In this paper, we introduce an automatic method for collusion detection in sequential games.  We achieve this through a novel object, called a collusion table, that aims to capture the effects of collusive behavior, i.e., advantage to the colluding parties, without committing to any particular pattern of behavior.  We demonstrate the effectiveness of this method in the domain of poker, a popular game where collusion is prohibited."
Automating Collusion Detection in Sequential Games,"Collusion is the practice of two parties deliberately cooperating to the detriment of others.  While such behavior may be desirable in certain circumstances, in many it is considered dishonest and unfair.  If agents otherwise hold strictly to the established rules, though, collusion can be challenging to police.  In this paper, we introduce an automatic method for collusion detection in sequential games.  We achieve this through a novel object, called a collusion table, that aims to capture the effects of collusive behavior, i.e., advantage to the colluding parties, without committing to any particular pattern of behavior.  We demonstrate the effectiveness of this method in the domain of poker, a popular game where collusion is prohibited."
Expectation Propagation in Gaussian Process Dynamical Systems,"Rich and complex time-series data, such as those generated from engineering sys-tems, financial markets, videos or neural recordings are now a common feature ofmodern data analysis. Explaining the phenomena underlying these diverse datasets requires flexible and accurate models. In this paper, we promote Gaussianprocess dynamical systems as a rich model class appropriate for such analysis. Inparticular, we present a message passing algorithm for approximate inference inGPDSs based on expectation propagation. By phrasing inference as a general mes-sage passing problem, we iterate forward-backward smoothing. We obtain moreaccurate posterior distributions over latent structures, resulting in improved pre-dictive performance compared to state-of-the-art GPDS smoothers, which are spe-cial cases of our general iterative message passing algorithm. Hence, we providea unifying approach within which to contextualize message passing in GPDSs."
Generalized quadratic models and moment-based neural dimensionality reduction,"A popular approach for investigating the neural code is to identify a low-dimensional subspace of stimuli that modulate a neuron's response. Here we describe a set of methods for neural characterization based on Generalized Quadratic Models (GQMs). These models contain a low-rank quadratic form Q that defines the neural subspace, a nonlinear transfer function f, and an exponential-family distribution function P. Special cases include the 2nd order Volterra model (with linear f and Gaussian P), the elliptical-LNP model (with arbitrary f and Poisson P), and the quadratic-logistic regression model (for logistic f and Bernoulli P).  Here we show that for ``canonical form'' GQMs, the first two response-weighted moments yield simplified maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes spike-triggered covariance analysis to analog and binary response data, and provides closed-form estimators under a variety of non-Gaussian stimulus distributions. In the linear-Gaussian case, we show that the corresponding estimator depends only on the first four moments of the raw stimulus distribution.  Finally, the GQM extends generalized linear models (GLMs) to allow multi-dimensional dependence on spike history.  We apply these methods to simulated and real neural data from retina and V1."
Generalized quadratic models and moment-based neural dimensionality reduction,"A popular approach for investigating the neural code is to identify a low-dimensional subspace of stimuli that modulate a neuron's response. Here we describe a set of methods for neural characterization based on Generalized Quadratic Models (GQMs). These models contain a low-rank quadratic form Q that defines the neural subspace, a nonlinear transfer function f, and an exponential-family distribution function P. Special cases include the 2nd order Volterra model (with linear f and Gaussian P), the elliptical-LNP model (with arbitrary f and Poisson P), and the quadratic-logistic regression model (for logistic f and Bernoulli P).  Here we show that for ``canonical form'' GQMs, the first two response-weighted moments yield simplified maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes spike-triggered covariance analysis to analog and binary response data, and provides closed-form estimators under a variety of non-Gaussian stimulus distributions. In the linear-Gaussian case, we show that the corresponding estimator depends only on the first four moments of the raw stimulus distribution.  Finally, the GQM extends generalized linear models (GLMs) to allow multi-dimensional dependence on spike history.  We apply these methods to simulated and real neural data from retina and V1."
Generalized quadratic models and moment-based neural dimensionality reduction,"A popular approach for investigating the neural code is to identify a low-dimensional subspace of stimuli that modulate a neuron's response. Here we describe a set of methods for neural characterization based on Generalized Quadratic Models (GQMs). These models contain a low-rank quadratic form Q that defines the neural subspace, a nonlinear transfer function f, and an exponential-family distribution function P. Special cases include the 2nd order Volterra model (with linear f and Gaussian P), the elliptical-LNP model (with arbitrary f and Poisson P), and the quadratic-logistic regression model (for logistic f and Bernoulli P).  Here we show that for ``canonical form'' GQMs, the first two response-weighted moments yield simplified maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes spike-triggered covariance analysis to analog and binary response data, and provides closed-form estimators under a variety of non-Gaussian stimulus distributions. In the linear-Gaussian case, we show that the corresponding estimator depends only on the first four moments of the raw stimulus distribution.  Finally, the GQM extends generalized linear models (GLMs) to allow multi-dimensional dependence on spike history.  We apply these methods to simulated and real neural data from retina and V1."
Generalized quadratic models and moment-based neural dimensionality reduction,"A popular approach for investigating the neural code is to identify a low-dimensional subspace of stimuli that modulate a neuron's response. Here we describe a set of methods for neural characterization based on Generalized Quadratic Models (GQMs). These models contain a low-rank quadratic form Q that defines the neural subspace, a nonlinear transfer function f, and an exponential-family distribution function P. Special cases include the 2nd order Volterra model (with linear f and Gaussian P), the elliptical-LNP model (with arbitrary f and Poisson P), and the quadratic-logistic regression model (for logistic f and Bernoulli P).  Here we show that for ``canonical form'' GQMs, the first two response-weighted moments yield simplified maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes spike-triggered covariance analysis to analog and binary response data, and provides closed-form estimators under a variety of non-Gaussian stimulus distributions. In the linear-Gaussian case, we show that the corresponding estimator depends only on the first four moments of the raw stimulus distribution.  Finally, the GQM extends generalized linear models (GLMs) to allow multi-dimensional dependence on spike history.  We apply these methods to simulated and real neural data from retina and V1."
Online Multi-Task Collaborative Filtering,"Traditional model based approaches for Collaborative Filtering are often based on batch learning algorithms, which assume all labeled data are given a priori before the learning tasks and the models often have to be re-trained when new data arrives in a recommendation task. Such techniques have several critical limitations, e.g., low efficiency and poor scalability for large-scale online applications. Recently, online collaborative filtering (OCF) has emerged as a promising technique to overcome the limitations, which sequentially learns the model over a sequence of data in an online learning fashion. Despite the advantage of high efficiency, the existing approach to OCF using a simple online gradient descent algorithm suffers from slow convergence. In this paper, we propose a new online collaborative filtering framework, that is, online multi-task collaborative filtering (OMTCF), which tackles the online collaborative filtering task by exploiting the idea of online multi-task learning. Unlike the existing OCF approach, OMTCF, by defining a user interaction matrix, effectively updates the models of multiple users simultaneously at each learning iteration, which is able to converge significantly faster. Encouraging results on real-world datasets show that the proposed technique is considerably more effective than the state-of-the-art OCF algorithm."
Online Multi-Task Collaborative Filtering,"Traditional model based approaches for Collaborative Filtering are often based on batch learning algorithms, which assume all labeled data are given a priori before the learning tasks and the models often have to be re-trained when new data arrives in a recommendation task. Such techniques have several critical limitations, e.g., low efficiency and poor scalability for large-scale online applications. Recently, online collaborative filtering (OCF) has emerged as a promising technique to overcome the limitations, which sequentially learns the model over a sequence of data in an online learning fashion. Despite the advantage of high efficiency, the existing approach to OCF using a simple online gradient descent algorithm suffers from slow convergence. In this paper, we propose a new online collaborative filtering framework, that is, online multi-task collaborative filtering (OMTCF), which tackles the online collaborative filtering task by exploiting the idea of online multi-task learning. Unlike the existing OCF approach, OMTCF, by defining a user interaction matrix, effectively updates the models of multiple users simultaneously at each learning iteration, which is able to converge significantly faster. Encouraging results on real-world datasets show that the proposed technique is considerably more effective than the state-of-the-art OCF algorithm."
Efficient Inference and Learning of switching Kalman filters and their Application to Gesture Recognition,"Computational models for high dimensional time series  such as video sequences, spectral trajectories of a speechsignal or the kinematic measurements of skilled human activity hold considerable interest,particularly models that capture the inherent stochastic variability in the signal.  The hidden Markov Model (HMM) is widely used  for modeling such data.  More complex models such as switching linear dynamical systems (S-LDS) account better for the continuity of the observations, which an HMM assumes to be conditionally independent, but they lack efficient learning and inference procedures.  This paper makes three advances to address these limitations\begin{enumerate}\item A previously known inference technique by Barber \cite{barber2006,mesot2007switching} is extended to S-LDS learning.  This extension provides computationally tractable EM-based estimation of S-LDS parameters.\item Under the diagonal assumption on the observation noise, a dynamic programming algorithm is proposed to speed up the per-frame inference-complexity from cubic to linear in the observation dimension.\item A system identification algorithm is provided for initializing the parameters of an S-LDS, leading to effective S-LDS learning.\end{enumerate}The effectiveness of the new algorithms is demonstrated in gesture recognition from kinematic measurements in a robot-assisted minimally invasive surgery (RMIS) task: S-LDS models show significant improvement in recognition accuracy over comparable factor analyzed HMMs.The ability to perform automatic gesture recognition in RMIS has several applications, such as assessing dexterity or manipulative skills during surgical training or providing guidance or assistance during tele-operated surgery."
Entangled Monte Carlo,"We propose a novel method for scalable parallelization of SMC algorithms,Entangled Monte Carlo simulation (EMC).  EMC avoids the transmission ofparticles between  nodes, and instead reconstructs them from the particlegenealogy. In particular, we show that we can reduce the communication tothe particle weights for each machine while efficiently maintaining implicitglobal coherence of the parallel simulation. We explain methods toefficiently maintain a genealogy of particles from which any particle can bereconstructed. We demonstrate using examples from Bayesian phylogeneticthat the computational gain from parallelization using EMCsignificantly outweighs the cost of particle reconstruction. The timingexperiments show that reconstruction of particles is indeed much more efficientas compared to transmission of particles."
Near-Optimal MAP Inference for Determinantal Point Processes,"  Determinantal point processes (DPPs) have recently been proposed as  computationally efficient probabilistic models of diverse sets for a  variety of applications, including document summarization, image  search, and pose estimation.  Many DPP inference operations,  including normalization and sampling, are tractable; however,  finding the most likely configuration (MAP), which is often required  in practice for decoding, is NP-hard, so we must resort to  approximate inference.  Because DPP probabilities are  log-submodular, greedy algorithms have been used in the past with  some empirical success; however, these methods only give  approximation guarantees in the special case of DPPs with monotone  kernels.  In this paper we propose a new algorithm for approximating  the MAP problem based on continuous techniques for submodular  function maximization.  Our method involves a novel continuous  relaxation of the log-probability function, which, in contrast to  the multilinear extension used for general submodular functions, can  be evaluated and differentiated exactly and efficiently.  We obtain  a practical algorithm with a 1/4-approximation guarantee for a  general class of non-monotone DPPs.  Our algorithm also extends to  MAP inference under complex polytope constraints, making it possible  to combine DPPs with Markov random fields, weighted matchings, and  other models.  We demonstrate that our approach outperforms greedy  methods on both synthetic and real-world data."
Near-Optimal MAP Inference for Determinantal Point Processes,"  Determinantal point processes (DPPs) have recently been proposed as  computationally efficient probabilistic models of diverse sets for a  variety of applications, including document summarization, image  search, and pose estimation.  Many DPP inference operations,  including normalization and sampling, are tractable; however,  finding the most likely configuration (MAP), which is often required  in practice for decoding, is NP-hard, so we must resort to  approximate inference.  Because DPP probabilities are  log-submodular, greedy algorithms have been used in the past with  some empirical success; however, these methods only give  approximation guarantees in the special case of DPPs with monotone  kernels.  In this paper we propose a new algorithm for approximating  the MAP problem based on continuous techniques for submodular  function maximization.  Our method involves a novel continuous  relaxation of the log-probability function, which, in contrast to  the multilinear extension used for general submodular functions, can  be evaluated and differentiated exactly and efficiently.  We obtain  a practical algorithm with a 1/4-approximation guarantee for a  general class of non-monotone DPPs.  Our algorithm also extends to  MAP inference under complex polytope constraints, making it possible  to combine DPPs with Markov random fields, weighted matchings, and  other models.  We demonstrate that our approach outperforms greedy  methods on both synthetic and real-world data."
Near-Optimal MAP Inference for Determinantal Point Processes,"  Determinantal point processes (DPPs) have recently been proposed as  computationally efficient probabilistic models of diverse sets for a  variety of applications, including document summarization, image  search, and pose estimation.  Many DPP inference operations,  including normalization and sampling, are tractable; however,  finding the most likely configuration (MAP), which is often required  in practice for decoding, is NP-hard, so we must resort to  approximate inference.  Because DPP probabilities are  log-submodular, greedy algorithms have been used in the past with  some empirical success; however, these methods only give  approximation guarantees in the special case of DPPs with monotone  kernels.  In this paper we propose a new algorithm for approximating  the MAP problem based on continuous techniques for submodular  function maximization.  Our method involves a novel continuous  relaxation of the log-probability function, which, in contrast to  the multilinear extension used for general submodular functions, can  be evaluated and differentiated exactly and efficiently.  We obtain  a practical algorithm with a 1/4-approximation guarantee for a  general class of non-monotone DPPs.  Our algorithm also extends to  MAP inference under complex polytope constraints, making it possible  to combine DPPs with Markov random fields, weighted matchings, and  other models.  We demonstrate that our approach outperforms greedy  methods on both synthetic and real-world data."
Learning Networks of Heterogeneous Influence,"Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities. "
Learning Networks of Heterogeneous Influence,"Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities. "
Learning Networks of Heterogeneous Influence,"Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities. "
Bellman Error Based Feature Generation using Random Projections on Sparse Spaces,"We address the problem of automatic generation of features for value function approximation.Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error.  Empirical results demonstrate the strength of this method."
Using Both Supervised and Latent Shared Topics for Multitask Learning,"Since its introduction, Latent Dirichlet Allocation (LDA) has been extended to include two different types ofdocument-level supervision: topic labels and category labels. We introduce a new framework, Doubly SupervisedLatent Dirichlet Allocation (DSLDA), that integrates both types of supervision. We demonstrate thatthis approach is particularly useful for multitask learning, in which both supervised and latent (unsupervised)topics are shared between multiple categories. Experimental results on document classification show thatboth types of supervision improve the performance of DSLDA and that sharing both latent and supervisedtopics allows for better multitask learning."
Using Both Supervised and Latent Shared Topics for Multitask Learning,"Since its introduction, Latent Dirichlet Allocation (LDA) has been extended to include two different types ofdocument-level supervision: topic labels and category labels. We introduce a new framework, Doubly SupervisedLatent Dirichlet Allocation (DSLDA), that integrates both types of supervision. We demonstrate thatthis approach is particularly useful for multitask learning, in which both supervised and latent (unsupervised)topics are shared between multiple categories. Experimental results on document classification show thatboth types of supervision improve the performance of DSLDA and that sharing both latent and supervisedtopics allows for better multitask learning."
Using Both Supervised and Latent Shared Topics for Multitask Learning,"Since its introduction, Latent Dirichlet Allocation (LDA) has been extended to include two different types ofdocument-level supervision: topic labels and category labels. We introduce a new framework, Doubly SupervisedLatent Dirichlet Allocation (DSLDA), that integrates both types of supervision. We demonstrate thatthis approach is particularly useful for multitask learning, in which both supervised and latent (unsupervised)topics are shared between multiple categories. Experimental results on document classification show thatboth types of supervision improve the performance of DSLDA and that sharing both latent and supervisedtopics allows for better multitask learning."
A tree-decomposed EM algorithm for covariance selection in noisy graphical models,"Gaussian graphical models (GGMs) are widely used in computer science, and have also enjoyed wide applicability in a number of scientific areas. We consider the problem of covariance selection, i.e. estimation of the (inverse) covariance matrix of the joint probability distribution of random variables on a high dimensional graph. To extend the applicability of GGMs, we consider the case where observations for variables are also subject to additional measurement noise. Unfortunately, the the estimation of model parameters in this setting becomes complicated by the fact that the structure of the underlying graph no longer provides direct information about the location of zeros in the inverse covariance matrix. We propose an efficient EM algorithm which uses the tree decomposition of the underlying graph in order to perform the parameter estimation through local operations. We also explore the effect of the treelike structure of the graph on computational performance of the algorithm as well as the accuracy of the estimates by applying it to a wide range of random graph models."
Monte Carlo Tree Search Using Goal-Directed and Constraint-Biased Action Abstractions,"Monte Carlo Tree Search (MCTS) is a family of methods for planning in large domains. It focuses on finding a good action for a particular state, making its complexity independent of the size of the state space. Effective application of MCTS requires both a good pruning heuristic and a roll-out policy. In this paper we leverage the human tendency to decompose tasks into 1) goal-directed action sequences and 2) sets of constraints to generate stochastic policies that facilitateefficient exploration in MCTS.We demonstrate the efficacy of our approach in the PacMan domain and highlight its advantages over traditional MCTS."
Monte Carlo Tree Search Using Goal-Directed and Constraint-Biased Action Abstractions,"Monte Carlo Tree Search (MCTS) is a family of methods for planning in large domains. It focuses on finding a good action for a particular state, making its complexity independent of the size of the state space. Effective application of MCTS requires both a good pruning heuristic and a roll-out policy. In this paper we leverage the human tendency to decompose tasks into 1) goal-directed action sequences and 2) sets of constraints to generate stochastic policies that facilitateefficient exploration in MCTS.We demonstrate the efficacy of our approach in the PacMan domain and highlight its advantages over traditional MCTS."
Monte Carlo Tree Search Using Goal-Directed and Constraint-Biased Action Abstractions,"Monte Carlo Tree Search (MCTS) is a family of methods for planning in large domains. It focuses on finding a good action for a particular state, making its complexity independent of the size of the state space. Effective application of MCTS requires both a good pruning heuristic and a roll-out policy. In this paper we leverage the human tendency to decompose tasks into 1) goal-directed action sequences and 2) sets of constraints to generate stochastic policies that facilitateefficient exploration in MCTS.We demonstrate the efficacy of our approach in the PacMan domain and highlight its advantages over traditional MCTS."
Monte Carlo Tree Search Using Goal-Directed and Constraint-Biased Action Abstractions,"Monte Carlo Tree Search (MCTS) is a family of methods for planning in large domains. It focuses on finding a good action for a particular state, making its complexity independent of the size of the state space. Effective application of MCTS requires both a good pruning heuristic and a roll-out policy. In this paper we leverage the human tendency to decompose tasks into 1) goal-directed action sequences and 2) sets of constraints to generate stochastic policies that facilitateefficient exploration in MCTS.We demonstrate the efficacy of our approach in the PacMan domain and highlight its advantages over traditional MCTS."
Flexible Temporal Structure Learning,"This paper extends the recently introduced structure learning methods for Gaussian random fields and ``nonparanormal'' distributions to a multivariate non-Gaussian time series setting. Our approach is based on discriminative state-space models, and introduces sparsity constraints on dependence structures of multivariate outcomes, as well as other parameters of emission and transition distributions. This combines feature selection with time series modelling, giving rise to explainable representations of data and dependence structures at each latent state. In contrast to the recent literature on sparse graphical models, our approach allows for an easy integration of multi-modality and input variables. We apply our method to multivariate financial time series data. We show that it helps to uncover meaningful dependencies between stock prices and significantly outperforms common approaches for predicting share prices, offering potential for real applications."
Randomized Proximal Point Algorithm for Large Scale Multiple Kernel Learning,"We consider the problem of learning weights to combine kernels to learn a good predictor. We study the computational problem that arises. We propose a randomized version of the proximal point algorithm to avoid a linear dependence on the number of predictors in the convergence rate. We derive finite-time performance bounds for the new algorithm that show that under mild conditions it finds the optimum of our penalized empirical risk criterion in an efficient manner. Experiments with simulated and real data are used to illustrate the new algorithm, which is found to be computationally more efficient with performance competitive to state-of-the-art alternatives."
Randomized Proximal Point Algorithm for Large Scale Multiple Kernel Learning,"We consider the problem of learning weights to combine kernels to learn a good predictor. We study the computational problem that arises. We propose a randomized version of the proximal point algorithm to avoid a linear dependence on the number of predictors in the convergence rate. We derive finite-time performance bounds for the new algorithm that show that under mild conditions it finds the optimum of our penalized empirical risk criterion in an efficient manner. Experiments with simulated and real data are used to illustrate the new algorithm, which is found to be computationally more efficient with performance competitive to state-of-the-art alternatives."
Randomized Proximal Point Algorithm for Large Scale Multiple Kernel Learning,"We consider the problem of learning weights to combine kernels to learn a good predictor. We study the computational problem that arises. We propose a randomized version of the proximal point algorithm to avoid a linear dependence on the number of predictors in the convergence rate. We derive finite-time performance bounds for the new algorithm that show that under mild conditions it finds the optimum of our penalized empirical risk criterion in an efficient manner. Experiments with simulated and real data are used to illustrate the new algorithm, which is found to be computationally more efficient with performance competitive to state-of-the-art alternatives."
Randomized Proximal Point Algorithm for Large Scale Multiple Kernel Learning,"We consider the problem of learning weights to combine kernels to learn a good predictor. We study the computational problem that arises. We propose a randomized version of the proximal point algorithm to avoid a linear dependence on the number of predictors in the convergence rate. We derive finite-time performance bounds for the new algorithm that show that under mild conditions it finds the optimum of our penalized empirical risk criterion in an efficient manner. Experiments with simulated and real data are used to illustrate the new algorithm, which is found to be computationally more efficient with performance competitive to state-of-the-art alternatives."
Bayesian Pedigree Analysis using Measure Factorization,"Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease.  With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites.  Some pedigrees number in the thousands of individuals.  Meanwhile, analysis methods have remained limited to pedigrees of <100 individuals which limits analyses to many small independent pedigrees.Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited.  This is because linkage anlysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order.  LODs are difficult to interpret and nontrivial to extend to consider interactions among sites.  These developments and difficulties call for the creation of modern methods of pedigree analysis.Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models.   We show that these disease models can be turned into accurate and efficient estimators.  The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models.  This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction."
Bayesian Pedigree Analysis using Measure Factorization,"Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease.  With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites.  Some pedigrees number in the thousands of individuals.  Meanwhile, analysis methods have remained limited to pedigrees of <100 individuals which limits analyses to many small independent pedigrees.Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited.  This is because linkage anlysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order.  LODs are difficult to interpret and nontrivial to extend to consider interactions among sites.  These developments and difficulties call for the creation of modern methods of pedigree analysis.Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models.   We show that these disease models can be turned into accurate and efficient estimators.  The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models.  This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction."
Accelerated Training for Matrix-norm Regularization: A Boosting Approach,"Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees $\epsilon$ accuracy within $O(1/\epsilon)$ iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle."
Accelerated Training for Matrix-norm Regularization: A Boosting Approach,"Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees $\epsilon$ accuracy within $O(1/\epsilon)$ iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle."
The Raindrop Process:  Bayesian Nonparametric Latent Shape Models,"In difficult object segmentation tasks, utilizing image information alone is not sufficient; incorporation of object shape prior models has been shown to improve segmentation performance. Most formulations that incorporate both shape and image information are in the form of optimizing energy functionals. This paper introduces a nonparametric Bayesian model for segmenting multiple objects in an image taking both shape and image feature/appearance into account. The generative process starts by generating object locations from a spatial Poisson process, then shape parameters are generated from a shape prior model. This automatically partitions the image: pixels inside are assumed to be generated from an object observation/appearance model and pixels outside from a background model. We learn the model via Markov Chain Monte Carlo sampling and our experiments show that the model is capable of segmenting multiple objects."
The Raindrop Process:  Bayesian Nonparametric Latent Shape Models,"In difficult object segmentation tasks, utilizing image information alone is not sufficient; incorporation of object shape prior models has been shown to improve segmentation performance. Most formulations that incorporate both shape and image information are in the form of optimizing energy functionals. This paper introduces a nonparametric Bayesian model for segmenting multiple objects in an image taking both shape and image feature/appearance into account. The generative process starts by generating object locations from a spatial Poisson process, then shape parameters are generated from a shape prior model. This automatically partitions the image: pixels inside are assumed to be generated from an object observation/appearance model and pixels outside from a background model. We learn the model via Markov Chain Monte Carlo sampling and our experiments show that the model is capable of segmenting multiple objects."
Submodular Bregman Divergences with Applications,"We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular Bregman divergences. We  consider two kinds,  defined either from tight modular upper or tight modular lower  bounds of a submodular function. We show that the properties of  these divergences are analogous to the (standard continuous) Bregman  divergence. Further, we demonstrate how they generalize many useful  divergences, including the weighted Hamming distance, squared  weighted Hamming, weighted precision, recall, conditional mutual  information, and a generalized KL-divergence on sets. We also show  that the lower bound submodular Bregman is actually a special case  of the generalized Bregman divergence on the \lovasz{} extension of  a submodular function which we call the \lovasz{} Bregman divergence. We then  point out a number of applications of the submodular Bregman  divergences, and in particular show that a proximal  algorithm defined through the submodular Bregman divergences  provides a framework for many mirror-descent style algorithms related  to submodular function optimization. We also show that a  generalization of the k-means algorithm using the \lovasz{} Bregman divergence is natural in clustering scenarios where  the ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely efficient unlike the other order based distance measures. \extendedv{Finally we provide a    clustering framework for the submodular Bregman, and we derive    fast algorithms for clustering sets of binary vectors    (equivalently sets of sets)."
Submodular Bregman Divergences with Applications,"We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular Bregman divergences. We  consider two kinds,  defined either from tight modular upper or tight modular lower  bounds of a submodular function. We show that the properties of  these divergences are analogous to the (standard continuous) Bregman  divergence. Further, we demonstrate how they generalize many useful  divergences, including the weighted Hamming distance, squared  weighted Hamming, weighted precision, recall, conditional mutual  information, and a generalized KL-divergence on sets. We also show  that the lower bound submodular Bregman is actually a special case  of the generalized Bregman divergence on the \lovasz{} extension of  a submodular function which we call the \lovasz{} Bregman divergence. We then  point out a number of applications of the submodular Bregman  divergences, and in particular show that a proximal  algorithm defined through the submodular Bregman divergences  provides a framework for many mirror-descent style algorithms related  to submodular function optimization. We also show that a  generalization of the k-means algorithm using the \lovasz{} Bregman divergence is natural in clustering scenarios where  the ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely efficient unlike the other order based distance measures. \extendedv{Finally we provide a    clustering framework for the submodular Bregman, and we derive    fast algorithms for clustering sets of binary vectors    (equivalently sets of sets)."
Recklessly Approximate Sparse Coding ,"Introduction of the so called K-means features caused significant discussion in the deep learning community. Despite their simplicity, these features have achieved state of the art performance on several benchmark image classification tasks, beating out many more sophisticated learning methods. In this paper we demonstrate that a variant of these features arises as a one-step approximation to non-negative sparse coding with a fixed dictionary. This result connects these features to a broader theoretical framework and provides an explanation for their success."
Recklessly Approximate Sparse Coding ,"Introduction of the so called K-means features caused significant discussion in the deep learning community. Despite their simplicity, these features have achieved state of the art performance on several benchmark image classification tasks, beating out many more sophisticated learning methods. In this paper we demonstrate that a variant of these features arises as a one-step approximation to non-negative sparse coding with a fixed dictionary. This result connects these features to a broader theoretical framework and provides an explanation for their success."
The Dual Regularization Path of Lasso,"The Lasso regularization path has been extensively studied in the literature. It is now well understood that the path is well defined, unique and continuous piecewise linear under certain conditions. However, when the covariates in the active set (covaraites with greatest absolute correlations with the residual) are dependent, the regularization path is not unique and existing path following algorithms such as LARS may break down. In this paper, we systematically analyze the dual regularization path of Lasso, i.e., the path of the dual optimal solution. In contrast to the primal regularization path, the dual regularization path is well defined, unique and continuous piecewise linear without the independence assumption. We then propose how to compute the sequence of breakpoints of the dual regularization path with or without dependent covariates in the active set. Under the independence assumption, when the sign restriction is violated, the covariate which violates the sign restriction should be dropped and a new direction of the current line segment is updated accordingly. However, when the independence assumption fails, we show it is not necessary to drop the covariate which violates the sign restriction and change the direction of the current line segment, overcoming one of the major issues in deriving the primal regularization Lasso path. We systematically study different possibilities and show how to find the correct solution. Once we get the dual regularization path, we obtain the primal regularization path by the KKT conditions. Our simulation studies validate the correctness of the path generated by the proposed algorithm."
Keyword-supervised topic models,"Supervised topic models are able to use document labels to find topics that are predictive of the labels. However, these models only predict labels through indirect topic allocations and do not account for the fact that different words can have different degrees of effect on the label of a document. In this paper, we present the keyword-supervised latent Dirichlet allocation (ksLDA) model as a supervised model that allows different words to have a more direct effect on the model of a document's label and for the effect of the words to be perturbed by their context. In this paper, we show that this new model performs better than supervised latent Dirichlet allocation (sLDA) on real-world classification and regression tasks and on limited-size training sets."
On the Difficulty of Learning Power Law Graphical Models,"A power-law graph is any graph $G=(V,E)$, whose degree distribution follows a power law \emph{i.e.} the number of vertices in the graph with degree $i, \;y_i$, is proportional to $i^{-\beta}$ : $y_i\propto i^{-\beta}$. In this paper, we provide information-theoretic lower bounds on the sample complexity of learning such power-law graphical models \emph{i.e.} graphical models whose Markov graph obeys the power law. In addition, we briefly revisit some existing state of the art estimators, and explicitly derive their sample complexity for power-law graphs."
On the Difficulty of Learning Power Law Graphical Models,"A power-law graph is any graph $G=(V,E)$, whose degree distribution follows a power law \emph{i.e.} the number of vertices in the graph with degree $i, \;y_i$, is proportional to $i^{-\beta}$ : $y_i\propto i^{-\beta}$. In this paper, we provide information-theoretic lower bounds on the sample complexity of learning such power-law graphical models \emph{i.e.} graphical models whose Markov graph obeys the power law. In addition, we briefly revisit some existing state of the art estimators, and explicitly derive their sample complexity for power-law graphs."
24 Parallel Codes for Sparse PCA,"Given a multivariate data set, sparse principal component analysis aims to extract several linear combinations of the variables which together explain the variance in the data as much as possible, while controlling the number of nonzero loadings in these combinations. In this paper we consider 8 different optimization formulations for computing a single sparse loading vector; these are obtained by combining the following factors: we employ two norms for measuring  variance (L2, L1) and two sparsity-inducing norms (L0, L1), which are used in two different ways (constraint, penalty). Three of our formulations, notably the one with L0 constraint and L1 variance, have not been considered in the literature. We give a unifying reformulation which we propose to solve  via a natural alternating maximization method. Besides this, we provide one serial (single-core) and three parallel (multi-core, GPU, cluster) codes for each of the 8 problems. Parallelism in the methods is aimed at i) speeding up computations (our GPU code can be 100 times faster than an efficient serial code written in C), ii) obtaining solutions explaining more variance and iii) dealing with large-scale problems (our cluster code is able to solve a 357 GB problem in about a minute)."
The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification," Linear models are often much faster to learn and test than non-linear models. To enable non-linear (with  respect to the original feature space) learning with efficient linear learning algorithms, explicit  embeddings that approximate popular kernels have recently been proposed. However, the kernels are usually  designed so that their dot product in the high dimensional space is efficient, while we want the embedding  itself to be efficient (and rich enough). We propose a simple and effective pairwise piecewise-linear  embedding to approximate models under a factorization-like assumption. The method is based on discretization  and interpolation of individual features values and feature pairs.  The discretization allows us to model  different regimes of the feature space separately, while the interpolation preseves the original continuous  values.  Pairs allows us to approximate cross-feature relationships. Using this embedding within an SVM  strictly generalizes linear SVM. Additionally, some cross-features relations such as feature similarity can  be modeled exactly, while other cross-feature relations are approximated.  We conducted an extensive  experimental study and show results for a large number of datasets. We compared our method to linear,  polynomial, $\chi^2$-like and RBF kernels and embeddings. Our method consistently achieves good performance  significantly outperforming all other methods, including the RBF kernel on the majority of the  datasets. This is in contrast to other proposed embeddings that were faster than kernel methods, but with  lower accuracy. Additionally, our method is as efficient as the second polynomial explicit feature map. The  code will be made available if the paper is accepted."
The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification," Linear models are often much faster to learn and test than non-linear models. To enable non-linear (with  respect to the original feature space) learning with efficient linear learning algorithms, explicit  embeddings that approximate popular kernels have recently been proposed. However, the kernels are usually  designed so that their dot product in the high dimensional space is efficient, while we want the embedding  itself to be efficient (and rich enough). We propose a simple and effective pairwise piecewise-linear  embedding to approximate models under a factorization-like assumption. The method is based on discretization  and interpolation of individual features values and feature pairs.  The discretization allows us to model  different regimes of the feature space separately, while the interpolation preseves the original continuous  values.  Pairs allows us to approximate cross-feature relationships. Using this embedding within an SVM  strictly generalizes linear SVM. Additionally, some cross-features relations such as feature similarity can  be modeled exactly, while other cross-feature relations are approximated.  We conducted an extensive  experimental study and show results for a large number of datasets. We compared our method to linear,  polynomial, $\chi^2$-like and RBF kernels and embeddings. Our method consistently achieves good performance  significantly outperforming all other methods, including the RBF kernel on the majority of the  datasets. This is in contrast to other proposed embeddings that were faster than kernel methods, but with  lower accuracy. Additionally, our method is as efficient as the second polynomial explicit feature map. The  code will be made available if the paper is accepted."
Calibration in Cost-sensitive Multiclass Classification: an Application to Reinforcement Learning,"In this paper we propose a computationally efficient version of classification based policy iteration. The key idea of these algorithms is to view the problem of coming up with the next policy in policy iteration as a classification problem, where a policy is viewed as a classifier.The main novelty is that we propose to replace the non-convex optimization problem of earlier algorithms with a convex one, where a new cost-sensitive surrogate loss is optimized in each iteration.The new loss is shown to be classification calibrated, which makes it a ``sound'' surrogate loss. As far as we know, this is the first calibration result in the context of multiclass classification. As a result, we are able to extend  theoretical guarantees that existed for the previous inefficient classification-based policy algorithms to our efficient method, thereby giving the first computationally efficient, theoretically sound version of classification-based policy iteration."
Calibration in Cost-sensitive Multiclass Classification: an Application to Reinforcement Learning,"In this paper we propose a computationally efficient version of classification based policy iteration. The key idea of these algorithms is to view the problem of coming up with the next policy in policy iteration as a classification problem, where a policy is viewed as a classifier.The main novelty is that we propose to replace the non-convex optimization problem of earlier algorithms with a convex one, where a new cost-sensitive surrogate loss is optimized in each iteration.The new loss is shown to be classification calibrated, which makes it a ``sound'' surrogate loss. As far as we know, this is the first calibration result in the context of multiclass classification. As a result, we are able to extend  theoretical guarantees that existed for the previous inefficient classification-based policy algorithms to our efficient method, thereby giving the first computationally efficient, theoretically sound version of classification-based policy iteration."
Bayesian Inference Reveals Synapse-Specific Short-Term Plasticity in Neocortical Microcircuits,"Short-term synaptic plasticity is highly diverse and varies according to brain area, cortical layer, and developmental stage. Since this form of plasticity shapes neural dynamics, its diversity suggests a specific and essential role in neural information processing. Therefore, a correct identification of short-term plasticity is an important step towards understanding and modeling neural systems. Although accurate phenomenological models have been developed, they are usually fitted to experimental data using least-mean square methods. We demonstrate that, for typical synaptic dynamics, such fitting gives unreliable results. Instead, we introduce a Bayesian approach based on a Markov Chain Monte Carlo method, which provides the full posterior distribution over the parameters of the model. We test the approach on simulated data over different regimes and show that common short-term plasticity protocols yield broad distributions over some of the parameters. Finally, we infer the model parameters using experimental data from three different neocortical excitatory connection types, revealing novel synapse-specific distributions and synaptic transfer functions, while the approach yields more robust clustering results. We conclude that ? because short-term plasticity presumably provides key computational features ? our approach to demarcate synapse-specific synaptic dynamics is an important improvement on the state of the art."
"Causality Analysis in Time Series: Foundations, Consistency and New Development","Granger causality is the primary technique of causality analysis for time series data. While it has been well studied in the literature for two time series, its performance for multivariate time series in the presence of hidden variables as well as its connections to true causality has always been debated. In this work, we reexamine the theoretical foundations of Granger causality and strive to provide insights into three fundamental questions on Granger causality for time series data. Specifically, we resort to the Structural Equation Modeling (SEM),  a widely accepted framework for true causality analysis, and statistical consistency analysis, which reveals consistency behavior of statistical methods to uncover Granger causality, in order to answer the following questions: (1) what are the advantages of Granger causality in avoiding spurious causation;  (2) what are the consistency properties of two popular approaches to uncover Granger causality, including significance test and L1-penalized regression; (3) what are the consistency properties of advanced algorithms for nonlinear dependencies, i.e.,  semi-parametric approach, to uncover Granger causality for high-dimensional time series. Experiment results on synthetic datasets and social media application data are shown to support our theoretical analysis."
"Causality Analysis in Time Series: Foundations, Consistency and New Development","Granger causality is the primary technique of causality analysis for time series data. While it has been well studied in the literature for two time series, its performance for multivariate time series in the presence of hidden variables as well as its connections to true causality has always been debated. In this work, we reexamine the theoretical foundations of Granger causality and strive to provide insights into three fundamental questions on Granger causality for time series data. Specifically, we resort to the Structural Equation Modeling (SEM),  a widely accepted framework for true causality analysis, and statistical consistency analysis, which reveals consistency behavior of statistical methods to uncover Granger causality, in order to answer the following questions: (1) what are the advantages of Granger causality in avoiding spurious causation;  (2) what are the consistency properties of two popular approaches to uncover Granger causality, including significance test and L1-penalized regression; (3) what are the consistency properties of advanced algorithms for nonlinear dependencies, i.e.,  semi-parametric approach, to uncover Granger causality for high-dimensional time series. Experiment results on synthetic datasets and social media application data are shown to support our theoretical analysis."
Robust Crowd Labeling using Little Expertise,"Crowd Labeling emerged from the need to label large-scale and complex data, atedious, expensive and time-consuming task. Each object to label is generally annotatedby multiple crowd labelers, and the collected labels are combined to inferone final estimated label. One open problem is the quality and integration of differentlabels, especially when the labelers participating to the task are of unknownexpertise. In order to address this challenge, we propose a new framework thatautomatically combines and boosts bulk crowd labels with a limited number ofground truth labels from experts. We show through extensive experiments that,unlike other state-of-the-art approaches, our method is robust to estimate true labelseven with the presence of a large proportion of not-so-good labelers in thecrowd."
Robust Crowd Labeling using Little Expertise,"Crowd Labeling emerged from the need to label large-scale and complex data, atedious, expensive and time-consuming task. Each object to label is generally annotatedby multiple crowd labelers, and the collected labels are combined to inferone final estimated label. One open problem is the quality and integration of differentlabels, especially when the labelers participating to the task are of unknownexpertise. In order to address this challenge, we propose a new framework thatautomatically combines and boosts bulk crowd labels with a limited number ofground truth labels from experts. We show through extensive experiments that,unlike other state-of-the-art approaches, our method is robust to estimate true labelseven with the presence of a large proportion of not-so-good labelers in thecrowd."
Online Learning of Hierarchical Balancing Strategies for Bipedal Humanoid Robots,"  Bipedal humanoid robots will fall under unforeseen perturbations without active stabilization.  Humans use dynamic full body behaviors in response to perturbations, and recent bipedal robot controllers for balancing are based upon human biomechanical responses.  These controllers assume simple physical models and require very accurate state information, making them less effective on physical robots in uncertain environments.  To address this issue, we propose a hierarchical control architecture that learns to switch between three low-level biomechanically-motivated strategies in response to perturbations.  The high level strategy is learned in an online fashion from state trajectory information gathered during experimental trials.  This learning approach is evaluated in physics-based simulations as well as on a small humanoid robot. Our results demonstrate how well this method stabilizes the robot during walking and whole body manipulation tasks."
Online Learning of Hierarchical Balancing Strategies for Bipedal Humanoid Robots,"  Bipedal humanoid robots will fall under unforeseen perturbations without active stabilization.  Humans use dynamic full body behaviors in response to perturbations, and recent bipedal robot controllers for balancing are based upon human biomechanical responses.  These controllers assume simple physical models and require very accurate state information, making them less effective on physical robots in uncertain environments.  To address this issue, we propose a hierarchical control architecture that learns to switch between three low-level biomechanically-motivated strategies in response to perturbations.  The high level strategy is learned in an online fashion from state trajectory information gathered during experimental trials.  This learning approach is evaluated in physics-based simulations as well as on a small humanoid robot. Our results demonstrate how well this method stabilizes the robot during walking and whole body manipulation tasks."
Hierarchical spike coding of sound,"We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task."
Hierarchical spike coding of sound,"We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task."
Hierarchical spike coding of sound,"We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task."
Coarse-to-fine video segmentation using supervoxel trees,"Image and video segmentation is a task of immense importance in the field of computer vision. Existing algorithms like graph cuts solve this problem by considering the possibility of every adjoining pixel (superpixel) or voxel (supervoxel) getting different labels. However, real images tend to have spatial continuity and videos have additional temporal continuity. In this paper, we consider a hierarchical tree of supervoxels.We propose a coarse-to-fine video segmentation scheme whereby larger supervoxels belonging to the same label, need not be refined into finer supervoxels. For videos with significant spatio-temporal continuity, such a scheme can lead to significant computational savings. By using admissible heuristic estimates of the unary and binary potentials, we can show that this scheme leads to the exact segmentation that would have been obtained by considering the finest layer of supervoxels."
Coarse-to-fine video segmentation using supervoxel trees,"Image and video segmentation is a task of immense importance in the field of computer vision. Existing algorithms like graph cuts solve this problem by considering the possibility of every adjoining pixel (superpixel) or voxel (supervoxel) getting different labels. However, real images tend to have spatial continuity and videos have additional temporal continuity. In this paper, we consider a hierarchical tree of supervoxels.We propose a coarse-to-fine video segmentation scheme whereby larger supervoxels belonging to the same label, need not be refined into finer supervoxels. For videos with significant spatio-temporal continuity, such a scheme can lead to significant computational savings. By using admissible heuristic estimates of the unary and binary potentials, we can show that this scheme leads to the exact segmentation that would have been obtained by considering the finest layer of supervoxels."
Local Support Vector Machines: Formulation and Analysis,"We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs."
Local Support Vector Machines: Formulation and Analysis,"We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs."
No More Pesky Learning Rates,"The performance of stochastic gradient descent (SGD) dependscritically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations accross samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained throughsystematic search, and effectively removes the need for learning rate tuning. "
Cost-Sensitive Exploration in Bayesian Reinforcement Learning,"In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems."
Learning with Partially Absorbing Random Walks,"We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$, and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\mathcal{S}$, while dropping sharply outside $\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning."
Learning with Partially Absorbing Random Walks,"We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$, and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\mathcal{S}$, while dropping sharply outside $\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning."
Learning with Partially Absorbing Random Walks,"We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$, and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\mathcal{S}$, while dropping sharply outside $\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning."
Learning with Partially Absorbing Random Walks,"We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$, and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\mathcal{S}$, while dropping sharply outside $\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning."
Efficient MAP Inference in Binary Pairwise MRFs,"Markov random fields (MRFs) have broad application in many fields including computer vision. In general, however, finding the most likely (MAP) configuration of variables is NP-hard.  If we restrict attention to the class of associative (submodular) models then the task is tractable but further speed improvement would be welcome. Simplifications are revealed by reducing the MAP inference problem to maximum weight stable set (MWSS) on a compiled nand Markov random field (NMRF). This yields two results for general binary pairwise MRFs: (1) A rapid pre-processing algorithm that identifies an exact MAP configuration of a subset of the variables; the subset is larger for sparse MRFs with low associativity and random settings;local sensitivity parameters are also returned for every variable; and (2) We derive necessary and sufficient conditions for when such an MRF maps to a perfect NMRF, which guarantees efficient MAP inference."
Efficient MAP Inference in Binary Pairwise MRFs,"Markov random fields (MRFs) have broad application in many fields including computer vision. In general, however, finding the most likely (MAP) configuration of variables is NP-hard.  If we restrict attention to the class of associative (submodular) models then the task is tractable but further speed improvement would be welcome. Simplifications are revealed by reducing the MAP inference problem to maximum weight stable set (MWSS) on a compiled nand Markov random field (NMRF). This yields two results for general binary pairwise MRFs: (1) A rapid pre-processing algorithm that identifies an exact MAP configuration of a subset of the variables; the subset is larger for sparse MRFs with low associativity and random settings;local sensitivity parameters are also returned for every variable; and (2) We derive necessary and sufficient conditions for when such an MRF maps to a perfect NMRF, which guarantees efficient MAP inference."
Optimal integration of visual speed information across different spatiotemporal frequency channels,"How does the human visual system compute the speed of a coherent motion stimulus that contains motion energy in different spatiotemporal frequency bands? Here we argue that perceived speed is the result of optimal integration over different spatiotemporal frequency channels. We formalize this hypothesis with a Bayesian observer model that treats the channel responses as independent cues, and then optimally combines them together with a prior for slow speeds. We test the model against behavioral data from a 2AFC speed discrimination task with which we measured subjects' perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies, and of various combinations of these single gratings. We find that perceived speed of the combined stimuli is independent of the relative phase of the underlying grating components, and that the perceptual biases and discrimination thresholds are always smaller for the combined stimuli, supporting the cue combination hypothesis. The proposed Bayesian model fits the data well, accounting for perceptual biases and thresholds of both simple and combined stimuli. Fits are improved if we assume that the channel responses are subject to divisive normalization, which is in line with physiological evidence.  Our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for stimuli of arbitrary spatial structure.  "
Optimal integration of visual speed information across different spatiotemporal frequency channels,"How does the human visual system compute the speed of a coherent motion stimulus that contains motion energy in different spatiotemporal frequency bands? Here we argue that perceived speed is the result of optimal integration over different spatiotemporal frequency channels. We formalize this hypothesis with a Bayesian observer model that treats the channel responses as independent cues, and then optimally combines them together with a prior for slow speeds. We test the model against behavioral data from a 2AFC speed discrimination task with which we measured subjects' perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies, and of various combinations of these single gratings. We find that perceived speed of the combined stimuli is independent of the relative phase of the underlying grating components, and that the perceptual biases and discrimination thresholds are always smaller for the combined stimuli, supporting the cue combination hypothesis. The proposed Bayesian model fits the data well, accounting for perceptual biases and thresholds of both simple and combined stimuli. Fits are improved if we assume that the channel responses are subject to divisive normalization, which is in line with physiological evidence.  Our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for stimuli of arbitrary spatial structure.  "
Efficient and direct estimation of a neural subunit model for sensory coding,"Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a `subunit' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (``convolutional??) copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the `subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency."
Spectral Learning of Latent-Variable HMMs ,We derive a spectral algorithm for learning the parameters of a latent-variable HMM. This method avoids the problem of local optima and provides a consistent estimate of the parameters. We demonstrate the method on a phoneme recognition task and show that it performs competitively with EM. 
Spectral Learning of Latent-Variable HMMs ,We derive a spectral algorithm for learning the parameters of a latent-variable HMM. This method avoids the problem of local optima and provides a consistent estimate of the parameters. We demonstrate the method on a phoneme recognition task and show that it performs competitively with EM. 
One Permutation Hashing,"While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.In this paper, we develop a simple \textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing $k$ permutations to just one  would be much more \textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \& logistic regression also confirm the theoretical results."
A template model for fine-grained object recognition,"Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms. "
A template model for fine-grained object recognition,"Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms. "
A template model for fine-grained object recognition,"Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms. "
Large-Scale Sparse PCA through Low-rank Approximations,We introduce a novel scheme for sparse PCA that has provable approximation guarantees.We first introduce an algorithm that can exactly solve sparse PCA for matrices of constant rank in polynomial time. Given a full-rank frequency matrix we obtain a constant rank matrix approximation and subsequently execute the exact sparse PCA solver on this low-rank approximation. We surprisingly see that this low-rank approximation step introduces very small errors. We theoretically explain this behavior by showing that data sets with few high-degree words must have data matrices that are close to low-rank. Our formalism allows us to show that our sparse PCA algorithm is asymptotically tight. Experimentally we evaluate our algorithm in a very large Twitter data set. A feature elimination step allows us to perform sparse PCA in millions of Tweets in a few minutes. Our scheme typically captures $80-90\%$ of the data variance by using rank-3 or rank-4 approximations. 
Is Matching Pursuit Solving Convex Problems?,"Matching pursuit  (\texttt{MP}) algorithms have been successfullyapplied in signal processing and pattern recognition areas. However,as far as we know, it is still not clear whether any \texttt{MP}algorithm can solve a convex problem or not. In this paper, a novelconvex relaxation is proposed for a class of matching pursuitalgorithms, which includes the classical orthogonal matching pursuit(\texttt{OMP}) as a special case.  Based on the proposed scheme, ageneral matching pursuit (\texttt{GMP}) algorithm can be naturallyobtained. As it solves a convex problem, \texttt{GMP}  guarantees toconverge globally. In addition, a subspace exploratory search canfurther improve the performance. Finally, we show that \texttt{GMP}with an $\ell_1$ regularization term can recover the $k$-sparsesignals if the restricted isometry constant $\sigma_k\leq 0.307-\nu$,where $\nu$ can be arbitrarily close to 0. The proposed method can beeasily parallelized and the efficiency can be further improved.Simulations on an 8-core machine show that the proposed method cansuccessfully decode the problems of scale $2^{13} \times 2^{17}$within 10 seconds and scale $2^{10} \times 2^{20}$ within 2 seconds."
Is Matching Pursuit Solving Convex Problems?,"Matching pursuit  (\texttt{MP}) algorithms have been successfullyapplied in signal processing and pattern recognition areas. However,as far as we know, it is still not clear whether any \texttt{MP}algorithm can solve a convex problem or not. In this paper, a novelconvex relaxation is proposed for a class of matching pursuitalgorithms, which includes the classical orthogonal matching pursuit(\texttt{OMP}) as a special case.  Based on the proposed scheme, ageneral matching pursuit (\texttt{GMP}) algorithm can be naturallyobtained. As it solves a convex problem, \texttt{GMP}  guarantees toconverge globally. In addition, a subspace exploratory search canfurther improve the performance. Finally, we show that \texttt{GMP}with an $\ell_1$ regularization term can recover the $k$-sparsesignals if the restricted isometry constant $\sigma_k\leq 0.307-\nu$,where $\nu$ can be arbitrarily close to 0. The proposed method can beeasily parallelized and the efficiency can be further improved.Simulations on an 8-core machine show that the proposed method cansuccessfully decode the problems of scale $2^{13} \times 2^{17}$within 10 seconds and scale $2^{10} \times 2^{20}$ within 2 seconds."
Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression,"We present a new variational inference algorithm for Gaussianprocesses with non-conjugate likelihood functions.This includes binary and multi-class classification, as well asordinal regression.Our method constructs a convex lower bound, which can be optimizedby using an efficient fixed point update method.We then show empirically that our new approach is much faster than existing methods without any degradation in performance."
Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression,"We present a new variational inference algorithm for Gaussianprocesses with non-conjugate likelihood functions.This includes binary and multi-class classification, as well asordinal regression.Our method constructs a convex lower bound, which can be optimizedby using an efficient fixed point update method.We then show empirically that our new approach is much faster than existing methods without any degradation in performance."
Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression,"We present a new variational inference algorithm for Gaussianprocesses with non-conjugate likelihood functions.This includes binary and multi-class classification, as well asordinal regression.Our method constructs a convex lower bound, which can be optimizedby using an efficient fixed point update method.We then show empirically that our new approach is much faster than existing methods without any degradation in performance."
Entropy Estimations Using Correlated Symmetric Stable Random Projections,"Methods for efficiently estimating  the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of {\em Compressed Counting (CC)}~\cite{Proc:Li_Zhang_COLT11}  based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two \textbf{correlated} frequency moments estimated from  correlated samples of \textbf{symmetric} stable random variables. Interestingly, the  estimator for the moment we recommend for entropy estimation  barely has bounded variance itself, whereas the  geometric mean estimator (which has bounded higher-order moments) is not sufficient for entropy estimation. Our experiments confirm that this method is able to  well approximate the Shannon entropy using small storage (e.g., $100\sim 1000$ samples). A prior study~\cite{Proc:Zhao_IMC07}  approximated the Shannon entropy using symmetric stable random projections with (e.g.,) $10^5\sim1.6\times10^6$ \textbf{independent} samples."
Learning Causal Relationships From Multivariate Time Series,"This paper considers a natural and widely prevalent setting where a collection of time series evolve in a causal manner, and one is interested in inferring the graph governing the causality between the variables therein. We consider this problem in the special case where variables are discrete and updates are Markov. We develop a new algorithm to learn causal graph structure based on the notion of {\em directed information}, and analytically and empirically demonstrate its performance. Our algorithm is an adaptation of a greedy heuristic for learning undirected graphical models, but with modifications to leverage causality. Analytically, the challenge lies in establishing sample complexity, because the samples are dependent."
Learning Causal Relationships From Multivariate Time Series,"This paper considers a natural and widely prevalent setting where a collection of time series evolve in a causal manner, and one is interested in inferring the graph governing the causality between the variables therein. We consider this problem in the special case where variables are discrete and updates are Markov. We develop a new algorithm to learn causal graph structure based on the notion of {\em directed information}, and analytically and empirically demonstrate its performance. Our algorithm is an adaptation of a greedy heuristic for learning undirected graphical models, but with modifications to leverage causality. Analytically, the challenge lies in establishing sample complexity, because the samples are dependent."
Learning Causal Relationships From Multivariate Time Series,"This paper considers a natural and widely prevalent setting where a collection of time series evolve in a causal manner, and one is interested in inferring the graph governing the causality between the variables therein. We consider this problem in the special case where variables are discrete and updates are Markov. We develop a new algorithm to learn causal graph structure based on the notion of {\em directed information}, and analytically and empirically demonstrate its performance. Our algorithm is an adaptation of a greedy heuristic for learning undirected graphical models, but with modifications to leverage causality. Analytically, the challenge lies in establishing sample complexity, because the samples are dependent."
Learning Causal Relationships From Multivariate Time Series,"This paper considers a natural and widely prevalent setting where a collection of time series evolve in a causal manner, and one is interested in inferring the graph governing the causality between the variables therein. We consider this problem in the special case where variables are discrete and updates are Markov. We develop a new algorithm to learn causal graph structure based on the notion of {\em directed information}, and analytically and empirically demonstrate its performance. Our algorithm is an adaptation of a greedy heuristic for learning undirected graphical models, but with modifications to leverage causality. Analytically, the challenge lies in establishing sample complexity, because the samples are dependent."
A Minimum Frame Error Criterion for Hidden Markov Model Training,"Abstract Hidden Markov models (HMM) have been widely studied and applied over decades. The standard supervised learning method for HMM is maximum likelihood estimation (MLE) which maximizes the joint probability of training data. However, the most natural way of training would be finding the parameters that directly minimize the error rate of a given training set. In this article, we propose a novel learning method that minimizes the number of incorrectly decoded labels frame-wise. To do this, we construct a smooth function that is arbitrarily close to the exact frame error rate and minimize it directly using a gradient-based optimization algorithm. The proposed approach is intuitive and simple. We applied our method to the task of chord recognition in music, and the results show that it performs better than Maximum Likelihood Estimation and Minimum Classification Error. "
A Minimum Frame Error Criterion for Hidden Markov Model Training,"Abstract Hidden Markov models (HMM) have been widely studied and applied over decades. The standard supervised learning method for HMM is maximum likelihood estimation (MLE) which maximizes the joint probability of training data. However, the most natural way of training would be finding the parameters that directly minimize the error rate of a given training set. In this article, we propose a novel learning method that minimizes the number of incorrectly decoded labels frame-wise. To do this, we construct a smooth function that is arbitrarily close to the exact frame error rate and minimize it directly using a gradient-based optimization algorithm. The proposed approach is intuitive and simple. We applied our method to the task of chord recognition in music, and the results show that it performs better than Maximum Likelihood Estimation and Minimum Classification Error. "
A Minimum Frame Error Criterion for Hidden Markov Model Training,"Abstract Hidden Markov models (HMM) have been widely studied and applied over decades. The standard supervised learning method for HMM is maximum likelihood estimation (MLE) which maximizes the joint probability of training data. However, the most natural way of training would be finding the parameters that directly minimize the error rate of a given training set. In this article, we propose a novel learning method that minimizes the number of incorrectly decoded labels frame-wise. To do this, we construct a smooth function that is arbitrarily close to the exact frame error rate and minimize it directly using a gradient-based optimization algorithm. The proposed approach is intuitive and simple. We applied our method to the task of chord recognition in music, and the results show that it performs better than Maximum Likelihood Estimation and Minimum Classification Error. "
Herded Gibbs Sampling,"The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm that is entirely deterministic. We demonstrate with simple examples that herded Gibbs exhibits better convergence behavior for approximating the marginal distributions than Gibbs sampling. We use an image denoising example to demonstrate the effectiveness of herded Gibbs as an inference technique for Markov Random Field models. We also adopt herded Gibbs as the inference engine for Conditional Random Fields in Named Entity Recognition and show that it is competitive with the state of the art. The conclusion is that herded Gibbs, for graphical models with nodes of low degree, is very close to Gibbs sampling in terms of the complexity of the code and computation, but that it converges much faster. "
Herded Gibbs Sampling,"The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm that is entirely deterministic. We demonstrate with simple examples that herded Gibbs exhibits better convergence behavior for approximating the marginal distributions than Gibbs sampling. We use an image denoising example to demonstrate the effectiveness of herded Gibbs as an inference technique for Markov Random Field models. We also adopt herded Gibbs as the inference engine for Conditional Random Fields in Named Entity Recognition and show that it is competitive with the state of the art. The conclusion is that herded Gibbs, for graphical models with nodes of low degree, is very close to Gibbs sampling in terms of the complexity of the code and computation, but that it converges much faster. "
Herded Gibbs Sampling,"The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm that is entirely deterministic. We demonstrate with simple examples that herded Gibbs exhibits better convergence behavior for approximating the marginal distributions than Gibbs sampling. We use an image denoising example to demonstrate the effectiveness of herded Gibbs as an inference technique for Markov Random Field models. We also adopt herded Gibbs as the inference engine for Conditional Random Fields in Named Entity Recognition and show that it is competitive with the state of the art. The conclusion is that herded Gibbs, for graphical models with nodes of low degree, is very close to Gibbs sampling in terms of the complexity of the code and computation, but that it converges much faster. "
Continuous Relaxations for Discrete Hamiltonian Monte Carlo,"Continuous relaxations play an important role in discreteoptimization, but have not seen much use in approximate probabilisticinference. Here we show that a general form of the GaussianIntegral Trick makes it possible to transform a wide class ofdiscrete variable undirected models into fully continuous systems. Thecontinuous representation allows the use of gradient-based HamiltonianMonte Carlo for inference,  results in new ways of estimatingnormalization constants (partition functions), and in general opens upa number of new avenues for inference in difficult discretesystems. We demonstrate some of these continuous relaxation inference algorithmson a number of illustrative problems."
Mechanism Design for Machine Learning Problems,"While machine learning competitions like the Netflix Prize have had relative success on their own, they pave the way to think about procedural aspects of developing predictors. We believe that applying optimal structures designed using game theoretical thinking can make the process of development of machine learning solution much more efficient. However, there are some special features thatare specific to learning scenarios. In this paper, we make the initial steps towards achieving this. In particular, we address the issue that in a prediction problem the outcome of a mechanism must depend on a quantity unknown to all parties (i.e., how well the proposed algorithms will perform). We also propose a specific auction where the developers can submit multiple proposed predictors."
Mechanism Design for Machine Learning Problems,"While machine learning competitions like the Netflix Prize have had relative success on their own, they pave the way to think about procedural aspects of developing predictors. We believe that applying optimal structures designed using game theoretical thinking can make the process of development of machine learning solution much more efficient. However, there are some special features thatare specific to learning scenarios. In this paper, we make the initial steps towards achieving this. In particular, we address the issue that in a prediction problem the outcome of a mechanism must depend on a quantity unknown to all parties (i.e., how well the proposed algorithms will perform). We also propose a specific auction where the developers can submit multiple proposed predictors."
Scaling Bayesian Optimization to High-Dimensions via Random Embedding,"Bayesian optimization is a powerful strategy for finding the extrema of objective functions. It is applicable in situations where one does not have a closed-form expression for the objective function, but where one can obtain noisy evaluations of this function at sampled values. It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex. Because of these properties, its popularity has increased in many domains, including robotics, planning, sensor placement, news recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension. Several NIPS workshops on this topic have identified the scaling of Bayesian optimization to high-dimensions as a core challenge. Despite this, little progress has been made in this direction. In this paper, we introduce random projection tools to scale Bayesian optimization to higher dimensions. Our proposed techniques enable us to treat continuous and categorical choices simultaneously. They perform well on several challenging domains, including a synthetic example of low intrinsic dimensionality but embedded in a million dimensions, automatic configuration of a practical linear programming solver and automatic configuration of a random forests classifier for the Kinect sensor."
Scaling Bayesian Optimization to High-Dimensions via Random Embedding,"Bayesian optimization is a powerful strategy for finding the extrema of objective functions. It is applicable in situations where one does not have a closed-form expression for the objective function, but where one can obtain noisy evaluations of this function at sampled values. It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex. Because of these properties, its popularity has increased in many domains, including robotics, planning, sensor placement, news recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension. Several NIPS workshops on this topic have identified the scaling of Bayesian optimization to high-dimensions as a core challenge. Despite this, little progress has been made in this direction. In this paper, we introduce random projection tools to scale Bayesian optimization to higher dimensions. Our proposed techniques enable us to treat continuous and categorical choices simultaneously. They perform well on several challenging domains, including a synthetic example of low intrinsic dimensionality but embedded in a million dimensions, automatic configuration of a practical linear programming solver and automatic configuration of a random forests classifier for the Kinect sensor."
Scaling Bayesian Optimization to High-Dimensions via Random Embedding,"Bayesian optimization is a powerful strategy for finding the extrema of objective functions. It is applicable in situations where one does not have a closed-form expression for the objective function, but where one can obtain noisy evaluations of this function at sampled values. It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex. Because of these properties, its popularity has increased in many domains, including robotics, planning, sensor placement, news recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension. Several NIPS workshops on this topic have identified the scaling of Bayesian optimization to high-dimensions as a core challenge. Despite this, little progress has been made in this direction. In this paper, we introduce random projection tools to scale Bayesian optimization to higher dimensions. Our proposed techniques enable us to treat continuous and categorical choices simultaneously. They perform well on several challenging domains, including a synthetic example of low intrinsic dimensionality but embedded in a million dimensions, automatic configuration of a practical linear programming solver and automatic configuration of a random forests classifier for the Kinect sensor."
Scaling Bayesian Optimization to High-Dimensions via Random Embedding,"Bayesian optimization is a powerful strategy for finding the extrema of objective functions. It is applicable in situations where one does not have a closed-form expression for the objective function, but where one can obtain noisy evaluations of this function at sampled values. It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex. Because of these properties, its popularity has increased in many domains, including robotics, planning, sensor placement, news recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension. Several NIPS workshops on this topic have identified the scaling of Bayesian optimization to high-dimensions as a core challenge. Despite this, little progress has been made in this direction. In this paper, we introduce random projection tools to scale Bayesian optimization to higher dimensions. Our proposed techniques enable us to treat continuous and categorical choices simultaneously. They perform well on several challenging domains, including a synthetic example of low intrinsic dimensionality but embedded in a million dimensions, automatic configuration of a practical linear programming solver and automatic configuration of a random forests classifier for the Kinect sensor."
Scaling Bayesian Optimization to High-Dimensions via Random Embedding,"Bayesian optimization is a powerful strategy for finding the extrema of objective functions. It is applicable in situations where one does not have a closed-form expression for the objective function, but where one can obtain noisy evaluations of this function at sampled values. It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex. Because of these properties, its popularity has increased in many domains, including robotics, planning, sensor placement, news recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension. Several NIPS workshops on this topic have identified the scaling of Bayesian optimization to high-dimensions as a core challenge. Despite this, little progress has been made in this direction. In this paper, we introduce random projection tools to scale Bayesian optimization to higher dimensions. Our proposed techniques enable us to treat continuous and categorical choices simultaneously. They perform well on several challenging domains, including a synthetic example of low intrinsic dimensionality but embedded in a million dimensions, automatic configuration of a practical linear programming solver and automatic configuration of a random forests classifier for the Kinect sensor."
On Finite Alphabet Compressive Sensing,"This paper considers the problem of compressive sensing over a finite alphabet, where the finite alphabet may be inherent to the data or a result of quantization. There are multiple examples of finite alphabet static as well as time-series data with inherent sparse structure; and quantizing real values is an essential step while handling real data in practice. This paper shows that there are significant benefits to analyzing the problem while incorporating its finite alphabet nature, versus ignoring this and employing a conventional real-alphabet compressing sensing toolbox to the problem. Specifically, when the alphabet is finite, our techniques a. have a lower sample complexity than over reals when the sparsity is below a threshold, b. facilitate constructive designs of sensing matrices based on coding theoretic principles; c. enable one to solve the exact $\ell_0$-minimization problem directly in polynomial time rather than a approach of relaxation followed by sufficient conditions for when the relaxation matches the original problem; and finally, d. allow for smaller data storage (in bits) compared to its real counterpart."
On Finite Alphabet Compressive Sensing,"This paper considers the problem of compressive sensing over a finite alphabet, where the finite alphabet may be inherent to the data or a result of quantization. There are multiple examples of finite alphabet static as well as time-series data with inherent sparse structure; and quantizing real values is an essential step while handling real data in practice. This paper shows that there are significant benefits to analyzing the problem while incorporating its finite alphabet nature, versus ignoring this and employing a conventional real-alphabet compressing sensing toolbox to the problem. Specifically, when the alphabet is finite, our techniques a. have a lower sample complexity than over reals when the sparsity is below a threshold, b. facilitate constructive designs of sensing matrices based on coding theoretic principles; c. enable one to solve the exact $\ell_0$-minimization problem directly in polynomial time rather than a approach of relaxation followed by sufficient conditions for when the relaxation matches the original problem; and finally, d. allow for smaller data storage (in bits) compared to its real counterpart."
Conditional Likelihood Inference on Overlapping Figure-Ground Segment Hypotheses,"In this paper we present an inference procedure for the semantic segmentation of images, namely identifying the spatial layout and class labels of the objects present.Different from many CRF approaches that rely on dependencies modeled with unary and pairwisepixel or superpixel potentials, our method is entirely based on overlap estimates on diverse, potentially overlapping segments in the image. This enablesus to solve the statistical inference problem without using a random field and its associated dependencies. In the approach, posteriorsuperpixels are obtained by intersections of the overlapping regions.Then random variables are defined on such superpixels so that the overlap between each segment and the ground truth can be constructed from them. Inferenceis then performed using conditional maximum likelihood based on an EM formulation. In the PASCAL VOC challenge, the proposed approach is comparable with the 3-times winner SVRSEGM system, but in addition it successfully recognizes additional images with multiple interacting objects."
Conditional Likelihood Inference on Overlapping Figure-Ground Segment Hypotheses,"In this paper we present an inference procedure for the semantic segmentation of images, namely identifying the spatial layout and class labels of the objects present.Different from many CRF approaches that rely on dependencies modeled with unary and pairwisepixel or superpixel potentials, our method is entirely based on overlap estimates on diverse, potentially overlapping segments in the image. This enablesus to solve the statistical inference problem without using a random field and its associated dependencies. In the approach, posteriorsuperpixels are obtained by intersections of the overlapping regions.Then random variables are defined on such superpixels so that the overlap between each segment and the ground truth can be constructed from them. Inferenceis then performed using conditional maximum likelihood based on an EM formulation. In the PASCAL VOC challenge, the proposed approach is comparable with the 3-times winner SVRSEGM system, but in addition it successfully recognizes additional images with multiple interacting objects."
Accelerating model selection for classification using bandit-based methods,"This paper advances the state of the art on techniques to improve the computa-tional efficiency of cross-validation for model selection. Obtaining procedures formodel selection that are both computationally and statistically efficient is one ofthe core challenges of machine learning research. We demonstrate that significantadvances are made possible via Bayesian bandit techniques and Thompson sam-pling. In doing so, we also provide a much-needed comprehensive comparison ofthese methods."
Accelerating model selection for classification using bandit-based methods,"This paper advances the state of the art on techniques to improve the computa-tional efficiency of cross-validation for model selection. Obtaining procedures formodel selection that are both computationally and statistically efficient is one ofthe core challenges of machine learning research. We demonstrate that significantadvances are made possible via Bayesian bandit techniques and Thompson sam-pling. In doing so, we also provide a much-needed comprehensive comparison ofthese methods."
Accelerating model selection for classification using bandit-based methods,"This paper advances the state of the art on techniques to improve the computa-tional efficiency of cross-validation for model selection. Obtaining procedures formodel selection that are both computationally and statistically efficient is one ofthe core challenges of machine learning research. We demonstrate that significantadvances are made possible via Bayesian bandit techniques and Thompson sam-pling. In doing so, we also provide a much-needed comprehensive comparison ofthese methods."
Auto-WEKA: Automated Selection and Hyper-Parameter Optimization of Classification Algorithms,"There exist many different machine learning algorithms; as most of these can be configured via hyper-parameters, there is a staggeringly large number of possible alternatives overall. There has been a considerable amount of previous work on choosing among learning algorithms and, separately, on optimizing hyper-parameters (mostly when these are continuous and very few in number) in a given use context. However, we are aware of no work that addresses both problems together. Here, we demonstrate the feasibility of using a fully automated approach for choosing both a learning algorithm and its hyper-parameters, leveraging recent innovations in Bayesian optimization. Specifically, we apply this approach to the entire space of classifiers implemented in WEKA, spanning 3 ensemble methods, 14 meta-methods, 30 base classifiers, and a large range of hyper-parameter settings for each of these. On each of 10 data sets from the UCI repository, we show classification performance better than that of complete cross-validation over the default hyper-parameter settings of our 47 classification algorithms. We believe that our approach, which we dubbed Auto-WEKA, will enable typical users of machine learning algorithms to make better choices and thus to obtain better performance in a fully automated fashion."
Auto-WEKA: Automated Selection and Hyper-Parameter Optimization of Classification Algorithms,"There exist many different machine learning algorithms; as most of these can be configured via hyper-parameters, there is a staggeringly large number of possible alternatives overall. There has been a considerable amount of previous work on choosing among learning algorithms and, separately, on optimizing hyper-parameters (mostly when these are continuous and very few in number) in a given use context. However, we are aware of no work that addresses both problems together. Here, we demonstrate the feasibility of using a fully automated approach for choosing both a learning algorithm and its hyper-parameters, leveraging recent innovations in Bayesian optimization. Specifically, we apply this approach to the entire space of classifiers implemented in WEKA, spanning 3 ensemble methods, 14 meta-methods, 30 base classifiers, and a large range of hyper-parameter settings for each of these. On each of 10 data sets from the UCI repository, we show classification performance better than that of complete cross-validation over the default hyper-parameter settings of our 47 classification algorithms. We believe that our approach, which we dubbed Auto-WEKA, will enable typical users of machine learning algorithms to make better choices and thus to obtain better performance in a fully automated fashion."
Auto-WEKA: Automated Selection and Hyper-Parameter Optimization of Classification Algorithms,"There exist many different machine learning algorithms; as most of these can be configured via hyper-parameters, there is a staggeringly large number of possible alternatives overall. There has been a considerable amount of previous work on choosing among learning algorithms and, separately, on optimizing hyper-parameters (mostly when these are continuous and very few in number) in a given use context. However, we are aware of no work that addresses both problems together. Here, we demonstrate the feasibility of using a fully automated approach for choosing both a learning algorithm and its hyper-parameters, leveraging recent innovations in Bayesian optimization. Specifically, we apply this approach to the entire space of classifiers implemented in WEKA, spanning 3 ensemble methods, 14 meta-methods, 30 base classifiers, and a large range of hyper-parameter settings for each of these. On each of 10 data sets from the UCI repository, we show classification performance better than that of complete cross-validation over the default hyper-parameter settings of our 47 classification algorithms. We believe that our approach, which we dubbed Auto-WEKA, will enable typical users of machine learning algorithms to make better choices and thus to obtain better performance in a fully automated fashion."
Auto-WEKA: Automated Selection and Hyper-Parameter Optimization of Classification Algorithms,"There exist many different machine learning algorithms; as most of these can be configured via hyper-parameters, there is a staggeringly large number of possible alternatives overall. There has been a considerable amount of previous work on choosing among learning algorithms and, separately, on optimizing hyper-parameters (mostly when these are continuous and very few in number) in a given use context. However, we are aware of no work that addresses both problems together. Here, we demonstrate the feasibility of using a fully automated approach for choosing both a learning algorithm and its hyper-parameters, leveraging recent innovations in Bayesian optimization. Specifically, we apply this approach to the entire space of classifiers implemented in WEKA, spanning 3 ensemble methods, 14 meta-methods, 30 base classifiers, and a large range of hyper-parameter settings for each of these. On each of 10 data sets from the UCI repository, we show classification performance better than that of complete cross-validation over the default hyper-parameter settings of our 47 classification algorithms. We believe that our approach, which we dubbed Auto-WEKA, will enable typical users of machine learning algorithms to make better choices and thus to obtain better performance in a fully automated fashion."
Discounting Human Reward: Limitations of Episodicity,"Several studies have demonstrated that human-generated reward can be a powerful feedback signal for control learning algorithms. However, the algorithmic space for learning from human reward has hitherto not been explored systematically. Using model-based reinforcement learning from human reward in goal-based, episodic tasks, we investigate how anticipated future rewards should be discounted to create behavior that performs well on the task that the human trainer intends to teach. We identify a ``positive loops'' problem with low discounting (i.e., high discount factors) for episodic tasks that arises from an observed bias among humans towards giving positive reward. Empirical analysis verifies the existence of the positive loops problem and further indicates that high discounting (i.e., low discount factors) of human reward is necessary in goal-based, episodic tasks. Lastly, an alternate strategy for overcoming the positive loops problem --- converting the episodic task to a continuing one --- is shown to support a wide range of discounting and therefore provide greater algorithmic flexibility."
Discounting Human Reward: Limitations of Episodicity,"Several studies have demonstrated that human-generated reward can be a powerful feedback signal for control learning algorithms. However, the algorithmic space for learning from human reward has hitherto not been explored systematically. Using model-based reinforcement learning from human reward in goal-based, episodic tasks, we investigate how anticipated future rewards should be discounted to create behavior that performs well on the task that the human trainer intends to teach. We identify a ``positive loops'' problem with low discounting (i.e., high discount factors) for episodic tasks that arises from an observed bias among humans towards giving positive reward. Empirical analysis verifies the existence of the positive loops problem and further indicates that high discounting (i.e., low discount factors) of human reward is necessary in goal-based, episodic tasks. Lastly, an alternate strategy for overcoming the positive loops problem --- converting the episodic task to a continuing one --- is shown to support a wide range of discounting and therefore provide greater algorithmic flexibility."
Unfolding Latent Tree Structures using 4th Order Tensors,"Existing approaches for discovering latent structures often require the number ofhidden states as an input, a quantity usually unknown in practice. In this paper, wepropose a quartet based approachwhich is agnostic to this number. Our key contributionis a novel rank characterization of the tensor associated with the marginaldistribution of a quartet; and this characterization allows us to design a nuclearnorm based test for resolving the quartet relations. We then use this quartet testas a subroutine in a divide-and-conquer algorithm for recovering the latent treestructure. We show that, under certain conditions, the algorithm is consistent andits error probability decays exponentially with increasing sample size. In experiments,we demonstrate that our approach compares favorably to alternatives fordiscovering latent structures. In real world datasets, our approach also discoversmeaningful groupings of variables."
Unfolding Latent Tree Structures using 4th Order Tensors,"Existing approaches for discovering latent structures often require the number ofhidden states as an input, a quantity usually unknown in practice. In this paper, wepropose a quartet based approachwhich is agnostic to this number. Our key contributionis a novel rank characterization of the tensor associated with the marginaldistribution of a quartet; and this characterization allows us to design a nuclearnorm based test for resolving the quartet relations. We then use this quartet testas a subroutine in a divide-and-conquer algorithm for recovering the latent treestructure. We show that, under certain conditions, the algorithm is consistent andits error probability decays exponentially with increasing sample size. In experiments,we demonstrate that our approach compares favorably to alternatives fordiscovering latent structures. In real world datasets, our approach also discoversmeaningful groupings of variables."
Unfolding Latent Tree Structures using 4th Order Tensors,"Existing approaches for discovering latent structures often require the number ofhidden states as an input, a quantity usually unknown in practice. In this paper, wepropose a quartet based approachwhich is agnostic to this number. Our key contributionis a novel rank characterization of the tensor associated with the marginaldistribution of a quartet; and this characterization allows us to design a nuclearnorm based test for resolving the quartet relations. We then use this quartet testas a subroutine in a divide-and-conquer algorithm for recovering the latent treestructure. We show that, under certain conditions, the algorithm is consistent andits error probability decays exponentially with increasing sample size. In experiments,we demonstrate that our approach compares favorably to alternatives fordiscovering latent structures. In real world datasets, our approach also discoversmeaningful groupings of variables."
Focus of Attention for Linear Predictors,"We present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious. This trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets. We observe that some examples are easier to classify than others, a phenomenon which is characterized by the event when most of the features agree on the class of an example.By stopping the feature evaluation when encountering an easy-to-classify example, the predictor can achieve substantial gains in computation. Our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to include our attentive method we prove that the average number of features computed is $O(\sqrt{n \log \delta^{-0.5}})$ where $n$ is the original number of features, and $\delta$ is the error rate incurred due to early stopping. We demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim, Gisette, and synthetic datasets."
Improved Graph Laplacian by Geometric Consistency,"We consider the problem of setting the parameters used to construct the graph Laplacian for Euclidean data, most notably ?, the kernel bandwidth. We exploit the connection between geometry and the Laplace-Beltrami operator to evaluate how closely the graph Laplacian recovers the geometry of the data. In doing so, we can consider a family of graph Laplacians indexed by a finite number of parameters and select the values that best recover the geometry of the data."
Improved Graph Laplacian by Geometric Consistency,"We consider the problem of setting the parameters used to construct the graph Laplacian for Euclidean data, most notably ?, the kernel bandwidth. We exploit the connection between geometry and the Laplace-Beltrami operator to evaluate how closely the graph Laplacian recovers the geometry of the data. In doing so, we can consider a family of graph Laplacians indexed by a finite number of parameters and select the values that best recover the geometry of the data."
Online Multi-modal Similarity Learning for Large-scale Applications,"In many real-word scenarios, e.g., multimedia applications, data often originates from multiple heterogeneous sources or are given by diverse types of representation, which is referred to as multi-modal data. The definition of similarity between any two items/objects on multi-modal data is a key challenge encountered by many real-world applications, including multimedia information retrieval. In this paper, we present a novel online learning framework for learning similarity on multi-modal data through the combination of multiple kernels. We propose fast online multi-modal similarity learning (OMSL) algorithms which are significantly more efficient and scalable than the state-of-the-art techniques. Extensive experiments were conducted on large-scale multi-modal image retrieval applications. "
Online Multi-modal Similarity Learning for Large-scale Applications,"In many real-word scenarios, e.g., multimedia applications, data often originates from multiple heterogeneous sources or are given by diverse types of representation, which is referred to as multi-modal data. The definition of similarity between any two items/objects on multi-modal data is a key challenge encountered by many real-world applications, including multimedia information retrieval. In this paper, we present a novel online learning framework for learning similarity on multi-modal data through the combination of multiple kernels. We propose fast online multi-modal similarity learning (OMSL) algorithms which are significantly more efficient and scalable than the state-of-the-art techniques. Extensive experiments were conducted on large-scale multi-modal image retrieval applications. "
