title,abstract
High-Dimensional Feature Selection by Kernel-Based Feature-Wise Non-Linear Lasso,"The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features."
Efficient Local Image Description and Matching Based on Permutation Distances,"Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a randomsampling of pairwise comparisons of pixel intensities in an image patch.  Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a descriptor based on permutation distances between the ordering of intensities or RGB values between two patches. LUCID is computable in linear time of patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF yet up to five times more accuate, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster."
Efficient Local Image Description and Matching Based on Permutation Distances,"Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a randomsampling of pairwise comparisons of pixel intensities in an image patch.  Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a descriptor based on permutation distances between the ordering of intensities or RGB values between two patches. LUCID is computable in linear time of patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF yet up to five times more accuate, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster."
Efficient Local Image Description and Matching Based on Permutation Distances,"Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a randomsampling of pairwise comparisons of pixel intensities in an image patch.  Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a descriptor based on permutation distances between the ordering of intensities or RGB values between two patches. LUCID is computable in linear time of patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF yet up to five times more accuate, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster."
Efficient Local Image Description and Matching Based on Permutation Distances,"Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a randomsampling of pairwise comparisons of pixel intensities in an image patch.  Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a descriptor based on permutation distances between the ordering of intensities or RGB values between two patches. LUCID is computable in linear time of patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF yet up to five times more accuate, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster."
Clustering the Stochastic Block Model via Positions in the Network,"We consider the stochastic block model and analyze methods based on the positions of the nodes in the network.  This perspective was introduced by Burt (1976) and algorithmically amounts to embedding the graph by mapping the vertices to the corresponding rows of the adjacency matrix.  Once this is done, off-the-shelf methods for clustering points in Euclidean (or Hamming) space can be applied.  We study some popular dissimilarities in this context and provide theoretical guarantees for them that are sufficient for hierarchical clustering to succeed in correctly clustering the nodes.  The analysis is relatively simple in the context of a stochastic block model, yielding competitive performance bounds, particularly when the number of communities in the network grows with the number of nodes.  We evaluate some of these methods in some simulations, comparing them with spectral clustering.  "
Clustering the Stochastic Block Model via Positions in the Network,"We consider the stochastic block model and analyze methods based on the positions of the nodes in the network.  This perspective was introduced by Burt (1976) and algorithmically amounts to embedding the graph by mapping the vertices to the corresponding rows of the adjacency matrix.  Once this is done, off-the-shelf methods for clustering points in Euclidean (or Hamming) space can be applied.  We study some popular dissimilarities in this context and provide theoretical guarantees for them that are sufficient for hierarchical clustering to succeed in correctly clustering the nodes.  The analysis is relatively simple in the context of a stochastic block model, yielding competitive performance bounds, particularly when the number of communities in the network grows with the number of nodes.  We evaluate some of these methods in some simulations, comparing them with spectral clustering.  "
Multiscale Hidden Conditional Neural Fields with Adaptive-Rate Latent Variable Grouping,"We hypothesize that considering multiple levels of abstraction with adaptive-rate time quantization improves temporal sequence learning. To prove this empirically, we developed multiscale hidden conditional neural fields with adaptive-rate latent variable grouping. Our model is comprised of multiple layers, where each layer is recursively built from the preceding layer by aggregating observations that are similar in the latent space, representing the sequence at a coarser scale. We extract a nonlinear combination of features from grouped variables using a set of gate functions, learning the optimal abstraction of the sequence at each scale. This allows our model to learn higher level abstractions at ever more coarse-grained time scale as the layer gets higher. Optimization is performed layer-wise, making the complexity grow linearly with the number of layers. We evaluate our approach on three human activity datasets: ArmGesture, NATOPS, and Canal9. Our method achieves a near perfect recognition accuracy on the ArmGesture dataset, and outperforms all baseline models on the NATOPS and Canal9 dataset."
Multiscale Hidden Conditional Neural Fields with Adaptive-Rate Latent Variable Grouping,"We hypothesize that considering multiple levels of abstraction with adaptive-rate time quantization improves temporal sequence learning. To prove this empirically, we developed multiscale hidden conditional neural fields with adaptive-rate latent variable grouping. Our model is comprised of multiple layers, where each layer is recursively built from the preceding layer by aggregating observations that are similar in the latent space, representing the sequence at a coarser scale. We extract a nonlinear combination of features from grouped variables using a set of gate functions, learning the optimal abstraction of the sequence at each scale. This allows our model to learn higher level abstractions at ever more coarse-grained time scale as the layer gets higher. Optimization is performed layer-wise, making the complexity grow linearly with the number of layers. We evaluate our approach on three human activity datasets: ArmGesture, NATOPS, and Canal9. Our method achieves a near perfect recognition accuracy on the ArmGesture dataset, and outperforms all baseline models on the NATOPS and Canal9 dataset."
Jointly Segmenting Multiple Web Photo Streams,"As online sharing of personal photo streams is becoming popular and many of such photo streams often share overlapping contents, the cosegmentation can potentiate a wide range of intriguing Web applications. However, existing cosegmentation algorithms are still far limited for these new opportunities in that input images must be carefully prepared by human. In this paper, we address the problem of jointly segmenting an arbitrary number of unaligned and uncalibrated Web photo streams from multiple anonymous users. Given that the main difficulty of cosegmenting such photo streams lies in their extreme diversity in visual contents, we propose a multi-round segmentation algorithm that consists of the companion sampling  and cosegmentation  steps. Theoretically, we show that the developed method achieves the sublinear bound of regrets (i.e. the cumulative sum of difference between unknown ideal cosegmentation scores and actual scores). With experiments on more than 16K images of Flickr dataset and LabelMe dataset, wedemonstrate that our algorithm is more successful in both segmentation quality and scalability over other state-of-art methods."
Jointly Segmenting Multiple Web Photo Streams,"As online sharing of personal photo streams is becoming popular and many of such photo streams often share overlapping contents, the cosegmentation can potentiate a wide range of intriguing Web applications. However, existing cosegmentation algorithms are still far limited for these new opportunities in that input images must be carefully prepared by human. In this paper, we address the problem of jointly segmenting an arbitrary number of unaligned and uncalibrated Web photo streams from multiple anonymous users. Given that the main difficulty of cosegmenting such photo streams lies in their extreme diversity in visual contents, we propose a multi-round segmentation algorithm that consists of the companion sampling  and cosegmentation  steps. Theoretically, we show that the developed method achieves the sublinear bound of regrets (i.e. the cumulative sum of difference between unknown ideal cosegmentation scores and actual scores). With experiments on more than 16K images of Flickr dataset and LabelMe dataset, wedemonstrate that our algorithm is more successful in both segmentation quality and scalability over other state-of-art methods."
Active Learning of Model Evidence Using Bayesian Quadrature ,"Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy."
A Unified 3D Scene Parsing Framework,"We propose a unified framework for parsing an image to predict the scene category, the 3D boundary of the space, camera parameters, and all objects in the scene, represented by their 3D bounding boxes and categories. Using a structural SVM, we build a complete end-to-end system which learns all parameters together in a single step. We encode many novel image features and context rules into the structural SVM feature function and our framework automatically weighs the relative importance of all these rules based on training data. By optimizing a unified objective function, we do not require extra training of other models, nor an additional fusion step. We design an intuitive web-based tool to annotate 3D bounding boxes for objects, build our ?SUN3D? database and demonstrate that our model outperforms the state-of-the-art algorithms on several individual subtasks."
A Unified 3D Scene Parsing Framework,"We propose a unified framework for parsing an image to predict the scene category, the 3D boundary of the space, camera parameters, and all objects in the scene, represented by their 3D bounding boxes and categories. Using a structural SVM, we build a complete end-to-end system which learns all parameters together in a single step. We encode many novel image features and context rules into the structural SVM feature function and our framework automatically weighs the relative importance of all these rules based on training data. By optimizing a unified objective function, we do not require extra training of other models, nor an additional fusion step. We design an intuitive web-based tool to annotate 3D bounding boxes for objects, build our ?SUN3D? database and demonstrate that our model outperforms the state-of-the-art algorithms on several individual subtasks."
Coupling Nonparametric Mixtures via Latent Dirichlet Processes,"Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Specifically, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures, allowing mixture models to be coupled more flexibly. In addition, we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling."
Coupling Nonparametric Mixtures via Latent Dirichlet Processes,"Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Specifically, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures, allowing mixture models to be coupled more flexibly. In addition, we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling."
Bayesian Nonparametric Maximum Margin Matrix Factorization for Collaborative Prediction,"We present a probabilistic formulation to max-margin matrix factorization and build accordingly an infinite nonparametric Bayesian model to automatically resolve the unknown number of latent factors. Our work demonstrates a successful example that integrates Bayesian nonparametrics and max-margin learning, which are conventionally two separate paradigms and enjoy complementary advantages. We develop an efficient variational learning algorithm for posterior inference, and our extensive empirical studies on large-scale MovieLens and EachMovie data sets appear to demonstrate the advantages inherited from both max-margin matrix factorization and Bayesian nonparametrics."
Bayesian Nonparametric Maximum Margin Matrix Factorization for Collaborative Prediction,"We present a probabilistic formulation to max-margin matrix factorization and build accordingly an infinite nonparametric Bayesian model to automatically resolve the unknown number of latent factors. Our work demonstrates a successful example that integrates Bayesian nonparametrics and max-margin learning, which are conventionally two separate paradigms and enjoy complementary advantages. We develop an efficient variational learning algorithm for posterior inference, and our extensive empirical studies on large-scale MovieLens and EachMovie data sets appear to demonstrate the advantages inherited from both max-margin matrix factorization and Bayesian nonparametrics."
Stereoscopic Tracking with Neural Network Hardware,"A stereoscopic learning and tracking system is built using off-the-shelf parts: a pair of Cognimem V1KU neural network boards with onboard CMOS cameras; four HiTec servo motors; a Phidgets servo controller; and a laptop. A simple acrylic mount was constructed. Object learning and recognition occurs primarily within the Cognimem neural network chips, with tracking and triangulation done in software. A Kalman filter is used for predictive tracking. The resulting prototype can learn a new object quickly, usually within a second. It then can track the object?s motion in 3D space with depth-perception, at fairly fast speeds even in the presence of noisy background, without the use of structured light. The software is designed for extension to a variety of practical applications."
Super-Bit Locality-Sensitive Hashing,"Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method towards scalable nearest neighbor search in high dimensional data space, which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments."
Super-Bit Locality-Sensitive Hashing,"Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method towards scalable nearest neighbor search in high dimensional data space, which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments."
Super-Bit Locality-Sensitive Hashing,"Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method towards scalable nearest neighbor search in high dimensional data space, which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments."
Fisher?s Discriminant with Natual Image Priors,"We suggest a Bayesian framework to combine Fisher's discriminant and natural image statistics. The probability structure of Fisher's discriminant is expressed, and the idea of natural image statistics is utilized as prior probabilities (\emph{Natural Image Priors}). Previous methods which directly employed the spatial smoothness assumption of images can be shown as special cases within this framework, but with potentially improper priors which are different from the natural image prior. We also propose a novel method which is a \emph{maximum a posteriori probability} (MAP) estimate with the natural image prior. Experimental results on the Yale face database and the ETH-80 object categorization dataset show that the proposed method significantly outperforms the state-of-the-art methods for general image data."
Convergence Analysis and Ef?cient Algorithms for Hyper-Graph Matching,"This paper focuses on the theoretical and algorithmic design for hyper graph matching where the affinity is represented by a tensor. We start with the simple Gradient Assignment (HGA) that works in the discrete domain iteratively, and ?nd its m-point loop convergence property under rather weak conditions, where m is the tensor order. Then Hyper Constrained Gradient Assignment (HCGA) is proposed to avoid such unwanted iteration track, and we show this algorithm will ensure to converge to a unique discrete point. Then we further extend HCGA to the continuous domain: HSCGA which plays the role to the classical Graduate Assignment (HGAGM) as HCGA to HGA. Then we explore the underlying connection between HCGA and its continuous counterpart both theoretically and empirically: under weak conditions, we first prove HGAGM will converge to m-discrete point like HGA, then illustrate HSCGA have the same convergence property with HCGA. These findings build the theoretical connection between the proposed two algorithms. Experimental results on both synthetic and real data consent our theoretical analysis: both algorithms perform competitively to state-of-the-arts. While HCGA outstands due to its working in the discrete space, able to handle ill cases when Hungarian method return multiple solutions, and being an ef?cient and anytime algorithm."
Convergence Analysis and Ef?cient Algorithms for Hyper-Graph Matching,"This paper focuses on the theoretical and algorithmic design for hyper graph matching where the affinity is represented by a tensor. We start with the simple Gradient Assignment (HGA) that works in the discrete domain iteratively, and ?nd its m-point loop convergence property under rather weak conditions, where m is the tensor order. Then Hyper Constrained Gradient Assignment (HCGA) is proposed to avoid such unwanted iteration track, and we show this algorithm will ensure to converge to a unique discrete point. Then we further extend HCGA to the continuous domain: HSCGA which plays the role to the classical Graduate Assignment (HGAGM) as HCGA to HGA. Then we explore the underlying connection between HCGA and its continuous counterpart both theoretically and empirically: under weak conditions, we first prove HGAGM will converge to m-discrete point like HGA, then illustrate HSCGA have the same convergence property with HCGA. These findings build the theoretical connection between the proposed two algorithms. Experimental results on both synthetic and real data consent our theoretical analysis: both algorithms perform competitively to state-of-the-arts. While HCGA outstands due to its working in the discrete space, able to handle ill cases when Hungarian method return multiple solutions, and being an ef?cient and anytime algorithm."
Learning Mixed Graphical Models,We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables. The structure and parameters of this model are learned using the pseudo-likelihood approximation with group-sparsity regularization. The pairwise model is also extended to incorporate features. Two algorithms for solving the resulting optimization problem are presented. The proposed models are compared with competing methods on synthetic data and a survey dataset.
Learning Mixed Graphical Models,We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables. The structure and parameters of this model are learned using the pseudo-likelihood approximation with group-sparsity regularization. The pairwise model is also extended to incorporate features. Two algorithms for solving the resulting optimization problem are presented. The proposed models are compared with competing methods on synthetic data and a survey dataset.
ForeCA: Forecastable Component Analysis,"Blind source separation (BSS) techniques are often applied to multivariate time series with the goal to obtain better forecasts. But BSS and the need for better forecasts are often treated separately, in the sense that finding an optimally transformed (sub-)space has nothing to do with the aim to predict well. Here I introduce Forecastable \textbf{C}omponent Analysis (ForeCA), a new BSS technique for temporally dependent signals that uses forecastability as the explicit objective in finding an optimal transformation. It separates the signal into the forecastable, $\mathbf{F}$, and the orthogonal white noise space, $\mathbf{F}^{\bot}$. Simulations and applications to financial data show that ForeCA successfully finds signals that can be used to forecast. ForeCA therefore automatically discovers informative structure in multivariate signals."
Optimal Calculation of Tensor Learning Approaches,"Tensors provide a general framework for exploring multiple factors in learning. A tensor representation also helps to reduce the small sample size problem in discriminative subspace selection and the overfitting problem in vector-based learning. Most algorithms have been extended to the tensor space to create algorithm versions with direct tensor inputs. However, very unfortunately basically all objective functions of algorithms in the tensor space are non-convex. However, sub-problems constructed by fixing all the modes but one are often convex and very easy to solve. So most of the algorithms use alternating projection optimization procedure to solve the problem. However, this method may lead to difficulty converging; iterative algorithms sometimes get stuck in a local minimum and have difficulty converging to the global solution. Here, we propose a computational framework for constrained and unconstrained tensor methods. Using our methods, the algorithm convergence situation can be improved to some extent and better solutions obtained. We applied our technique to Uncorrelated Multilinear Principal Component Analysis (UMPCA), Tensor Rank one Discriminant Analysis (TR1DA) and Support Tensor Machines (STM); results showthe effectiveness of our method."
Constructing a Design Matrix by Stepping Vertex Features on Multiple Networks,"Suppose there are $n$ vertices which are embedded in $p$ networks and which take values $\mathbf{z}$.  We are interested in the simultaneous interactions between the vertices and the networks in which they are embedded.  In pursuit of this, we propose constructing an $n$ by $p$ design matrix by simply taking a step from $\mathbf{z}$ on each adjacency matrix (premultiplying each adjacency matrix by $\mathbf{z}$).  We then rely on traditional statistical techniques to analyze this constructed design matrix either with exploratory techniques or predictive techniques if the vertices take on a value $\mathbf{y}$ (we also consider the autoregressive case where $\mathbf{y} = \mathbf{z}$).  We compare this design matrix construction approach to methods specialized to handle the network analysis."
Constructing a Design Matrix by Stepping Vertex Features on Multiple Networks,"Suppose there are $n$ vertices which are embedded in $p$ networks and which take values $\mathbf{z}$.  We are interested in the simultaneous interactions between the vertices and the networks in which they are embedded.  In pursuit of this, we propose constructing an $n$ by $p$ design matrix by simply taking a step from $\mathbf{z}$ on each adjacency matrix (premultiplying each adjacency matrix by $\mathbf{z}$).  We then rely on traditional statistical techniques to analyze this constructed design matrix either with exploratory techniques or predictive techniques if the vertices take on a value $\mathbf{y}$ (we also consider the autoregressive case where $\mathbf{y} = \mathbf{z}$).  We compare this design matrix construction approach to methods specialized to handle the network analysis."
Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning,"Many problems in machine learning can be (re)formulated as linearly constrained convex programs. When there are only two variables, such problems can be efficiently solved by the alternating direction method (ADM) or its linearized version (LADM). However, more often there are more than two variables, but the corresponding theories on ADM and LADM are rather scarce. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-variable separable convex programs efficiently. LADMPSAP is particularly suitable for sparse representation and low rank recovery problems because its subproblems have closed form solutions and the sparsity and low rankness of the iterates can be preserved during the iteration. It is also highly parallelizable and hence fits for parallel or distributed computing. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the sufficient and necessary conditions} for global convergence. We further propose a simple optimality measure and reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, we devise a practical version of LADMPSAP for faster convergence. Numerical experiments on the latent low rank representation problem testify to the speed and accuracy advantages of LADMPSAP."
Ordered Rules for Classification: A Discrete Optimization Approach to Associative Classification,"We aim to design classifiers that have the interpretability of association rules yet have predictive power on par with the top machine learning algorithms for classification. We propose a novel mixed integer optimization (MIO) approach called Ordered Rules for Classification (ORC) for this task. Our method has two parts. The first part mines a particular frontier of solutions in the space of rules, and we show that this frontier contains the best rules according to a variety of interestingness measures. The second part learns an optimal ranking for the rules to build a decision list classifier that is simple and insightful. We report empirical evidence using several different datasets to demonstrate the performance of this method."
Ordered Rules for Classification: A Discrete Optimization Approach to Associative Classification,"We aim to design classifiers that have the interpretability of association rules yet have predictive power on par with the top machine learning algorithms for classification. We propose a novel mixed integer optimization (MIO) approach called Ordered Rules for Classification (ORC) for this task. Our method has two parts. The first part mines a particular frontier of solutions in the space of rules, and we show that this frontier contains the best rules according to a variety of interestingness measures. The second part learns an optimal ranking for the rules to build a decision list classifier that is simple and insightful. We report empirical evidence using several different datasets to demonstrate the performance of this method."
Ordered Rules for Classification: A Discrete Optimization Approach to Associative Classification,"We aim to design classifiers that have the interpretability of association rules yet have predictive power on par with the top machine learning algorithms for classification. We propose a novel mixed integer optimization (MIO) approach called Ordered Rules for Classification (ORC) for this task. Our method has two parts. The first part mines a particular frontier of solutions in the space of rules, and we show that this frontier contains the best rules according to a variety of interestingness measures. The second part learns an optimal ranking for the rules to build a decision list classifier that is simple and insightful. We report empirical evidence using several different datasets to demonstrate the performance of this method."
Positivity and Transportation,"We study in this paper positive definite kernels between discrete probability measures. We prove that the weighted volume -- or generating function -- of the set of integral transportation matrices between two integral histograms $r$ and $c$ of equal sum is a positive definite kernel of $r$ and $c$ when the set of considered weights forms a positive definite matrix. The computation of this quantity, despite being the subject of a significant research effort, remains computationally intractable for histograms of very modest dimensions. We propose an alternative kernel which, rather than considering all matrices of the transportation polytope, only focuses on a sub-sample of its vertices known as Northwestern corner solutions. The resulting kernel is positive definite and can be computed with a linear complexity $O(R^2d)$ in the dimension $d$, where $R^2$ --  the total amount of sampled vertices -- is a parameter that controls the complexity of the kernel."
Modeling the Forgetting Process using Image Regions,"While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten over time has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten over time, using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. "
Modeling the Forgetting Process using Image Regions,"While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten over time has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten over time, using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. "
Modeling the Forgetting Process using Image Regions,"While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten over time has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten over time, using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. "
Modeling the Forgetting Process using Image Regions,"While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten over time has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten over time, using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. "
Multi-agent Coordination through Adaptively Regularized Parallel Distributed Optimization,"Many problems require the analysis of large, high-dimensional datasets, which make the problem intractable for most optimization algorithms. In this paper, we propose using a parallel distributed adaptive regularizer (PDAR) to solve both the joint objective problems (JOP) and separated objective problems (SOP). The regularizer is dependent on the stepsize between iterations and the number of iterations executed, and the sub-problems are solved in parallel. We show convergence of our algorithm, and use a multi-agent two-bin resource allocation example to illustrate effectiveness. We also present example applications of our algorithm to biological and demand response problems. The numerical examples indicate that our parallel algorithm converges to the same optimal solution as sequential versionsdo, with significantly reduced computation time."
Multi-agent Coordination through Adaptively Regularized Parallel Distributed Optimization,"Many problems require the analysis of large, high-dimensional datasets, which make the problem intractable for most optimization algorithms. In this paper, we propose using a parallel distributed adaptive regularizer (PDAR) to solve both the joint objective problems (JOP) and separated objective problems (SOP). The regularizer is dependent on the stepsize between iterations and the number of iterations executed, and the sub-problems are solved in parallel. We show convergence of our algorithm, and use a multi-agent two-bin resource allocation example to illustrate effectiveness. We also present example applications of our algorithm to biological and demand response problems. The numerical examples indicate that our parallel algorithm converges to the same optimal solution as sequential versionsdo, with significantly reduced computation time."
Multi-agent Coordination through Adaptively Regularized Parallel Distributed Optimization,"Many problems require the analysis of large, high-dimensional datasets, which make the problem intractable for most optimization algorithms. In this paper, we propose using a parallel distributed adaptive regularizer (PDAR) to solve both the joint objective problems (JOP) and separated objective problems (SOP). The regularizer is dependent on the stepsize between iterations and the number of iterations executed, and the sub-problems are solved in parallel. We show convergence of our algorithm, and use a multi-agent two-bin resource allocation example to illustrate effectiveness. We also present example applications of our algorithm to biological and demand response problems. The numerical examples indicate that our parallel algorithm converges to the same optimal solution as sequential versionsdo, with significantly reduced computation time."
An Optimal Policy for Target Localization with Application to Electron Microscopy,"This paper considers the task of finding a target location by making a limited number of sequential observation. Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position. Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked. In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon. Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies."
Textual features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks."
Towards Massive Multi-Way Classification: Structured Sparse Output Coding,"Multi-way classification with massive classes is a practical and challenging problem. In this paper, we propose structured sparse output coding, a principled way for massive multi-way classification, where a sparse output coding matrix is learned to maximize codeword separation and accuracy of each bit predictor. Moreover, we provide a concave-convex procedure based algorithm for the resultant optimization problem, which solves a series of l1 regularized convex optimization problems under linear constraints, using dual proximal gradient method. Experimental results demonstrate the effectiveness of our proposed approach."
Scalable Manifold Learning,"High computational costs of manifold learning make its application prohibitive for large point sets. A common strategy to overcome this problem is to sample a subset of points, called landmarks, on which the dimensionality reduction is performed and to reconstruct the embedding of all points using the Nystr?m method. In this paper, we address the two main challenges that arise in this setup. First, the selected subset of landmarks in non-Euclidean geometries must result in a low reconstruction error. Second, the nearest neighbor graph construction on sparsely sampled subsets must be robust and approximate the original data well. We propose an extension for sampling from determinantal distributions on non-Euclidean spaces by opearting on the geodesic distance on the manifold. Since current determinantal sampling algorithms have the same complexity as manifold learning, we propose an efficient approximation running in $\mO(ndk)$. We achieve excellent results with the proposed algorithm for manifold sampling by restricting the probability update to local neighborhoods. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Mahalanobis distance increases the robustness of dimensionality reduction on sparsely sampled manifolds. "
Scalable Manifold Learning,"High computational costs of manifold learning make its application prohibitive for large point sets. A common strategy to overcome this problem is to sample a subset of points, called landmarks, on which the dimensionality reduction is performed and to reconstruct the embedding of all points using the Nystr?m method. In this paper, we address the two main challenges that arise in this setup. First, the selected subset of landmarks in non-Euclidean geometries must result in a low reconstruction error. Second, the nearest neighbor graph construction on sparsely sampled subsets must be robust and approximate the original data well. We propose an extension for sampling from determinantal distributions on non-Euclidean spaces by opearting on the geodesic distance on the manifold. Since current determinantal sampling algorithms have the same complexity as manifold learning, we propose an efficient approximation running in $\mO(ndk)$. We achieve excellent results with the proposed algorithm for manifold sampling by restricting the probability update to local neighborhoods. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Mahalanobis distance increases the robustness of dimensionality reduction on sparsely sampled manifolds. "
Non-parametric Approximate Dynamic Programming via the Kernel	Method,"This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful, dimension-independent approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our non-parametric procedure is competitive with parametric ADP approaches."
Sampling GMRFs by Subgraph Correction,"The problem of efficiently drawing samples from a Gaussian Markov random field is studied. In this paper, we introduce the subgraph correction sampling algorithm, which makes use of any pre-existing tractable sampling algorithm for a subgraph by perturbing this algorithm so as to yield asymptotically exact samples for the intended distribution. The subgraph can have any structure for which efficient sampling algorithms exist: for example, tree-structured, with low tree-width, or with a small feedback vertex set. Experimental results demonstrate that the subgraph correction algorithm yields accurate samples much faster than many traditional sampling methods---such as Gibbs sampling---for many graph topologies."
Sampling GMRFs by Subgraph Correction,"The problem of efficiently drawing samples from a Gaussian Markov random field is studied. In this paper, we introduce the subgraph correction sampling algorithm, which makes use of any pre-existing tractable sampling algorithm for a subgraph by perturbing this algorithm so as to yield asymptotically exact samples for the intended distribution. The subgraph can have any structure for which efficient sampling algorithms exist: for example, tree-structured, with low tree-width, or with a small feedback vertex set. Experimental results demonstrate that the subgraph correction algorithm yields accurate samples much faster than many traditional sampling methods---such as Gibbs sampling---for many graph topologies."
Sampling GMRFs by Subgraph Correction,"The problem of efficiently drawing samples from a Gaussian Markov random field is studied. In this paper, we introduce the subgraph correction sampling algorithm, which makes use of any pre-existing tractable sampling algorithm for a subgraph by perturbing this algorithm so as to yield asymptotically exact samples for the intended distribution. The subgraph can have any structure for which efficient sampling algorithms exist: for example, tree-structured, with low tree-width, or with a small feedback vertex set. Experimental results demonstrate that the subgraph correction algorithm yields accurate samples much faster than many traditional sampling methods---such as Gibbs sampling---for many graph topologies."
Multidimensional Membership Mixture Models,"We present the multidimensional membership mixture (M3) models where every dimension of the membership represents an independent mixture model and each data point is generated from the selected mixture components jointly. This is helpful when the data has a certain shared structure. For example, three unique means and three unique variances can effectively form a Gaussian mixture model with nine components, while requiring only six parameters to fully describe it. In this paper, we present three instantiations of M3 models (together with the learning and inference algorithms): infinite, finite, and hybrid, depending on whether the number of mixtures is fixed or not. They are built upon Dirichlet process mixture models, latent Dirichlet allocation, and a combination respectively. We then consider two applications: topic modeling and learning 3D object arrangements. Our experiments show that our M3 models achieve better performance using fewer topics than many classic topic models. We also observe that topics from the different dimensions of M3 models are meaningful and orthogonal to each other. "
Multidimensional Membership Mixture Models,"We present the multidimensional membership mixture (M3) models where every dimension of the membership represents an independent mixture model and each data point is generated from the selected mixture components jointly. This is helpful when the data has a certain shared structure. For example, three unique means and three unique variances can effectively form a Gaussian mixture model with nine components, while requiring only six parameters to fully describe it. In this paper, we present three instantiations of M3 models (together with the learning and inference algorithms): infinite, finite, and hybrid, depending on whether the number of mixtures is fixed or not. They are built upon Dirichlet process mixture models, latent Dirichlet allocation, and a combination respectively. We then consider two applications: topic modeling and learning 3D object arrangements. Our experiments show that our M3 models achieve better performance using fewer topics than many classic topic models. We also observe that topics from the different dimensions of M3 models are meaningful and orthogonal to each other. "
Multidimensional Membership Mixture Models,"We present the multidimensional membership mixture (M3) models where every dimension of the membership represents an independent mixture model and each data point is generated from the selected mixture components jointly. This is helpful when the data has a certain shared structure. For example, three unique means and three unique variances can effectively form a Gaussian mixture model with nine components, while requiring only six parameters to fully describe it. In this paper, we present three instantiations of M3 models (together with the learning and inference algorithms): infinite, finite, and hybrid, depending on whether the number of mixtures is fixed or not. They are built upon Dirichlet process mixture models, latent Dirichlet allocation, and a combination respectively. We then consider two applications: topic modeling and learning 3D object arrangements. Our experiments show that our M3 models achieve better performance using fewer topics than many classic topic models. We also observe that topics from the different dimensions of M3 models are meaningful and orthogonal to each other. "
Optimal Regularized Dual Averaging Methods for Stochastic Optimization,"This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth.  We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the optimal  rate of $O(\frac{1}{N}+\frac{1}{N^2})$ for $N$  iterations, which improves the best known rate $O(\frac{\log N }{N})$ of previous stochastic dual averaging algorithms. In addition, our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., $\ell_1$-norm), it has the advantage of encouraging sparser solutions. We further develop a multi-stage extension using the proposed algorithm as a subroutine, which achieves the uniformly-optimal rate $O(\frac{1}{N}+\exp\{-N\})$ for strongly convex loss."
Optimal Regularized Dual Averaging Methods for Stochastic Optimization,"This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth.  We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the optimal  rate of $O(\frac{1}{N}+\frac{1}{N^2})$ for $N$  iterations, which improves the best known rate $O(\frac{\log N }{N})$ of previous stochastic dual averaging algorithms. In addition, our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., $\ell_1$-norm), it has the advantage of encouraging sparser solutions. We further develop a multi-stage extension using the proposed algorithm as a subroutine, which achieves the uniformly-optimal rate $O(\frac{1}{N}+\exp\{-N\})$ for strongly convex loss."
Learning global properties of scene images from conditional correlational structure,"Scene images with similar spatial layout properties often display characteristic statistical regularities on a global scale. In order to develop an efficient code for these global properties that reflects their inherent regularities, we train a hierarchical probabilistic model to infer conditional correlational information from scene images. Fitting a model to a scene database yields a compact representation of global information that encodes salient visual structures with low dimensional latent variables. Using perceptual ratings and scene similarities based on spatial layouts of scene images, we demonstrate that the model representation is more consistent with perceptual similarities of scene images than the metrics based on the state-of-the-art visual features. "
The variational hierarchical EM algorithm for clustering hidden Markov models.,"In this paper, we derive a novel algorithm to cluster  hidden Markov models (HMMs) according to their probability distributions.We propose a variational hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ``cluster center'', i.e., a novel HMM that is representative for the group.We illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging."
The variational hierarchical EM algorithm for clustering hidden Markov models.,"In this paper, we derive a novel algorithm to cluster  hidden Markov models (HMMs) according to their probability distributions.We propose a variational hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ``cluster center'', i.e., a novel HMM that is representative for the group.We illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging."
Consistency Analysis of  Empirical MEE Algorithm,"In this paper we study the consistency of the empirical minimum error entropy (MEE) algorithm for regression learning.Two types of consistency are studied. The error entropy consistency,which requires the error entropy of the learned functionapproximates the minimum error entropy, is shown to be always trueif the bandwidth parameter tends to 0 at an appropriate rate. Theregression consistency, which requires the learned functionapproximates the regression function, however, is a complicatedissue. We prove that the error entropy consistency implies theregression consistency for homoskedastic models where the noise isindependent of the input variable. But for heteroskedastic models, acounter-example is used to show the two types of consistency do notcoincide. A surprising result is that the regression consistency isalways true, provided that the bandwidth parameter tends to infinityat certain rates. This result, however, contradicts the motivationof MEE principle because the minimum error entropy is believed to benot approximated well with this choice of bandwidth parameter."
Consistency Analysis of  Empirical MEE Algorithm,"In this paper we study the consistency of the empirical minimum error entropy (MEE) algorithm for regression learning.Two types of consistency are studied. The error entropy consistency,which requires the error entropy of the learned functionapproximates the minimum error entropy, is shown to be always trueif the bandwidth parameter tends to 0 at an appropriate rate. Theregression consistency, which requires the learned functionapproximates the regression function, however, is a complicatedissue. We prove that the error entropy consistency implies theregression consistency for homoskedastic models where the noise isindependent of the input variable. But for heteroskedastic models, acounter-example is used to show the two types of consistency do notcoincide. A surprising result is that the regression consistency isalways true, provided that the bandwidth parameter tends to infinityat certain rates. This result, however, contradicts the motivationof MEE principle because the minimum error entropy is believed to benot approximated well with this choice of bandwidth parameter."
Multi-Granularity Image Categorization via Probabilistic Decoding,"Modern image data sets organize classes in a hierarchical taxonomy structure, such as a tree or DAG. Usually formulated as a multi-way classification, classical image categorization approaches can only predict leaf labels in such hierarchy. In this paper, based on the error correcting output coding formulation for multi-way classification, we propose a probabilistic decoding approach for both single-granularity, where only leaf level labels are allowed, and multi-granularity image categorization, which could generate internal labels from the taxonomy hierarchy if it is uncertain on leaf level labels. Experimental results demonstrate the effectiveness of our proposed image categorization approach on both single-granularity scenario and multi-granularity case."
3D Gaze Concurrences from Head-mounted Cameras,"A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanating from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the fixed relationship between the primary gaze ray and the head-mounted camera pose. The resulting gaze model enables us to build a social saliency field in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent mode-seeking in the social saliency field. Our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth."
3D Gaze Concurrences from Head-mounted Cameras,"A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanating from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the fixed relationship between the primary gaze ray and the head-mounted camera pose. The resulting gaze model enables us to build a social saliency field in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent mode-seeking in the social saliency field. Our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth."
3D Gaze Concurrences from Head-mounted Cameras,"A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanating from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the fixed relationship between the primary gaze ray and the head-mounted camera pose. The resulting gaze model enables us to build a social saliency field in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent mode-seeking in the social saliency field. Our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth."
Information-Theoretic Limits on Model Selection for Gaussian Markov Random Fields in the High-Dimensional Setting,"This paper focuses on the information-theoretic limitations of model selection for Gaussian Markov random fields in the high-dimensional setting, where the graph size $p$ and the number of edges $k$ are allowed to scale with the sample size $n$. We provide an a rigorous analysis of this problem for generic graphs in an ensemble. Our result establishes a necessary condition on the sample size $n(p,k)$ for any procedure, regardless of its computational complexity, to consistently recover the underlying graph. Moreover, our analysis implies a connection between that graphical model selection limits and eigenvalues of concentration matrices. The key way out of the difficulty is found via investigating the orthogonal systems from concentration matrices, making it possible to calculate the symmetric Kullback-Leibler divergence between generic graphs and obtain the final simple result. Our method of analyzing generic graphs using orthogonal systems would be of use to other model selection problems."
Learning Useful Abstractions from the Web: Case Study on Patient Medications and Outcomes,"The successful application of machine learning to electronic medical records typically turns on the construction of an appropriate feature vector.  That often depends upon the ability to find an appropriate way to abstract the large number of variables found in such records.  In this paper, we explore the use of topic modeling to design feature vectors in an automated manner by harnessing expertise available on the Web. We test the proposed methods on the task of inferring useful abstractions from a list of thousands of medications. Using Latent Dirichlet Allocation we learn a topic model based on Web entries corresponding to each drug in the list. Using only knowledge from Wikipedia pages, we were able to learn a model that is similar to the curated drug classification scheme that serves as an industry standard. We further demonstrate the utility of these learned abstractions through the construction of a kernel based on the earth mover's distance and derived from the learned topic model. Applied to a corpus of 25,000 patient admissions, we use this kernel to predict three different adverse outcomes (death, an abnormally long stay, or admission through the emergency room) for the next hospital admission.  Somewhat surprisingly,  the classifiers built using the learned abstractions outperform classifiers learned from the curated drug classification scheme."
Learning Useful Abstractions from the Web: Case Study on Patient Medications and Outcomes,"The successful application of machine learning to electronic medical records typically turns on the construction of an appropriate feature vector.  That often depends upon the ability to find an appropriate way to abstract the large number of variables found in such records.  In this paper, we explore the use of topic modeling to design feature vectors in an automated manner by harnessing expertise available on the Web. We test the proposed methods on the task of inferring useful abstractions from a list of thousands of medications. Using Latent Dirichlet Allocation we learn a topic model based on Web entries corresponding to each drug in the list. Using only knowledge from Wikipedia pages, we were able to learn a model that is similar to the curated drug classification scheme that serves as an industry standard. We further demonstrate the utility of these learned abstractions through the construction of a kernel based on the earth mover's distance and derived from the learned topic model. Applied to a corpus of 25,000 patient admissions, we use this kernel to predict three different adverse outcomes (death, an abnormally long stay, or admission through the emergency room) for the next hospital admission.  Somewhat surprisingly,  the classifiers built using the learned abstractions outperform classifiers learned from the curated drug classification scheme."
Learning Model-Based Sparsity via Projected Gradient Descent,"Several convex formulation methods have been proposed previously for statistical estimation with structured sparsity as the prior. These methods often require a carefully tuned regularization parameter, often a cumbersome or heuristic exercise. Furthermore, the estimate that these methods produce might not belong to the desired sparsity model, albeit accurately approximating the true parameter. Therefore, greedy-type algorithms could often be more desirable in estimating structured-sparse parameters. So far, these greedy methods have mostly focused on linear statistical models. In this paper we study the projected gradient descent with non-convex structured-sparse parameter model as the constraint set. Should the cost function have a Stable Model-Restricted Hessian the algorithm converges to the desired minimizer up to an approximation error. As an example we elaborate on application of the main results to estimation in Generalized Linear Model."
Learning Model-Based Sparsity via Projected Gradient Descent,"Several convex formulation methods have been proposed previously for statistical estimation with structured sparsity as the prior. These methods often require a carefully tuned regularization parameter, often a cumbersome or heuristic exercise. Furthermore, the estimate that these methods produce might not belong to the desired sparsity model, albeit accurately approximating the true parameter. Therefore, greedy-type algorithms could often be more desirable in estimating structured-sparse parameters. So far, these greedy methods have mostly focused on linear statistical models. In this paper we study the projected gradient descent with non-convex structured-sparse parameter model as the constraint set. Should the cost function have a Stable Model-Restricted Hessian the algorithm converges to the desired minimizer up to an approximation error. As an example we elaborate on application of the main results to estimation in Generalized Linear Model."
Discriminatively Activated Sparselets,"As the number of object classes becomes large, redundancy amonglearned object models increases substantially and thus naturallymotivates the idea of compact intermediate representations that canbe shared across classes for efficient multiclass inference. Recently,a new universal intermediate representation for multiclass objectdetection, sparselets, was introduced yieldingone to two orders of magnitude reduction in inference time andenabling real-time multiclass object detection. However, as computationalefficiency is gained by making the sparselet activations increasingly sparse, the task performance of reconstructive sparseletmodels degrades unfavorably.  This paper provides a generalformalism where sparselet activations are learned discriminativelyin a structured output prediction framework. Our experimental resultson multiclass object detection and multiclass image classificationdemonstrate that the proposed discriminative sparselet activationsmaintain high task performance while achieving greater sparsity,which in turn significantly improves inference efficiency."
Discriminatively Activated Sparselets,"As the number of object classes becomes large, redundancy amonglearned object models increases substantially and thus naturallymotivates the idea of compact intermediate representations that canbe shared across classes for efficient multiclass inference. Recently,a new universal intermediate representation for multiclass objectdetection, sparselets, was introduced yieldingone to two orders of magnitude reduction in inference time andenabling real-time multiclass object detection. However, as computationalefficiency is gained by making the sparselet activations increasingly sparse, the task performance of reconstructive sparseletmodels degrades unfavorably.  This paper provides a generalformalism where sparselet activations are learned discriminativelyin a structured output prediction framework. Our experimental resultson multiclass object detection and multiclass image classificationdemonstrate that the proposed discriminative sparselet activationsmaintain high task performance while achieving greater sparsity,which in turn significantly improves inference efficiency."
On Consistent Classification with Imbalanced Classes,"We consider the problem of imbalanced classes in binary classification, where one class is rare compared to the other. This problem arises frequently in practice and has been widely studied. However very little is understood in terms of the theoretical properties of the problem or of the algorithms proposed: what performance measures are appropriate, how these affect the learning process, and whether the algorithms are statistically consistent with respect to the desired performance measures. In this paper, we initiate a formal study of these issues, focusing on the balanced 0-1 error that evaluates errors on the majority and minority classes separately and effectively balances the two. The underlying balanced 0-1 loss bears similarity to cost-sensitive losses; however a critical difference between the two is that the balanced loss depends on the underlying distribution, while cost-sensitive losses are defined independent of the distribution. We establish statistical consistency of two types of algorithms with respect to the balanced 0-1 error: plug-in rules that use an empirically determined threshold, and certain types of empirically balanced risk minimization algorithms. Our experiments support our theoretical results, showing that both these approaches perform as well as (or better than) under-/over-sampling methods that are currently viewed as the state of the art."
Patient Risk Stratification for Hospital-Associated C. Diff as a Time-Series Classification Task,"A patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state. We explore representing patient risk as a time series. In doing so, patient risk stratification becomes a time-series classification task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted. Thus, we begin by defining and extracting approximate \textit{risk processes}, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns. We apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \textit{Clostridium Difficile}. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05). "
Patient Risk Stratification for Hospital-Associated C. Diff as a Time-Series Classification Task,"A patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state. We explore representing patient risk as a time series. In doing so, patient risk stratification becomes a time-series classification task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted. Thus, we begin by defining and extracting approximate \textit{risk processes}, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns. We apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \textit{Clostridium Difficile}. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05). "
Human Activity Learning using Object Affordances from RGB-D Videos,"Human activities comprise several sub-activities performed in a sequence and involve interactions with various objects. This makes reasoning about the object affordances a central task for activity recognition. In this work, we consider the problem of jointly labeling the object affordances and human activities from RGB-D videos. We frame the problem as a Markov Random Field where the nodes represent  objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural SVM approach, where labeling over various alternate temporal segmentations are considered as latent variables. We tested our method on a dataset comprising 120 activity videos collected from four subjects, and obtained an end-to-end precision of 81.8% and recall of 80.0% for labeling the activities."
Human Activity Learning using Object Affordances from RGB-D Videos,"Human activities comprise several sub-activities performed in a sequence and involve interactions with various objects. This makes reasoning about the object affordances a central task for activity recognition. In this work, we consider the problem of jointly labeling the object affordances and human activities from RGB-D videos. We frame the problem as a Markov Random Field where the nodes represent  objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural SVM approach, where labeling over various alternate temporal segmentations are considered as latent variables. We tested our method on a dataset comprising 120 activity videos collected from four subjects, and obtained an end-to-end precision of 81.8% and recall of 80.0% for labeling the activities."
Learning to Discover Social Circles in Ego Networks,"Our personal social networks are big and cluttered, and currently there is no good way to organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g. `circles' on Google+, and `lists' on Facebook and Twitter), however they are laborious to construct and must be updated whenever a user's network grows. We define a novel machine learning task of identifying users' social circles. We pose the problem as a node clustering problem on a user's ego-network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user profile information. For each circle we learn its members and the circle-specific user profile similarity metric. Modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter for all of which we obtain hand-labeled ground-truth data."
Ensemble weighted kernel estimators for multivariate entropy estimation,"The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, kernel plug-in estimators suffer from the curse of dimensionality, wherein the MSE rate of convergence is glacially slow - of order  $O(T^{-{\gamma}/{d}})$, where $T$ is the number of samples, and $\gamma>0$ is a rate parameter. In this paper, it is shown that for sufficiently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order $O(T^{-1})$. Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed offline. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates."
Ensemble weighted kernel estimators for multivariate entropy estimation,"The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, kernel plug-in estimators suffer from the curse of dimensionality, wherein the MSE rate of convergence is glacially slow - of order  $O(T^{-{\gamma}/{d}})$, where $T$ is the number of samples, and $\gamma>0$ is a rate parameter. In this paper, it is shown that for sufficiently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order $O(T^{-1})$. Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed offline. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates."
Efficient high dimensional maximum entropy modeling via symmetric partition functions,"  The application of the maximum entropy principle to sequence  modeling has been popularized by methods such as Conditional Random  Fields (CRFs).  However, these approaches are generally limited to  modeling paths in discrete spaces of low dimensionality.  We  consider the problem of modeling distributions over paths in  continuous spaces of high dimensionality---a problem for which  inference is generally intractable.  Our main contribution is to  show that maximum entropy modeling of high-dimensional, continuous  paths is tractable as long as the constrained features   possess a certain kind of low dimensional structure.  In this case, we show that the associated {\em partition function} is  symmetric and that this symmetry can be exploited to compute the  partition function efficiently in a compressed form.  Empirical  results are given showing an application of our method to maximum  entropy modeling of high dimensional human motion capture data."
Efficient high dimensional maximum entropy modeling via symmetric partition functions,"  The application of the maximum entropy principle to sequence  modeling has been popularized by methods such as Conditional Random  Fields (CRFs).  However, these approaches are generally limited to  modeling paths in discrete spaces of low dimensionality.  We  consider the problem of modeling distributions over paths in  continuous spaces of high dimensionality---a problem for which  inference is generally intractable.  Our main contribution is to  show that maximum entropy modeling of high-dimensional, continuous  paths is tractable as long as the constrained features   possess a certain kind of low dimensional structure.  In this case, we show that the associated {\em partition function} is  symmetric and that this symmetry can be exploited to compute the  partition function efficiently in a compressed form.  Empirical  results are given showing an application of our method to maximum  entropy modeling of high dimensional human motion capture data."
Shifting Weights: Adapting Object Detectors from Image to Video,"Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video."
Shifting Weights: Adapting Object Detectors from Image to Video,"Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video."
Shifting Weights: Adapting Object Detectors from Image to Video,"Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video."
Shifting Weights: Adapting Object Detectors from Image to Video,"Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video."
Recursive Deep Learning on 3D Point Clouds,"Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a novel model based on sparse and recursive autoencoders (RAE) for learning both features and object categories from raw 3D point clouds as well as standard images. The model differs from previous RAE models in that it fixes the tree structures and includes short-circuit connections from all tree nodes to the final classifier. This allows the model to take into consideration both low-level features as well as global features of the object. Using our fully learned architecture, we achieve state of the art performance on a standard RGB-D object recognition dataset, rivaling random forest classifiers on hand-designed features such as SIFT and spin images. Our method is very fast and can classify 71 images in 1 second on a standard desktop machine in Matlab. This is possible because the method only requires 16 matrix multiplications to classify each image into one of 51 household objects."
Recursive Deep Learning on 3D Point Clouds,"Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a novel model based on sparse and recursive autoencoders (RAE) for learning both features and object categories from raw 3D point clouds as well as standard images. The model differs from previous RAE models in that it fixes the tree structures and includes short-circuit connections from all tree nodes to the final classifier. This allows the model to take into consideration both low-level features as well as global features of the object. Using our fully learned architecture, we achieve state of the art performance on a standard RGB-D object recognition dataset, rivaling random forest classifiers on hand-designed features such as SIFT and spin images. Our method is very fast and can classify 71 images in 1 second on a standard desktop machine in Matlab. This is possible because the method only requires 16 matrix multiplications to classify each image into one of 51 household objects."
Recursive Deep Learning on 3D Point Clouds,"Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a novel model based on sparse and recursive autoencoders (RAE) for learning both features and object categories from raw 3D point clouds as well as standard images. The model differs from previous RAE models in that it fixes the tree structures and includes short-circuit connections from all tree nodes to the final classifier. This allows the model to take into consideration both low-level features as well as global features of the object. Using our fully learned architecture, we achieve state of the art performance on a standard RGB-D object recognition dataset, rivaling random forest classifiers on hand-designed features such as SIFT and spin images. Our method is very fast and can classify 71 images in 1 second on a standard desktop machine in Matlab. This is possible because the method only requires 16 matrix multiplications to classify each image into one of 51 household objects."
Weighted regret-based likelihood: a new approach to describing uncertainty,"A new representation of likelihood is proposed, based on the notion ofregret, and is completely characterized."
Semantic GIST: Probabilistic Modelling of Scenes using Scenelet,"In this paper, we propose a probabilistic modeling framework for scenes to encode semantic information of images into a compact Semantic Gist representation. The representation is based on a key concept called {\em scenelets}, which serves as building blocks for scenes. We learn these scenelets using a topic model to group correlated objects such that the learned set of scenelets maximally retain the semantic saliency of images in terms of KL-divergence. Our model also integrates information from individual discriminative object detectors and global image features by coding them as priors. Empirical results demonstrate the power of our model. We first show that using a small set of scenelet classifiers, we can predict the existence of a large set of objects without running individual object detectors.Furthermore, we can even predict the presence of objects without running large sets of object detectors by MAP estimation using our model.We also show that the framework can improve the performance of individual detectors by incorporating the contextual object and scenelet information. Experiments on challenging datasets including PASCAL and SUN09  demonstrate that our model outperforms other state-of-the-art ones."
Semantic GIST: Probabilistic Modelling of Scenes using Scenelet,"In this paper, we propose a probabilistic modeling framework for scenes to encode semantic information of images into a compact Semantic Gist representation. The representation is based on a key concept called {\em scenelets}, which serves as building blocks for scenes. We learn these scenelets using a topic model to group correlated objects such that the learned set of scenelets maximally retain the semantic saliency of images in terms of KL-divergence. Our model also integrates information from individual discriminative object detectors and global image features by coding them as priors. Empirical results demonstrate the power of our model. We first show that using a small set of scenelet classifiers, we can predict the existence of a large set of objects without running individual object detectors.Furthermore, we can even predict the presence of objects without running large sets of object detectors by MAP estimation using our model.We also show that the framework can improve the performance of individual detectors by incorporating the contextual object and scenelet information. Experiments on challenging datasets including PASCAL and SUN09  demonstrate that our model outperforms other state-of-the-art ones."
Density-Difference Estimation,"We address the problem of estimating the difference betweentwo probability densities.A naive approach is a two-step procedure of first estimating two densities separatelyand then computing their difference.However, such a two-step procedure does not necessarily work wellbecause the first step is performed without regard to the second stepand thus a small error incurred in the first stage can cause a big error in the second stage.In this paper, we propose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities.We derive a non-parametric finite-sample error boundfor the proposed single-shot density-difference estimatorand show that it achieves the optimal convergence rate.We experimentally demonstrate the usefulness of the proposed methodin class-prior estimation and change-point detection."
Search-Based Understanding of Hierarchical Dirichlet Process Topic Models,"Bayesian nonparametric models are widely used to allow unsupervised learning of statistical model structure from data.  Conventional learning algorithms, based on Gibbs sampling or variational Bayesian approximations, can be undesirably prone to local optima and sensitive to initialization.  We explore this issue in the context of the hierarchical Dirichlet process, which with combined with Dirichlet-multinomial likelihoods provides a nonparametric topic model.  We present a more efficient learning algorithm based on the Maximization-Expectation (ME) algorithm, based on a novel combinatorial formulation of the HDP marginal likelihood and implemented with carefully crafted search moves.  Experiments on synthetic data with statistics similar to real text corpora, and four real document collections, show consistent improvement over sampling algorithms in both recovery of the true number of topics, and predictive likelihood of test data."
Localizing 3D cuboids in single-view images,"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners."
Localizing 3D cuboids in single-view images,"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners."
Active Metric Learning For Ground Level To Aerial Image Matching,"Image-based geolocation is a challenging problem that has recently captured the interest of computer vision and machine learning researchers.  In this work we focus on the specific problem of matching ground level images to 45 degree aerial images. Matching ground level and aerial images is a very hard problem due to wide disparities in viewpoint and imaging conditions, the combination of which leads to low level feature matching failure.  To overcome this problem, we propose an active metric learning framework that allows a human user to solve the problem collaboratively with the machine.  Our approach selects pairs of regions of interest based on an information gain criterion and asks the human user to establish correspondences between them.  Those pairs are subsequently used to update a metric to improve the correspondences. This process continues until the system finds the correct location of the ground level image.  We introduce a new Ground Level to Aerial Image Dataset (GLAID) to assess strengths and weaknesses of our proposed framework. Our experiments show that our system allows user to find the correct location with significantly reduced effort."
Active Metric Learning For Ground Level To Aerial Image Matching,"Image-based geolocation is a challenging problem that has recently captured the interest of computer vision and machine learning researchers.  In this work we focus on the specific problem of matching ground level images to 45 degree aerial images. Matching ground level and aerial images is a very hard problem due to wide disparities in viewpoint and imaging conditions, the combination of which leads to low level feature matching failure.  To overcome this problem, we propose an active metric learning framework that allows a human user to solve the problem collaboratively with the machine.  Our approach selects pairs of regions of interest based on an information gain criterion and asks the human user to establish correspondences between them.  Those pairs are subsequently used to update a metric to improve the correspondences. This process continues until the system finds the correct location of the ground level image.  We introduce a new Ground Level to Aerial Image Dataset (GLAID) to assess strengths and weaknesses of our proposed framework. Our experiments show that our system allows user to find the correct location with significantly reduced effort."
Active Metric Learning For Ground Level To Aerial Image Matching,"Image-based geolocation is a challenging problem that has recently captured the interest of computer vision and machine learning researchers.  In this work we focus on the specific problem of matching ground level images to 45 degree aerial images. Matching ground level and aerial images is a very hard problem due to wide disparities in viewpoint and imaging conditions, the combination of which leads to low level feature matching failure.  To overcome this problem, we propose an active metric learning framework that allows a human user to solve the problem collaboratively with the machine.  Our approach selects pairs of regions of interest based on an information gain criterion and asks the human user to establish correspondences between them.  Those pairs are subsequently used to update a metric to improve the correspondences. This process continues until the system finds the correct location of the ground level image.  We introduce a new Ground Level to Aerial Image Dataset (GLAID) to assess strengths and weaknesses of our proposed framework. Our experiments show that our system allows user to find the correct location with significantly reduced effort."
Active Metric Learning For Ground Level To Aerial Image Matching,"Image-based geolocation is a challenging problem that has recently captured the interest of computer vision and machine learning researchers.  In this work we focus on the specific problem of matching ground level images to 45 degree aerial images. Matching ground level and aerial images is a very hard problem due to wide disparities in viewpoint and imaging conditions, the combination of which leads to low level feature matching failure.  To overcome this problem, we propose an active metric learning framework that allows a human user to solve the problem collaboratively with the machine.  Our approach selects pairs of regions of interest based on an information gain criterion and asks the human user to establish correspondences between them.  Those pairs are subsequently used to update a metric to improve the correspondences. This process continues until the system finds the correct location of the ground level image.  We introduce a new Ground Level to Aerial Image Dataset (GLAID) to assess strengths and weaknesses of our proposed framework. Our experiments show that our system allows user to find the correct location with significantly reduced effort."
Learning to Align from Scratch,"  Unsupervised joint alignment of images has been demonstrated to  improve performance on recognition tasks such as face verification.  Such alignment reduces undesired variability due to factors such as  pose, while only requiring weak supervision in the form of poorly  aligned examples.  However, prior work on unsupervised alignment of  complex, real world images has required the careful selection of  feature representation based on hand-crafted image descriptors, in  order to achieve an appropriate, smooth optimization landscape.  In this paper, we instead propose a novel combination of  unsupervised joint alignment with unsupervised feature learning.  Specifically, we incorporate deep learning into the {\em congealing}  alignment framework.  Through deep learning, we obtain features that  can represent the image at differing resolutions based on network  depth, and that are tuned to the statistics of the specific data  being aligned.  In addition, we modify the learning algorithm for  the restricted Boltzmann machine by incorporating a group sparsity  penalty, leading to a topographic organization on the learned  filters and improving subsequent alignment results.  We apply our method to the Labeled Faces in the Wild database  (LFW). Using the aligned images produced by our proposed  unsupervised algorithm, we achieve a significantly higher accuracy  in face verification than obtained using the original face images,  prior work in unsupervised alignment, and prior work in supervised  alignment.  We also match the accuracy for the best available, but  unpublished method."
Multiclass Semi-Supervised Learning on Graphs,"We present a graph-based variational algorithm for multiclassclassification of high dimensional data. The variational energy is basedon a diffuse interface model, and we introduce an alternative measure ofsmoothness appropriate for the multiclass segmentation problem. Wedemonstrate that the multiclass diffuse interface model outperformsclassical spectral clustering methods, and that it obtains resultscompetitive with the state of the art among other graph-based algorithms."
Multiclass Semi-Supervised Learning on Graphs,"We present a graph-based variational algorithm for multiclassclassification of high dimensional data. The variational energy is basedon a diffuse interface model, and we introduce an alternative measure ofsmoothness appropriate for the multiclass segmentation problem. Wedemonstrate that the multiclass diffuse interface model outperformsclassical spectral clustering methods, and that it obtains resultscompetitive with the state of the art among other graph-based algorithms."
Regularized Nonnegative Matrix Factorization using Minimum Mean Square Error Estimates under Gaussian Mixture Prior Models with online Learning for the Uncertainties,"We propose a new method to enforce priors on the solution of the nonnegative matrix factorization (NMF). The proposed algorithm can be used for denoising or single-channel source separation (SCSS) applications. The NMF solution is guided to follow the Minimum Mean Square Error (MMSE) estimate under Gaussian mixture prior models (GMM) for the source signal. In SCSS applications, the spectra of the observed mixed signal are decomposed as a weighted linear combination of trained basis vectors for each source using NMF. In this work, the NMF decomposition weight matrices are treated as a distorted image by a distortion operator, which is learned directly from the observed signals. The MMSE estimate of the weights matrix under GMM prior and log-normal distribution for the distribution is then found to improve the NMF decomposition results. The MMSE estimate is embedded within the NMF cost function as a regularized NMF, and their corresponding update rules are driven in this paper. Experimental results show that, the proposed regularized NMF algorithm improves the source separation performance compared with using NMF only."
Regularized Nonnegative Matrix Factorization using Minimum Mean Square Error Estimates under Gaussian Mixture Prior Models with online Learning for the Uncertainties,"We propose a new method to enforce priors on the solution of the nonnegative matrix factorization (NMF). The proposed algorithm can be used for denoising or single-channel source separation (SCSS) applications. The NMF solution is guided to follow the Minimum Mean Square Error (MMSE) estimate under Gaussian mixture prior models (GMM) for the source signal. In SCSS applications, the spectra of the observed mixed signal are decomposed as a weighted linear combination of trained basis vectors for each source using NMF. In this work, the NMF decomposition weight matrices are treated as a distorted image by a distortion operator, which is learned directly from the observed signals. The MMSE estimate of the weights matrix under GMM prior and log-normal distribution for the distribution is then found to improve the NMF decomposition results. The MMSE estimate is embedded within the NMF cost function as a regularized NMF, and their corresponding update rules are driven in this paper. Experimental results show that, the proposed regularized NMF algorithm improves the source separation performance compared with using NMF only."
Diagnosing learners' knowledge from their actions using inverse reinforcement learning,"The use of computerized environments in which students complete complex tasks is increasingly common in education. Data about students' actions in these environments has the potential to provide information about these students' knowledge, but can be difficult to interpret.In this paper, we focus on instances in which a student must take a series of actions to complete a goal, and develop a framework for automatically inferring the student's underlying beliefs based on these observed actions. This framework relies on modeling how student actions follow from beliefs about the effects of those actions. By framing the problem in terms of a Markov decision process, we specify a general model that can be applied to a wide range of situations. We first validate that this model can recover learners' beliefs in a lab experiment, and then use it to model data from an educational game. In the lab experiment, the model's inferences reflect participants' stated beliefs, and for the educational game, the model's inferences are consistent with conventional assessment measures."
Diagnosing learners' knowledge from their actions using inverse reinforcement learning,"The use of computerized environments in which students complete complex tasks is increasingly common in education. Data about students' actions in these environments has the potential to provide information about these students' knowledge, but can be difficult to interpret.In this paper, we focus on instances in which a student must take a series of actions to complete a goal, and develop a framework for automatically inferring the student's underlying beliefs based on these observed actions. This framework relies on modeling how student actions follow from beliefs about the effects of those actions. By framing the problem in terms of a Markov decision process, we specify a general model that can be applied to a wide range of situations. We first validate that this model can recover learners' beliefs in a lab experiment, and then use it to model data from an educational game. In the lab experiment, the model's inferences reflect participants' stated beliefs, and for the educational game, the model's inferences are consistent with conventional assessment measures."
Diagnosing learners' knowledge from their actions using inverse reinforcement learning,"The use of computerized environments in which students complete complex tasks is increasingly common in education. Data about students' actions in these environments has the potential to provide information about these students' knowledge, but can be difficult to interpret.In this paper, we focus on instances in which a student must take a series of actions to complete a goal, and develop a framework for automatically inferring the student's underlying beliefs based on these observed actions. This framework relies on modeling how student actions follow from beliefs about the effects of those actions. By framing the problem in terms of a Markov decision process, we specify a general model that can be applied to a wide range of situations. We first validate that this model can recover learners' beliefs in a lab experiment, and then use it to model data from an educational game. In the lab experiment, the model's inferences reflect participants' stated beliefs, and for the educational game, the model's inferences are consistent with conventional assessment measures."
Adaptive Compressive Network Modeling,"Network data is ubiquitous nowadays, such as social networks, bio-networks, computer networks, and complex information network. However, the research of network data is still mostly heuristic and lacks rigorous theoretical underpinnings. Until very recently, a new line of work named compressive network modeling sheds light on this problem.  Specifically, [13] proposes a general framework to use compressed sensing techniques to analyze network data. This method explores a large latent dictionary which is able to recover the hidden structure within the network. However, one limitation of this framework is that the dictionary used to model the network must be pre-given, e.g. clique spaces. Such a pre-determined dictionary is not adaptive to unknown datasets. In this paper, instead of assuming the dictionary is prefixed, we propose a method to automatically learn a dictionary based on the observed networks, which better adapts to the empirical data. Our approach, named adaptive compressive network modeling, is amenable to theoretical analysis, computationally tractable and can be successfully applied to many areas. "
Adaptive Compressive Network Modeling,"Network data is ubiquitous nowadays, such as social networks, bio-networks, computer networks, and complex information network. However, the research of network data is still mostly heuristic and lacks rigorous theoretical underpinnings. Until very recently, a new line of work named compressive network modeling sheds light on this problem.  Specifically, [13] proposes a general framework to use compressed sensing techniques to analyze network data. This method explores a large latent dictionary which is able to recover the hidden structure within the network. However, one limitation of this framework is that the dictionary used to model the network must be pre-given, e.g. clique spaces. Such a pre-determined dictionary is not adaptive to unknown datasets. In this paper, instead of assuming the dictionary is prefixed, we propose a method to automatically learn a dictionary based on the observed networks, which better adapts to the empirical data. Our approach, named adaptive compressive network modeling, is amenable to theoretical analysis, computationally tractable and can be successfully applied to many areas. "
Adaptive Compressive Network Modeling,"Network data is ubiquitous nowadays, such as social networks, bio-networks, computer networks, and complex information network. However, the research of network data is still mostly heuristic and lacks rigorous theoretical underpinnings. Until very recently, a new line of work named compressive network modeling sheds light on this problem.  Specifically, [13] proposes a general framework to use compressed sensing techniques to analyze network data. This method explores a large latent dictionary which is able to recover the hidden structure within the network. However, one limitation of this framework is that the dictionary used to model the network must be pre-given, e.g. clique spaces. Such a pre-determined dictionary is not adaptive to unknown datasets. In this paper, instead of assuming the dictionary is prefixed, we propose a method to automatically learn a dictionary based on the observed networks, which better adapts to the empirical data. Our approach, named adaptive compressive network modeling, is amenable to theoretical analysis, computationally tractable and can be successfully applied to many areas. "
Adaptive Compressive Network Modeling,"Network data is ubiquitous nowadays, such as social networks, bio-networks, computer networks, and complex information network. However, the research of network data is still mostly heuristic and lacks rigorous theoretical underpinnings. Until very recently, a new line of work named compressive network modeling sheds light on this problem.  Specifically, [13] proposes a general framework to use compressed sensing techniques to analyze network data. This method explores a large latent dictionary which is able to recover the hidden structure within the network. However, one limitation of this framework is that the dictionary used to model the network must be pre-given, e.g. clique spaces. Such a pre-determined dictionary is not adaptive to unknown datasets. In this paper, instead of assuming the dictionary is prefixed, we propose a method to automatically learn a dictionary based on the observed networks, which better adapts to the empirical data. Our approach, named adaptive compressive network modeling, is amenable to theoretical analysis, computationally tractable and can be successfully applied to many areas. "
Online Egocentric Models for Citation Networks,"With the emergence of large-scale longitudinal network data, dynamic network analysis(DNA) has become a very hot research topic in recent years. Although a lot of DNA methods have been proposed by researchers from different communities, most of them can only model snapshot data recorded at a very rough temporal granularity. Recently, a novel method, called dynamic egocentric model(DEM), has been proposed for DNA which can be used to model large-scale citation networks at a fine temporal granularity. However, DEM suffers from a significant decrease of accuracy over time because the learned parameters and topic features of DEM are static (fixed) during the prediction process of time-varying citation networks. In this paper, we propose an online extension of DEM, called online egocentric model (OEM), to learn time-varying parameters and topic features for dynamic citation networks. Experimental results on real-world citation networks show that our OEM can not only prevent the prediction accuracy from decreasing over time but also uncover the evolution of topics in the citation networks."
Online Egocentric Models for Citation Networks,"With the emergence of large-scale longitudinal network data, dynamic network analysis(DNA) has become a very hot research topic in recent years. Although a lot of DNA methods have been proposed by researchers from different communities, most of them can only model snapshot data recorded at a very rough temporal granularity. Recently, a novel method, called dynamic egocentric model(DEM), has been proposed for DNA which can be used to model large-scale citation networks at a fine temporal granularity. However, DEM suffers from a significant decrease of accuracy over time because the learned parameters and topic features of DEM are static (fixed) during the prediction process of time-varying citation networks. In this paper, we propose an online extension of DEM, called online egocentric model (OEM), to learn time-varying parameters and topic features for dynamic citation networks. Experimental results on real-world citation networks show that our OEM can not only prevent the prediction accuracy from decreasing over time but also uncover the evolution of topics in the citation networks."
Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form,"We consider minimizing convex objective functions in \emph{composite form}\begin{align*}  \minimize_{x\in\R^n} f(x) := g(x) + h(x),\end{align*}where $g$ is convex and twice-continuously differentiable and $h:\R^n\to\R$ is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efficiently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. Many problems of relevance in high-dimensional statistics, machine learning, and signal processing can be formulated in composite form. We prove such methods are globally convergent to a minimizer and achieve quadratic rates of convergence in the vicinity of a unique minimizer. We also demonstrate the performance of such methods using problems of relevance in machine learning and high-dimensional statistics."
Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form,"We consider minimizing convex objective functions in \emph{composite form}\begin{align*}  \minimize_{x\in\R^n} f(x) := g(x) + h(x),\end{align*}where $g$ is convex and twice-continuously differentiable and $h:\R^n\to\R$ is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efficiently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. Many problems of relevance in high-dimensional statistics, machine learning, and signal processing can be formulated in composite form. We prove such methods are globally convergent to a minimizer and achieve quadratic rates of convergence in the vicinity of a unique minimizer. We also demonstrate the performance of such methods using problems of relevance in machine learning and high-dimensional statistics."
Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form,"We consider minimizing convex objective functions in \emph{composite form}\begin{align*}  \minimize_{x\in\R^n} f(x) := g(x) + h(x),\end{align*}where $g$ is convex and twice-continuously differentiable and $h:\R^n\to\R$ is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efficiently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. Many problems of relevance in high-dimensional statistics, machine learning, and signal processing can be formulated in composite form. We prove such methods are globally convergent to a minimizer and achieve quadratic rates of convergence in the vicinity of a unique minimizer. We also demonstrate the performance of such methods using problems of relevance in machine learning and high-dimensional statistics."
Multi-criteria Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria."
Multi-criteria Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria."
Multi-criteria Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria."
Multi-criteria Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria."
Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes,"To learn reliable rules that can generalize to novel situations, the brain must be capable of imposing some form of regularization. Here we suggest, through theoretical and computational arguments, that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system. The functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise. Noise on the inputs is shown to impose regularization, and when synchronization upstream induces time-varying correlations across noise variables, the degree of regularization can be calibrated over time. The resulting qualitative behavior matches experimental data from visual cortex."
Max-Margin Transforms for Visual Domain Adaptation,"We present a new algorithm for training linear support vector machine classifiers across image domains. Our algorithm learns a linear transformation that maps points from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce a novel cost function for transformation learning based on the misclassification loss of the target points transformed into the source domain. One advantage of our method over previous SVM-based domain adaptation algorithms is that it performs multi-task adaptation, learning a shared component of the domain shift across all categories.  Experiments on both synthetic data and real image datasets demonstrate strong performance and computational advantages compared to previous approaches."
Max-Margin Transforms for Visual Domain Adaptation,"We present a new algorithm for training linear support vector machine classifiers across image domains. Our algorithm learns a linear transformation that maps points from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce a novel cost function for transformation learning based on the misclassification loss of the target points transformed into the source domain. One advantage of our method over previous SVM-based domain adaptation algorithms is that it performs multi-task adaptation, learning a shared component of the domain shift across all categories.  Experiments on both synthetic data and real image datasets demonstrate strong performance and computational advantages compared to previous approaches."
Max-Margin Transforms for Visual Domain Adaptation,"We present a new algorithm for training linear support vector machine classifiers across image domains. Our algorithm learns a linear transformation that maps points from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce a novel cost function for transformation learning based on the misclassification loss of the target points transformed into the source domain. One advantage of our method over previous SVM-based domain adaptation algorithms is that it performs multi-task adaptation, learning a shared component of the domain shift across all categories.  Experiments on both synthetic data and real image datasets demonstrate strong performance and computational advantages compared to previous approaches."
Max-Margin Transforms for Visual Domain Adaptation,"We present a new algorithm for training linear support vector machine classifiers across image domains. Our algorithm learns a linear transformation that maps points from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce a novel cost function for transformation learning based on the misclassification loss of the target points transformed into the source domain. One advantage of our method over previous SVM-based domain adaptation algorithms is that it performs multi-task adaptation, learning a shared component of the domain shift across all categories.  Experiments on both synthetic data and real image datasets demonstrate strong performance and computational advantages compared to previous approaches."
Fast Exact MAP Inference by Passing Incomplete Messages,"We propose a novel approach to faster exact MAP inference that builds on max-product (min-sum) message passing on clique trees and exploits the branch-and-bound idea. The high level procedure is to propagate incomplete messages over the clique tree while maintaining local upper bounds at sepsets. Our algorithm is guaranteed to converge and find the global optimal solution. Empirically we show that our method consistently demonstrates large savings over state-of-the-art methods on several different models with both synthetic and real world data. Our approach also suggests an interesting connection between exact and approximate MAP inference: If we can find tighter relaxations (over sub-graphs), we can make use of the lower bounds to perform exact inference even faster."
Fast Exact MAP Inference by Passing Incomplete Messages,"We propose a novel approach to faster exact MAP inference that builds on max-product (min-sum) message passing on clique trees and exploits the branch-and-bound idea. The high level procedure is to propagate incomplete messages over the clique tree while maintaining local upper bounds at sepsets. Our algorithm is guaranteed to converge and find the global optimal solution. Empirically we show that our method consistently demonstrates large savings over state-of-the-art methods on several different models with both synthetic and real world data. Our approach also suggests an interesting connection between exact and approximate MAP inference: If we can find tighter relaxations (over sub-graphs), we can make use of the lower bounds to perform exact inference even faster."
Emergence of Flexible Prediction-Based Discrete Decision Making and Continuous Motion Generation through Actor-Q-Learning,"In this paper, through the learning of invisible-target-capturing task by Actor-Q-learning with a recurrent neural network (RNN), the followings are shown.(1) Prediction-based continuous motions that should be varied complicatedly by several factors can be acquired only from rewards and punishments through reinforcement learning (RL) with a RNN.(2) Actor-Q-learning that is a RL method for both discrete decision(action) making and continuous motion generation works in a task other than active perception-and-recognition. It can also learn prediction-required action and motion by using a RNN.(3) As initial connection weights in RNN, local and regular connection and positive self-feedback connection improve the performance of learning with vision-like sensor inputs.(4) Two problem-solving strategies one of which is selectively used can be acquired in just one neural network as a parallel processing-and-learning system only from rewards and punishments through RL. The strategy-switching caused by situation changes also emerges without any explicit switching element or any prior knowledge."
Efficient Random Walk with Gaussian Kernels,"Implicit manifolds is a technique used with random walk-based semi-supervised learning and graph clustering methods to implicitly construct a dense similarity matrix, reducing the cost of manipulations on the matrix from $O(n^2)$ to linear. Specifically, propagating labels through an $O(n^2)$ matrix can be replaced with propagating through a series of sparse matrices. While similarity functions such as cosine similarity are easily ``plugged into'' the implicit manifold framework, it is not straightforward to integrate the Gaussian kernel. In this paper we propose two methods to do this and provide experimental results."
Efficient Random Walk with Gaussian Kernels,"Implicit manifolds is a technique used with random walk-based semi-supervised learning and graph clustering methods to implicitly construct a dense similarity matrix, reducing the cost of manipulations on the matrix from $O(n^2)$ to linear. Specifically, propagating labels through an $O(n^2)$ matrix can be replaced with propagating through a series of sparse matrices. While similarity functions such as cosine similarity are easily ``plugged into'' the implicit manifold framework, it is not straightforward to integrate the Gaussian kernel. In this paper we propose two methods to do this and provide experimental results."
Searching for objects driven by context,"The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired.We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image. Their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set.In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy."
Timely Object Recognition,"In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the eminent PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains $66\%$ better AP than a random ordering, and $14\%$ better performance than an intelligent baseline. On the timeliness measure, our method obtains at least $11\%$ better performance. Our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning."
Timely Object Recognition,"In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the eminent PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains $66\%$ better AP than a random ordering, and $14\%$ better performance than an intelligent baseline. On the timeliness measure, our method obtains at least $11\%$ better performance. Our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning."
Timely Object Recognition,"In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the eminent PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains $66\%$ better AP than a random ordering, and $14\%$ better performance than an intelligent baseline. On the timeliness measure, our method obtains at least $11\%$ better performance. Our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning."
A New Fast Stochastic Bayesian Learning Automata: a Machine Learning Perspective,"One of the drawbacks of Learning Automata is having a relatively slow rate of convergence, thus the main challenge of Learning Automata theory is designing faster learning algorithms. In this paper, we propose a new fast learning algorithm from a machine learning perspective. The key idea is that the estimator which estimates the probability of stochastic environment rewarding each action, is considered as reconstructing Bernoulli distribution from sequential data, and is formalized based on exponential conjugate family which enables our designed Learning Automata having relatively simple format and hence easy to be implemented. We emphasize that this approach is quite generous and applicable to existing estimator based Learning Automata. Meanwhile,the -optimality of the proposed Learn-ing Automata referred to as Generalized Bayesian Stochastic Estimator Learning Automata is also presented. Extensive experimental results on benchmark environments demonstrate our proposed learning scheme is faster than the LA state of the art."
Fast variational inference for stochastic differential equations,We introduce a Gaussian variational mean field approximation for inference in continuous time stochastic differential equations. This approach allows us to express the variational free energy as a functional of the marginal moments of the approximating Gaussian process. A restriction of moments to piecewise polynomial functions over time makes the complexity of approximate inference for stochastic differential equation models comparable to that of  discrete time hidden Markov models. We demonstrate the algorithm on state and parameter estimation for nonlinear problems with up to forty state variables. 
Fast variational inference for stochastic differential equations,We introduce a Gaussian variational mean field approximation for inference in continuous time stochastic differential equations. This approach allows us to express the variational free energy as a functional of the marginal moments of the approximating Gaussian process. A restriction of moments to piecewise polynomial functions over time makes the complexity of approximate inference for stochastic differential equation models comparable to that of  discrete time hidden Markov models. We demonstrate the algorithm on state and parameter estimation for nonlinear problems with up to forty state variables. 
Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states."
Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states."
Learning with Marginalized Corrupted Features,"An important goal of machine learning is to develop predictors that are robust to noise in the observations. In this paper, we consider a particular type of observation noise in which features are ``blanked out'' with some probability. Such blank-out noise occurs, \emph{e.g.} when sensors measuring features (temporarily) break down or when particular words related to a topic are not observed in a document. A simple way to train predictors that are robust to such noise is to extend the training data with training examples in which some of the variables are blanked out at random, but such an approach is computationally costly. This paper presents a new approach, called \emph{marginalized corrupted features} (MCF), that trains robust predictors by minimizing the expected value of the loss function under the blank-out noise model. Experimental evaluation of our approach reveals that the resulting predictors are not only more robust to sensors breaking down, but that they also perform substantially better on data with high-dimensional, heavy-tailed features, such as bag-of-words text documents. "
Learning with Marginalized Corrupted Features,"An important goal of machine learning is to develop predictors that are robust to noise in the observations. In this paper, we consider a particular type of observation noise in which features are ``blanked out'' with some probability. Such blank-out noise occurs, \emph{e.g.} when sensors measuring features (temporarily) break down or when particular words related to a topic are not observed in a document. A simple way to train predictors that are robust to such noise is to extend the training data with training examples in which some of the variables are blanked out at random, but such an approach is computationally costly. This paper presents a new approach, called \emph{marginalized corrupted features} (MCF), that trains robust predictors by minimizing the expected value of the loss function under the blank-out noise model. Experimental evaluation of our approach reveals that the resulting predictors are not only more robust to sensors breaking down, but that they also perform substantially better on data with high-dimensional, heavy-tailed features, such as bag-of-words text documents. "
Accuracy at the Top,"We introduce a new notion of classification accuracy based on the top $\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines. We define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems. We also present margin-based guarantees for this algorithm based on the $\tau$-quantile of the functions in the hypothesis set. Finally, we report the results of several experiments evaluating the performance of our algorithm. In a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top."
Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
Near-optimal Differentially Private Principal Components,"Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension.  Many current data sets of interest contain private or sensitive information about individuals.  Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs.  Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs.  In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output.  We demonstrate that on real data, there this a large performance gap between the existing methods and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling."
Near-optimal Differentially Private Principal Components,"Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension.  Many current data sets of interest contain private or sensitive information about individuals.  Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs.  Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs.  In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output.  We demonstrate that on real data, there this a large performance gap between the existing methods and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling."
Graph Estimation From Multi-attribute Data,"Many real world network problems often concern multivariate nodal  attributes such as image, textual, and multi-view feature vectors on  nodes, rather than simple univariate nodal attributes. The existing  graph estimation methods built on Gaussian graphical models and  covariance selection algorithms can not handle such data, neither can  the theories developed around such methods be directly  applied. In this paper, we propose a new principled framework for  estimating multi-attribute networks. Instead of estimating the  partial correlation as in current literature, our method estimates  the {\it partial canonical correlations} that naturally accommodate  complex nodal features.  Computationally, we provide an efficient  algorithm which utilizes the multi-attribue  structure. Theoretically, we provide sufficient conditions which  guarantee consistent graph recovery. Empirically, we apply our  method on a genomic dataset to illustrate its usefulness. "
Graph Estimation From Multi-attribute Data,"Many real world network problems often concern multivariate nodal  attributes such as image, textual, and multi-view feature vectors on  nodes, rather than simple univariate nodal attributes. The existing  graph estimation methods built on Gaussian graphical models and  covariance selection algorithms can not handle such data, neither can  the theories developed around such methods be directly  applied. In this paper, we propose a new principled framework for  estimating multi-attribute networks. Instead of estimating the  partial correlation as in current literature, our method estimates  the {\it partial canonical correlations} that naturally accommodate  complex nodal features.  Computationally, we provide an efficient  algorithm which utilizes the multi-attribue  structure. Theoretically, we provide sufficient conditions which  guarantee consistent graph recovery. Empirically, we apply our  method on a genomic dataset to illustrate its usefulness. "
Recovering Block-structured Activations Using Compressive Measurements ,"We consider the problem of detection and localization of a small block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. However, contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that in our problem the magnitude of the weakest signals one can reliably localize is strongly influenced by both structure and the ability to choose measurements adaptively. We derive tight upper and lower bounds for the detection and estimation problems, under both adaptive and non-adaptive measurement schemes. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation."
Recovering Block-structured Activations Using Compressive Measurements ,"We consider the problem of detection and localization of a small block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. However, contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that in our problem the magnitude of the weakest signals one can reliably localize is strongly influenced by both structure and the ability to choose measurements adaptively. We derive tight upper and lower bounds for the detection and estimation problems, under both adaptive and non-adaptive measurement schemes. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation."
Recovering Block-structured Activations Using Compressive Measurements ,"We consider the problem of detection and localization of a small block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. However, contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that in our problem the magnitude of the weakest signals one can reliably localize is strongly influenced by both structure and the ability to choose measurements adaptively. We derive tight upper and lower bounds for the detection and estimation problems, under both adaptive and non-adaptive measurement schemes. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation."
Recovering Block-structured Activations Using Compressive Measurements ,"We consider the problem of detection and localization of a small block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. However, contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that in our problem the magnitude of the weakest signals one can reliably localize is strongly influenced by both structure and the ability to choose measurements adaptively. We derive tight upper and lower bounds for the detection and estimation problems, under both adaptive and non-adaptive measurement schemes. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation."
Recovering Block-structured Activations Using Compressive Measurements ,"We consider the problem of detection and localization of a small block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. However, contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that in our problem the magnitude of the weakest signals one can reliably localize is strongly influenced by both structure and the ability to choose measurements adaptively. We derive tight upper and lower bounds for the detection and estimation problems, under both adaptive and non-adaptive measurement schemes. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation."
Classification with Asymmetric Label Noise,"We consider the problem of binary classification when the training labels are noisy. Previous theoretical work on this problem assumes that the two classes are separable, or that the label noise is independent of the class label. We present a general framework that allows for overlapping class-conditional distributions (so that the true labels are not deterministic) and that accommodates asymmetric label noise. This problem is motivated by several applications including nuclear particle classification, where background radiation and other environmental factors make it impossible to obtain pure training samples. We first identify a necessary and sufficient condition that makes the unknown noise proportions identifiable. Under this assumption, we describe estimation of the Type I and Type II errors, and use these estimates to design a classification rule that is consistent with respect to the minmax criterion. Our approach depends critically on recent results for mixture proportion estimation, which is the problem of estimating the percentage of one distribution that is present in another."
Classification with Asymmetric Label Noise,"We consider the problem of binary classification when the training labels are noisy. Previous theoretical work on this problem assumes that the two classes are separable, or that the label noise is independent of the class label. We present a general framework that allows for overlapping class-conditional distributions (so that the true labels are not deterministic) and that accommodates asymmetric label noise. This problem is motivated by several applications including nuclear particle classification, where background radiation and other environmental factors make it impossible to obtain pure training samples. We first identify a necessary and sufficient condition that makes the unknown noise proportions identifiable. Under this assumption, we describe estimation of the Type I and Type II errors, and use these estimates to design a classification rule that is consistent with respect to the minmax criterion. Our approach depends critically on recent results for mixture proportion estimation, which is the problem of estimating the percentage of one distribution that is present in another."
Classification with Asymmetric Label Noise,"We consider the problem of binary classification when the training labels are noisy. Previous theoretical work on this problem assumes that the two classes are separable, or that the label noise is independent of the class label. We present a general framework that allows for overlapping class-conditional distributions (so that the true labels are not deterministic) and that accommodates asymmetric label noise. This problem is motivated by several applications including nuclear particle classification, where background radiation and other environmental factors make it impossible to obtain pure training samples. We first identify a necessary and sufficient condition that makes the unknown noise proportions identifiable. Under this assumption, we describe estimation of the Type I and Type II errors, and use these estimates to design a classification rule that is consistent with respect to the minmax criterion. Our approach depends critically on recent results for mixture proportion estimation, which is the problem of estimating the percentage of one distribution that is present in another."
Classification with Asymmetric Label Noise,"We consider the problem of binary classification when the training labels are noisy. Previous theoretical work on this problem assumes that the two classes are separable, or that the label noise is independent of the class label. We present a general framework that allows for overlapping class-conditional distributions (so that the true labels are not deterministic) and that accommodates asymmetric label noise. This problem is motivated by several applications including nuclear particle classification, where background radiation and other environmental factors make it impossible to obtain pure training samples. We first identify a necessary and sufficient condition that makes the unknown noise proportions identifiable. Under this assumption, we describe estimation of the Type I and Type II errors, and use these estimates to design a classification rule that is consistent with respect to the minmax criterion. Our approach depends critically on recent results for mixture proportion estimation, which is the problem of estimating the percentage of one distribution that is present in another."
Mining the brain with a theory of visual attention,"We propose a new target objective for BCI systems in which a parametric model of early visual perception follows the EEG decoder stage. This approach enables the supervised extraction of EEG components that jointly predict behavioral responses. We analyze the pre-stimulus EEG activity from a letter-recognition task using two EEG decoders running in stereo, and detect distinct components of the EEG that predict separable attentional parameters on a single-trial level."
Mining the brain with a theory of visual attention,"We propose a new target objective for BCI systems in which a parametric model of early visual perception follows the EEG decoder stage. This approach enables the supervised extraction of EEG components that jointly predict behavioral responses. We analyze the pre-stimulus EEG activity from a letter-recognition task using two EEG decoders running in stereo, and detect distinct components of the EEG that predict separable attentional parameters on a single-trial level."
Predicting a Neural Spiking Probability Map,"This paper models signals and noise for extracellular neural recording. Although recorded data approximately follow Gaussian distribution, there are slight deviations that are critical for signal detection: a statistical examination of neural data in Hilbert space shows that noise forms an exponential term while signals form a polynomial term. These two terms can be used to estimate a spiking probability map which tells the probability of spike presence in any time window. The predictions of the two terms and the spiking probability map are quantitatively assessed with both animal and synthesized data. To demonstrate the usefulness of the work, a few application examples are presented with quantitative experimental results that have shown improved signal processing reliability. Last, we report an algorithm implementation in a 130nm CMOS process for on-the-fly processing multi-channel neural data."
Predicting a Neural Spiking Probability Map,"This paper models signals and noise for extracellular neural recording. Although recorded data approximately follow Gaussian distribution, there are slight deviations that are critical for signal detection: a statistical examination of neural data in Hilbert space shows that noise forms an exponential term while signals form a polynomial term. These two terms can be used to estimate a spiking probability map which tells the probability of spike presence in any time window. The predictions of the two terms and the spiking probability map are quantitatively assessed with both animal and synthesized data. To demonstrate the usefulness of the work, a few application examples are presented with quantitative experimental results that have shown improved signal processing reliability. Last, we report an algorithm implementation in a 130nm CMOS process for on-the-fly processing multi-channel neural data."
Predicting a Neural Spiking Probability Map,"This paper models signals and noise for extracellular neural recording. Although recorded data approximately follow Gaussian distribution, there are slight deviations that are critical for signal detection: a statistical examination of neural data in Hilbert space shows that noise forms an exponential term while signals form a polynomial term. These two terms can be used to estimate a spiking probability map which tells the probability of spike presence in any time window. The predictions of the two terms and the spiking probability map are quantitatively assessed with both animal and synthesized data. To demonstrate the usefulness of the work, a few application examples are presented with quantitative experimental results that have shown improved signal processing reliability. Last, we report an algorithm implementation in a 130nm CMOS process for on-the-fly processing multi-channel neural data."
Predicting a Neural Spiking Probability Map,"This paper models signals and noise for extracellular neural recording. Although recorded data approximately follow Gaussian distribution, there are slight deviations that are critical for signal detection: a statistical examination of neural data in Hilbert space shows that noise forms an exponential term while signals form a polynomial term. These two terms can be used to estimate a spiking probability map which tells the probability of spike presence in any time window. The predictions of the two terms and the spiking probability map are quantitatively assessed with both animal and synthesized data. To demonstrate the usefulness of the work, a few application examples are presented with quantitative experimental results that have shown improved signal processing reliability. Last, we report an algorithm implementation in a 130nm CMOS process for on-the-fly processing multi-channel neural data."
Symmetry Detection by Distributed Synchrony of Spiking VLSI Neurons,"The detection of geometrical symmetries in visual scenes plays a key role in both animal perception and machine vision. Such types of sophisticated pattern detection can be obtained via spike-to-spike synchrony in recurrent networks of Integrate & Fire (I&F) neurons. To determine the network properties and the conditions required for synchronization we apply a formal contraction theory analysis using weakly coupled oscillator models and show how, under these conditions, the stability of the synchronous state can be guaranteed. These conditions can be experimentally verified through the measurement of the Phase Response Curve (PRC). To demonstrate the reliability of the method and its robustness to noise and parameter variability we used it to implement a bilateral symmetry detection network in analog/digital neuromorphic hardware and applied the system to real-time symmetry detection in response to real-world sensory data, provided by an event-based silicon retina. The silicon neurons quickly synchronize when a symmetric object is aligned with respect to the scene vertical axis. The presence of a symmetric input stimulus is reported by a separate read-out network of I&F neurons used as coincidence detectors. Our results demonstrate how this theory can be used to successfully configure low-power neuromorphic system for robust real-time pattern detection, making a central use of precise spike timing to detect user-specified symmetries present in visual scenes."
Delay Compensation with Dynamical Synapses,"Time delay is pervasive in neural information processing. To achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system. In the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner. The state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents. The parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network. We also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude."
A Gaussian Latent Variable Model for Ranking,"We describe a Gaussian latent variable model for ranking. The model learns a linear scoring function to maximize the probability that randomly chosen pairs of examples are correctly ranked. We show how to perform inference in this model and derive the Expectation-Maximization (EM) algorithm that monotonically increases the likelihood of correct ranking. We also highlight the intuitive form of the EM algorithm: at each iteration, the weight vector is re-estimated by a simple least-squares update. Finally, we explore two extensions of the model, based on kernels and boosting, to learn nonlinear ranking functions. The model?s effectiveness is demonstrated on problems in AUC maximization and information retrieval."
A Gaussian Latent Variable Model for Ranking,"We describe a Gaussian latent variable model for ranking. The model learns a linear scoring function to maximize the probability that randomly chosen pairs of examples are correctly ranked. We show how to perform inference in this model and derive the Expectation-Maximization (EM) algorithm that monotonically increases the likelihood of correct ranking. We also highlight the intuitive form of the EM algorithm: at each iteration, the weight vector is re-estimated by a simple least-squares update. Finally, we explore two extensions of the model, based on kernels and boosting, to learn nonlinear ranking functions. The model?s effectiveness is demonstrated on problems in AUC maximization and information retrieval."
Cost-Sensitive Trees of Classifiers,"Recently, machine learning algorithms have started to successfully enter large-scale real-world industrial applications. In these settings, test-time CPU usage needs to be budgeted and accounted for. Addressing the trade-off between classifier accuracy and test-time cost in a principled fashion has become a major challenge for machine learning. This test-time cost consists of classifier evaluation time and feature extraction time, with the latter varying dramatically from feature to feature. In this paper, we propose a meta-learning algorithm that learns a tree of classifiers. Test-inputs traverse the tree along different paths and features are only extracted for subsets of inputs where they are beneficial. Experimental results on a real-world data set demonstrates that our algorithm significantly improves over the current state-of-the-art in test-time cost-sensitive learning. "
Cost-Sensitive Trees of Classifiers,"Recently, machine learning algorithms have started to successfully enter large-scale real-world industrial applications. In these settings, test-time CPU usage needs to be budgeted and accounted for. Addressing the trade-off between classifier accuracy and test-time cost in a principled fashion has become a major challenge for machine learning. This test-time cost consists of classifier evaluation time and feature extraction time, with the latter varying dramatically from feature to feature. In this paper, we propose a meta-learning algorithm that learns a tree of classifiers. Test-inputs traverse the tree along different paths and features are only extracted for subsets of inputs where they are beneficial. Experimental results on a real-world data set demonstrates that our algorithm significantly improves over the current state-of-the-art in test-time cost-sensitive learning. "
Cost-Sensitive Trees of Classifiers,"Recently, machine learning algorithms have started to successfully enter large-scale real-world industrial applications. In these settings, test-time CPU usage needs to be budgeted and accounted for. Addressing the trade-off between classifier accuracy and test-time cost in a principled fashion has become a major challenge for machine learning. This test-time cost consists of classifier evaluation time and feature extraction time, with the latter varying dramatically from feature to feature. In this paper, we propose a meta-learning algorithm that learns a tree of classifiers. Test-inputs traverse the tree along different paths and features are only extracted for subsets of inputs where they are beneficial. Experimental results on a real-world data set demonstrates that our algorithm significantly improves over the current state-of-the-art in test-time cost-sensitive learning. "
Cost-Sensitive Trees of Classifiers,"Recently, machine learning algorithms have started to successfully enter large-scale real-world industrial applications. In these settings, test-time CPU usage needs to be budgeted and accounted for. Addressing the trade-off between classifier accuracy and test-time cost in a principled fashion has become a major challenge for machine learning. This test-time cost consists of classifier evaluation time and feature extraction time, with the latter varying dramatically from feature to feature. In this paper, we propose a meta-learning algorithm that learns a tree of classifiers. Test-inputs traverse the tree along different paths and features are only extracted for subsets of inputs where they are beneficial. Experimental results on a real-world data set demonstrates that our algorithm significantly improves over the current state-of-the-art in test-time cost-sensitive learning. "
Recognizing Activities by Attribute Dynamics ,"The problem of modeling the dynamic structure of the attributes of human activities is considered. Video is first represented in a semantic feature space, where each feature encodes the probability of occurrence of an action attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining a binary observation variable with a hidden Gauss-Markov state process. In this way, it combines the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by the popular dynamic texture method for LDS learning, is proposed. A similarity measure between BDSs, which generalizes the Binet-Cauchy LDS kernel, is then introduced and used to design activity classifiers. These are shown to outperform similar classifiers derived from the kernel-LDS and state-of-the-art approaches to dynamics-based or attribute-based action recognition."
Recognizing Activities by Attribute Dynamics ,"The problem of modeling the dynamic structure of the attributes of human activities is considered. Video is first represented in a semantic feature space, where each feature encodes the probability of occurrence of an action attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining a binary observation variable with a hidden Gauss-Markov state process. In this way, it combines the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by the popular dynamic texture method for LDS learning, is proposed. A similarity measure between BDSs, which generalizes the Binet-Cauchy LDS kernel, is then introduced and used to design activity classifiers. These are shown to outperform similar classifiers derived from the kernel-LDS and state-of-the-art approaches to dynamics-based or attribute-based action recognition."
Training sparse natural image models with a fast Gibbs sampler of an extended state space,"We present a new learning strategy based on an efficient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models."
A Mixed-Membership Model for Learning Genomic Subtype Signatures,We address the problem of identification of genomic signatures from mixed tumor samples. Most methods for identifying tumors on the basis of their genomic mutations are all-or-none classifiers whereas real tumors are complex mixtures. We present a method for identifying sparse subtype signatures from mixed samples using a hierarchical Bayesian model. We apply this method to identify signatures for subtypes of glioblastoma from RNA expression data obtained as part of the Cancer Genome Atlas (TCGA) project and find one subtype is associated with genes involved in DCX-mediated invasion and another subtype is associated with high POSTN expression and mesenchymal-like tumors.
GenDeR: A Generic Diversified Ranking Algorithm,"Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling,product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a wide range of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm."
Shape Priors for Weakly Labeled Segmentation,"In this paper we tackle the problem of weakly labeled image segmentation.  Towards this goal, we propose a novel generative model of segmentation based on transformed hierarchical Pitman-Yor processes, where we augment each object class with a shape prior.  Our model exploits weakly label data, as it does not require a training set composed of  pixel-wise annotations. Instead, it learns appearance models for each object using  as labels only bounding boxes around the object of interest as well as a  shape prior. We demonstrate the effectiveness of our approach on the PASCAL 2010 dataset and show that we outperformed a set of baselines, improving $8\%$ absolute error over the unsupervised version of our model as well as $9\%$ over the detector, which is theinput to our approach. Importantly, our approach performs similarly to fully-supervised approaches.  "
Shape Priors for Weakly Labeled Segmentation,"In this paper we tackle the problem of weakly labeled image segmentation.  Towards this goal, we propose a novel generative model of segmentation based on transformed hierarchical Pitman-Yor processes, where we augment each object class with a shape prior.  Our model exploits weakly label data, as it does not require a training set composed of  pixel-wise annotations. Instead, it learns appearance models for each object using  as labels only bounding boxes around the object of interest as well as a  shape prior. We demonstrate the effectiveness of our approach on the PASCAL 2010 dataset and show that we outperformed a set of baselines, improving $8\%$ absolute error over the unsupervised version of our model as well as $9\%$ over the detector, which is theinput to our approach. Importantly, our approach performs similarly to fully-supervised approaches.  "
Shape Priors for Weakly Labeled Segmentation,"In this paper we tackle the problem of weakly labeled image segmentation.  Towards this goal, we propose a novel generative model of segmentation based on transformed hierarchical Pitman-Yor processes, where we augment each object class with a shape prior.  Our model exploits weakly label data, as it does not require a training set composed of  pixel-wise annotations. Instead, it learns appearance models for each object using  as labels only bounding boxes around the object of interest as well as a  shape prior. We demonstrate the effectiveness of our approach on the PASCAL 2010 dataset and show that we outperformed a set of baselines, improving $8\%$ absolute error over the unsupervised version of our model as well as $9\%$ over the detector, which is theinput to our approach. Importantly, our approach performs similarly to fully-supervised approaches.  "
Bayesian Sparse Partial Least Squares,"Born in the bosom of the chemometrics discipline, partial least squares (PLS) is a class of methods that makes use of a set of latent or unobserved variables to model the relation between (typically) two sets of input and output variables, respectively. Several flavors, consisting on how the latent variables or components are computed, have been developed over the last years. In this paper, we propose a Bayesian formulation of PLS along with some extensions. In a nutshell, we provide sparsity at the input space level and an automatic estimation of the optimal number of latent components. We follow the variational approach to infer the parameter distributions. We have successfully tested the proposed methods on electrocorticogram data associated with several motor outputs in monkeys."
Bayesian Sparse Partial Least Squares,"Born in the bosom of the chemometrics discipline, partial least squares (PLS) is a class of methods that makes use of a set of latent or unobserved variables to model the relation between (typically) two sets of input and output variables, respectively. Several flavors, consisting on how the latent variables or components are computed, have been developed over the last years. In this paper, we propose a Bayesian formulation of PLS along with some extensions. In a nutshell, we provide sparsity at the input space level and an automatic estimation of the optimal number of latent components. We follow the variational approach to infer the parameter distributions. We have successfully tested the proposed methods on electrocorticogram data associated with several motor outputs in monkeys."
Sampling with Deterministic Constraints,"Deterministic and near-deterministic relationships among subsets of variables in multivariate systems are known to causeserious problems for Monte Carlo algorithms. We examine the family of problems in whichthe relationship $Z = f(X_1,\ldots,X_k)$ holds and we wish to obtain exact samples from the conditional distribution $P(X_1,\ldots,X_k\mid Z= z)$. We begin with the case where $f$ is addition,showing that the problem is NP-hard even when the $X_i$s are independent and each has only two possible values.In more restricted cases---for example, i.i.d. Boolean or uniform continuous $X_i$s---efficient exact samplers have been obtained previously.For the case where each $X_i$ has a bounded range of integer values, we derive an $O(k)$ dynamic programming algorithm called {\em exact constrained sequential sampling} (ECSS).For the more general, continuous case, we propose a {\em dynamic scaling} algorithm (DYSC),a form of importance sampling. We evaluate these algorithms on several examplesand derive generalized forms that operate with any function $f$ that satisfies certain natural conditions."
Sampling with Deterministic Constraints,"Deterministic and near-deterministic relationships among subsets of variables in multivariate systems are known to causeserious problems for Monte Carlo algorithms. We examine the family of problems in whichthe relationship $Z = f(X_1,\ldots,X_k)$ holds and we wish to obtain exact samples from the conditional distribution $P(X_1,\ldots,X_k\mid Z= z)$. We begin with the case where $f$ is addition,showing that the problem is NP-hard even when the $X_i$s are independent and each has only two possible values.In more restricted cases---for example, i.i.d. Boolean or uniform continuous $X_i$s---efficient exact samplers have been obtained previously.For the case where each $X_i$ has a bounded range of integer values, we derive an $O(k)$ dynamic programming algorithm called {\em exact constrained sequential sampling} (ECSS).For the more general, continuous case, we propose a {\em dynamic scaling} algorithm (DYSC),a form of importance sampling. We evaluate these algorithms on several examplesand derive generalized forms that operate with any function $f$ that satisfies certain natural conditions."
Bidirectional Noisy-Max Model,"Noisy-Or/Max models describe relations between graded variables in asymmetric directed relations, where multiple variables have effect on one variable through a Or/Max gate. However, it is common in real life that such relations exist in symmetric correlations. This paper presents a bidirectional noisy-Max model to represent it. The model is established on neighboring relations between variables. It shows how to obtain an exact joint distribution in the model. It also presents an inference algorithm based on message passing to compute marginal distributions of variables efficiently."
Modeling Human-Object Interactions for Action Recognition in Real-World Videos,"This paper deals with the interesting problem of recognizing human actions in real-world videos. Such videos usually present large variation in background and camera motion, which makes the performance of low-level appearance and motion features unsatisfactory, particularly in the case of video classes sharing similar objects and background (e.g. ``snatch'' and ``clean-jerk'' weightlifting actions). In this paper, we tackle the problem through representation of action classes as human and object interactions (HOI). HOI is modeled as the spatio-temporal relationship between human and object tracks along with their appearance descriptions in a video. However, such a representation requires accurate detection of human and object tracks. This is a difficult task in its own right when dealt separately. We address the issue by extracting candidate tracks from a video and modeling the choice of correct tracks as latent variables in a latent SVM framework. This formulation enables the task of HOI modeling and action recognition without accurate initialization of human and object tracks. We demonstrate promising action classification results on the challenging Olympic Sports [1] and TRECVID11-MED [2] datasets, where our method outperforms state-of-the-art approaches."
Modeling Human-Object Interactions for Action Recognition in Real-World Videos,"This paper deals with the interesting problem of recognizing human actions in real-world videos. Such videos usually present large variation in background and camera motion, which makes the performance of low-level appearance and motion features unsatisfactory, particularly in the case of video classes sharing similar objects and background (e.g. ``snatch'' and ``clean-jerk'' weightlifting actions). In this paper, we tackle the problem through representation of action classes as human and object interactions (HOI). HOI is modeled as the spatio-temporal relationship between human and object tracks along with their appearance descriptions in a video. However, such a representation requires accurate detection of human and object tracks. This is a difficult task in its own right when dealt separately. We address the issue by extracting candidate tracks from a video and modeling the choice of correct tracks as latent variables in a latent SVM framework. This formulation enables the task of HOI modeling and action recognition without accurate initialization of human and object tracks. We demonstrate promising action classification results on the challenging Olympic Sports [1] and TRECVID11-MED [2] datasets, where our method outperforms state-of-the-art approaches."
Modeling Human-Object Interactions for Action Recognition in Real-World Videos,"This paper deals with the interesting problem of recognizing human actions in real-world videos. Such videos usually present large variation in background and camera motion, which makes the performance of low-level appearance and motion features unsatisfactory, particularly in the case of video classes sharing similar objects and background (e.g. ``snatch'' and ``clean-jerk'' weightlifting actions). In this paper, we tackle the problem through representation of action classes as human and object interactions (HOI). HOI is modeled as the spatio-temporal relationship between human and object tracks along with their appearance descriptions in a video. However, such a representation requires accurate detection of human and object tracks. This is a difficult task in its own right when dealt separately. We address the issue by extracting candidate tracks from a video and modeling the choice of correct tracks as latent variables in a latent SVM framework. This formulation enables the task of HOI modeling and action recognition without accurate initialization of human and object tracks. We demonstrate promising action classification results on the challenging Olympic Sports [1] and TRECVID11-MED [2] datasets, where our method outperforms state-of-the-art approaches."
Learning curves for multi-task Gaussian process regression,"  We study the average case performance of multi-task Gaussian process (GP)  regression as captured in the learning curve, i.e.\ the average Bayes error  for a chosen task versus the total number of examples $n$ for all  tasks. For GP covariances that are the product of an  input-dependent covariance function and a free-form inter-task  covariance matrix, we  show that accurate approximations for the learning curve can be  obtainedfor an arbitrary number of tasks $T$.  We use  these to study the asymptotic learning behaviour for large  $n$. Surprisingly, multi-task learning can be asymptotically essentially  useless: examples from other tasks only help when the  degree of inter-task correlation, $\rho$, is near its maximal value  $\rho=1$. This effect is most extreme for learning of smooth target  functions as described by e.g.\ squared exponential kernels. We also  demonstrate that when learning {\em many} tasks, the learning curves  separate into an initial phase, where the Bayes error on each task  is reduced down to a plateau value by ``collective learning''   even though most tasks have not seen examples,  and a final decay that occurs only once the number of examples is  proportional to the number of tasks."
Group Regularization of Correlated Features in Conditional Random Fields,"Conditional random fields allow extracting completely arbitrary linguistic dependencies between labels and observations. Optimization and even storage of billions of features is a problem; that is why a number of model selection approaches have been proposed. Although modern feature selection methods are efficient, they usually do not take into consideration correlation between  dependencies.  In this contribution we consider application of group and hierarchical linguistic dependencies using composite norms. We propose a new optimization approach which exploits the matrix of the second derivatives that keeps important information about correlations between model dependencies.  We illustrate by experiments on standard natural language learning data sets that the proposed approach is efficient. "
Transformed Poisson-Dirichlet Processes for Differential Topic Modeling,"We want to compare topics from a number of different document collections:some of these topics capture shared content, others capture the different andunique aspects that the collections may contain. We propose the transformedPoisson-Dirichlet process (TPDP), which is defined to be a class of hierarchicalPoisson-Dirichlet processes (HPDP) with transformed base measures, to build differential topic model among different groups of data. The main challenge of using the TPDP is the non-conjugacy between the prior and likelihood. We propose an efficient sampling algorithm by introducing auxiliary variables, which effectively resolve this problem. Experiment results show a dramatic reduced test perplexity compared to existing approximating methods on a variety of text and image collections. The model also gives an insightful analysis of the Democrat versus Republican blogs leading up to the 2008 USA election."
Learning to Classify Actions using Image-level Labels,"We consider the problem of classifying whether a given person in an image is performing an action of interest. Unlike previous methods that rely on onerous and expensive annotations of training images (a tight bounding box of the person and his/her ground truth action label), we propose a  weakly supervised formulation that only requires image level labels indicating the presence or an absence of an action in an image. Specifically, we consider a set of candidate objects obtained automatically from an object detector, and treat the true action of each object as a latent variable. This allows us to adapt the recently proposed dissimilarity coefficient learning framework for our task. In order to address the commonly encountered problem of imbalance in the dataset---the negative samples far outnumber the positive samples---we extend our learning framework by introducing a relative weight for each candidate object. These relative weights are adaptively changed depending on the current estimate of the latent variables. Using the largest publicly available dataset, namely the PASCAL VOC action classification challenge, we show that our approach greatly reduces the burden of annotation while preserving the accuracy of the model."
Learning to Classify Actions using Image-level Labels,"We consider the problem of classifying whether a given person in an image is performing an action of interest. Unlike previous methods that rely on onerous and expensive annotations of training images (a tight bounding box of the person and his/her ground truth action label), we propose a  weakly supervised formulation that only requires image level labels indicating the presence or an absence of an action in an image. Specifically, we consider a set of candidate objects obtained automatically from an object detector, and treat the true action of each object as a latent variable. This allows us to adapt the recently proposed dissimilarity coefficient learning framework for our task. In order to address the commonly encountered problem of imbalance in the dataset---the negative samples far outnumber the positive samples---we extend our learning framework by introducing a relative weight for each candidate object. These relative weights are adaptively changed depending on the current estimate of the latent variables. Using the largest publicly available dataset, namely the PASCAL VOC action classification challenge, we show that our approach greatly reduces the burden of annotation while preserving the accuracy of the model."
Regularization of Latent Variable Models to Obtain Sparsity,"We present a pseudo-observed variable based regularization technique for latent variable mixed-membership models that provides a mechanism to impose preferences on the characteristics of aggregate functions of latent and observed variables.  The regularization framework is used to regularize topic models, which are latent variable mixed membership models for language modeling.  In many domains, documents and words often exhibit only a slight degree of mixed-membership behavior that is inadequately modeled by topic models which are overly liberal in permitting mixed-membership behavior.  The regularization introduced in the paper is used to control the degree of polysemy of words permitted by topic models and to prefer sparsity in topic distributions of documents.  The utility of the regularization is evaluated internally using document perplexity and externally by using the models to predict star counts in movie and product reviews based on the content of the reviews.  Results of our experiments show that using the regularization to finely control the behavior of topic models leads to better perplexity and lower mean squared error rates in the star-prediction task."
Regularization of Latent Variable Models to Obtain Sparsity,"We present a pseudo-observed variable based regularization technique for latent variable mixed-membership models that provides a mechanism to impose preferences on the characteristics of aggregate functions of latent and observed variables.  The regularization framework is used to regularize topic models, which are latent variable mixed membership models for language modeling.  In many domains, documents and words often exhibit only a slight degree of mixed-membership behavior that is inadequately modeled by topic models which are overly liberal in permitting mixed-membership behavior.  The regularization introduced in the paper is used to control the degree of polysemy of words permitted by topic models and to prefer sparsity in topic distributions of documents.  The utility of the regularization is evaluated internally using document perplexity and externally by using the models to predict star counts in movie and product reviews based on the content of the reviews.  Results of our experiments show that using the regularization to finely control the behavior of topic models leads to better perplexity and lower mean squared error rates in the star-prediction task."
Estimating Unknown Sparsity in Compressed Sensing,"Within the framework of compressed sensing, many theoretical guarantees for signal reconstruction require that the number of linear of measurements $n$ exceed the sparsity $\|x\|_0$ of the unknown signal $x\in\R^p$. However, when the sparsity parameter $\|x\|_0$ is unknown, the choice of $n$ remains problematic. In this paper, we consider the problem of directly estimating $\|x\|_0$ from a small number of linear measurements---without making any prior assumptions about the sparsity of $x$. Although we show that estimation of $\|x\|_0$ is generally intractable in this framework, we consider an alternative measure of sparsity $s(x):=\|x\|_1^2\big/\|x\|_2^2$, which is a sharp lower bound on $\|x\|_0$, and is more amenable to estimation. When $x$ is a non-negative signal, we propose a computationally inexpensive estimator $\hat{s}(x)$ for $s(x)$, and derive concentration bounds that imply $\hat{s}(x)/s(x)\to 1$ almost surely as $(n,p)\to\infty$. Remarkably, the quality of estimation is \emph{dimension-free}, which ensures that $\hat{s}(x)$ is well-suited to the high-dimensional regime where $n\ll p$. These results also extend naturally to the problems of using linear measurements to estimate the rank of a positive semidefinite matrix, or the sparsity of a non-negative matrix. Finally, we show that if no structural assumption (such as non-negativity) is made on the signal $x$, then the quantity $s(x)$ cannot generally be estimated when $n\ll p$."
Modelling Shapes and Segmentations by Fields of Expert Mixtures,"We introduce a Gibbs Random Field (GRF) for modelling segmentations, i.e., the shape and the spatial relations of segments. The latent variables of the model form a field of expert mixtures (i.e.~naive Bayes models). The resulting probability distribution for segmentations (obtained by marginalising over the expert field) is a GRF with factors of arity higher than two.We show how to formulate the corresponding learning and inference tasks for the resulting model and propose algorithms for their approximative solution. The resulting approach is experimentally analysed on the example of two prototypical segmentation tasks."
Modelling Shapes and Segmentations by Fields of Expert Mixtures,"We introduce a Gibbs Random Field (GRF) for modelling segmentations, i.e., the shape and the spatial relations of segments. The latent variables of the model form a field of expert mixtures (i.e.~naive Bayes models). The resulting probability distribution for segmentations (obtained by marginalising over the expert field) is a GRF with factors of arity higher than two.We show how to formulate the corresponding learning and inference tasks for the resulting model and propose algorithms for their approximative solution. The resulting approach is experimentally analysed on the example of two prototypical segmentation tasks."
A Spectral Learning Approach to Range-Only SLAM,"We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our  approach does not need to linearize a transition or measurement model; such linearizations can cause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly for the highly non-Gaussian posteriors encountered in range-only SLAM. We provide a theoretical analysis of our method, including finite-sample error bounds.  Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost."
A Spectral Learning Approach to Range-Only SLAM,"We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our  approach does not need to linearize a transition or measurement model; such linearizations can cause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly for the highly non-Gaussian posteriors encountered in range-only SLAM. We provide a theoretical analysis of our method, including finite-sample error bounds.  Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost."
Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation,"This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture.Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP.Extensive experiments on different data sets demonstrate that EC-PBP achieves a higher topic modeling accuracyand reduces more than $80\%$ communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm."
Maximum Composite Likelihood Estimators for Gaussian Process Classification,"  Type-II Maximum Likelihood point estimation of covariance function  parameters is the inference framework of choice for the majority of  serious applications of Gaussian Process based discriminant analysis  and object classification. These estimators are typically derived  from approximations to the non-analytic marginal likelihood based on  Variational bounds or Expectation Propagation. However, the  frequentist properties of such estimators, such as the     unbiasedness, consistency and efficiency cannot  generally be determined analytically or indeed guaranteed. This  paper suggests that estimators with good frequentist properties for  binary Gaussian Process classifiers can be obtained by the use of  Composite Likelihoods for the estimation of the parameters of the  Gaussian Process. Furthermore, because of the structure of the  Composite Likelihood, the computational scaling achievable is vastly  superior to the typical ${\cal O}(n^3)$ scaling for Gaussian  Processes; the explicit computation of determinants, inverses and  derivatives of covariance matrices of arbitrary $n \times n$  dimension is obviated because exact analytical expressions can be  exploited in an independent and distributed manner."
Maximum Composite Likelihood Estimators for Gaussian Process Classification,"  Type-II Maximum Likelihood point estimation of covariance function  parameters is the inference framework of choice for the majority of  serious applications of Gaussian Process based discriminant analysis  and object classification. These estimators are typically derived  from approximations to the non-analytic marginal likelihood based on  Variational bounds or Expectation Propagation. However, the  frequentist properties of such estimators, such as the     unbiasedness, consistency and efficiency cannot  generally be determined analytically or indeed guaranteed. This  paper suggests that estimators with good frequentist properties for  binary Gaussian Process classifiers can be obtained by the use of  Composite Likelihoods for the estimation of the parameters of the  Gaussian Process. Furthermore, because of the structure of the  Composite Likelihood, the computational scaling achievable is vastly  superior to the typical ${\cal O}(n^3)$ scaling for Gaussian  Processes; the explicit computation of determinants, inverses and  derivatives of covariance matrices of arbitrary $n \times n$  dimension is obviated because exact analytical expressions can be  exploited in an independent and distributed manner."
Maximum Composite Likelihood Estimators for Gaussian Process Classification,"  Type-II Maximum Likelihood point estimation of covariance function  parameters is the inference framework of choice for the majority of  serious applications of Gaussian Process based discriminant analysis  and object classification. These estimators are typically derived  from approximations to the non-analytic marginal likelihood based on  Variational bounds or Expectation Propagation. However, the  frequentist properties of such estimators, such as the     unbiasedness, consistency and efficiency cannot  generally be determined analytically or indeed guaranteed. This  paper suggests that estimators with good frequentist properties for  binary Gaussian Process classifiers can be obtained by the use of  Composite Likelihoods for the estimation of the parameters of the  Gaussian Process. Furthermore, because of the structure of the  Composite Likelihood, the computational scaling achievable is vastly  superior to the typical ${\cal O}(n^3)$ scaling for Gaussian  Processes; the explicit computation of determinants, inverses and  derivatives of covariance matrices of arbitrary $n \times n$  dimension is obviated because exact analytical expressions can be  exploited in an independent and distributed manner."
The Variational Garrote,"In this paper, we present a new model for sparse regression using L0 regularization. The model introduces a sparseness mechanism in the likelihood, instead of in the prior, as is done in the spike and slab model. The posterior probability is computed in the variational approximation. The variational parameters appear in the approximate model in a way that is similar to Breiman's Garrote model.  We refer to this method as the variational Garrote (VG).  We show that the combination of the variational approximation and L0 regularization has the effect of making the problem effectively of maximal rank even when the number of samples is small compared to the number of variables.  The VG is compared numerically with the Lasso method and with ridge regression.  Numerical results on synthetic data show that the VG yields more accurate predictions and more accurately reconstructs the true model than the other methods. It is shown that the VG finds correct solutions when the Lasso solution is inconsistent due to large input correlations.  The naive implementation of the VG scales cubic with the number of features.  By introducing Lagrange multipliers we obtain a dual formulation of the problem that scales cubic in the number of samples, but close to linear in the number of features."
The Variational Garrote,"In this paper, we present a new model for sparse regression using L0 regularization. The model introduces a sparseness mechanism in the likelihood, instead of in the prior, as is done in the spike and slab model. The posterior probability is computed in the variational approximation. The variational parameters appear in the approximate model in a way that is similar to Breiman's Garrote model.  We refer to this method as the variational Garrote (VG).  We show that the combination of the variational approximation and L0 regularization has the effect of making the problem effectively of maximal rank even when the number of samples is small compared to the number of variables.  The VG is compared numerically with the Lasso method and with ridge regression.  Numerical results on synthetic data show that the VG yields more accurate predictions and more accurately reconstructs the true model than the other methods. It is shown that the VG finds correct solutions when the Lasso solution is inconsistent due to large input correlations.  The naive implementation of the VG scales cubic with the number of features.  By introducing Lagrange multipliers we obtain a dual formulation of the problem that scales cubic in the number of samples, but close to linear in the number of features."
An Alternative Kernel Method for the Two-Sample Problem,"We present an alternative kernel method for the two-sample problem that is based on Friedman's approach of using any binary classification learning machine to score the data.  When the learning machine is chosen to be a support vector machine, we show that this approach is a generalization of the permutation $t$-test.  Previous work has yielded a normal rate of convergence bound using Stein's Method in the simple setting of univariate data and a linear kernel with simulations, suggesting that this proof technique may be extended to address a more general setting.  Despite a lack of tuning of the SVM parameters, this method is shown to be competitive with the Maximum Mean Discrepancy (MMD) test."
An Alternative Kernel Method for the Two-Sample Problem,"We present an alternative kernel method for the two-sample problem that is based on Friedman's approach of using any binary classification learning machine to score the data.  When the learning machine is chosen to be a support vector machine, we show that this approach is a generalization of the permutation $t$-test.  Previous work has yielded a normal rate of convergence bound using Stein's Method in the simple setting of univariate data and a linear kernel with simulations, suggesting that this proof technique may be extended to address a more general setting.  Despite a lack of tuning of the SVM parameters, this method is shown to be competitive with the Maximum Mean Discrepancy (MMD) test."
Learning with Feature Concatenation,"We propose a flexible hierarchical Bayesian framework for integrating multiple views based on feature concatenation, which can seamlessly incorporate any priors from generative model and any convex losses used in discriminative model. By virtue of the probabilistic interpretation of this framework, we uncover the connection from feature concatenation to multiple kernel learning (MKL) and weighted voting of view classifiers. This connection inspires a novel feature concatenation formula with the exponential distribution as the prior in the fully Bayesian inference, and also provides an elegant extension of many existing MKL formulations via the generalized maximum likelihood method. Moreover, under certain mild condition, the resulting optimization problems are convex. Experiments on image classification and UCI datasets illustrate the benefits of our proposed framework."
Learning Low-rank Nonparametric Kernel Matrices From the Point of View of Matrix Completion,"Many existing nonparametric kernel learning methods suffer from high computational cost, which limits theirapplications to large scale real-world problems. In this paper, we propose a novel nonparametric kernel learning method based on the matrix completion technique, which emphasizes the low rank property of the learned kernel matrices. Given some pairwise constraints, we formulate the nonparametric kernel learning problem into a rank minimization problem with  constraints that enforce the learned similarities between the known data pairs equal the values of the known pairwise constraints. The resulting optimization problem can be relaxed to a convex optimization problem of minimizing nuclear norm with graph Laplacian regularization. We then develop a singular value thresholding like algorithm to solve the constrained convex problem using the similar techniques as in the matrix completion problems. Preliminary experimental results show that our algorithm performs better than or comparably to the existing best method BCDNPKL of [10] in terms of clustering accuracy and scalability."
Nonparametric Bayesian Double Articulation Analyzer,"In this paper, we propose a new Bayesian model for fully unsupervised segmentation and chunking method for continuous time series data. The proposed method, nonparametric Bayesian double articulation analyzer, presume that there are double articulation structure inside of the observed time series data and estimated the hidden double articulation structure without knowing the number of hidden states (letters) and the number of chunk types (words). An efficient approximate sampling procedure is introduced to extracts hidden words and letters. Our experiments using synthetic data shows that the nonparametric Bayesian double articulation analyzer could find hidden word changing points.  "
Multi-Stage Multi-Task Feature Learning,"Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex regularization problem, which is usually suboptimal, due to its looseness for approximating an $\ell_0$-type regularizer. In this paper, we propose a non-convex regularized formulation for multi-task sparse feature learning; we propose to solve the non-convex optimization problem by employing a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms."
Neural Network Models for Multilabel Learning,"Multilabel learning is an extension of standard binary classification where the goal is to predict a set of labels (which we call tags) for a given input example. There have been many models proposed for this problem, such as subspace learning, kernel methods, and nearest neighbour schemes. Recently, the probabilistic classifier chain method was proposed, which learns a series of probabilistic models that attempt to capture tag correlations. In this paper, we show how this model may be interpreted as a neural network with connections amongst output nodes. We argue that using an explicit hidden layer instead brings several advantages, such as tractable test-time inference, and removing the need for fixing a tag ordering. Further, the hidden units capture nonlinear latent structure that both improves classification performance and allows for interpretability. Compared to previous neural network models for multilabel learning, we discuss several design decisions that have a significant impact on training the network. Empirical results show that the model outperforms several existing methods."
Neural Network Models for Multilabel Learning,"Multilabel learning is an extension of standard binary classification where the goal is to predict a set of labels (which we call tags) for a given input example. There have been many models proposed for this problem, such as subspace learning, kernel methods, and nearest neighbour schemes. Recently, the probabilistic classifier chain method was proposed, which learns a series of probabilistic models that attempt to capture tag correlations. In this paper, we show how this model may be interpreted as a neural network with connections amongst output nodes. We argue that using an explicit hidden layer instead brings several advantages, such as tractable test-time inference, and removing the need for fixing a tag ordering. Further, the hidden units capture nonlinear latent structure that both improves classification performance and allows for interpretability. Compared to previous neural network models for multilabel learning, we discuss several design decisions that have a significant impact on training the network. Empirical results show that the model outperforms several existing methods."
Neural Network Models for Multilabel Learning,"Multilabel learning is an extension of standard binary classification where the goal is to predict a set of labels (which we call tags) for a given input example. There have been many models proposed for this problem, such as subspace learning, kernel methods, and nearest neighbour schemes. Recently, the probabilistic classifier chain method was proposed, which learns a series of probabilistic models that attempt to capture tag correlations. In this paper, we show how this model may be interpreted as a neural network with connections amongst output nodes. We argue that using an explicit hidden layer instead brings several advantages, such as tractable test-time inference, and removing the need for fixing a tag ordering. Further, the hidden units capture nonlinear latent structure that both improves classification performance and allows for interpretability. Compared to previous neural network models for multilabel learning, we discuss several design decisions that have a significant impact on training the network. Empirical results show that the model outperforms several existing methods."
Robust Bijective Vector-Valued Function Learning by Jointly Learning Its Inverse,"We discuss about a quite challenging problem in this paper: given the train data with large proportion of the outliers, the goal is to robustly estimate a bijective vector-valued target function. The existing methods only learn the target function itself or learn its inverse respectively and thus can?t handle the outliers well. To address this problem, we propose a robust method for bijective vector-valued function learning. In this approach, the target function and its inverse are bounded together and then jointly optimized under the maximum likelihood estimation (MLE) framework. By associating each sample with a latent variable that indicates whether the sample is an inlier, Expectation Maximization algorithm is employed to solve the MLE problem. To show its usefulness, the proposed method is applied to solve a fundamental problem of computer vision tasks. In detailed, given a set of putative point correspondences between two images that large proportion of correspondences are mismatches, the objective is to estimate a mapping function which can identify correct matches as inliers and distinguish the mismatches as the outliers. The experimental results have demonstrated that our proposed method is very robust and outperforms the state-of-the-art methods."
A Coarse-to-Fine Approach to Flexible Activity Discovery and Data Segmentation,"The growing number of mobile sensors allow collection of activity data at differentlevels of complexity and at fine-grained temporal resolution. However,accurately translating unlabeled sensor streams into meaningful activity classesremains non-trivial. The primitives that comprise an activity are usually foundin multiple activity classes; activities exhibit high-order temporal dependenciesamong primitives; and these higher-order dependencies vary from activity to activityand are blended together in the data. This paper presents a novel unsupervisedcoarse-to-fine activity discovery framework that handles the temporal heterogeneitypresent in sequences of sensor data and automatically segments activitieseven when the dependency order is not known and not fixed across activities.Our framework designs a Mixed Memory Latent Dirichlet Allocation (MM-LDA)model that iteratively discovers sequential transition patterns, segments the sensordata accordingly, and groups contiguous activity samples using a locality metric.We demonstrate the effectiveness of the approach by empirical experimentationon real-world datasets."
Operator-valued kernel-based autoregressive models with application to biological network inference,"Reverse-modeling of gene regulatory network from time-series of gene expression still remains a challenging problem in computational systems biology. Works concerning network inference from temporal data usually rely on sparse linear models or Granger causality tools. A very few address the issue in the nonlinear case. In this work, we propose a nonparametric approach to dynamical system modeling that makes no assumption about the nature of the underlying nonlinear system. We introduce a new family of vector autoregressive models based on operator-valued kernels to identify the dynamical system and retrieve the target network. As in the linear case, a key issue is to control the model's sparsity. We propose an alternate minimization procedure to learn both the kernel and the basis vectors. We show very good results both in  estimation on DREAM benchmarks as well as on the IRMA datasets."
Learning features for image classification from text,"The principle of cross-modal regularization, where data from an auxiliary modality is used to regularize classifiers of a principal modality, is exploited to improve image classification. Images and text are first represented by semantic descriptors, composed of their classification scores under classical image and text classifiers.A measure of cross-modal similarity, which defines the similarity of any image to the training texts, is then learned, and used to implement a soft label transfer mechanism, that transfers labels from training texts to the image. This mechanism is finally used to learn a set of cross-modal image classifiers, i.e. classifiers that classify images according to 1) their similarity to training text, and 2) the labels of the latter. The scores of these cross-modal image classifiers are used to augment the semantic image descriptors, acting as {\it cross-modalregularizing features\/}.  This regularization is shown to significantly improve the state-of-the-art semantics based image classification, on three challenging datasets."
Learning features for image classification from text,"The principle of cross-modal regularization, where data from an auxiliary modality is used to regularize classifiers of a principal modality, is exploited to improve image classification. Images and text are first represented by semantic descriptors, composed of their classification scores under classical image and text classifiers.A measure of cross-modal similarity, which defines the similarity of any image to the training texts, is then learned, and used to implement a soft label transfer mechanism, that transfers labels from training texts to the image. This mechanism is finally used to learn a set of cross-modal image classifiers, i.e. classifiers that classify images according to 1) their similarity to training text, and 2) the labels of the latter. The scores of these cross-modal image classifiers are used to augment the semantic image descriptors, acting as {\it cross-modalregularizing features\/}.  This regularization is shown to significantly improve the state-of-the-art semantics based image classification, on three challenging datasets."
Sample selection bias in unsupervised clustering,Sample selection bias has been studied in supervised settings when the training and test data are drawn from different distributions. We consider the effect of sample selection bias in an unsupervised setting when mixed-membership models are used for population stratification and topic modeling.We examined the effect of biased sampling on the accuracy of unsupervised clustering in terms of learning the low-dimensional representations of objects (documents or individual genotypes). We found that the accuracy of unsupervised clustering using a mixed-membership model is affected to a large extent by the sample used for analysis and how representative it is of the underlying populations. We also propose a correction for sample selection bias that is effective in real applications.
Sample selection bias in unsupervised clustering,Sample selection bias has been studied in supervised settings when the training and test data are drawn from different distributions. We consider the effect of sample selection bias in an unsupervised setting when mixed-membership models are used for population stratification and topic modeling.We examined the effect of biased sampling on the accuracy of unsupervised clustering in terms of learning the low-dimensional representations of objects (documents or individual genotypes). We found that the accuracy of unsupervised clustering using a mixed-membership model is affected to a large extent by the sample used for analysis and how representative it is of the underlying populations. We also propose a correction for sample selection bias that is effective in real applications.
Learning and Decision Making,"This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization."
Learning and Decision Making,"This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization."
On the Numerical Stability and Value Function Stability of Value-Directed Compression for POMDPs,"Value-directed compression (VDC) improves the tractability of a large scale POMDP by finding a basis to project its high-dimensional belief space into a low-dimensional approximation, where the problem can be solved with less computations. Our empirical findings indicate lossless VDC may sometimes produce larger compression errors than lossy VDC truncated to the same compression level due to the trade-off between residual threshold and numerical stability.  This paper analyses the numerical stability and residual error of the lossless and lossy VDC algorithms according to their column selection heuristics, and proposes a slight modification of lossless VDC that has a more tractable condition number. In addition, we show that the factorability of a problem is not the main determiner of the learnability of compressed problems. We discuss a built-indeficiency of VDC that can possibly magnify a distortion in the value function caused by compression errors to an infinitely great degree, which in the worst case will make the quality of the policy optimised based on the compressed POMDP arbitrarily low. This work contributes to the fundamental underlying theory of VDC, with supporting empirical evidence using benchmark POMDP problems."
On the Numerical Stability and Value Function Stability of Value-Directed Compression for POMDPs,"Value-directed compression (VDC) improves the tractability of a large scale POMDP by finding a basis to project its high-dimensional belief space into a low-dimensional approximation, where the problem can be solved with less computations. Our empirical findings indicate lossless VDC may sometimes produce larger compression errors than lossy VDC truncated to the same compression level due to the trade-off between residual threshold and numerical stability.  This paper analyses the numerical stability and residual error of the lossless and lossy VDC algorithms according to their column selection heuristics, and proposes a slight modification of lossless VDC that has a more tractable condition number. In addition, we show that the factorability of a problem is not the main determiner of the learnability of compressed problems. We discuss a built-indeficiency of VDC that can possibly magnify a distortion in the value function caused by compression errors to an infinitely great degree, which in the worst case will make the quality of the policy optimised based on the compressed POMDP arbitrarily low. This work contributes to the fundamental underlying theory of VDC, with supporting empirical evidence using benchmark POMDP problems."
On the Numerical Stability and Value Function Stability of Value-Directed Compression for POMDPs,"Value-directed compression (VDC) improves the tractability of a large scale POMDP by finding a basis to project its high-dimensional belief space into a low-dimensional approximation, where the problem can be solved with less computations. Our empirical findings indicate lossless VDC may sometimes produce larger compression errors than lossy VDC truncated to the same compression level due to the trade-off between residual threshold and numerical stability.  This paper analyses the numerical stability and residual error of the lossless and lossy VDC algorithms according to their column selection heuristics, and proposes a slight modification of lossless VDC that has a more tractable condition number. In addition, we show that the factorability of a problem is not the main determiner of the learnability of compressed problems. We discuss a built-indeficiency of VDC that can possibly magnify a distortion in the value function caused by compression errors to an infinitely great degree, which in the worst case will make the quality of the policy optimised based on the compressed POMDP arbitrarily low. This work contributes to the fundamental underlying theory of VDC, with supporting empirical evidence using benchmark POMDP problems."
Spike triggered covariance for strongly correlated Gaussian stimuli,"Characterizing feature selectivity is an important problem because it can shed light on how neurons process their inputs. The spike triggered covariance method (STCM) is a very commonly used method to extract the relevant set of stimulus features to which a neuron responds. One of the main advantages of STCM is that it can determine the dimensionality of the cell's relevant subspace. The method has been previously thought to be applicable when stimuli are drawn from a Gaussian ensemble, with or without stimulus correlations.  Here we use random matrix theory to show that when STCM is used with strongly correlated Gaussian stimuli, the null distribution of eigenvalues has a large outstanding mode. As a result, STCM can either yield an extra feature, which often corresponds to the strongest eigenvalue, or fail to yield any significant dimensions. We present a simple correction scheme that removes this artifact and illustrate its effectiveness by analyzing model neurons and recordings from retinal ganglion cells probed with correlated Gaussian stimuli whose second-order statistics was matched to natural stimuli. Our results can serve as guidelines for design of reverse correlation experiments that can help illuminate how neurons are optimized to code natural stimuli."
Spike triggered covariance for strongly correlated Gaussian stimuli,"Characterizing feature selectivity is an important problem because it can shed light on how neurons process their inputs. The spike triggered covariance method (STCM) is a very commonly used method to extract the relevant set of stimulus features to which a neuron responds. One of the main advantages of STCM is that it can determine the dimensionality of the cell's relevant subspace. The method has been previously thought to be applicable when stimuli are drawn from a Gaussian ensemble, with or without stimulus correlations.  Here we use random matrix theory to show that when STCM is used with strongly correlated Gaussian stimuli, the null distribution of eigenvalues has a large outstanding mode. As a result, STCM can either yield an extra feature, which often corresponds to the strongest eigenvalue, or fail to yield any significant dimensions. We present a simple correction scheme that removes this artifact and illustrate its effectiveness by analyzing model neurons and recordings from retinal ganglion cells probed with correlated Gaussian stimuli whose second-order statistics was matched to natural stimuli. Our results can serve as guidelines for design of reverse correlation experiments that can help illuminate how neurons are optimized to code natural stimuli."
A Sparsity Nonnegative Matrix Factorization Technique for Correspondences Problems,"Graph matching is an essential problem in computer vision and pattern recognition. In this paper, we present a robust graph matching method based on nonnegative matrix factorization with sparsity constraints. We show that our sparsity NMF based solution is sparse and thus naturally imposes the discrete mapping constraints strongly in the optimization process. Promising experimental results on both synthetic point matching and real world image feature matching tasks show the effectiveness of our graph matching method."
No voodoo here! Learning discrete graphical models via inverse covariance estimation,"We investigate the relationship between the support of the inverses of generalized covariance matrices and the structure of a discrete graphical model. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph. Our work extends results which were previously established only for multivariate Gaussian distributions, and partially answers an open question about the meaning of the inverse covariance matrix of a non-Gaussian distribution. We propose graph selection methods for a general discrete graphical model with bounded degree based on possibly corrupted observations, and verify our theoretical results via simulations. Along the way, we also establish new results for support recovery in the setting of sparse high-dimensional linear regression based on corrupted and missing observations."
No voodoo here! Learning discrete graphical models via inverse covariance estimation,"We investigate the relationship between the support of the inverses of generalized covariance matrices and the structure of a discrete graphical model. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph. Our work extends results which were previously established only for multivariate Gaussian distributions, and partially answers an open question about the meaning of the inverse covariance matrix of a non-Gaussian distribution. We propose graph selection methods for a general discrete graphical model with bounded degree based on possibly corrupted observations, and verify our theoretical results via simulations. Along the way, we also establish new results for support recovery in the setting of sparse high-dimensional linear regression based on corrupted and missing observations."
Efficient 3D Kernel Estimation for Non-uniform Camera Shake Removal,"Non-uniform camera shake removal is a knotty problem which plagues the researchers due to the huge computational cost of high-dimensional blur kernel estimation. To address this problem, we propose to estimate a 3D blur kernel from its 2D projections, which are computed from image patches, by solving a linear equation system. Under this scheme, we propose a perpendicular acquisition system to increase the projection variance of 3D kernel and thus decrease ill-posedness. Correspondingly, the 3D kernel estimator is obtained by efficient intersection operation. Finally, a RANSAC-based framework is developed to raise the robustness to estimation error of 2D local blur kernels. In experiments, we test our algorithm on both synthetic and real captured data, and both results show that results validate the effectiveness and efficiency of our approach."
Gradient-based kernel method for feature extraction and variable selection,"We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter finds a subset of predictors. First, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method.  In comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets.  Second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by Chen et al. (2010).  Experimental results show that the proposed methods successfully find effective features and variables without parametric models."
Domain-Unifying Embedding,"We propose Domain Unifying Embedding, together with its kernelized version, as a consolidated framework for domain adaptation and cross-domain recognition. Our approach embeds samples from one or more source domains and a target domain into a single latent shared domain, with the embedding represented by a linear or kernel transformation. In addition to allowing an arbitrary number of source domains, the approach allows these sources to be heterogeneous in the sense of having different dimensions. It also allows simultaneously exploiting a variety of types of semi-supervision, including target samples with explicit class labels when available, and instances for which corresponding, but unlabeled, samples are available in two or more domains."
Rational impatience in perceptual decision-making: a Bayesian account of discrepancy between two-alternative forced choice and Go/NoGo Behavior,"Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes."
Rational impatience in perceptual decision-making: a Bayesian account of discrepancy between two-alternative forced choice and Go/NoGo Behavior,"Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes."
3D Scene Grammar for Parsing RGB-D Pointclouds,"We pose 3D scene-understanding as a problem of parsing in a grammar.  A grammar helps us capture the compositional structure of real-word objects, e.g., a chair is composed of a seat, a back-rest and some legs. Having multiple rules for an object helps us capture structural variations in objects, e.g., a  chair can optionally also have arm-rests. Finally, having rules to capture composition at different levels helps us formulate the  entire scene-processing pipeline as a single problem of finding most likely parse-tree---small segments combine to form parts of objects, parts to objects and objects to a scene. We attach a generative probability model to our grammar by having a feature-dependent probability function for every rule. Our model can be trained very efficiently (within seconds), and it scales only linearly in with the number of rules in the grammar.   We show that we obtain good parse trees on 84 real point-clouds obtained from RGB-D cameras."
3D Scene Grammar for Parsing RGB-D Pointclouds,"We pose 3D scene-understanding as a problem of parsing in a grammar.  A grammar helps us capture the compositional structure of real-word objects, e.g., a chair is composed of a seat, a back-rest and some legs. Having multiple rules for an object helps us capture structural variations in objects, e.g., a  chair can optionally also have arm-rests. Finally, having rules to capture composition at different levels helps us formulate the  entire scene-processing pipeline as a single problem of finding most likely parse-tree---small segments combine to form parts of objects, parts to objects and objects to a scene. We attach a generative probability model to our grammar by having a feature-dependent probability function for every rule. Our model can be trained very efficiently (within seconds), and it scales only linearly in with the number of rules in the grammar.   We show that we obtain good parse trees on 84 real point-clouds obtained from RGB-D cameras."
On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks,"In this paper, we argue for representing networks as a bag of {\it triangular motifs},particularly for important network problems that current model-based approaches handle poorlydue to computational bottlenecks incurred by using edge representations.Such approaches require both 1-edges and 0-edges(missing edges) to be provided as input, and as a consequence, approximate inference algorithms for thesemodels usually require $\Omega(N^2)$ time per iteration, precluding their application to larger real-world networks.In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality.A triangular motif is a vertex triple containing 2 or 3 edges, andthe number of such motifs is $\Theta(\sum_{i}D_{i}^{2})$ (where $D_i$ is the degree of vertex $i$),which is much smaller than $N^2$ for low-maximum-degree networks. Using this representation, we develop a novelmixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networkswith high maximum degree, the triangular motifs can be naturally subsampled in a {\it node-centric}fashion, allowing for much faster inference at a small cost in accuracy.Empirically, we demonstrate that our approach, when compared to that of an edge-based model,has faster runtime and improved accuracy for mixed-membership community detection.We conclude with a large-scale demonstration on an $N\approx 280,000$-node network, which isinfeasible for network models with $\Omega(N^2)$ inference cost."
On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks,"In this paper, we argue for representing networks as a bag of {\it triangular motifs},particularly for important network problems that current model-based approaches handle poorlydue to computational bottlenecks incurred by using edge representations.Such approaches require both 1-edges and 0-edges(missing edges) to be provided as input, and as a consequence, approximate inference algorithms for thesemodels usually require $\Omega(N^2)$ time per iteration, precluding their application to larger real-world networks.In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality.A triangular motif is a vertex triple containing 2 or 3 edges, andthe number of such motifs is $\Theta(\sum_{i}D_{i}^{2})$ (where $D_i$ is the degree of vertex $i$),which is much smaller than $N^2$ for low-maximum-degree networks. Using this representation, we develop a novelmixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networkswith high maximum degree, the triangular motifs can be naturally subsampled in a {\it node-centric}fashion, allowing for much faster inference at a small cost in accuracy.Empirically, we demonstrate that our approach, when compared to that of an edge-based model,has faster runtime and improved accuracy for mixed-membership community detection.We conclude with a large-scale demonstration on an $N\approx 280,000$-node network, which isinfeasible for network models with $\Omega(N^2)$ inference cost."
On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks,"In this paper, we argue for representing networks as a bag of {\it triangular motifs},particularly for important network problems that current model-based approaches handle poorlydue to computational bottlenecks incurred by using edge representations.Such approaches require both 1-edges and 0-edges(missing edges) to be provided as input, and as a consequence, approximate inference algorithms for thesemodels usually require $\Omega(N^2)$ time per iteration, precluding their application to larger real-world networks.In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality.A triangular motif is a vertex triple containing 2 or 3 edges, andthe number of such motifs is $\Theta(\sum_{i}D_{i}^{2})$ (where $D_i$ is the degree of vertex $i$),which is much smaller than $N^2$ for low-maximum-degree networks. Using this representation, we develop a novelmixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networkswith high maximum degree, the triangular motifs can be naturally subsampled in a {\it node-centric}fashion, allowing for much faster inference at a small cost in accuracy.Empirically, we demonstrate that our approach, when compared to that of an edge-based model,has faster runtime and improved accuracy for mixed-membership community detection.We conclude with a large-scale demonstration on an $N\approx 280,000$-node network, which isinfeasible for network models with $\Omega(N^2)$ inference cost."
Learning a network from multiple data sources,"Recent methods on estimating the structure of undirected Gaussian graphical models have focused on estimation from a single data source. However, in many real world applications, multiple data sources are available that give information about the same set of nodes. We propose NP-MuScL (non-paranormal multi source learning) to estimate the structure of a sparse undirected graphical model that is consistent with multiple sources of data, having the same underlying relationships between the nodes. We use the semiparametric Gaussian copula to model the distribution of the different data sources, and show how to estimate such a model in the high dimensional scenario. Results are reported on synthetic data, where NP-MuScL outperforms baseline algorithms significantly, even in the presence of noisy data sources. Experiments are also run on two different data sets of yeast microarray, where NP-MuScL predicts a higher number of known gene interactions than existing techniques. "
Learning a network from multiple data sources,"Recent methods on estimating the structure of undirected Gaussian graphical models have focused on estimation from a single data source. However, in many real world applications, multiple data sources are available that give information about the same set of nodes. We propose NP-MuScL (non-paranormal multi source learning) to estimate the structure of a sparse undirected graphical model that is consistent with multiple sources of data, having the same underlying relationships between the nodes. We use the semiparametric Gaussian copula to model the distribution of the different data sources, and show how to estimate such a model in the high dimensional scenario. Results are reported on synthetic data, where NP-MuScL outperforms baseline algorithms significantly, even in the presence of noisy data sources. Experiments are also run on two different data sets of yeast microarray, where NP-MuScL predicts a higher number of known gene interactions than existing techniques. "
Robust exponential binary pattern storage in Little-Hopfield networks,"The Little-Hopfield network is an auto-associative computational model of neural memory storage and retrieval.  This model is known to robustly store collections of randomly generated binary patterns as stable-points of the network dynamics.  However, the number of binary memories so storable scales linearly in the number of neurons, and it has been a long-standing open problem whether robust exponential storage of binary patterns was possible in such a network memory model.  In this note, we design elementary families of Little-Hopfield networks that solve this problem affirmitavely."
Robust exponential binary pattern storage in Little-Hopfield networks,"The Little-Hopfield network is an auto-associative computational model of neural memory storage and retrieval.  This model is known to robustly store collections of randomly generated binary patterns as stable-points of the network dynamics.  However, the number of binary memories so storable scales linearly in the number of neurons, and it has been a long-standing open problem whether robust exponential storage of binary patterns was possible in such a network memory model.  In this note, we design elementary families of Little-Hopfield networks that solve this problem affirmitavely."
Efficient and optimal Little-Hopfield auto-associative memory storage using minimum probability flow,"We present an algorithm to store binary memories in a Hopfield neural network using minimum probability flow, a recent technique to fit parameters in energy-based probabilistic models.  In the case of memories without noise, our algorithm provably achieves optimal pattern storage (which we show is at least one pattern per neuron) and outperforms classical methods both in speed and memory recovery.  Moreover, when trained on noisy or corrupted versions of a fixed set of binary patterns, our algorithm finds networks which correctly store the originals.  We also demonstrate this finding visually with the unsupervised storage and clean-up of large binary fingerprint images from significantly corrupted samples."
Efficient and optimal Little-Hopfield auto-associative memory storage using minimum probability flow,"We present an algorithm to store binary memories in a Hopfield neural network using minimum probability flow, a recent technique to fit parameters in energy-based probabilistic models.  In the case of memories without noise, our algorithm provably achieves optimal pattern storage (which we show is at least one pattern per neuron) and outperforms classical methods both in speed and memory recovery.  Moreover, when trained on noisy or corrupted versions of a fixed set of binary patterns, our algorithm finds networks which correctly store the originals.  We also demonstrate this finding visually with the unsupervised storage and clean-up of large binary fingerprint images from significantly corrupted samples."
Active Learning on Low-Rank Matrices,"Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. With various criteria, we can actively choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many large points as possible. We evaluate our methods on simulated data and show their applicability on movie ratings prediction as well as discovering drug-target interactions."
Active Learning on Low-Rank Matrices,"Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. With various criteria, we can actively choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many large points as possible. We evaluate our methods on simulated data and show their applicability on movie ratings prediction as well as discovering drug-target interactions."
Active Learning on Low-Rank Matrices,"Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. With various criteria, we can actively choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many large points as possible. We evaluate our methods on simulated data and show their applicability on movie ratings prediction as well as discovering drug-target interactions."
HGLMMF: Generalizing Matrix Factorization with Hierarchical Generalized Linear Model,"Matrix factorization (MF) has become the dominant method of collaborative filtering. Recently, various MF methods have been proposed and tried to jointly model multiple relations. However, such methods are vulnerable to the changes of data or sub-models. Moreover, data often follows the Pareto rule, which may lead to a poor result due to the global bias caused by such imbalanced data. To overcome these defects, we designed a generalized MF method based on hierarchical generalized linear models (HGLMMF) that augments knowledge with learned extra information from other related models. More specifically, HGLMMF uses one portion of the augmented knowledge to construct augmented covariates to better capture fixed effects while the other portion is used to model the cluster-specific random effects to adjust the global bias problem. We also demonstrate that a number of state-of-the-art MF models can be viewed as special cases of HGLMMF."
HGLMMF: Generalizing Matrix Factorization with Hierarchical Generalized Linear Model,"Matrix factorization (MF) has become the dominant method of collaborative filtering. Recently, various MF methods have been proposed and tried to jointly model multiple relations. However, such methods are vulnerable to the changes of data or sub-models. Moreover, data often follows the Pareto rule, which may lead to a poor result due to the global bias caused by such imbalanced data. To overcome these defects, we designed a generalized MF method based on hierarchical generalized linear models (HGLMMF) that augments knowledge with learned extra information from other related models. More specifically, HGLMMF uses one portion of the augmented knowledge to construct augmented covariates to better capture fixed effects while the other portion is used to model the cluster-specific random effects to adjust the global bias problem. We also demonstrate that a number of state-of-the-art MF models can be viewed as special cases of HGLMMF."
HGLMMF: Generalizing Matrix Factorization with Hierarchical Generalized Linear Model,"Matrix factorization (MF) has become the dominant method of collaborative filtering. Recently, various MF methods have been proposed and tried to jointly model multiple relations. However, such methods are vulnerable to the changes of data or sub-models. Moreover, data often follows the Pareto rule, which may lead to a poor result due to the global bias caused by such imbalanced data. To overcome these defects, we designed a generalized MF method based on hierarchical generalized linear models (HGLMMF) that augments knowledge with learned extra information from other related models. More specifically, HGLMMF uses one portion of the augmented knowledge to construct augmented covariates to better capture fixed effects while the other portion is used to model the cluster-specific random effects to adjust the global bias problem. We also demonstrate that a number of state-of-the-art MF models can be viewed as special cases of HGLMMF."
Affine Independent Variational Inference,"We present a method for approximate inference for a broad class of non-conjugate probabilistic models. In particular, for the family of generalized linear model target densities we describe a rich class of variational approximating densities which can be best fit to the target by minimizing the Kullback-Leibler divergence.  Our approach is based on using the Fourier representation which we show results in efficient and scalable inference."
Tracking 3-D Rotations with the Quaternion Bingham Filter,"A deterministic method for sequential estimation of 3-D rotationsis presented.  The Bingham distribution is used to representuncertainty directly on the unit quaternion hypersphere.  Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions.  Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic.  We present two versions of the  QBF--suitable for tracking the state of first- and second-order rotating dynamical systems."
Tracking 3-D Rotations with the Quaternion Bingham Filter,"A deterministic method for sequential estimation of 3-D rotationsis presented.  The Bingham distribution is used to representuncertainty directly on the unit quaternion hypersphere.  Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions.  Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic.  We present two versions of the  QBF--suitable for tracking the state of first- and second-order rotating dynamical systems."
Unsupervised Structure Discovery for Semantic Analysis of Audio,"Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines."
Unsupervised Structure Discovery for Semantic Analysis of Audio,"Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines."
Growing a List,"We would like to intelligently grow a long list, starting from a small seed of examples. Our algorithm for solving this problem takes advantage of the wisdom of the crowd, in the sense that there are many experts who post lists of things on the Internet. We want to find these experts and aggregate their lists in an intelligent way in order to produce a single concise, complete, and meaningful list. Our solution to the list aggregation problem has several simple components: i) a combinatorial search over pairs of seed items, leveraging the speed of search engines, ii) a fast clustering algorithm (Bayesian Sets), and iii) an implicit feedback loop where the most relevant terms are added to the seed. The algorithm is extremely fast, and we show experimental results on two problems: creating a list of planned events in and around Boston, and creating a list of Jewish foods. We find that Bayesian Sets clusters well even when its underlying independence assumption is violated, and we provide a theoretical generalization bound to explain its ability to cluster well in general."
Growing a List,"We would like to intelligently grow a long list, starting from a small seed of examples. Our algorithm for solving this problem takes advantage of the wisdom of the crowd, in the sense that there are many experts who post lists of things on the Internet. We want to find these experts and aggregate their lists in an intelligent way in order to produce a single concise, complete, and meaningful list. Our solution to the list aggregation problem has several simple components: i) a combinatorial search over pairs of seed items, leveraging the speed of search engines, ii) a fast clustering algorithm (Bayesian Sets), and iii) an implicit feedback loop where the most relevant terms are added to the seed. The algorithm is extremely fast, and we show experimental results on two problems: creating a list of planned events in and around Boston, and creating a list of Jewish foods. We find that Bayesian Sets clusters well even when its underlying independence assumption is violated, and we provide a theoretical generalization bound to explain its ability to cluster well in general."
Growing a List,"We would like to intelligently grow a long list, starting from a small seed of examples. Our algorithm for solving this problem takes advantage of the wisdom of the crowd, in the sense that there are many experts who post lists of things on the Internet. We want to find these experts and aggregate their lists in an intelligent way in order to produce a single concise, complete, and meaningful list. Our solution to the list aggregation problem has several simple components: i) a combinatorial search over pairs of seed items, leveraging the speed of search engines, ii) a fast clustering algorithm (Bayesian Sets), and iii) an implicit feedback loop where the most relevant terms are added to the seed. The algorithm is extremely fast, and we show experimental results on two problems: creating a list of planned events in and around Boston, and creating a list of Jewish foods. We find that Bayesian Sets clusters well even when its underlying independence assumption is violated, and we provide a theoretical generalization bound to explain its ability to cluster well in general."
Building an Attribute based Semantic Hierarchy,"We propose a new framework to build attribute based hierarchies from visual datasets. Our desiderata is to construct a tree structure in which attributes which are used more frequently are associated with nodes which are closer to the root, whereas attributes which are used less frequently are associated with lower levels in the tree. Most of the existing works that are concerned with learning visual and semantic taxonomies are based on hierarchical topic models which entail the bag of features representation. Such approaches are therefore not suitable for dealing with an attribute based representation. An attribute based representation can facilitate information transfer from previously observed instances into new images, and therefore an attribute based hierarchy can capture richer semantics while limiting the use of costly annotation data. We develop a new generative model for hierarchical clustering of binary vectors, which we refer to as the attribute tree process (ATP), and which is based on a tree-structured stick breaking process. The ATP allows us to estimate the entire structure of the hierarchy and the model parameters in an unsupervised fashion. We evaluate the proposed framework using several widely available datasets, and demonstrate that the ATP is capable of constructing semantically meaningful hierarchical representations of the data. "
Building an Attribute based Semantic Hierarchy,"We propose a new framework to build attribute based hierarchies from visual datasets. Our desiderata is to construct a tree structure in which attributes which are used more frequently are associated with nodes which are closer to the root, whereas attributes which are used less frequently are associated with lower levels in the tree. Most of the existing works that are concerned with learning visual and semantic taxonomies are based on hierarchical topic models which entail the bag of features representation. Such approaches are therefore not suitable for dealing with an attribute based representation. An attribute based representation can facilitate information transfer from previously observed instances into new images, and therefore an attribute based hierarchy can capture richer semantics while limiting the use of costly annotation data. We develop a new generative model for hierarchical clustering of binary vectors, which we refer to as the attribute tree process (ATP), and which is based on a tree-structured stick breaking process. The ATP allows us to estimate the entire structure of the hierarchy and the model parameters in an unsupervised fashion. We evaluate the proposed framework using several widely available datasets, and demonstrate that the ATP is capable of constructing semantically meaningful hierarchical representations of the data. "
Building an Attribute based Semantic Hierarchy,"We propose a new framework to build attribute based hierarchies from visual datasets. Our desiderata is to construct a tree structure in which attributes which are used more frequently are associated with nodes which are closer to the root, whereas attributes which are used less frequently are associated with lower levels in the tree. Most of the existing works that are concerned with learning visual and semantic taxonomies are based on hierarchical topic models which entail the bag of features representation. Such approaches are therefore not suitable for dealing with an attribute based representation. An attribute based representation can facilitate information transfer from previously observed instances into new images, and therefore an attribute based hierarchy can capture richer semantics while limiting the use of costly annotation data. We develop a new generative model for hierarchical clustering of binary vectors, which we refer to as the attribute tree process (ATP), and which is based on a tree-structured stick breaking process. The ATP allows us to estimate the entire structure of the hierarchy and the model parameters in an unsupervised fashion. We evaluate the proposed framework using several widely available datasets, and demonstrate that the ATP is capable of constructing semantically meaningful hierarchical representations of the data. "
Building an Attribute based Semantic Hierarchy,"We propose a new framework to build attribute based hierarchies from visual datasets. Our desiderata is to construct a tree structure in which attributes which are used more frequently are associated with nodes which are closer to the root, whereas attributes which are used less frequently are associated with lower levels in the tree. Most of the existing works that are concerned with learning visual and semantic taxonomies are based on hierarchical topic models which entail the bag of features representation. Such approaches are therefore not suitable for dealing with an attribute based representation. An attribute based representation can facilitate information transfer from previously observed instances into new images, and therefore an attribute based hierarchy can capture richer semantics while limiting the use of costly annotation data. We develop a new generative model for hierarchical clustering of binary vectors, which we refer to as the attribute tree process (ATP), and which is based on a tree-structured stick breaking process. The ATP allows us to estimate the entire structure of the hierarchy and the model parameters in an unsupervised fashion. We evaluate the proposed framework using several widely available datasets, and demonstrate that the ATP is capable of constructing semantically meaningful hierarchical representations of the data. "
Refining Models for Percutaneous Coronary Intervention through Transfer Learning,"Identifying patients at risk of complications during percutaneous coronary intervention (PCI), such as arrhythmias and bleeding, is essential in guiding patient care at the bedside and in streamlining healthcare delivery across hospitals with varying clinical resources (e.g., with or without on-site cardiac surgery). The traditional approach to develop models for PCI care is largely centralized and uses patient data aggregated across a growing number of hospitals. While this approach is effective in increasing the amount of training data available for model training, and in improving the generality of the models learned, it suffers from the pooled data abstracting the PCI population in a way that fails to reflect variations across individual hospitals in terms of both patients and caregivers. We address this shortcoming by exploring the hypothesis that models for PCI care can be improved through the use of transfer learning. In particular, we study how transfer learning can be applied to adapt models derived from multi-hospital PCI data for use at individual hospitals, in an effort to simultaneously leverage the benefits of both aggregating clinical datasets and fitting to the specific characteristics of the patient/caregiver mix at individual hospitals. When studied on a registry of patients undergoing PCI, this approach of model adaptation through transfer learning improved reclassification at the individual healthcare provider level for many complications associated with PCI."
Refining Models for Percutaneous Coronary Intervention through Transfer Learning,"Identifying patients at risk of complications during percutaneous coronary intervention (PCI), such as arrhythmias and bleeding, is essential in guiding patient care at the bedside and in streamlining healthcare delivery across hospitals with varying clinical resources (e.g., with or without on-site cardiac surgery). The traditional approach to develop models for PCI care is largely centralized and uses patient data aggregated across a growing number of hospitals. While this approach is effective in increasing the amount of training data available for model training, and in improving the generality of the models learned, it suffers from the pooled data abstracting the PCI population in a way that fails to reflect variations across individual hospitals in terms of both patients and caregivers. We address this shortcoming by exploring the hypothesis that models for PCI care can be improved through the use of transfer learning. In particular, we study how transfer learning can be applied to adapt models derived from multi-hospital PCI data for use at individual hospitals, in an effort to simultaneously leverage the benefits of both aggregating clinical datasets and fitting to the specific characteristics of the patient/caregiver mix at individual hospitals. When studied on a registry of patients undergoing PCI, this approach of model adaptation through transfer learning improved reclassification at the individual healthcare provider level for many complications associated with PCI."
Refining Models for Percutaneous Coronary Intervention through Transfer Learning,"Identifying patients at risk of complications during percutaneous coronary intervention (PCI), such as arrhythmias and bleeding, is essential in guiding patient care at the bedside and in streamlining healthcare delivery across hospitals with varying clinical resources (e.g., with or without on-site cardiac surgery). The traditional approach to develop models for PCI care is largely centralized and uses patient data aggregated across a growing number of hospitals. While this approach is effective in increasing the amount of training data available for model training, and in improving the generality of the models learned, it suffers from the pooled data abstracting the PCI population in a way that fails to reflect variations across individual hospitals in terms of both patients and caregivers. We address this shortcoming by exploring the hypothesis that models for PCI care can be improved through the use of transfer learning. In particular, we study how transfer learning can be applied to adapt models derived from multi-hospital PCI data for use at individual hospitals, in an effort to simultaneously leverage the benefits of both aggregating clinical datasets and fitting to the specific characteristics of the patient/caregiver mix at individual hospitals. When studied on a registry of patients undergoing PCI, this approach of model adaptation through transfer learning improved reclassification at the individual healthcare provider level for many complications associated with PCI."
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Posterior contraction of the population polytope in finite admixture models,"We study the posterior contraction behavior of the latent population structure that arises in admixture models as the amount of data increases. An admixture model  --- alternatively known as a topic model --- specifies $k$ populations (or topics), each of which is characterized by vector of frequencies for generating a set of discrete values of observations. The population polytope is defined as the convex hull of the $k$ frequency vectors. Given a prior distribution over the space of population polytopes, we establish rates at which the posterior distribution contracts to $G_0$, under the Hausdorff metric and a minimum matching Euclidean metric, as the amount of data tends to infinity. Rates are obtained for the overfitted setting, i.e., when the number of extreme points of $G_0$ is bounded above by $k$, and for the setting in which the number of extreme points of $G_0$ is known. Minimax lower bounds are also established. Our analysis combines posterior asymptotics techniques for the estimation of mixing measures in hierarchical models with arguments in convex geometry."
Exact and Efficient Parallel Inference for Nonparametric Mixture Models,"Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to sample from the true posterior in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods."
Exact and Efficient Parallel Inference for Nonparametric Mixture Models,"Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to sample from the true posterior in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods."
Dimension Independent Similarity Computation,"We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO)to compute all pairwise similarities between very high dimensional sparse vectors.All of our results are provably independent of dimension, meaningapart from the initial cost of trivially reading in the data, all subsequentoperations are independent of the dimension, thus the dimension can be very large.We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash.Our results are geared toward the MapReduce framework. We empirically validate ourtheorems at large scale using data from the social networking site Twitter."
Dimension Independent Similarity Computation,"We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO)to compute all pairwise similarities between very high dimensional sparse vectors.All of our results are provably independent of dimension, meaningapart from the initial cost of trivially reading in the data, all subsequentoperations are independent of the dimension, thus the dimension can be very large.We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash.Our results are geared toward the MapReduce framework. We empirically validate ourtheorems at large scale using data from the social networking site Twitter."
Symmetric Correspondence Topic Models for Multilingual Text Analysis,"Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models."
Infinite Structured Hidden Markov Model,"We present the infinite structured hidden Markov model (ISHMM). An ISHMM is an HMM that possesses an unbounded number of states, parameterizes state dwell-time distributions explicitly, and can constrain what kinds of state transitions are possible. We present two parameterizations of the ISHMM. The first is a novel construction for an infinite explicit duration HMM. The second is an entirely novel infinite left-to-right HMM. We provide inference algorithms for the ISHMM and show results from using the ISHMM to analyze both real and synthetic data."
Understanding Indoor Scenes with Latent Interaction Template Models,"Visual scene understanding is a difficult problem, interleaving object detection, geometric reasoning and scene classification. In this paper, we present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the latent Interaction Template Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings, while also improving individual object detections."
Understanding Indoor Scenes with Latent Interaction Template Models,"Visual scene understanding is a difficult problem, interleaving object detection, geometric reasoning and scene classification. In this paper, we present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the latent Interaction Template Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings, while also improving individual object detections."
Understanding Indoor Scenes with Latent Interaction Template Models,"Visual scene understanding is a difficult problem, interleaving object detection, geometric reasoning and scene classification. In this paper, we present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the latent Interaction Template Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings, while also improving individual object detections."
Towards Sparse Representation on Cosine Distance,"Sparse code is a regularized least squares solution by $L_1$ or $L_0$ constraint, based on Euclidean distance between original and reconstructed signals with respect to a pre-defined dictionary.  The Euclidean distance, however, is not a good metric for many visual feature descriptors especially histogram features,~\eg~SIFT, HOG, LBP and Spatial Pyramid.  Instead, a cosine distance is a semantically meaningful metric for the visual features.  To leverage the benefit of cosine distance in sparse representation, we formulate a new sparse coding objective function based on approximate cosine distance by forcing a norm of reconstructed signal to be close to the norm of original signal.  We evaluate our new formulation on two datasets: Extended YaleB and AR dataset.  Our formulation shows consistent improvement over the traditional Euclidean distance based sparse coding formulation in our evaluations and achieve the state-of-the-art performance on the datasets."
Eliciting Predictions from a Connected Crowd of Traders,"We study an online trading community where traders can communicate with each other as well as perform trades. We discuss characteristics of social influence on trading decisions within this connected crowd. We discover traders are still heavily affected by social influence even when every trade is with their own money, and social influence often negatively affects their returns. Based on our observations, we implement three trading strategies to elicit the crowd?s prediction. In particular, we design a novel way of inferring predictions by modeling the crowd reasoning process under social influence. We find that even complex social dynamics can dramatically effect trades, it is still possible to infer knowledge from the crowd. Our novel algorithm achieves the best performance by modeling decision making processes under influence rather than the decisions from the crowd."
Eliciting Predictions from a Connected Crowd of Traders,"We study an online trading community where traders can communicate with each other as well as perform trades. We discuss characteristics of social influence on trading decisions within this connected crowd. We discover traders are still heavily affected by social influence even when every trade is with their own money, and social influence often negatively affects their returns. Based on our observations, we implement three trading strategies to elicit the crowd?s prediction. In particular, we design a novel way of inferring predictions by modeling the crowd reasoning process under social influence. We find that even complex social dynamics can dramatically effect trades, it is still possible to infer knowledge from the crowd. Our novel algorithm achieves the best performance by modeling decision making processes under influence rather than the decisions from the crowd."
Eliciting Predictions from a Connected Crowd of Traders,"We study an online trading community where traders can communicate with each other as well as perform trades. We discuss characteristics of social influence on trading decisions within this connected crowd. We discover traders are still heavily affected by social influence even when every trade is with their own money, and social influence often negatively affects their returns. Based on our observations, we implement three trading strategies to elicit the crowd?s prediction. In particular, we design a novel way of inferring predictions by modeling the crowd reasoning process under social influence. We find that even complex social dynamics can dramatically effect trades, it is still possible to infer knowledge from the crowd. Our novel algorithm achieves the best performance by modeling decision making processes under influence rather than the decisions from the crowd."
CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem,"While compressive sensing (CS) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models. This limits its application and excludes many areas where CS ideas could make a difference. This paper presents a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose a novel solution using a lifting technique -- CPRL, which relaxes the NP-hard problem to a nonsmooth semidefinite program. Our analysis shows that CPRL inherits many desirable properties from CS, such as guarantees for exact recovery. We further provide scalable numerical solvers to accelerate its implementation. The source code of our algorithms will be provided to the public. "
CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem,"While compressive sensing (CS) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models. This limits its application and excludes many areas where CS ideas could make a difference. This paper presents a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose a novel solution using a lifting technique -- CPRL, which relaxes the NP-hard problem to a nonsmooth semidefinite program. Our analysis shows that CPRL inherits many desirable properties from CS, such as guarantees for exact recovery. We further provide scalable numerical solvers to accelerate its implementation. The source code of our algorithms will be provided to the public. "
CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem,"While compressive sensing (CS) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models. This limits its application and excludes many areas where CS ideas could make a difference. This paper presents a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose a novel solution using a lifting technique -- CPRL, which relaxes the NP-hard problem to a nonsmooth semidefinite program. Our analysis shows that CPRL inherits many desirable properties from CS, such as guarantees for exact recovery. We further provide scalable numerical solvers to accelerate its implementation. The source code of our algorithms will be provided to the public. "
CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem,"While compressive sensing (CS) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models. This limits its application and excludes many areas where CS ideas could make a difference. This paper presents a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose a novel solution using a lifting technique -- CPRL, which relaxes the NP-hard problem to a nonsmooth semidefinite program. Our analysis shows that CPRL inherits many desirable properties from CS, such as guarantees for exact recovery. We further provide scalable numerical solvers to accelerate its implementation. The source code of our algorithms will be provided to the public. "
Visually-grounded Bayesian Word Learning,"Learning the meaning of a novel noun from a few labelled objects is one of the simplest aspects of learning a language, but approximating human performance on this task is still a significant challenge for current machine learning systems. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for given visual stimulus. Recent work in cognitive science on Bayesian models of word learning partially addresses this challenge, but assumes that objects are perfectly recognized and has only been evaluated in small domains. We present a system for learning words directly from images, using probabilistic predictions generated by visual classifiers as the input to Bayesian word learning, and compare this system to human performance in a large-scale automated experiment. The system captures a significant proportion of the variance in human responses. Combining the uncertain outputs of the visual classifiers with the ability to identify an appropriate level of abstraction that comes from Bayesian word learning allows the system to outperform alternatives that assume perfect recognition or use a more conventional computer vision approach."
Visually-grounded Bayesian Word Learning,"Learning the meaning of a novel noun from a few labelled objects is one of the simplest aspects of learning a language, but approximating human performance on this task is still a significant challenge for current machine learning systems. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for given visual stimulus. Recent work in cognitive science on Bayesian models of word learning partially addresses this challenge, but assumes that objects are perfectly recognized and has only been evaluated in small domains. We present a system for learning words directly from images, using probabilistic predictions generated by visual classifiers as the input to Bayesian word learning, and compare this system to human performance in a large-scale automated experiment. The system captures a significant proportion of the variance in human responses. Combining the uncertain outputs of the visual classifiers with the ability to identify an appropriate level of abstraction that comes from Bayesian word learning allows the system to outperform alternatives that assume perfect recognition or use a more conventional computer vision approach."
Visually-grounded Bayesian Word Learning,"Learning the meaning of a novel noun from a few labelled objects is one of the simplest aspects of learning a language, but approximating human performance on this task is still a significant challenge for current machine learning systems. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for given visual stimulus. Recent work in cognitive science on Bayesian models of word learning partially addresses this challenge, but assumes that objects are perfectly recognized and has only been evaluated in small domains. We present a system for learning words directly from images, using probabilistic predictions generated by visual classifiers as the input to Bayesian word learning, and compare this system to human performance in a large-scale automated experiment. The system captures a significant proportion of the variance in human responses. Combining the uncertain outputs of the visual classifiers with the ability to identify an appropriate level of abstraction that comes from Bayesian word learning allows the system to outperform alternatives that assume perfect recognition or use a more conventional computer vision approach."
Visually-grounded Bayesian Word Learning,"Learning the meaning of a novel noun from a few labelled objects is one of the simplest aspects of learning a language, but approximating human performance on this task is still a significant challenge for current machine learning systems. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for given visual stimulus. Recent work in cognitive science on Bayesian models of word learning partially addresses this challenge, but assumes that objects are perfectly recognized and has only been evaluated in small domains. We present a system for learning words directly from images, using probabilistic predictions generated by visual classifiers as the input to Bayesian word learning, and compare this system to human performance in a large-scale automated experiment. The system captures a significant proportion of the variance in human responses. Combining the uncertain outputs of the visual classifiers with the ability to identify an appropriate level of abstraction that comes from Bayesian word learning allows the system to outperform alternatives that assume perfect recognition or use a more conventional computer vision approach."
A Sparse and Adaptive Prior for Time-Dependent Model Parameters,"We consider the scenario where the parameters of a probabilistic model are expected to vary over time. We construct a novel prior distribution that promotessparsity and adapts the strength of correlation between parameters at successivetimesteps, based on the data. We derive approximate variational inference procedures for learning and prediction with this prior. We test the approach on twotasks: forecasting ?nancial quanitities from relevant text, and modeling languagecontingent on time-varying ?nancial measurements."
A Sparse and Adaptive Prior for Time-Dependent Model Parameters,"We consider the scenario where the parameters of a probabilistic model are expected to vary over time. We construct a novel prior distribution that promotessparsity and adapts the strength of correlation between parameters at successivetimesteps, based on the data. We derive approximate variational inference procedures for learning and prediction with this prior. We test the approach on twotasks: forecasting ?nancial quanitities from relevant text, and modeling languagecontingent on time-varying ?nancial measurements."
A Sparse and Adaptive Prior for Time-Dependent Model Parameters,"We consider the scenario where the parameters of a probabilistic model are expected to vary over time. We construct a novel prior distribution that promotessparsity and adapts the strength of correlation between parameters at successivetimesteps, based on the data. We derive approximate variational inference procedures for learning and prediction with this prior. We test the approach on twotasks: forecasting ?nancial quanitities from relevant text, and modeling languagecontingent on time-varying ?nancial measurements."
Leveraging for Fitting Linear Models in Large-scale Data,"Recent empirical and theoretical work has focused on using the empirical statistical leverage scores of data matrices in order to develop improved algorithms for common matrix problems such as least-squares approximation and low-rank matrix approximation.  Existing work focuses on algorithmic issues such as worst-case running times or on the usefulness of this approach in downstream data applications.  Here, we examine the statistical properties of this leveraging paradigm in the context of fitting a linear model to data.  We derive the mean squared errors for two related leveraging-based estimates and for uniform sampling estimates.  Depending on the the mean, variance and skewness of the leverage scores, one procedure or another is preferred.  We also describe the empirical behavior of these procedures on several synthetic and real data sets."
Leveraging for Fitting Linear Models in Large-scale Data,"Recent empirical and theoretical work has focused on using the empirical statistical leverage scores of data matrices in order to develop improved algorithms for common matrix problems such as least-squares approximation and low-rank matrix approximation.  Existing work focuses on algorithmic issues such as worst-case running times or on the usefulness of this approach in downstream data applications.  Here, we examine the statistical properties of this leveraging paradigm in the context of fitting a linear model to data.  We derive the mean squared errors for two related leveraging-based estimates and for uniform sampling estimates.  Depending on the the mean, variance and skewness of the leverage scores, one procedure or another is preferred.  We also describe the empirical behavior of these procedures on several synthetic and real data sets."
Dual Semi-Supervised Co-Clustering Informed by Geometry,"Co-clustering algorithms, which group a data matrix based on the similarities of both rows (samples) and columns (features), often yield impressive performanceimprovement over traditional one-side clustering approaches. Efficient utilizing partial supervision in the form of row labels as well as column labels is still a challenge, especially when the number of labels is small. Moreover, since many real world data are sampled from a low dimensional manifold, effective co-clusteringalgorithms will depend upon the intrinsic structures of rows as well as columns of the data matrix. In this paper we propose dual semi-supervised co-clusteringinformed by geometry (DSCIG) to address these two issues. First, we provide a general framework for co-clustering that incorporates partial supervision informationand preserve local geometry. Second, we augment this framework with an additional step for similarity propagation that generate richer supervision information.To manage the different applications of DSCIG, we derive an alternative optimization procedure and show the convergence is guaranteed theoretically ."
Privacy Aware Learning,"We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.  In this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator."
Privacy Aware Learning,"We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.  In this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator."
Privacy Aware Learning,"We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.  In this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator."
Multi-Armed Bandit Problem with Budget Constraint and Variable Costs,"In this paper, we study the multi-armed bandit problem with budget constraint and variable costs (MAB-BV). In this setting, pulling each arm is associated with an unknown and variable cost, and the objective of a learning algorithm is to pull a sequence of arms in order to maximize the expected total reward with the number of pulled arms complying with the budget constraint. This new setting describes many Internet applications (e.g., sponsored search and cloud computing) in a more accurate manner than previous settings that either assume the pulling of arms is costless or with a fixed cost. To tackle this new kind of multi-armed bandit problem, we extend the UCB algorithms by selecting arms according to the reward-cost ratio, and propose a new algorithm called UCB-BV. Our empirical results verify the effectiveness of this algorithm. Although the extension in UCB-BV seems natural and simple, and it is practically effective, the theoretical analysis on its regret bound turns out to be very difficult. We develop a set of new proof techniques and obtain a regret bound of $O(\ln B)$. Furthermore, we show that when applying the proposed algorithm to the setting with fixed costs (which is our special case), one can improve the corresponding regret bound obtained so far."
Smooth and Monotone Covariance Regularization,"The dangers of using the sample covariance matrix obtained from scarce data in high-dimensional settings are well recognized. In particular, the inconsistency of its eigenvalue spectrum has grave implications for modeling risk within the Markowitz portfolios framework. A variety of approaches to improve the covariance estimates exploit knowledge of structure in the data, including low-rank models (principal component and factor analysis), banded models, sparse inverse covariances, and parametric models. We investigate a different nonparametric prior for random vectors indexed along a low-dimensional manifold: we assume that the covariance matrix is monotone and smooth with respect to this indexing. This fits a variety of problems including interest-rate risk modeling in econometrics, and sensor array noise modeling. We formulate the estimation problem in a convex-optimization framework as a semidefinite-programming problem, and develop efficient first-order methods to solve it. We apply our framework on a number of examples with limited, missing and asynchronous data, and show that it has the potential to provide more accurate covariance matrix estimates than existing methods, and exhibits a desirable eigenvalue-spectrum correction effect."
Smooth and Monotone Covariance Regularization,"The dangers of using the sample covariance matrix obtained from scarce data in high-dimensional settings are well recognized. In particular, the inconsistency of its eigenvalue spectrum has grave implications for modeling risk within the Markowitz portfolios framework. A variety of approaches to improve the covariance estimates exploit knowledge of structure in the data, including low-rank models (principal component and factor analysis), banded models, sparse inverse covariances, and parametric models. We investigate a different nonparametric prior for random vectors indexed along a low-dimensional manifold: we assume that the covariance matrix is monotone and smooth with respect to this indexing. This fits a variety of problems including interest-rate risk modeling in econometrics, and sensor array noise modeling. We formulate the estimation problem in a convex-optimization framework as a semidefinite-programming problem, and develop efficient first-order methods to solve it. We apply our framework on a number of examples with limited, missing and asynchronous data, and show that it has the potential to provide more accurate covariance matrix estimates than existing methods, and exhibits a desirable eigenvalue-spectrum correction effect."
Smooth and Monotone Covariance Regularization,"The dangers of using the sample covariance matrix obtained from scarce data in high-dimensional settings are well recognized. In particular, the inconsistency of its eigenvalue spectrum has grave implications for modeling risk within the Markowitz portfolios framework. A variety of approaches to improve the covariance estimates exploit knowledge of structure in the data, including low-rank models (principal component and factor analysis), banded models, sparse inverse covariances, and parametric models. We investigate a different nonparametric prior for random vectors indexed along a low-dimensional manifold: we assume that the covariance matrix is monotone and smooth with respect to this indexing. This fits a variety of problems including interest-rate risk modeling in econometrics, and sensor array noise modeling. We formulate the estimation problem in a convex-optimization framework as a semidefinite-programming problem, and develop efficient first-order methods to solve it. We apply our framework on a number of examples with limited, missing and asynchronous data, and show that it has the potential to provide more accurate covariance matrix estimates than existing methods, and exhibits a desirable eigenvalue-spectrum correction effect."
Nearest Nonnegative Affine Subspace Classification Using Lotka-Volterra,"Nearest Subspace Classifier (NSC) is an important and well knownmethod in many multiclass classification problems. NSC uses theshortest projection distance between the testing data and thesubspaces spanned by the training samples of each class ascriteria for classification. To achieve good classificationresults, NSC requires that each test data is located on or atleast very close to the associated subspace. However, thisrequirement cannot be always met. Taking an example, objectclassification, it can be considered that an object withindifferent angles can span a subspace. There is an ideal propertyfor such images, i.e., the testing images from the same candidateare in fact located in the spanned subspace. In actualapplications, the image dimension is usually too high to dealwith. Reducing the dimension is necessary in many situations. Aproblem is that by reducing the dimension, the ideal propertycannot be guaranteed. This paper proposes a new classificationmethod: Nearest Nonnegative Affine Subspace Classifier (NNASC).Unlike the NSC, NNASC uses the shortest distance between thetesting data and the associated nonnegative affine subspace ascriteria for classification. Lotka-Volterra Recurrent NeuralNetworks(LV RNNs) are employed to solve the NNASC optimizationproblem. Three different kinds of databases are tested by theproposed algorithm. It demonstrates that NNASC outperforms the NSCand some other classifiers, especially, if the data is in lowdimensions."
Nearest Nonnegative Affine Subspace Classification Using Lotka-Volterra,"Nearest Subspace Classifier (NSC) is an important and well knownmethod in many multiclass classification problems. NSC uses theshortest projection distance between the testing data and thesubspaces spanned by the training samples of each class ascriteria for classification. To achieve good classificationresults, NSC requires that each test data is located on or atleast very close to the associated subspace. However, thisrequirement cannot be always met. Taking an example, objectclassification, it can be considered that an object withindifferent angles can span a subspace. There is an ideal propertyfor such images, i.e., the testing images from the same candidateare in fact located in the spanned subspace. In actualapplications, the image dimension is usually too high to dealwith. Reducing the dimension is necessary in many situations. Aproblem is that by reducing the dimension, the ideal propertycannot be guaranteed. This paper proposes a new classificationmethod: Nearest Nonnegative Affine Subspace Classifier (NNASC).Unlike the NSC, NNASC uses the shortest distance between thetesting data and the associated nonnegative affine subspace ascriteria for classification. Lotka-Volterra Recurrent NeuralNetworks(LV RNNs) are employed to solve the NNASC optimizationproblem. Three different kinds of databases are tested by theproposed algorithm. It demonstrates that NNASC outperforms the NSCand some other classifiers, especially, if the data is in lowdimensions."
Nearest Nonnegative Affine Subspace Classification Using Lotka-Volterra,"Nearest Subspace Classifier (NSC) is an important and well knownmethod in many multiclass classification problems. NSC uses theshortest projection distance between the testing data and thesubspaces spanned by the training samples of each class ascriteria for classification. To achieve good classificationresults, NSC requires that each test data is located on or atleast very close to the associated subspace. However, thisrequirement cannot be always met. Taking an example, objectclassification, it can be considered that an object withindifferent angles can span a subspace. There is an ideal propertyfor such images, i.e., the testing images from the same candidateare in fact located in the spanned subspace. In actualapplications, the image dimension is usually too high to dealwith. Reducing the dimension is necessary in many situations. Aproblem is that by reducing the dimension, the ideal propertycannot be guaranteed. This paper proposes a new classificationmethod: Nearest Nonnegative Affine Subspace Classifier (NNASC).Unlike the NSC, NNASC uses the shortest distance between thetesting data and the associated nonnegative affine subspace ascriteria for classification. Lotka-Volterra Recurrent NeuralNetworks(LV RNNs) are employed to solve the NNASC optimizationproblem. Three different kinds of databases are tested by theproposed algorithm. It demonstrates that NNASC outperforms the NSCand some other classifiers, especially, if the data is in lowdimensions."
Discovering Voxel-Level Functional Connectivity Between Cortical Regions,"Functional connectivity patterns are known to exist in the human brain at the millimeter scale, but the standard fMRI connectivity measure only computes functional correlations at a coarse level. We present the first method which identifies fine-grained functional connectivity between any two brain regions by simultaneously learning voxel-level connectivity maps over both regions. We show how to formulate this problem as a constrained least-squared optimization, with a spatial regularization term that allows connectivity maps to be learned much more efficiently. This optimization problem can be solved using a trust region approach, and can automatically discover connectivity between multiple distinct voxel clusters in the two regions. We validate our method in two experiments, demonstrating that we can successfully learn subregion connectivity structures from a small amount of training data. Our approach is shown to be substantially better at estimating fine-grained connectivity differences than state-of-the-art subregion connectivity methods, all of which learn maps over only one region at a time."
Discovering Voxel-Level Functional Connectivity Between Cortical Regions,"Functional connectivity patterns are known to exist in the human brain at the millimeter scale, but the standard fMRI connectivity measure only computes functional correlations at a coarse level. We present the first method which identifies fine-grained functional connectivity between any two brain regions by simultaneously learning voxel-level connectivity maps over both regions. We show how to formulate this problem as a constrained least-squared optimization, with a spatial regularization term that allows connectivity maps to be learned much more efficiently. This optimization problem can be solved using a trust region approach, and can automatically discover connectivity between multiple distinct voxel clusters in the two regions. We validate our method in two experiments, demonstrating that we can successfully learn subregion connectivity structures from a small amount of training data. Our approach is shown to be substantially better at estimating fine-grained connectivity differences than state-of-the-art subregion connectivity methods, all of which learn maps over only one region at a time."
Discovering Voxel-Level Functional Connectivity Between Cortical Regions,"Functional connectivity patterns are known to exist in the human brain at the millimeter scale, but the standard fMRI connectivity measure only computes functional correlations at a coarse level. We present the first method which identifies fine-grained functional connectivity between any two brain regions by simultaneously learning voxel-level connectivity maps over both regions. We show how to formulate this problem as a constrained least-squared optimization, with a spatial regularization term that allows connectivity maps to be learned much more efficiently. This optimization problem can be solved using a trust region approach, and can automatically discover connectivity between multiple distinct voxel clusters in the two regions. We validate our method in two experiments, demonstrating that we can successfully learn subregion connectivity structures from a small amount of training data. Our approach is shown to be substantially better at estimating fine-grained connectivity differences than state-of-the-art subregion connectivity methods, all of which learn maps over only one region at a time."
Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods,"We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{\dim}$ in convergence rate over traditional stochastic gradient methods, where $\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors."
Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods,"We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{\dim}$ in convergence rate over traditional stochastic gradient methods, where $\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors."
Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods,"We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{\dim}$ in convergence rate over traditional stochastic gradient methods, where $\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors."
Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods,"We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{\dim}$ in convergence rate over traditional stochastic gradient methods, where $\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors."
The impact on mid?level vision of statistically optimal divisive normalization in V1,"The first two areas of the primate visual cortex (V1 and V2) provide a paradigmatic example of hierarchical computation in the brain. However both the interactions between the two areas, and the functional properties of V2, are not well understood. Here we present insights gained from statistical models of natural scenes. In particular, we study the impact of V1 output nonlinearities on the statistics seen by V2. We focus on divisive normalization, a canonical computation that has been found in many neural areas and modalities. We consider models of V1 complex cells with (and without) different forms of surround normalization derived from the Gaussian Scale Mixture (GSM) generative model of natural scenes, and a Mixture of GSMs that also accounts for image non?homogeneities. When surround normalization is used, followed by ordinary PCA to linearly combine V1 responses across space, then V2-like receptive fields emerge. To provide a more quantitative assessment, we compare the resulting 2?stage models on perceptual tasks of figure/ground judgment and object recognition; in both cases we find systematic advantages for using a V1 stage with statistically optimal surround normalization."
The impact on mid?level vision of statistically optimal divisive normalization in V1,"The first two areas of the primate visual cortex (V1 and V2) provide a paradigmatic example of hierarchical computation in the brain. However both the interactions between the two areas, and the functional properties of V2, are not well understood. Here we present insights gained from statistical models of natural scenes. In particular, we study the impact of V1 output nonlinearities on the statistics seen by V2. We focus on divisive normalization, a canonical computation that has been found in many neural areas and modalities. We consider models of V1 complex cells with (and without) different forms of surround normalization derived from the Gaussian Scale Mixture (GSM) generative model of natural scenes, and a Mixture of GSMs that also accounts for image non?homogeneities. When surround normalization is used, followed by ordinary PCA to linearly combine V1 responses across space, then V2-like receptive fields emerge. To provide a more quantitative assessment, we compare the resulting 2?stage models on perceptual tasks of figure/ground judgment and object recognition; in both cases we find systematic advantages for using a V1 stage with statistically optimal surround normalization."
Hard and Easy Distributions of Bayesian Networks for Junction Tree Computation ,"The effort associated with Bayesian network computation is vital in many infer-ence and machine learning settings. In this paper, we introduce a novel algorithm,GPART, that generates synthetic Bayesian networks that reflect several input pa-rameters. Using the algorithm, we investigate how various parameters of Bayesiannetworks can affect junction tree characteristics and hence computation time. Wegeneralize previous approaches to randomly generating Bayesian network by (i)introducing a novel depth parameter as well as (ii) allowing state space size andnumber of parameters for a non-root node to be probability distributions. In ex-periments, we surprisingly find that increasing our novel depth parameter dramati-cally increases clique tree size and computation time. Using parameters computedfrom application networks as parameters in GPART, and comparing the resultingjunction trees, we better understand the similarities and differences between ap-plication and synthetic Bayesian networks."
Hard and Easy Distributions of Bayesian Networks for Junction Tree Computation ,"The effort associated with Bayesian network computation is vital in many infer-ence and machine learning settings. In this paper, we introduce a novel algorithm,GPART, that generates synthetic Bayesian networks that reflect several input pa-rameters. Using the algorithm, we investigate how various parameters of Bayesiannetworks can affect junction tree characteristics and hence computation time. Wegeneralize previous approaches to randomly generating Bayesian network by (i)introducing a novel depth parameter as well as (ii) allowing state space size andnumber of parameters for a non-root node to be probability distributions. In ex-periments, we surprisingly find that increasing our novel depth parameter dramati-cally increases clique tree size and computation time. Using parameters computedfrom application networks as parameters in GPART, and comparing the resultingjunction trees, we better understand the similarities and differences between ap-plication and synthetic Bayesian networks."
Hard and Easy Distributions of Bayesian Networks for Junction Tree Computation ,"The effort associated with Bayesian network computation is vital in many infer-ence and machine learning settings. In this paper, we introduce a novel algorithm,GPART, that generates synthetic Bayesian networks that reflect several input pa-rameters. Using the algorithm, we investigate how various parameters of Bayesiannetworks can affect junction tree characteristics and hence computation time. Wegeneralize previous approaches to randomly generating Bayesian network by (i)introducing a novel depth parameter as well as (ii) allowing state space size andnumber of parameters for a non-root node to be probability distributions. In ex-periments, we surprisingly find that increasing our novel depth parameter dramati-cally increases clique tree size and computation time. Using parameters computedfrom application networks as parameters in GPART, and comparing the resultingjunction trees, we better understand the similarities and differences between ap-plication and synthetic Bayesian networks."
Robust Structural Metric Learning,"Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degradesaccordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation,while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methodsin both high- and low-noise settings."
Robust Structural Metric Learning,"Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degradesaccordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation,while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methodsin both high- and low-noise settings."
Robust Structural Metric Learning,"Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degradesaccordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation,while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methodsin both high- and low-noise settings."
Multiclass Clustering using a Semidefinite Relaxation,"Spectral and other cut-based relaxations have been applied to graph clustering problems. In this paper, we propose a novel semidefinite relaxation for graph clustering known as Max-cut clustering. The clustering problem is formulated in terms of a discrete optimization problem and then relaxed to a SDP. To make the optimization scalable, we represent the SDP by a low-rank factorized approximation that reduces the number of variables, and then use a simple projected gradient method to solve it. To obtain the clustering, we propose a reweighted rounding scheme to get integral solutions. We also extend this formulation to a global approach to multi-class clustering and MAP inference in graphical models. Experimental results indicate that we outperform state-of-art several clustering methods. The algorithm is extended to perform MAP inference in graphical models and outperforms competing methods."
Higher-order Nonparametric Models for Recognition by Analogy,"Nonparametric classification methods such as nearest neighbor offer the ability tolearn by association, leveraging large amounts of data, avoiding a training phase,and placing no assumptions on the structure of label space. However, such meth-ods often perform poorly due to the limited ability of typical distance functions tocapture complex relationships in the data. We propose a method for learning dis-tance functions using higher-order nonparametric models, resulting in better per-formance while still learning by association with no training phase. Our methodreplaces single example association with pair association, and can be interpretedas finding analogies among training and test examples. We test our method onRGB-D [1], a multi-view object data set, which lets us learn implicitly when dif-ferent 2D shapes describe a similar 3D structure. We show that our method isparticularly beneficial in the one-shot transfer regime, where only one exampleis available for a test category. Where traditional supervised learning methodsperform poorly, our method can use the relationships between objects in differentcategories to learn the structure of categories with impoverished training data."
Higher-order Nonparametric Models for Recognition by Analogy,"Nonparametric classification methods such as nearest neighbor offer the ability tolearn by association, leveraging large amounts of data, avoiding a training phase,and placing no assumptions on the structure of label space. However, such meth-ods often perform poorly due to the limited ability of typical distance functions tocapture complex relationships in the data. We propose a method for learning dis-tance functions using higher-order nonparametric models, resulting in better per-formance while still learning by association with no training phase. Our methodreplaces single example association with pair association, and can be interpretedas finding analogies among training and test examples. We test our method onRGB-D [1], a multi-view object data set, which lets us learn implicitly when dif-ferent 2D shapes describe a similar 3D structure. We show that our method isparticularly beneficial in the one-shot transfer regime, where only one exampleis available for a test category. Where traditional supervised learning methodsperform poorly, our method can use the relationships between objects in differentcategories to learn the structure of categories with impoverished training data."
A Performance Function for Multi-class Classification,"A performance function for multi-class classification is proposed in this paper. This performance function takes Nearest Subspace (NS) residual together with Collaborative Representation (CR) residual as variables. Strong underlying geometric explanations make those well-known residual measurements effective for multi-class classification problem. Nearest Subspace Classification (NSC) is a local measurement that considers distance between testing sample and each class, while Collaborative Representation based Classifier (CRC) is a global method measuring both intra-class and inter-class measurements. These two measurements are independent to each other. The first and the second order Taylor series of the performance function are analyzed, which characterizes this function well in some degree. A Second Order Performance Function (SOPF) is derived by involving a quadratic term of the first order terms and a product term. The SOPF contains two parameters with a positive factor constraint. A classifier based on SOPF is proposed, which improves a recent reported classifier called CROC(Collaborative Representation Optimized Classifier). The proposed algorithm is tested against human face and handwritten digits recognition and it achieves competitive classification result comparing to baseline methods. A large range of parameter configuration is acceptable once the positive factor constraint is satisfied."
A Performance Function for Multi-class Classification,"A performance function for multi-class classification is proposed in this paper. This performance function takes Nearest Subspace (NS) residual together with Collaborative Representation (CR) residual as variables. Strong underlying geometric explanations make those well-known residual measurements effective for multi-class classification problem. Nearest Subspace Classification (NSC) is a local measurement that considers distance between testing sample and each class, while Collaborative Representation based Classifier (CRC) is a global method measuring both intra-class and inter-class measurements. These two measurements are independent to each other. The first and the second order Taylor series of the performance function are analyzed, which characterizes this function well in some degree. A Second Order Performance Function (SOPF) is derived by involving a quadratic term of the first order terms and a product term. The SOPF contains two parameters with a positive factor constraint. A classifier based on SOPF is proposed, which improves a recent reported classifier called CROC(Collaborative Representation Optimized Classifier). The proposed algorithm is tested against human face and handwritten digits recognition and it achieves competitive classification result comparing to baseline methods. A large range of parameter configuration is acceptable once the positive factor constraint is satisfied."
Communication-Efficient Algorithms for Statistical Optimization,"We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$. Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset."
Communication-Efficient Algorithms for Statistical Optimization,"We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$. Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset."
Communication-Efficient Algorithms for Statistical Optimization,"We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$. Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset."
Preparing Deep Belief Networks for Pratical Tasks,"Deep Belief Networks (DBNs) is a probabilistic generative models composed of multiple layers of stochastic, latent variables. The network can learn many layers of features on various type of data such as gray scaled images, color images and acoustic data. This paper further examined the ability of DBNs to interpret the binary representation of data. The performance is validated by learning given distributions such as normal distribution, Poisson distribution and random number generator. We have shown that Deep Believe Networks can successfully learn the probability distribution with binary encoded dataset. With this property, we can further extend DBNs into states or properties prediction application,we will provide a example showing that DBNs can take multiple binary encoded parameter as input vector and predict the belong category of these input. Generally, the sensory input of DBNs contains information belong to a certain timestep, that is, the prediction depends only on the input. However, in some practical tasks, prediction often depend not only on the current state but also the history of states. We propose a method combining DBNs with Echo State Networks(ESNs), using the properties of ESNs' reservoir to encoded the history of previous states which gives us an idea of artificial dreaming."
Learning Hierarchical Spatial Tiling Representation for Scene Tagging,"In order to name and localize semantic tags/attributes on natural scene images, in this paper, we first propose a structure learning method to learn a novel representation for scene modeling, namely Hierarchical Space Tiling (HST). It is able to account for the structure variations of scene configurations using different parts/words in the learned tilling dictionary. Then, the association relationship between a part and a semantic tag/attribute is discovered by exploring their mutual information on scene images. Finally, given a naked image, we first parse it into a tree structure with the learned HST model, then assign tags to the terminal nodes/parts on the hierarchy. We evaluate the advantages of the proposed method from three aspects. (i) The proposed HST is compact and less ambiguous in constructing the compositions of scenes. (ii) The semantic tags are named and localized accurately on scene images. (iii) It has scalability potential to real applications by showing that the parsing + tagging is extremely fast. "
Constructing ?2-graph for Clustering,"Constructing a sparse similarity graph is an important step in graph-oriented clustering algorithms. In a similarity graph, the vertex denotes a data point and the connection weight between two data points represents the similarity. Some recent works use ?1-minimization based sparse coefficients to construct the graph for various applications, and impressive results are achieved. This paper proposes a method to construct the similarity graph, called ?2-graph, by using ?2-minimization based representation coefficients. This graph can produce a block-sparse similarity graph via enforcing locality onto the non-sparse representation. The representation is derived via solving an optimization problem to obtain an interesting closed-form solution. Experimental results using several facial databases demonstrate that the proposed method outperforms two state-of-the-art ?1-minimization based clustering algorithms, i.e., Sparse Subspace Clustering [1, 2] and Low Rank Representation [3], in accuracy, robustness and time saving."
Constructing ?2-graph for Clustering,"Constructing a sparse similarity graph is an important step in graph-oriented clustering algorithms. In a similarity graph, the vertex denotes a data point and the connection weight between two data points represents the similarity. Some recent works use ?1-minimization based sparse coefficients to construct the graph for various applications, and impressive results are achieved. This paper proposes a method to construct the similarity graph, called ?2-graph, by using ?2-minimization based representation coefficients. This graph can produce a block-sparse similarity graph via enforcing locality onto the non-sparse representation. The representation is derived via solving an optimization problem to obtain an interesting closed-form solution. Experimental results using several facial databases demonstrate that the proposed method outperforms two state-of-the-art ?1-minimization based clustering algorithms, i.e., Sparse Subspace Clustering [1, 2] and Low Rank Representation [3], in accuracy, robustness and time saving."
Learning Granger Graphical Models via Alternating Direction Method of Multipliers,"This paper presents a recent powerful algorithm, namely, the alternating direction method of multipliers (ADMM) for solving topology selection problems in Granger graphical models of autoregressive processes. The existence of a directed edge from node $j$ to node $i$ in the graph can be specified by the nonzero $(i,j)$ entry of the autoregressive coefficients. The problem of estimating the graph topology is formulated as a least-squares problem with an $\ell_1$-type regularization and can be regarded as a variant of Group Lasso problem. The value of the regularization parameter which controls the density of the estimated graph can be determined by minimizing Bayes information criterion score. We illustrate the idea and verify the performance of the ADMM algorithm on randomly generated data sets. This approach is finally applied on Google Flu Trends data set to learn a causal structure of flu activities from $51$ states in the USA."
Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions,"We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a $\order(\pdim/T)$ convergencerate for strongly convex objectives in $\pdim$ dimensions and $\order(\sqrt{\spindex( \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our algorithm is based on successively solving a series of $\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error ofour solution after $T$ iterations is at most$\order(\spindex(\log\pdim)/T)$, with natural extensions toapproximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem."
Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions,"We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a $\order(\pdim/T)$ convergencerate for strongly convex objectives in $\pdim$ dimensions and $\order(\sqrt{\spindex( \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our algorithm is based on successively solving a series of $\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error ofour solution after $T$ iterations is at most$\order(\spindex(\log\pdim)/T)$, with natural extensions toapproximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem."
Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions,"We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a $\order(\pdim/T)$ convergencerate for strongly convex objectives in $\pdim$ dimensions and $\order(\sqrt{\spindex( \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our algorithm is based on successively solving a series of $\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error ofour solution after $T$ iterations is at most$\order(\spindex(\log\pdim)/T)$, with natural extensions toapproximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem."
Active Sensing as Bayes-Optimal Sequential Decision-Making,"Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience.  An important but poorly understood aspect of sensory processing is the role of active sensing: the use of self-motion to selectively process the most rewarding or informative aspects of the environment. Here, we present a Bayes-optimal inference and decision-making framework for active sensing, which directly minimizes a cost function that takes into account behavioral costs such as response delay, error, and effort. Unlike previously proposed algorithms that optimize heuristic objectives such as expected entropy reduction [Butko and Movellan, 2010] or one-step look-ahead accuracy [Najemnik and Geisler, 2005], this optimal policy can account for search duration as well as location, and is sensitive to contextual factors such as the relative importance of time, error, and effort.  We implement the optimal policy, along with the two heuristic policies, for an example visual search task, and illustrate how the heuristic policies deviate from optimal performance in various contexts.  We show that the discrepancy is especially large when the cost of time and the cost of switching between sensing locations are high. We demonstrate a potential route for overcoming the computational complexity of the optimal algorithm, especially problematic in large real-world applications, by exploiting the concavity and smoothness of the value function.  We show that a basis function approximation to the value function, several orders of magnitude reduced in dimensionality and complexity, achieves near-optimal performance."
Active Sensing as Bayes-Optimal Sequential Decision-Making,"Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience.  An important but poorly understood aspect of sensory processing is the role of active sensing: the use of self-motion to selectively process the most rewarding or informative aspects of the environment. Here, we present a Bayes-optimal inference and decision-making framework for active sensing, which directly minimizes a cost function that takes into account behavioral costs such as response delay, error, and effort. Unlike previously proposed algorithms that optimize heuristic objectives such as expected entropy reduction [Butko and Movellan, 2010] or one-step look-ahead accuracy [Najemnik and Geisler, 2005], this optimal policy can account for search duration as well as location, and is sensitive to contextual factors such as the relative importance of time, error, and effort.  We implement the optimal policy, along with the two heuristic policies, for an example visual search task, and illustrate how the heuristic policies deviate from optimal performance in various contexts.  We show that the discrepancy is especially large when the cost of time and the cost of switching between sensing locations are high. We demonstrate a potential route for overcoming the computational complexity of the optimal algorithm, especially problematic in large real-world applications, by exploiting the concavity and smoothness of the value function.  We show that a basis function approximation to the value function, several orders of magnitude reduced in dimensionality and complexity, achieves near-optimal performance."
Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation,"Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented."
Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation,"Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented."
Sparse Online Topic Models,"Probabilistic online topic models have been developed for discovering latent semantic representations from massive data corpora. However, due to normalization constraints, probabilistic topic models can be ineffective in controlling the sparsity of discovered representations. In this paper, we present a sparse online topic model, which directly controls the sparsity of word and document codes by imposing sparsity-inducing regularization. The topical dictionary is learned by an online algorithm, which is efficient and guaranteed to converge. We extensively evaluate the basic sparse online topic model as well as its collapsed and supervised extensions on large-scale data sets. Our results demonstrate appealing performance."
Monte Carlo Methods for Maximum Margin Supervised Topic Models,"An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihood-based supervised topic models, of which posterior inference can be carried out using the Bayes' rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper, we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency."
Monte Carlo Methods for Maximum Margin Supervised Topic Models,"An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihood-based supervised topic models, of which posterior inference can be carried out using the Bayes' rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper, we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency."
Monte Carlo Methods for Maximum Margin Supervised Topic Models,"An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihood-based supervised topic models, of which posterior inference can be carried out using the Bayes' rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper, we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency."
Input Variable Selection for Linear Regression Model using Nearest Correlation Spectral Clustering,"Linear regression models have been widely accepted in many scientific and engineering fields for the estimation or interpretation of phenomena.  When a linear regression model is built, appropriate input variables have to be selected to achieve high estimation performance.  This work proposes new methodologies for selecting input variables for linear regression models using nearest correlation spectral clustering (NCSC), which is a correlation-based clustering method.  In the present work, NCSC is used for variable group construction, and a few variable groups are selected by their contribution to estimates or by group Lasso; they are referred to as NCSC-based variable selection (NCSC-VS) and NCSC-group Lasso (NCSC-GL). The performances of the proposed NCSC-VS and NCSC-GL are examined through a case study of chemometrics data."
Analysis of Differential Privacy Based on Importance Weighting,"This paper introduces and analyzes a novel data-publishing mechanism based on computing weights that make an existing dataset, for which there are no confidentiality issues, analogous to the dataset that must be kept private. The existing dataset may be genuine but public already, or it can be synthetic. The only necessary requirement is that it have similar schema as the private dataset. The weights are importance sampling weights, but they are regularized and have noise added. The weights allow statistical queries to be answered approximately while provably guaranteeing differential privacy. We derive expressions for the variance of the approximate answers. Experiments show that the new mechanism performs well even when the public dataset is quite different from the private dataset,and the privacy budget is small."
Analysis of Differential Privacy Based on Importance Weighting,"This paper introduces and analyzes a novel data-publishing mechanism based on computing weights that make an existing dataset, for which there are no confidentiality issues, analogous to the dataset that must be kept private. The existing dataset may be genuine but public already, or it can be synthetic. The only necessary requirement is that it have similar schema as the private dataset. The weights are importance sampling weights, but they are regularized and have noise added. The weights allow statistical queries to be answered approximately while provably guaranteeing differential privacy. We derive expressions for the variance of the approximate answers. Experiments show that the new mechanism performs well even when the public dataset is quite different from the private dataset,and the privacy budget is small."
Temporal Coding of Local Spectrogram Features for Robust Sound Recognition,"There is much evidence to suggest that the human auditory system uses localised time-frequency information for the robust recognition of sounds. Despite this, conventional systems typically rely on features extracted from short windowed frames over time, covering the whole frequency spectrum. Such approaches are not inherently robust to noise, as each frame will contain a mixture of the spectral information from noise and signal.Here, we propose a novel approach based on the temporal coding of Local Spectrogram Features (LSFs), which generate spikes that are used to train a Spiking Neural Network (SNN) with temporal learning. LSFs represent robust location information in the spectrogram surrounding keypoints, which are detected in a signal-driven manner, such that the effect of noise on the temporal coding is reduced. Our system models characteristic clusters of LSFs in an unsupervised way, using tonotopic learning based on Self Organising Maps (SOMs).Our experiments demonstrate the robust performance of our approach across a variety of noise conditions, such that it is able to outperform the conventional frame-based baseline methods."
Temporal Coding of Local Spectrogram Features for Robust Sound Recognition,"There is much evidence to suggest that the human auditory system uses localised time-frequency information for the robust recognition of sounds. Despite this, conventional systems typically rely on features extracted from short windowed frames over time, covering the whole frequency spectrum. Such approaches are not inherently robust to noise, as each frame will contain a mixture of the spectral information from noise and signal.Here, we propose a novel approach based on the temporal coding of Local Spectrogram Features (LSFs), which generate spikes that are used to train a Spiking Neural Network (SNN) with temporal learning. LSFs represent robust location information in the spectrogram surrounding keypoints, which are detected in a signal-driven manner, such that the effect of noise on the temporal coding is reduced. Our system models characteristic clusters of LSFs in an unsupervised way, using tonotopic learning based on Self Organising Maps (SOMs).Our experiments demonstrate the robust performance of our approach across a variety of noise conditions, such that it is able to outperform the conventional frame-based baseline methods."
Isotropic Hashing ,"Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances."
Isotropic Hashing ,"Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances."
Sparse Optimal Control Signals for Natural Human Movements Using the Infinity Norm,"Optimal control models have been a successful tool in describing many aspects and characteristics of human movements. While such models have a sound theoretical foundation, their interpretation and neuronal implementation in the Central Nervous System (CNS) is not clear. We propose that the CNS not only utilizes control policies that are optimal with respect to a criterion, but also satisfy sparsity constraints. In recent years sparsity has played a pivotal role in theoretical neuroscience for information processing (such as vision). Typically, sparsity is imposed by introducing a cardinality constraint or penalty measured or approximate by the one-norm. In this work, to obtain sparse control signals, however, the $L_{\infty}$ norm is used as a penalty on the control signal. Even though such sparse control signals are discontinuous, the movements that result are continuous and smooth.  In addition, such sparse control signals are more biologically realistic and have a clear neuronal interpretation with a sequence of neuronal spikes. We show that moreover sparse optimal control signals quantitatively describe real human arm movements with high accuracy. "
Sparse Optimal Control Signals for Natural Human Movements Using the Infinity Norm,"Optimal control models have been a successful tool in describing many aspects and characteristics of human movements. While such models have a sound theoretical foundation, their interpretation and neuronal implementation in the Central Nervous System (CNS) is not clear. We propose that the CNS not only utilizes control policies that are optimal with respect to a criterion, but also satisfy sparsity constraints. In recent years sparsity has played a pivotal role in theoretical neuroscience for information processing (such as vision). Typically, sparsity is imposed by introducing a cardinality constraint or penalty measured or approximate by the one-norm. In this work, to obtain sparse control signals, however, the $L_{\infty}$ norm is used as a penalty on the control signal. Even though such sparse control signals are discontinuous, the movements that result are continuous and smooth.  In addition, such sparse control signals are more biologically realistic and have a clear neuronal interpretation with a sequence of neuronal spikes. We show that moreover sparse optimal control signals quantitatively describe real human arm movements with high accuracy. "
Sparse Optimal Control Signals for Natural Human Movements Using the Infinity Norm,"Optimal control models have been a successful tool in describing many aspects and characteristics of human movements. While such models have a sound theoretical foundation, their interpretation and neuronal implementation in the Central Nervous System (CNS) is not clear. We propose that the CNS not only utilizes control policies that are optimal with respect to a criterion, but also satisfy sparsity constraints. In recent years sparsity has played a pivotal role in theoretical neuroscience for information processing (such as vision). Typically, sparsity is imposed by introducing a cardinality constraint or penalty measured or approximate by the one-norm. In this work, to obtain sparse control signals, however, the $L_{\infty}$ norm is used as a penalty on the control signal. Even though such sparse control signals are discontinuous, the movements that result are continuous and smooth.  In addition, such sparse control signals are more biologically realistic and have a clear neuronal interpretation with a sequence of neuronal spikes. We show that moreover sparse optimal control signals quantitatively describe real human arm movements with high accuracy. "
"A meta algorithm making centralized graph computation faster, distributed and at times better","In this paper, we present a meta algorithm that takes existing centralized algorithms for graph computation and makes them distributed and faster. In a nutshell, the meta algorithm creates a randomized partition of  the graph, with each partition being a small subgraph, and it then runs the centralized algorithm on each partition separately and stitches the resulting solutions to produce a global solution. We illustrate this meta algorithm with two popular problems: computation of Maximum A Posteriori (MAP) assignment in an arbitrary pairwise Markov Random Field (MRF), and modularity optimization for clustering and community detection.We show that the resulting distributed algorithms for these problems essentially run in linear time  and that they perform as well -- or even better -- than the original centralized algorithm as long as the graphs have geometric structure. More precisely, if the centralized algorithm is a constant factor approximation, the resulting distributed algorithm is also a constant factor approximation with constant slightly bigger; but if the centralized algorithm is a non-constant (e.g. logarithmic) factor approximation, then the resulting distributed algorithm becomes a constant factor approximation. For general graphs (not necessarily geometric), we  compute explicit bounds on the loss of  performance of the distributed algorithm with respect to the centralized algorithm."
On the connections between saliency and tracking,"A model connecting visual tracking and saliency has recently been proposed. Thismodel is based on the saliency hypothesis for tracking which postulates that trackingis achieved by the top-down tuning, based on target features, of discriminantcenter-surround saliency mechanisms over time. In this work, we identify threemain predictions that must hold if the hypothesis were true: 1) tracking reliabilityshould be larger for salient than for non-salient targets, 2) tracking reliabilityshould have a dependence on the defining variables of saliency, namely featurecontrast and distractor heterogeneity, and must replicate the dependence ofsaliency on these variables, and 3) saliency and tracking can be implemented withcommon low level neural mechanisms. We confirm that the first two predictionshold by reporting results from a set of human behavior studies on the connectionbetween saliency and tracking. We also show that the third prediction holds byconstructing a common neurophysiologically plausible architecture that can computationallysolve both saliency and tracking. This architecture is fully compliantwith the standard physiological models of V1 and MT, and with what is knownabout attentional control in area LIP, while explaining the results of the humanbehavior experiments."
Infinite EFH: an Infinite Undirected Latent Variable Model,"Bayesian nonparametrics has been promising in learning Bayesian networks, but very few attempts have been made under the context of undirected Markov networks. This paper presents infinite exponential family Harmoniums (iEFH), an attempt to broaden the use of Bayesian nonparametrics to automatically resolve the unknown number of hidden units in undirected latent variable models. We further generalize iEFH to the supervised infinite max-margin Harmoniums (iMMH), which directly regularizes the latent representations via imposing max-margin constraints for discovering predictive latent representations that are good for classification. We use the sparsity-inducing Indian buffet process prior to select latent units from an infinite pool. Our extensive experiments on real text and image datasets appear to demonstrate the benefits of iEFH and iMMH inherited from both Bayesian nonparametrics and max-margin learning."
Infinite EFH: an Infinite Undirected Latent Variable Model,"Bayesian nonparametrics has been promising in learning Bayesian networks, but very few attempts have been made under the context of undirected Markov networks. This paper presents infinite exponential family Harmoniums (iEFH), an attempt to broaden the use of Bayesian nonparametrics to automatically resolve the unknown number of hidden units in undirected latent variable models. We further generalize iEFH to the supervised infinite max-margin Harmoniums (iMMH), which directly regularizes the latent representations via imposing max-margin constraints for discovering predictive latent representations that are good for classification. We use the sparsity-inducing Indian buffet process prior to select latent units from an infinite pool. Our extensive experiments on real text and image datasets appear to demonstrate the benefits of iEFH and iMMH inherited from both Bayesian nonparametrics and max-margin learning."
Transfer learning in the blind with the personal perceptron,"We consider the transfer learning scenario where test data is randomly sampled from several, possibly unknown distributions. This scenario is interesting for many natural language processing services, for example, where the domain, genre and style of the next input example is not known in advance. We introduce a perceptron learning algorithm that learns so-called 'error prints' for intermediate models that are averaged at test time to adapt the final model to new data points. An error print marks a region where a model is more likely to misclassify examples. We evaluate our algorithm on 6 sentiment analysis datasets (12 problems) and show that the method is effective, in particular when test data is not sampled from the source distribution. We show that the personal perceptron is more expressive than other large-margin perceptron algorithms and establish an relation to nearest neighbor methods. "
On Generalization Performance of Unified Learning Model,"Recently, A.Anonymous [1] showed a unified formulation based on robust optimization thatembraces several kinds of classification methods and gave a geometric interpretation and statistical interpretation for the unified formulation. This paper extends the unified model to cover not only  binary classification but also  regression and outlier (or novelty) detection. Then we show that the unified model minimizes a well-known financial risk measure. Moreover, after deriving generalization bounds using such risk measures, we prove that the unified model gives a solution that minimizes the generalization bounds. "
Simple Models for Shunting Inhibition,"The integration of excitatory and inhibitory inputs at a neuron follows a nonlinear process, which is generally termed shunting inhibition. The experimental data has revealed that the effect of shunting inhibitionon the somatic potential can be largely expressed as a simple arithmetic rule, in which the contribution of shunting inhibition is proportional to the product between the contributions of excitatory and inhibitory inputs when they are applied individually. In this study, we develop simple neuron and network models for shunting inhibition. Our simple neuron models reproduce the experimental results qualitatively. We show that shunting inhibition can provide a mechanism to retain persistent activity in a network, andcan be well approximated as divisive normalization in describing the stationary states of continuous attractor neural networks."
Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model,"We consider linear regression in the high-dimensional regime in which the number of observations $n$ is smaller than the number of parameters $p$. A very successful approach in this setting uses $\ell_1$-penalized least squares (a.k.a. the Lasso) to search for a subset of $s_0< n$  parameters that best explain the data, while setting the other parameters to zero. A considerable amount of work has been devoted to characterizing the estimation and model selection problems within this approach. In this paper we consider instead the fundamental --but  far less understood-- question of statistical significance. Roughly speaking, when the Lasso estimates a specific parameter to be zero (or non-zero), \emph{how certain is this conclusion}? We study this problem under the random design model in which the rows of the design matrix are i.i.d. and drawn from an unknown high-dimensional Gaussian distribution. This situation arises --for instance-- in learning high-dimensional Gaussian graphical models. Leveraging on an asymptotic distributional characterization of regularized least squares estimators, we develop a procedure for computing p-values and hence assessing statistical significance for hypothesis testing. We characterize the power of this procedure, and evaluate it on synthetic and real data, comparing it with earlier proposals."
Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model,"We consider linear regression in the high-dimensional regime in which the number of observations $n$ is smaller than the number of parameters $p$. A very successful approach in this setting uses $\ell_1$-penalized least squares (a.k.a. the Lasso) to search for a subset of $s_0< n$  parameters that best explain the data, while setting the other parameters to zero. A considerable amount of work has been devoted to characterizing the estimation and model selection problems within this approach. In this paper we consider instead the fundamental --but  far less understood-- question of statistical significance. Roughly speaking, when the Lasso estimates a specific parameter to be zero (or non-zero), \emph{how certain is this conclusion}? We study this problem under the random design model in which the rows of the design matrix are i.i.d. and drawn from an unknown high-dimensional Gaussian distribution. This situation arises --for instance-- in learning high-dimensional Gaussian graphical models. Leveraging on an asymptotic distributional characterization of regularized least squares estimators, we develop a procedure for computing p-values and hence assessing statistical significance for hypothesis testing. We characterize the power of this procedure, and evaluate it on synthetic and real data, comparing it with earlier proposals."
Direct 0-1 Loss Minimization and Margin Maximization with Boosting,"We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensembled classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples. Once the training classification error is reduced to its minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching the maximum of the margins. Experimental results on a synthetic data and a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost and LogitBoost."
Direct 0-1 Loss Minimization and Margin Maximization with Boosting,"We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensembled classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples. Once the training classification error is reduced to its minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching the maximum of the margins. Experimental results on a synthetic data and a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost and LogitBoost."
Direct 0-1 Loss Minimization and Margin Maximization with Boosting,"We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensembled classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples. Once the training classification error is reduced to its minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching the maximum of the margins. Experimental results on a synthetic data and a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost and LogitBoost."
Direct 0-1 Loss Minimization and Margin Maximization with Boosting,"We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensembled classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples. Once the training classification error is reduced to its minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching the maximum of the margins. Experimental results on a synthetic data and a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost and LogitBoost."
Poset View and Energy Distribution Criteria for Monotonic Dual Decomposition,"Dual decomposition algorithms based on block coordinate descent are efficient techniques for approximate MAP inference in graphical models. They optimize a local dual function at each step to monotonically increase the dual function value. In this paper, we present a unified framework for constructing and optimizing the local dual function based on the partially ordered set (poset). To maximize the local dual function, we first introduce the concept of the energy distribution ratio, and then derive an explicit and globally optimal solution, which covers all the existing algorithms. We show that the differences of the monotonic algorithms can be summarized in the local dual functions and the energy distribution ratios. Furthermore, we investigate the effect of energy distribution ratios on convergence and introduce energy distribution criteria for fast convergence. New algorithms are proposed based on the criteria, and the experimental results show they outperform the existing algorithms on convergence performance."
Poset View and Energy Distribution Criteria for Monotonic Dual Decomposition,"Dual decomposition algorithms based on block coordinate descent are efficient techniques for approximate MAP inference in graphical models. They optimize a local dual function at each step to monotonically increase the dual function value. In this paper, we present a unified framework for constructing and optimizing the local dual function based on the partially ordered set (poset). To maximize the local dual function, we first introduce the concept of the energy distribution ratio, and then derive an explicit and globally optimal solution, which covers all the existing algorithms. We show that the differences of the monotonic algorithms can be summarized in the local dual functions and the energy distribution ratios. Furthermore, we investigate the effect of energy distribution ratios on convergence and introduce energy distribution criteria for fast convergence. New algorithms are proposed based on the criteria, and the experimental results show they outperform the existing algorithms on convergence performance."
Data Representation with Rank Regularized PCA,"Trace-norm is often used in low-rank data representation models. In this paper, we point out some drawbacks of the trace norm based approach and propose a rank regularized formulation which can be solved very efficiently. We did extensive experiments on six datasets. Experiments show the advantage of the proposed approach."
Clustered Bandits,"We consider a multi-armed bandit setting that is inspired by real-world applications in e-commerce.  In our setting, there are a few types of users, each with a specific response to the different arms. When a user enters the system, his type is unknown to the decision maker.  The decision maker can either treat each user separately ignoring the previously observed users, or can attempt to take advantage of knowing that only few types exist and cluster the users according to their response to the arms. We devise algorithms that combine the usual exploration-exploitation tradeoff with clustering of users and demonstrate the value of clustering.  In the process of developing algorithms for the clustered setting, we propose and analyze simple algorithms for the setup where a decision maker knows that a user belongs to one of few types, but does not know which one."
Spectral Differential Privacy,"Positive semidefinite matrices are important for a number of machine learning applications. We consider the problem of differentially private publication of positive semidefinite matrices computed from private information. Differential privacy is typically achieved by adding random noise.However, when the outputs form positive semidefinite matrices, element-wise additive randomization causes problems. First, when not a single element, but the entire matrix is released, the scale of noises to provide differential privacy can be too large. Second, such randomization not only destroys the positive semidefiniteness, but may be statistically denoised in some cases.For these problems, we introduce a new randomization mechanism which separately randomizes eigenvectors and eigenvalues so that the randomization does not completely destroy the spectral features. Furthermore, noting that low-rank approximation preserves useful information of matrices while discarding unnecessarily details, we incorporate low-rank approximation into randomization.We prove that the scale of perturbation required to guarantee differential privacy is inversely proportional to the rank of the output matrices in the proposed randomization mechanism. Thus, if a data analyst does not need the output matrix itself, but needs only a low-rank approximation, the scale of perturbation can be relatively smaller without sacrificing privacy. This is convenient for machine learning applications which work well even with lower-rank approximation.We experimentally demonstrate that low-rank approximation helps to implicitly control the accuracy-privacy trade-off with  a collaborative filtering example."
On Pre-training Shallow Networks with Support Vector Machine Primals,"We present a methodology to pre-train shallow neural networks with Support Vector Machine primals. We train a Support Vector Machine and extract the primal weights to embed them as pre-trained prior knowledge in a shallow neural network; we then proceed to apply backpropagation to leverage and fine tune this knowledge. This contrasts with previous work on pre-training, in which unsupervised pre-training has been used as feature extractors in deep learning. In our MNIST experimental results, we find that using even only $\frac{1}{60}$ of the original dataset for the Support Vector Machine primal pre-training yielded a consistently faster convergence in the network. We believe this paper opens up interesting opportunities for pre-training shallow networks using prior knowledge."
On Pre-training Shallow Networks with Support Vector Machine Primals,"We present a methodology to pre-train shallow neural networks with Support Vector Machine primals. We train a Support Vector Machine and extract the primal weights to embed them as pre-trained prior knowledge in a shallow neural network; we then proceed to apply backpropagation to leverage and fine tune this knowledge. This contrasts with previous work on pre-training, in which unsupervised pre-training has been used as feature extractors in deep learning. In our MNIST experimental results, we find that using even only $\frac{1}{60}$ of the original dataset for the Support Vector Machine primal pre-training yielded a consistently faster convergence in the network. We believe this paper opens up interesting opportunities for pre-training shallow networks using prior knowledge."
On Pre-training Shallow Networks with Support Vector Machine Primals,"We present a methodology to pre-train shallow neural networks with Support Vector Machine primals. We train a Support Vector Machine and extract the primal weights to embed them as pre-trained prior knowledge in a shallow neural network; we then proceed to apply backpropagation to leverage and fine tune this knowledge. This contrasts with previous work on pre-training, in which unsupervised pre-training has been used as feature extractors in deep learning. In our MNIST experimental results, we find that using even only $\frac{1}{60}$ of the original dataset for the Support Vector Machine primal pre-training yielded a consistently faster convergence in the network. We believe this paper opens up interesting opportunities for pre-training shallow networks using prior knowledge."
Multiresolution Value Function Approximation in Reinforcement Learning using the Wavelet Basis,"We present the wavelet basis, a linear value function approximation scheme that enables multiresolution value function approximation in continuous state spaces. We apply the wavelet basis to two standard reinforcement learning domains, and show that it performs as well as or better than existing commonly used basis functions when used as a fixed basis.We also briefly demonstrate how it can be used to add representational power to better represent spatially local detail."
Off-Policy Actor-Critic with Function Approximation,"We present a new off-policy learning algorithm with an actor-critic architecture that is convergent to a locally optimal solution. Off-policy learning---learning about a policy different from the one being followed---plays an important role in reinforcement learning (RL) due to exploration-exploitation tradeoff. Recent advances in off-policy Temporal-Difference (TD) learning, such as Greedy-GQ, have been hitherto  limited  to value-function based methods and have not been fully extended to policy gradient methods,  which can represent a larger class of policies and also can handle problems with large (or continuous) action space.  Among policy gradient methods, actor-critic methods substantially have been considered for large-scale applications due to their desirable algorithmic features---e.g., they use bootstrapping methods such as TD learning that can reduce variance, and generally are easy to use with function approximation. The critic in our algorithm is based on recent gradient-TD prediction (GTD) methods with linear function approximation and the actor updates the policy parameters via stochastic gradient-ascent of a performance measure. Recently, Degris et al. (2012) have presented an off-policy actor-critic algorithm (OPAC) with similar objectives. However, OPAC does not update the actor via gradient-ascent and, as we will establish, does not always converge. In this paper, we address this issue by proposing an algorithm, called GTD-AC, that shares several of OPACs desirable features: online operation, incremental updating, linear complexity both in terms of memory and per-time-step computation, and in addition it maintains the same number of tuning parameters. Most importantly, we establish a convergence guarantee."
Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. "
Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. "
Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. "
Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. "
Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. "
Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. "
Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. "
The topographic unsupervised learning of natural sounds in the auditory cortex,"The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efficient coding strategy and that the disorder in A1 reflects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reflected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner."
Conditional Distance Variance and Correlation,"Recently a new dependence measure, the distance correlation, has been proposed to measure the dependence between continuous random variables. A nice property of this measure is that it can be consistently estimated with the empirical average of the products of certain distances between the sample points. Here we generalize this quantity to measure the conditional dependence between random variables, and show that this can also be estimated with a statistic using a weighted empirical average of the products of distances between the sample points. We demonstrate the applicability of the estimators with numerical experiments on real and simulated data sets."
Conditional Distance Variance and Correlation,"Recently a new dependence measure, the distance correlation, has been proposed to measure the dependence between continuous random variables. A nice property of this measure is that it can be consistently estimated with the empirical average of the products of certain distances between the sample points. Here we generalize this quantity to measure the conditional dependence between random variables, and show that this can also be estimated with a statistic using a weighted empirical average of the products of distances between the sample points. We demonstrate the applicability of the estimators with numerical experiments on real and simulated data sets."
Limits of Adaptation in Crowdsourcing,"Crowdsourcing systems, where numerous tasks are electronically distributed to an unidentified pool of workers through an open call, has emerged as an effective tool for human-powered solving of data intensive tasks such as image classification, video annotation, product categorization, and transcription. Since these low-paid workers can be unreliable, all crowdsourcers need to devise a way to cope with the errors and ensure a certain reliability in their answers. A common solution is to add redundancy by asking each question to multiple workers and combining their answers using some scheme such as majority voting. A fundamental question of interest for such systems is how much redundancy is necessary to achieve a certain accuracy in our answers? In this paper, we investigate the fundamental limit on the minimum number of queries necessary to achieve the target error probability. In particular, we want to identify how much we can gain by switching to an adaptive algorithm from an existing low-complexity and non-adaptive algorithms. To establish  this result, we provid a lower bound on the probability of error achieved by the optimal adaptive algorithm.  Compared to a known upper bound for a practical and non-adaptive algorithm, this shows that there is no significant gain in using adaptive algorithms. In terms of the budget required to achieve the target error probability, the gain of using an adaptive scheme is at most a constant factor. "
Limits of Adaptation in Crowdsourcing,"Crowdsourcing systems, where numerous tasks are electronically distributed to an unidentified pool of workers through an open call, has emerged as an effective tool for human-powered solving of data intensive tasks such as image classification, video annotation, product categorization, and transcription. Since these low-paid workers can be unreliable, all crowdsourcers need to devise a way to cope with the errors and ensure a certain reliability in their answers. A common solution is to add redundancy by asking each question to multiple workers and combining their answers using some scheme such as majority voting. A fundamental question of interest for such systems is how much redundancy is necessary to achieve a certain accuracy in our answers? In this paper, we investigate the fundamental limit on the minimum number of queries necessary to achieve the target error probability. In particular, we want to identify how much we can gain by switching to an adaptive algorithm from an existing low-complexity and non-adaptive algorithms. To establish  this result, we provid a lower bound on the probability of error achieved by the optimal adaptive algorithm.  Compared to a known upper bound for a practical and non-adaptive algorithm, this shows that there is no significant gain in using adaptive algorithms. In terms of the budget required to achieve the target error probability, the gain of using an adaptive scheme is at most a constant factor. "
A Gaussian Approximation of Feature Space for Fast Image Similarity,"We introduce a fast technique for the robust computation of image similarity. It builds on a re-interpretation of the recent exemplar-based SVM approach, where a linear SVM is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. Although it is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of-the-art performance on the PASCAL VOC 2007 detection task despite its simplicity. We re-interpret it by viewing the SVM between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. We show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. We then use a simple Gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. Given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. This allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. We further show that our approach is equivalent to feature-space whitening and has links to image saliency."
A Gaussian Approximation of Feature Space for Fast Image Similarity,"We introduce a fast technique for the robust computation of image similarity. It builds on a re-interpretation of the recent exemplar-based SVM approach, where a linear SVM is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. Although it is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of-the-art performance on the PASCAL VOC 2007 detection task despite its simplicity. We re-interpret it by viewing the SVM between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. We show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. We then use a simple Gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. Given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. This allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. We further show that our approach is equivalent to feature-space whitening and has links to image saliency."
Bayesian models for Large-scale Hierarchical Classification ,"A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods. "
Bayesian models for Large-scale Hierarchical Classification ,"A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods. "
Recovery of Sparse Probability Measures via Convex Programming,"We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. It's well-known that the classical $\ell_1$ regularizer fails to promote sparsity on the probability simplex since $\ell_1$ norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on $\ell_1$ norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm."
Recovery of Sparse Probability Measures via Convex Programming,"We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. It's well-known that the classical $\ell_1$ regularizer fails to promote sparsity on the probability simplex since $\ell_1$ norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on $\ell_1$ norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm."
Recovery of Sparse Probability Measures via Convex Programming,"We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. It's well-known that the classical $\ell_1$ regularizer fails to promote sparsity on the probability simplex since $\ell_1$ norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on $\ell_1$ norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm."
{Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning,"We consider the estimation of an i.i.d.\ vector $\xbf \in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possiblynonlinear) measurement channel. We present a method, calledadaptive generalized approximate message passing(Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\xbf$.The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriorsin the E-steps are computed via approximate message passing.The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yieldasymptotically consistent parameter estimates, which implies that the algorithmachieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic,general and computationally efficient methodapplicable to a large range of complex linear-nonlinearmodels with provable guarantees."
Optimal Stochastic Convex Optimization Through The Lens Of Active Learning,"The large fields of convex optimization and active learning have been developed fairly independent of each other, from the design of algorithms to the techniques of proof. Given the growing literature in both these subjects, we believe that understanding the connections between them is important to people in both areas. Here, we establish few such interesting relationships in upper and lower bound techniques that bring out these similarities. Our prime result is showing upper and lower bounds for precisely how the minimax rate for optimizing a given function depends solely on a flatness/noise condition for the function around its minimum."
Optimal Stochastic Convex Optimization Through The Lens Of Active Learning,"The large fields of convex optimization and active learning have been developed fairly independent of each other, from the design of algorithms to the techniques of proof. Given the growing literature in both these subjects, we believe that understanding the connections between them is important to people in both areas. Here, we establish few such interesting relationships in upper and lower bound techniques that bring out these similarities. Our prime result is showing upper and lower bounds for precisely how the minimax rate for optimizing a given function depends solely on a flatness/noise condition for the function around its minimum."
"Halo, Hyperbole, and the Pragmatic Interpretation of Numbers","Numbers are interpreted flexibly in everyday language: imprecision, exaggeration, and hyperbole are everywhere. We propose a computational model of the pragmatic interpretation of numbers, building upon recent models of pragmatics as rational inference. We assume that speaker and listener perform a social inference regarding the intended meaning, precision, and affective subtext of a numerical utterance. This model predicts two pragmatic effects, pragmatic halo and hyperbole, and their interaction. We demonstrate that the model accurately predicts the qualitative effects of human interpretation of number words in five real-world domains."
"Halo, Hyperbole, and the Pragmatic Interpretation of Numbers","Numbers are interpreted flexibly in everyday language: imprecision, exaggeration, and hyperbole are everywhere. We propose a computational model of the pragmatic interpretation of numbers, building upon recent models of pragmatics as rational inference. We assume that speaker and listener perform a social inference regarding the intended meaning, precision, and affective subtext of a numerical utterance. This model predicts two pragmatic effects, pragmatic halo and hyperbole, and their interaction. We demonstrate that the model accurately predicts the qualitative effects of human interpretation of number words in five real-world domains."
"Halo, Hyperbole, and the Pragmatic Interpretation of Numbers","Numbers are interpreted flexibly in everyday language: imprecision, exaggeration, and hyperbole are everywhere. We propose a computational model of the pragmatic interpretation of numbers, building upon recent models of pragmatics as rational inference. We assume that speaker and listener perform a social inference regarding the intended meaning, precision, and affective subtext of a numerical utterance. This model predicts two pragmatic effects, pragmatic halo and hyperbole, and their interaction. We demonstrate that the model accurately predicts the qualitative effects of human interpretation of number words in five real-world domains."
"Halo, Hyperbole, and the Pragmatic Interpretation of Numbers","Numbers are interpreted flexibly in everyday language: imprecision, exaggeration, and hyperbole are everywhere. We propose a computational model of the pragmatic interpretation of numbers, building upon recent models of pragmatics as rational inference. We assume that speaker and listener perform a social inference regarding the intended meaning, precision, and affective subtext of a numerical utterance. This model predicts two pragmatic effects, pragmatic halo and hyperbole, and their interaction. We demonstrate that the model accurately predicts the qualitative effects of human interpretation of number words in five real-world domains."
"Predicting Human Gaze Using Low-, Object- and Social- Saliency: A Dataset and Computational Models","Previous models to predict where people look in natural scenes focused on low-level image features. To bridge the semantic gap between the predictive power of computational saliency models and human behavior, we propose a new saliency architecture that incorporates information at three layers: low-level image features, object-level features, and social-level features. Object- and social-level information is frequently ignored, or only a few sample object categories are discussed where scaling to a large number of object categories is not feasible nor neutrally plausible. To address this problem, this work constructs a principled vocabulary of basic attributes to describe object- and social-level information thus not restricting to a limited number of object categories. We build a new dataset of 700 images with eye tracking data of 15 viewers and annotation data of 5551 segmented objects with fine contours and 12 social attributes (publicly available with the paper). Experimental results demonstrate the importance of the objectand social-level information in the prediction of visual attention."
"Predicting Human Gaze Using Low-, Object- and Social- Saliency: A Dataset and Computational Models","Previous models to predict where people look in natural scenes focused on low-level image features. To bridge the semantic gap between the predictive power of computational saliency models and human behavior, we propose a new saliency architecture that incorporates information at three layers: low-level image features, object-level features, and social-level features. Object- and social-level information is frequently ignored, or only a few sample object categories are discussed where scaling to a large number of object categories is not feasible nor neutrally plausible. To address this problem, this work constructs a principled vocabulary of basic attributes to describe object- and social-level information thus not restricting to a limited number of object categories. We build a new dataset of 700 images with eye tracking data of 15 viewers and annotation data of 5551 segmented objects with fine contours and 12 social attributes (publicly available with the paper). Experimental results demonstrate the importance of the objectand social-level information in the prediction of visual attention."
"Predicting Human Gaze Using Low-, Object- and Social- Saliency: A Dataset and Computational Models","Previous models to predict where people look in natural scenes focused on low-level image features. To bridge the semantic gap between the predictive power of computational saliency models and human behavior, we propose a new saliency architecture that incorporates information at three layers: low-level image features, object-level features, and social-level features. Object- and social-level information is frequently ignored, or only a few sample object categories are discussed where scaling to a large number of object categories is not feasible nor neutrally plausible. To address this problem, this work constructs a principled vocabulary of basic attributes to describe object- and social-level information thus not restricting to a limited number of object categories. We build a new dataset of 700 images with eye tracking data of 15 viewers and annotation data of 5551 segmented objects with fine contours and 12 social attributes (publicly available with the paper). Experimental results demonstrate the importance of the objectand social-level information in the prediction of visual attention."
Detecting Local Manifold Structure for Unsupervised Feature Selection,"Unsupervised feature selection is fundamental in statistical pattern recognition, and has drawn persistent attention in the past several decades. Recently, much work have shown that feature selection can be formulated as nonlinear dimensionality reduction with discrete constraints. This line of research emphasizes the manifold learning techniques, where the Laplacian eigenmap has been extensively studied. In this paper, we propose a new feature selection perspective from locally linear embedding (LLE), which is another popular manifold learning method. Our algorithm, called locally linear selection (LLS), can select the feature subset which optimally represents the underlying data manifold. We further develop a locally linear rotation-selection (LLRS) algorithm which extends LLS to identify the optimal coordinate subset from a new space. Experimental results on five real-world datasets show that our method can be more effective than Laplacian eigenmap based feature selection methods. "
Detecting Local Manifold Structure for Unsupervised Feature Selection,"Unsupervised feature selection is fundamental in statistical pattern recognition, and has drawn persistent attention in the past several decades. Recently, much work have shown that feature selection can be formulated as nonlinear dimensionality reduction with discrete constraints. This line of research emphasizes the manifold learning techniques, where the Laplacian eigenmap has been extensively studied. In this paper, we propose a new feature selection perspective from locally linear embedding (LLE), which is another popular manifold learning method. Our algorithm, called locally linear selection (LLS), can select the feature subset which optimally represents the underlying data manifold. We further develop a locally linear rotation-selection (LLRS) algorithm which extends LLS to identify the optimal coordinate subset from a new space. Experimental results on five real-world datasets show that our method can be more effective than Laplacian eigenmap based feature selection methods. "
Detecting Local Manifold Structure for Unsupervised Feature Selection,"Unsupervised feature selection is fundamental in statistical pattern recognition, and has drawn persistent attention in the past several decades. Recently, much work have shown that feature selection can be formulated as nonlinear dimensionality reduction with discrete constraints. This line of research emphasizes the manifold learning techniques, where the Laplacian eigenmap has been extensively studied. In this paper, we propose a new feature selection perspective from locally linear embedding (LLE), which is another popular manifold learning method. Our algorithm, called locally linear selection (LLS), can select the feature subset which optimally represents the underlying data manifold. We further develop a locally linear rotation-selection (LLRS) algorithm which extends LLS to identify the optimal coordinate subset from a new space. Experimental results on five real-world datasets show that our method can be more effective than Laplacian eigenmap based feature selection methods. "
Fitting community models to large sparse networks,"Many algorithms have been proposed for fitting network models with communities but most of them do not scale well to large networks, and often fail on sparse networks.   Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees.   We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs.  We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood.   "
Fitting community models to large sparse networks,"Many algorithms have been proposed for fitting network models with communities but most of them do not scale well to large networks, and often fail on sparse networks.   Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees.   We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs.  We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood.   "
Fitting community models to large sparse networks,"Many algorithms have been proposed for fitting network models with communities but most of them do not scale well to large networks, and often fail on sparse networks.   Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees.   We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs.  We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood.   "
Learning Manifolds with K-Means and K-Flats,"We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-?ats, and analyze their performance. We extend previous results  for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-?ats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-?ats, both the results and the mathematical tools are  new."
Learning Manifolds with K-Means and K-Flats,"We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-?ats, and analyze their performance. We extend previous results  for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-?ats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-?ats, both the results and the mathematical tools are  new."
Iterative ranking from pair-wise comparisons ,"The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR?s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, finding ?scores? for each object (e.g. player?s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efficacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1]."
Iterative ranking from pair-wise comparisons ,"The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR?s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, finding ?scores? for each object (e.g. player?s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efficacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1]."
Learning Probability Measures with respect to  Optimal Transport Metrics,"We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures."
Robust elastic-net nonnegative matrix factorization with box constraints,"In this paper, we propose an elastic-net nonnegative matrix factorization (NMF) with box constraints to remove grouped outliers and recover the inherent nonnegative low-rank structure of the given high dimensional noisy image data. Based on the augmented Lagrangian framework, we solve the linearly constrained minimization reformulation of the elastic-net NMF with the successive overrelaxed outer product iteration (SOOPI). We evaluate the performance of the proposed method for the background modeling of video image sequence and removal of varying illumination and grossly corrupted artifacts in face images. The numerical results show that our proposed elastic-net NMF model does better recover low-rank structure than the state-of-the-art nuclear norm based robust principal component analysis (PCA) and other robust NMF models."
Semi-supervised Eigenvectors for Locally-biased Learning,"In many applications, one has information, e.g., labels that are provided in a semi-supervised manner, about a specific target region of a large data set, and one wants to perform machine learning and data analysis tasks nearby that pre-specified target region.  Locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools.At root, the reason is that eigenvectors are inherently global quantities.In this paper, we address this issue by providing a methodology to construct semi-supervised eigenvectors of a graph Laplacian, and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning.These semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance, conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner.We also provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning."
Restricting exchangeable nonparametric distributions,"Distributions over exchangeable matrices with infinitely many columns, such as the Indian buffet process, are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution."
Exponential Concentration for Mutual Information Estimation with Application to Forests,"We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph."
A Soft-Label Model with Impact for Active Graph Search,"We consider the problem of active search on a graph where we seek nodes belonging to a certain positive class by iteratively selecting nodes to query for their class label. The problem has similarities with active learning on a graph except that the performance is measured by number of positives identified rather than classification accuracy. Good solutions must tradeoff exploration to better fit a model against exploitation to collect likely positives and thus the problem has similarities with bandit problems as well. However, bandit algorithms are hard to adapt to the  problem since we will never choose the same node more than once.Previous work showed that the optimal active search algorithm requires a look ahead evaluation of expected utility that is exponential in the number of node selections to be made and considered heuristics that do a truncated look ahead [1]. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs.  We test the algorithm empirically on citation and wikipedia graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps."
A Soft-Label Model with Impact for Active Graph Search,"We consider the problem of active search on a graph where we seek nodes belonging to a certain positive class by iteratively selecting nodes to query for their class label. The problem has similarities with active learning on a graph except that the performance is measured by number of positives identified rather than classification accuracy. Good solutions must tradeoff exploration to better fit a model against exploitation to collect likely positives and thus the problem has similarities with bandit problems as well. However, bandit algorithms are hard to adapt to the  problem since we will never choose the same node more than once.Previous work showed that the optimal active search algorithm requires a look ahead evaluation of expected utility that is exponential in the number of node selections to be made and considered heuristics that do a truncated look ahead [1]. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs.  We test the algorithm empirically on citation and wikipedia graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps."
Non-linear Metric Learning,"In this paper, we introduce two novel metric learning algorithms, ?2-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: ?2-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear ?2-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of ?2-LMNN, obtain best results in 19 out of 20 learning settings. "
Non-linear Metric Learning,"In this paper, we introduce two novel metric learning algorithms, ?2-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: ?2-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear ?2-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of ?2-LMNN, obtain best results in 19 out of 20 learning settings. "
Non-linear Metric Learning,"In this paper, we introduce two novel metric learning algorithms, ?2-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: ?2-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear ?2-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of ?2-LMNN, obtain best results in 19 out of 20 learning settings. "
Non-linear Metric Learning,"In this paper, we introduce two novel metric learning algorithms, ?2-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: ?2-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear ?2-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of ?2-LMNN, obtain best results in 19 out of 20 learning settings. "
Ancestral Sampling for Particle Gibbs,"We present a novel method in the family of particle MCMC methods that we refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the existing PG with backward simulation (PG-BS) procedure, we use backward sampling to (considerably) improve the mixing of the PG kernel. Instead of using separate forward and backward sweeps as in PG-BS, however, we achieve the same effect in a single forward sweep. We apply the PG-AS framework to the challenging class of non-Markovian state-space models. We develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the PG-AS framework. In particular, as we show in a simulation study, PG-AS can yield an order-of-magnitude improved accuracy relative to PG-BS due to its robustness to the truncation error. Several application examples are discussed, including Rao-Blackwellized particle smoothing and inference in degenerate state-space models."
Ancestral Sampling for Particle Gibbs,"We present a novel method in the family of particle MCMC methods that we refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the existing PG with backward simulation (PG-BS) procedure, we use backward sampling to (considerably) improve the mixing of the PG kernel. Instead of using separate forward and backward sweeps as in PG-BS, however, we achieve the same effect in a single forward sweep. We apply the PG-AS framework to the challenging class of non-Markovian state-space models. We develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the PG-AS framework. In particular, as we show in a simulation study, PG-AS can yield an order-of-magnitude improved accuracy relative to PG-BS due to its robustness to the truncation error. Several application examples are discussed, including Rao-Blackwellized particle smoothing and inference in degenerate state-space models."
Ancestral Sampling for Particle Gibbs,"We present a novel method in the family of particle MCMC methods that we refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the existing PG with backward simulation (PG-BS) procedure, we use backward sampling to (considerably) improve the mixing of the PG kernel. Instead of using separate forward and backward sweeps as in PG-BS, however, we achieve the same effect in a single forward sweep. We apply the PG-AS framework to the challenging class of non-Markovian state-space models. We develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the PG-AS framework. In particular, as we show in a simulation study, PG-AS can yield an order-of-magnitude improved accuracy relative to PG-BS due to its robustness to the truncation error. Several application examples are discussed, including Rao-Blackwellized particle smoothing and inference in degenerate state-space models."
Multi-Task Active Learning for Hierarchical Classification," In this paper, we present a novel combination of Active Learning and Multi-Task Learning for minimizing the training data required for effective Hierarchical Classification. For Multi-Task Learning, we describe a novel hierarchical regularization strategy that utilizes the learnt parameters of a parent category as regularizers for its children categories. For Active Learning, we leverage the multi-task relationships to selectively acquire training data that is effective for improving classification at a category as well as other categories that it influences through the regularization framework. We formulate a stochastic gradient descent solution for Multi-Task learning and an online decision criterion for Active learning to make our approach scalable for large-scale deployment of Active Hierarchical Classification. In spite of being a jointly learnt multi-task model, the approach can be easily adapted to the popular MapReduce, OpenMP and MPI frameworks for large-scale learning through differential message passing amongst categories. Through experiments on well-known hierarchical classification datasets, we demonstrate the superior performance of our approach as compared to learning the hierarchical categories in isolation (single-task setting), especially for categories with limited positive training instances. Our experiments also show significant reduction in the amount of required training data when it is selected with our novel multi-task active learning approach as compared to conventional active learning approaches that select instances for each category in isolation."
Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential ?1-Minimization,"We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where $A_k$ and $e_k$ denote the measurement matrix and noise, respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ \emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity."
Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential ?1-Minimization,"We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where $A_k$ and $e_k$ denote the measurement matrix and noise, respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ \emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity."
Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems ,"We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes.More recently, an asymptotic regret bound of $\tilde{O}(\sqrt{T})$ was shown for $T \gg p$ where $p$ is the dimension of the state space.In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large.We present an adaptive control scheme that for $p \gg 1$ and $T \gg \polylog(p)$ achieves a regret bound of $\tilde{O}(p \sqrt{T})$.In particular, our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$.This is in comparison to previous work on the dense dynamics where the algorithm needs $\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy.We believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks."
Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems ,"We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes.More recently, an asymptotic regret bound of $\tilde{O}(\sqrt{T})$ was shown for $T \gg p$ where $p$ is the dimension of the state space.In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large.We present an adaptive control scheme that for $p \gg 1$ and $T \gg \polylog(p)$ achieves a regret bound of $\tilde{O}(p \sqrt{T})$.In particular, our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$.This is in comparison to previous work on the dense dynamics where the algorithm needs $\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy.We believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks."
Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems ,"We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes.More recently, an asymptotic regret bound of $\tilde{O}(\sqrt{T})$ was shown for $T \gg p$ where $p$ is the dimension of the state space.In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large.We present an adaptive control scheme that for $p \gg 1$ and $T \gg \polylog(p)$ achieves a regret bound of $\tilde{O}(p \sqrt{T})$.In particular, our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$.This is in comparison to previous work on the dense dynamics where the algorithm needs $\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy.We believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks."
Emergence of Object-Selective Features in Unsupervised Feature Learning," Recent work in unsupervised feature learning has focused on the goal  of discovering high-level features from unlabeled images.  Much  progress has been made in this direction, but in most cases it is  still standard to use a large amount of labeled data in order to  construct detectors sensitive to object classes or other complex  patterns in the data.  In this paper, we aim to test the hypothesis  that unsupervised feature learning methods, provided with only  unlabeled data, can learn high-level, invariant features that are  sensitive to commonly-occurring objects.  Though a handful of prior  results suggest that this is possible when each object class  accounts for a large fraction of the data (as in many labeled  datasets), it is unclear whether something similar can be  accomplished when dealing with completely unlabeled data.  A major  obstacle to this test, however, is scale: we cannot expect to  succeed with small datasets or with small numbers of learned  features.  Here, we propose a large-scale feature learning system  that enables us to carry out this experiment, learning 150,000  features from tens of millions of unlabeled images.  Based on two  scalable clustering algorithms (K-means and agglomerative  clustering), we find that our simple system can discover features  sensitive to a commonly occurring object class (human faces) and can  also combine these into detectors invariant to significant global  distortions like large translations and scale."
Emergence of Object-Selective Features in Unsupervised Feature Learning," Recent work in unsupervised feature learning has focused on the goal  of discovering high-level features from unlabeled images.  Much  progress has been made in this direction, but in most cases it is  still standard to use a large amount of labeled data in order to  construct detectors sensitive to object classes or other complex  patterns in the data.  In this paper, we aim to test the hypothesis  that unsupervised feature learning methods, provided with only  unlabeled data, can learn high-level, invariant features that are  sensitive to commonly-occurring objects.  Though a handful of prior  results suggest that this is possible when each object class  accounts for a large fraction of the data (as in many labeled  datasets), it is unclear whether something similar can be  accomplished when dealing with completely unlabeled data.  A major  obstacle to this test, however, is scale: we cannot expect to  succeed with small datasets or with small numbers of learned  features.  Here, we propose a large-scale feature learning system  that enables us to carry out this experiment, learning 150,000  features from tens of millions of unlabeled images.  Based on two  scalable clustering algorithms (K-means and agglomerative  clustering), we find that our simple system can discover features  sensitive to a commonly occurring object class (human faces) and can  also combine these into detectors invariant to significant global  distortions like large translations and scale."
Emergence of Object-Selective Features in Unsupervised Feature Learning," Recent work in unsupervised feature learning has focused on the goal  of discovering high-level features from unlabeled images.  Much  progress has been made in this direction, but in most cases it is  still standard to use a large amount of labeled data in order to  construct detectors sensitive to object classes or other complex  patterns in the data.  In this paper, we aim to test the hypothesis  that unsupervised feature learning methods, provided with only  unlabeled data, can learn high-level, invariant features that are  sensitive to commonly-occurring objects.  Though a handful of prior  results suggest that this is possible when each object class  accounts for a large fraction of the data (as in many labeled  datasets), it is unclear whether something similar can be  accomplished when dealing with completely unlabeled data.  A major  obstacle to this test, however, is scale: we cannot expect to  succeed with small datasets or with small numbers of learned  features.  Here, we propose a large-scale feature learning system  that enables us to carry out this experiment, learning 150,000  features from tens of millions of unlabeled images.  Based on two  scalable clustering algorithms (K-means and agglomerative  clustering), we find that our simple system can discover features  sensitive to a commonly occurring object class (human faces) and can  also combine these into detectors invariant to significant global  distortions like large translations and scale."
Annotation on the cheap,"We consider the task of producing a high-quality labeling of a new data set, given access to a human annotator who is to be used sparingly. Our approach involves the active learning of a classifier that is allowed to abstain on difficult inputs."
Annotation on the cheap,"We consider the task of producing a high-quality labeling of a new data set, given access to a human annotator who is to be used sparingly. Our approach involves the active learning of a classifier that is allowed to abstain on difficult inputs."
"Burn-in, bias, and the rationality of anchoring","Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as burn-in. Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks. Our theoretical and empirical results suggest that the anchoring bias is consistent with approximate Bayesian inference."
"Burn-in, bias, and the rationality of anchoring","Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as burn-in. Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks. Our theoretical and empirical results suggest that the anchoring bias is consistent with approximate Bayesian inference."
Learning-driven Exploration in Embodied Action-Perception Loops,"Extracting the structure underlying observed data points is a recurring problem in machine learning. When data can be actively collected in the context of a closed-action perception loop, behavior becomes a fundamental determinant of learning efficiency. Previous machine learning studies in closed action-perception loops, however, have largely focused on the control problem of maximizing acquisition of rewards and often treat the learning of structure as a secondary objective deriving from the search for rewards. Psychology, in contrast, has long argued that learning itself is a primary motivation in both human and animal behavior. Here, we study explorative behavioral control in the absence of external reward structure. Instead, we take the quality of an agent's internal model as the primary objective. In a simple probabilistic framework, we derive an estimate, predicted information gain (PIG), for the amount of information about an (unknown) environment that an agent can expect to receive by taking an action. We develop an explorative strategy through approximate maximization of information gain by backwards propagation of future PIG using a value-iteration algorithm. Across a range of environments, we demonstrate that the proposed behavioral policy learns significantly faster than previous reward-free explorative strategies. Finally, we address the possible evolutionary advantage of reward-free exploration by demonstrating that agents which explore efficiently when rewards are not available, are later better able to accomplish a range of goal-directed tasks."
Learning-driven Exploration in Embodied Action-Perception Loops,"Extracting the structure underlying observed data points is a recurring problem in machine learning. When data can be actively collected in the context of a closed-action perception loop, behavior becomes a fundamental determinant of learning efficiency. Previous machine learning studies in closed action-perception loops, however, have largely focused on the control problem of maximizing acquisition of rewards and often treat the learning of structure as a secondary objective deriving from the search for rewards. Psychology, in contrast, has long argued that learning itself is a primary motivation in both human and animal behavior. Here, we study explorative behavioral control in the absence of external reward structure. Instead, we take the quality of an agent's internal model as the primary objective. In a simple probabilistic framework, we derive an estimate, predicted information gain (PIG), for the amount of information about an (unknown) environment that an agent can expect to receive by taking an action. We develop an explorative strategy through approximate maximization of information gain by backwards propagation of future PIG using a value-iteration algorithm. Across a range of environments, we demonstrate that the proposed behavioral policy learns significantly faster than previous reward-free explorative strategies. Finally, we address the possible evolutionary advantage of reward-free exploration by demonstrating that agents which explore efficiently when rewards are not available, are later better able to accomplish a range of goal-directed tasks."
On the Sample Complexity of Ranking,"Learning to rank is a core machine learning problem. When the truescoring functions are hard to learn or training data is scarce, thesample complexity for predicting a ranking with small error is ofconsiderable interest. We present a lower bound for such a samplecomplexity for any algorithm that estimates a broad class of scoringfunction based on randomly sampled binary comparisons. Additionally,we demonstrate two simple algorithms that achieve the bound inexpectation.  While one algorithm predicts rankings with roughlyuniform quality across the ranking, the other predicts more accuratelynear the top of the ranking than the bottom. Results are presented onsynthetic examples and on an application to epitope (peptide) ranking."
On the Sample Complexity of Ranking,"Learning to rank is a core machine learning problem. When the truescoring functions are hard to learn or training data is scarce, thesample complexity for predicting a ranking with small error is ofconsiderable interest. We present a lower bound for such a samplecomplexity for any algorithm that estimates a broad class of scoringfunction based on randomly sampled binary comparisons. Additionally,we demonstrate two simple algorithms that achieve the bound inexpectation.  While one algorithm predicts rankings with roughlyuniform quality across the ranking, the other predicts more accuratelynear the top of the ranking than the bottom. Results are presented onsynthetic examples and on an application to epitope (peptide) ranking."
Density Propagation and Improved Bounds on the Partition Function,"Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds."
Density Propagation and Improved Bounds on the Partition Function,"Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds."
Density Propagation and Improved Bounds on the Partition Function,"Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds."
Distributed large-scale natural graph factorization,"More and more natural graphs arise in numerous Web applications, such as socialnetworks, email, and instant messaging. Such large-scalegraphs today easily contain hundreds of millions of nodes and billions of edges,and are expected to grow even larger in the future. While severaltheoretical models have been proposed for such networks, their analysisis still difficult due to their scale and nature. Inthis paper we propose a distributed framework for large-scale graph factorizationthat allows for easier analysis and more compact representation oflarge natural graphs. "
Statistical Consistency of Finite-dimensional Unregularized Linear Classification,"This manuscript studies statistical properties of linear classifiers obtained through minimization of an unregularized convex risk over a finite sample. Although the results are explicitly finite-dimensional, inputs may be passed through feature maps; in this way, in addition to treating the consistency of logistic regression, this analysis also handles boosting over a finite weak learning class with, for instance, the exponential, logistic, and hinge losses.  In this finite-dimensional setting, it is still possible to fit arbitrary decision boundaries: scaling the complexity of the weak learning class with the sample size leads to the optimal classification risk almost surely."
Multiclass Learning  with Simplex Coding,"In this paper we dicuss a novel  framework for multiclass learning, defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classesa relaxation approach commonly used in binary classification.In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized methods with training/tuning complexity which is {\em independent} to the number of classes.Tools from convex analysis are introduced that can be used beyond the scope of this paper. "
Multiclass Learning  with Simplex Coding,"In this paper we dicuss a novel  framework for multiclass learning, defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classesa relaxation approach commonly used in binary classification.In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized methods with training/tuning complexity which is {\em independent} to the number of classes.Tools from convex analysis are introduced that can be used beyond the scope of this paper. "
Multiclass Learning  with Simplex Coding,"In this paper we dicuss a novel  framework for multiclass learning, defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classesa relaxation approach commonly used in binary classification.In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized methods with training/tuning complexity which is {\em independent} to the number of classes.Tools from convex analysis are introduced that can be used beyond the scope of this paper. "
Multiclass Learning  with Simplex Coding,"In this paper we dicuss a novel  framework for multiclass learning, defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classesa relaxation approach commonly used in binary classification.In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized methods with training/tuning complexity which is {\em independent} to the number of classes.Tools from convex analysis are introduced that can be used beyond the scope of this paper. "
FastEx: Fast Clustering with Exponential Families," Clustering is a key component in data analysis toolbox. Despite its  importance, scalable algorithms often eschew rich statistical models  in favor of simpler descriptions such as $k$-means clustering. In  this paper we present a sampler, capable of estimating  mixtures of exponential families. At its heart lies a novel proposal distribution using random  projections to achieve high throughput in generating proposals, which is crucial  for clustering models with large numbers of clusters. "
A tree-decomposed EM algorithm for covariance selection in noisy graphical models,"Gaussian graphical models (GGMs) are widely used in computer science, and have also enjoyed wide applicability in a number of scientific areas. We consider the problem of covariance selection, i.e. estimation of the (inverse) covariance matrix of the joint probability distribution of random variables on a high dimensional graph. To extend the applicability of GGMs, we consider the case where observations for variables are also subject to additional measurement noise. Unfortunately, the the estimation of model parameters in this setting becomes complicated by the fact that the structure of the underlying graph no longer provides direct information about the location of zeros in the inverse covariance matrix. We propose an efficient EM algorithm which uses the tree decomposition of the underlying graph in order to perform the parameter estimation through local operations. We also explore the effect of the treelike structure of the graph on computational performance of the algorithm as well as the accuracy of the estimates by applying it to a wide range of random graph models."
Learning with Recursive Perceptual Representations,"Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks."
Learning with Recursive Perceptual Representations,"Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks."
Changepoint Detection over Graphs with the Spectral Scan Statistic,"We consider the change-point detection problem of deciding, based on noisy measurements, whether an unknown signal over a given graph is constant or is instead piecewise constant over two connected induced subgraphs of relatively low cut size. We analyze the corresponding generalized likelihood ratio (GLR) statistics and relate it to the problem of finding a sparsest cut in a graph. We develop a tractable relaxation of the GLR statistic based on the combinatorial Laplacian of the graph, which we call the spectral scan statistic, and analyze its properties. We show how its performance as a testing procedure depends directly on the spectrum of the graph, and use this  result to explicitly derive its asymptotic properties on few significant graph topologies. Finally, we demonstrate both theoretically and by simulations that the spectral scan statistic can outperform naive testing procedures based on edge thresholding and $\chi^2$ testing. "
Changepoint Detection over Graphs with the Spectral Scan Statistic,"We consider the change-point detection problem of deciding, based on noisy measurements, whether an unknown signal over a given graph is constant or is instead piecewise constant over two connected induced subgraphs of relatively low cut size. We analyze the corresponding generalized likelihood ratio (GLR) statistics and relate it to the problem of finding a sparsest cut in a graph. We develop a tractable relaxation of the GLR statistic based on the combinatorial Laplacian of the graph, which we call the spectral scan statistic, and analyze its properties. We show how its performance as a testing procedure depends directly on the spectrum of the graph, and use this  result to explicitly derive its asymptotic properties on few significant graph topologies. Finally, we demonstrate both theoretically and by simulations that the spectral scan statistic can outperform naive testing procedures based on edge thresholding and $\chi^2$ testing. "
Changepoint Detection over Graphs with the Spectral Scan Statistic,"We consider the change-point detection problem of deciding, based on noisy measurements, whether an unknown signal over a given graph is constant or is instead piecewise constant over two connected induced subgraphs of relatively low cut size. We analyze the corresponding generalized likelihood ratio (GLR) statistics and relate it to the problem of finding a sparsest cut in a graph. We develop a tractable relaxation of the GLR statistic based on the combinatorial Laplacian of the graph, which we call the spectral scan statistic, and analyze its properties. We show how its performance as a testing procedure depends directly on the spectrum of the graph, and use this  result to explicitly derive its asymptotic properties on few significant graph topologies. Finally, we demonstrate both theoretically and by simulations that the spectral scan statistic can outperform naive testing procedures based on edge thresholding and $\chi^2$ testing. "
Overlapping Decomposition for High-Order Directed Graphical Modeling,"We propose to estimate the dependence structure in high-order directed graph by decomposing it into subgraphswith overlaps. We first introduce a lasso type model to formulate this problem, and further transfer it into estimating a set of group variable selection problems.Specifically, we establish a generic hierarchical lasso method for the estimation, where scalable norms are employed for controlling the structure of subgraphs flexibly. The asymptotic properties of the proposed method are discussed with detailed analysis. We also develop an efficient algorithm to compute such model. Finally, we evaluate our model on both synthetic data and real traffic data."
Sparse Approximate Manifolds for Differential Geometric MCMC,"One of the enduring challenges in Markov chain Monte Carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost. The recent introduction of locally adaptive MCMC methods based on the natural underlying Riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration.In this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid MCMC algorithm that extends the applicability of Riemannian Manifold MCMC methods to statistical models that do not admit an analytically computable metric tensor. Secondly, we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made. We demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust student-t error model, for which the expected Fisher Information is analytically intractable."
Learning with Multiple Models,"The standard approach to analyzing data in supervised or unsupervised learning is to assume that a certain specific model generated the data. The goal of the learning process is typically to recover the generating model, or a good approximation thereof. But in many cases, the data are generated by multiple models rather than a single one. In this paper we study the problem of learning when multiple models are considered, generalizing well known schemes such as clustering and multi-subspace approximation. The objective is to learn several models that explain the data best, and the loss for any given data point is the minimal loss among all considered models. We develop an efficient iterative optimization based procedure for the multiple model setup and provide sample complexity bounds."
Spatial Coarse-to-Fine Processing in a Neural Model with Cortical Feedback,"Methods utilizing coarse-to-fine processing, in which the general features of a stimulus are processed before more detailed structure, have found success in several applications, such as natural language processing, image processing, and computer vision. Such dynamics have also been observed in several neurological pathways, including the visual system. In this paper, we consider mechanisms of the spatial coarse-to-fine process in the central visual pathway. We present a model of the lateral geniculate  nucleus (LGN) and visual cortex (V1) incorporating both feedforward and feedback connections. We show that cortical feedback has a substantial effect on spatial dynamics in the LGN. Our results suggest that the LGN may use these recurrent connections to ?learn? the coarse-to-fine dynamic during development. We provide an ideal framework within which to explore this process through more computationally intensive simulations."
Spatial Coarse-to-Fine Processing in a Neural Model with Cortical Feedback,"Methods utilizing coarse-to-fine processing, in which the general features of a stimulus are processed before more detailed structure, have found success in several applications, such as natural language processing, image processing, and computer vision. Such dynamics have also been observed in several neurological pathways, including the visual system. In this paper, we consider mechanisms of the spatial coarse-to-fine process in the central visual pathway. We present a model of the lateral geniculate  nucleus (LGN) and visual cortex (V1) incorporating both feedforward and feedback connections. We show that cortical feedback has a substantial effect on spatial dynamics in the LGN. Our results suggest that the LGN may use these recurrent connections to ?learn? the coarse-to-fine dynamic during development. We provide an ideal framework within which to explore this process through more computationally intensive simulations."
Spatial Coarse-to-Fine Processing in a Neural Model with Cortical Feedback,"Methods utilizing coarse-to-fine processing, in which the general features of a stimulus are processed before more detailed structure, have found success in several applications, such as natural language processing, image processing, and computer vision. Such dynamics have also been observed in several neurological pathways, including the visual system. In this paper, we consider mechanisms of the spatial coarse-to-fine process in the central visual pathway. We present a model of the lateral geniculate  nucleus (LGN) and visual cortex (V1) incorporating both feedforward and feedback connections. We show that cortical feedback has a substantial effect on spatial dynamics in the LGN. Our results suggest that the LGN may use these recurrent connections to ?learn? the coarse-to-fine dynamic during development. We provide an ideal framework within which to explore this process through more computationally intensive simulations."
The Raindrop Process:  Bayesian Nonparametric Latent Shape Models,"In difficult object segmentation tasks, utilizing image information alone is not sufficient; incorporation of object shape prior models has been shown to improve segmentation performance. Most formulations that incorporate both shape and image information are in the form of optimizing energy functionals. This paper introduces a nonparametric Bayesian model for segmenting multiple objects in an image taking both shape and image feature/appearance into account. The generative process starts by generating object locations from a spatial Poisson process, then shape parameters are generated from a shape prior model. This automatically partitions the image: pixels inside are assumed to be generated from an object observation/appearance model and pixels outside from a background model. We learn the model via Markov Chain Monte Carlo sampling and our experiments show that the model is capable of segmenting multiple objects."
Discovering Sparse Networks In Spiking Data,"  In order to reason about the interacting processes giving rise to a given set of data, we must understand the causal relationships between those latent processes. One way to uncover these relationships is to discover a directed network structure from the data. Often, these data have the form of discrete events --- for example, neural spikes --- which can be modeled effectively via interacting point processes; the Hawkes process is the classical example of such a joint process. Here we provide a fully-Bayesian treatment of the Hawkes process, introducing 1)~convenient conjugate priors for the model parameters and 2)~a sparsity-promoting  spike-and-slab prior on the elements of the interaction matrix.  We demonstrate how to perform efficient inference in this model with Markov chain Monte Carlo.  This enables us to both recover posterior samples of the network structure and infer the characteristics of the underlying temporal dynamics.  We validate our approach on a simulated dataset with known ground truth before examining two real-world data sets --- neural spike train data and financial tick streams.  In each case, we uncover sparse networks with meaningful parameters, suggesting that this method is widely applicable for network discovery."
Divide and prosper --- fault tolerant scalable sketches,"We describe a family of algorithms that can be used to extend  sketches such as the CountMin sketch and SpaceSaving, to settings  where fault tolerance and scalability are crucial. We show how tools  from systems research, namely consistent and proportional hashing  can be used to increase accuracy and throughput linearly in the  number of processors while simultaneously decreasing the failure  probability exponentially. We provide both tight theoretical  guarantees and experimental results that corroborate our findings."
Minimizing Uncertainty in Pipelines,"In this paper, we consider the problem of debugging large pipelines by human labeling. We represent the execution of a pipeline using a directed acyclic graph of AND and OR nodes, where each node represents a data item produced by some operator in the pipeline. We assume that each operator assigns a confidence to each of its output data. We want to reduce the uncertainty in the output by issuing queries to a human expert, where a query consists of checking if a given data item is correct. In this paper, we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty. We perform a detailed evaluation of the complexity of the problem for various classes of graphs. We give efficient algorithms for the problem for trees, and show that, for a general dag, the problem is intractable."
First-Order Models for POMDPs,"Interest in relational and first-order languages for probabilitymodels has grown rapidly in recent years, and with it the possibilityof extending such languages to handle decision processes---both fullyand partially observable.  We examine the problem of extending afirst-order, open-universe language to describe POMDPs and identifynon-trivial representational issues in describing an agent'scapability for observation and action---issues that were avoided inprevious work only by making strong and restrictive assumptions. Wepresent a solution based on ideas from modal logic, and show how tohandle cases like being able to act upon an object thathas been detected through one's observations."
First-Order Models for POMDPs,"Interest in relational and first-order languages for probabilitymodels has grown rapidly in recent years, and with it the possibilityof extending such languages to handle decision processes---both fullyand partially observable.  We examine the problem of extending afirst-order, open-universe language to describe POMDPs and identifynon-trivial representational issues in describing an agent'scapability for observation and action---issues that were avoided inprevious work only by making strong and restrictive assumptions. Wepresent a solution based on ideas from modal logic, and show how tohandle cases like being able to act upon an object thathas been detected through one's observations."
Detecting Activations over Graphs using Spanning Tree Wavelet Bases,"We consider the detection activations over graphs under Gaussian noise, where signals are supposed to be peice-wise constant over the graph.Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with proveable theoretical guarantees for general graph topologies.We first provide a universal necessary condition for asymptotic distinguishability of the null and alternative hypotheses.We introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph.We prove that for any spanning tree, we can hope to correctly detect signals in a low signal-to-noise regime using spanning tree wavelets.We propose a randomized test, in which we use a uniform spanning tree in the basis construction.Using electrical network theory, we show that the uniform spanning tree provides strong theoretical guarantees for arbitrary graphs that in many cases match our necessary condition.We prove that for edge transitive graphs, $k$-nearest neighbor graphs, and $\epsilon$-graphs we obtain nearly optimal performance with the uniform spanning tree wavelet detector."
Detecting Activations over Graphs using Spanning Tree Wavelet Bases,"We consider the detection activations over graphs under Gaussian noise, where signals are supposed to be peice-wise constant over the graph.Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with proveable theoretical guarantees for general graph topologies.We first provide a universal necessary condition for asymptotic distinguishability of the null and alternative hypotheses.We introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph.We prove that for any spanning tree, we can hope to correctly detect signals in a low signal-to-noise regime using spanning tree wavelets.We propose a randomized test, in which we use a uniform spanning tree in the basis construction.Using electrical network theory, we show that the uniform spanning tree provides strong theoretical guarantees for arbitrary graphs that in many cases match our necessary condition.We prove that for edge transitive graphs, $k$-nearest neighbor graphs, and $\epsilon$-graphs we obtain nearly optimal performance with the uniform spanning tree wavelet detector."
Detecting Activations over Graphs using Spanning Tree Wavelet Bases,"We consider the detection activations over graphs under Gaussian noise, where signals are supposed to be peice-wise constant over the graph.Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with proveable theoretical guarantees for general graph topologies.We first provide a universal necessary condition for asymptotic distinguishability of the null and alternative hypotheses.We introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph.We prove that for any spanning tree, we can hope to correctly detect signals in a low signal-to-noise regime using spanning tree wavelets.We propose a randomized test, in which we use a uniform spanning tree in the basis construction.Using electrical network theory, we show that the uniform spanning tree provides strong theoretical guarantees for arbitrary graphs that in many cases match our necessary condition.We prove that for edge transitive graphs, $k$-nearest neighbor graphs, and $\epsilon$-graphs we obtain nearly optimal performance with the uniform spanning tree wavelet detector."
Nonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction,"We show how to incorporate information from labeled examples into nonnegative matrix factorization (NMF). In addition to mapping the data into a space of lower dimensionality, our approach aims to preserve the nonnegative components of the data that are important for classification. We identify these components from the support vectors of large-margin classifiers and derive iterative updates to preserve them in a semi-supervised version of NMF. These updates have a simple multiplicative form like their unsupervised counterparts. We evaluate these updates for dimensionality reduction when they are used as a precursor to linear classification.  We find that they yield much better performance than their unsupervised counterparts. We also find one unexpected benefit of the low dimensional representations discovered by our approach: often they yield more accurate classifiers than both ordinary and transductive SVMs trained in the original input space."
Nonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction,"We show how to incorporate information from labeled examples into nonnegative matrix factorization (NMF). In addition to mapping the data into a space of lower dimensionality, our approach aims to preserve the nonnegative components of the data that are important for classification. We identify these components from the support vectors of large-margin classifiers and derive iterative updates to preserve them in a semi-supervised version of NMF. These updates have a simple multiplicative form like their unsupervised counterparts. We evaluate these updates for dimensionality reduction when they are used as a precursor to linear classification.  We find that they yield much better performance than their unsupervised counterparts. We also find one unexpected benefit of the low dimensional representations discovered by our approach: often they yield more accurate classifiers than both ordinary and transductive SVMs trained in the original input space."
Modeling Laminar Recordings from Visual Cortex with Semi-Restricted Boltzmann Machines,"The proliferation of high density recording techniques presents us with new challenges for characterizing the statistics of neural activity over populations of many neurons. The Ising model, which is the maximum entropy model for pairwise correlations, has been used to model the instantaneous state of a population of neurons.  This model suffers from two major limitations: 1) Estimation for large models becomes computationally intractable, and 2) it cannot capture higher-order dependencies.  We propose applying a more general maximum entropy model, the semi-restricted Boltzmann machine (sRBM), which extends the Ising model to capture higher order dependencies using hidden units. Estimation of large models is made practical using minimum probability flow, a recently developed parameter estimation method for energy-based models. The partition functions of the models are estimated using annealed importance sampling, which allows for comparing models in terms of likelihood.  Applied to 32-channel polytrode data recorded from cat visual cortex, these higher order models significantly outperform Ising models. In addition, extending the model to spatiotemporal sequences of states allows us to predict spiking based on network history. Our results highlight the importance of modeling higher order interactions across space and time to characterize activity in cortical networks."
Modeling Laminar Recordings from Visual Cortex with Semi-Restricted Boltzmann Machines,"The proliferation of high density recording techniques presents us with new challenges for characterizing the statistics of neural activity over populations of many neurons. The Ising model, which is the maximum entropy model for pairwise correlations, has been used to model the instantaneous state of a population of neurons.  This model suffers from two major limitations: 1) Estimation for large models becomes computationally intractable, and 2) it cannot capture higher-order dependencies.  We propose applying a more general maximum entropy model, the semi-restricted Boltzmann machine (sRBM), which extends the Ising model to capture higher order dependencies using hidden units. Estimation of large models is made practical using minimum probability flow, a recently developed parameter estimation method for energy-based models. The partition functions of the models are estimated using annealed importance sampling, which allows for comparing models in terms of likelihood.  Applied to 32-channel polytrode data recorded from cat visual cortex, these higher order models significantly outperform Ising models. In addition, extending the model to spatiotemporal sequences of states allows us to predict spiking based on network history. Our results highlight the importance of modeling higher order interactions across space and time to characterize activity in cortical networks."
Modeling Laminar Recordings from Visual Cortex with Semi-Restricted Boltzmann Machines,"The proliferation of high density recording techniques presents us with new challenges for characterizing the statistics of neural activity over populations of many neurons. The Ising model, which is the maximum entropy model for pairwise correlations, has been used to model the instantaneous state of a population of neurons.  This model suffers from two major limitations: 1) Estimation for large models becomes computationally intractable, and 2) it cannot capture higher-order dependencies.  We propose applying a more general maximum entropy model, the semi-restricted Boltzmann machine (sRBM), which extends the Ising model to capture higher order dependencies using hidden units. Estimation of large models is made practical using minimum probability flow, a recently developed parameter estimation method for energy-based models. The partition functions of the models are estimated using annealed importance sampling, which allows for comparing models in terms of likelihood.  Applied to 32-channel polytrode data recorded from cat visual cortex, these higher order models significantly outperform Ising models. In addition, extending the model to spatiotemporal sequences of states allows us to predict spiking based on network history. Our results highlight the importance of modeling higher order interactions across space and time to characterize activity in cortical networks."
Near-Tight Bounds for Cross-Validation via Loss Stability,Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds.  In this work we introduce a new and weak measure of stability (\emph{loss stability}) and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal.  Our work thus quantitatively improves the currentbest bounds on cross-validation.
Convergence Rate Analysis of MAP Coordinate Minimization Algorithms,"Finding maximum aposteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used.Solving these relaxations efficiently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However,these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence.Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima."
Projection Retrieval for Classification,"In many applications classification systems often require in the loop human intervention. In such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution. To tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users."
Projection Retrieval for Classification,"In many applications classification systems often require in the loop human intervention. In such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution. To tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users."
Dual-view Dirichlet Process Mixture Models for Cross-modal Data Analysis,"We propose Dual-view Dirichlet Process Mixture Models for analyzing cross-modal data. This model is a Bayesian nonparametric model incorporating a prior of infinite mixture distribution of data in any single modality, and it also captures the correspondences between mixture components from different modalities. We develop an efficient variational inference algorithm for learning the joint distribution of cross-modal data which can contribute to identifying latent structures. For prediction tasks, we provide fast approximated methods based on a latent subspace derived from this generative model and kernel regression. Comparisons of experimental results to other state of the art models on benchmark datasets demonstrate the superiority of our model in significantly improving performances on cross-modal information retrieval and image annotation."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Mixing-time Regularized Policy Gradient,"Policy gradient reinforcement learning (PGRL) methods have received substantial attention as a mean for seeking stochastic policies that maximize a cumulative reward. However, PRRL methods can often take a huge number of learning steps before it finds a reasonable stochastic policy. This learning speed depends on the mixing time of the Markov chains that are given by the policies that PGRL explores. In this paper, we give a new PGRL approach that regularizes the rule of updating the policy with the hitting time that bounds the mixing time.  In particular, hitting-time regressions based on temporal-difference learning are proposed. This will keep the Markov chain compact and can improve the learning efficiency. Numerical experiments show the proposed method outperforms the conventional PG methods."
Buy-in-Bulk Active Learning,"In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time.This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch.In this work, we study the label complexity of active learning algorithms thatrequest labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once.  In particluar, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increasethe total number of labels requested, it reduces the total cost requiredfor learning."
Buy-in-Bulk Active Learning,"In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time.This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch.In this work, we study the label complexity of active learning algorithms thatrequest labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once.  In particluar, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increasethe total number of labels requested, it reduces the total cost requiredfor learning."
Forward Model Extraction from Neural Population Activity,"Internal forward models are believed to explain the nervous system's ability to compensate for sensory feedback delays and adapt to changes in effector dynamics.  Single-neuron and behavioral studies have provided evidence of forward models, but to our knowledge it has not yet been possible to extract a full forward model of the effector directly from neural activity.  Here, we develop a novel probabilistic framework for forward model extraction that integrates neural commands with sensory feedback.  Using this framework, we can i) extract the subject's forward model, which is represented as parameters in the probabilistic model, and ii) infer the subject's timestep-by-timestep internal estimates of the motor effector position, which are latent variables in the model.  We leverage brain-computer interface (BCI) infrastructure, in which all neural commands driving the effector and sensory feedback are fully observed. We applied this framework to neural population activity recorded in macaque motor cortex during BCI control of a computer cursor to acquire visual targets. We found that recorded neural commands were more consistent with aiming straight toward targets from the subject's internal estimates of cursor position, as inferred by our probabilistic framework, than from the cursor positions displayed during online control. The extracted forward models explain about 75% of the subject's aiming errors. We believe that the probabilistic framework developed provides a critical link between sensory feedback and motor commands, and will likely facilitate the study of feedback motor control and motor learning."
Forward Model Extraction from Neural Population Activity,"Internal forward models are believed to explain the nervous system's ability to compensate for sensory feedback delays and adapt to changes in effector dynamics.  Single-neuron and behavioral studies have provided evidence of forward models, but to our knowledge it has not yet been possible to extract a full forward model of the effector directly from neural activity.  Here, we develop a novel probabilistic framework for forward model extraction that integrates neural commands with sensory feedback.  Using this framework, we can i) extract the subject's forward model, which is represented as parameters in the probabilistic model, and ii) infer the subject's timestep-by-timestep internal estimates of the motor effector position, which are latent variables in the model.  We leverage brain-computer interface (BCI) infrastructure, in which all neural commands driving the effector and sensory feedback are fully observed. We applied this framework to neural population activity recorded in macaque motor cortex during BCI control of a computer cursor to acquire visual targets. We found that recorded neural commands were more consistent with aiming straight toward targets from the subject's internal estimates of cursor position, as inferred by our probabilistic framework, than from the cursor positions displayed during online control. The extracted forward models explain about 75% of the subject's aiming errors. We believe that the probabilistic framework developed provides a critical link between sensory feedback and motor commands, and will likely facilitate the study of feedback motor control and motor learning."
Forward Model Extraction from Neural Population Activity,"Internal forward models are believed to explain the nervous system's ability to compensate for sensory feedback delays and adapt to changes in effector dynamics.  Single-neuron and behavioral studies have provided evidence of forward models, but to our knowledge it has not yet been possible to extract a full forward model of the effector directly from neural activity.  Here, we develop a novel probabilistic framework for forward model extraction that integrates neural commands with sensory feedback.  Using this framework, we can i) extract the subject's forward model, which is represented as parameters in the probabilistic model, and ii) infer the subject's timestep-by-timestep internal estimates of the motor effector position, which are latent variables in the model.  We leverage brain-computer interface (BCI) infrastructure, in which all neural commands driving the effector and sensory feedback are fully observed. We applied this framework to neural population activity recorded in macaque motor cortex during BCI control of a computer cursor to acquire visual targets. We found that recorded neural commands were more consistent with aiming straight toward targets from the subject's internal estimates of cursor position, as inferred by our probabilistic framework, than from the cursor positions displayed during online control. The extracted forward models explain about 75% of the subject's aiming errors. We believe that the probabilistic framework developed provides a critical link between sensory feedback and motor commands, and will likely facilitate the study of feedback motor control and motor learning."
Noisy Bayesian Active Learning,"We consider the problem of noisy Bayesian active learning, where we are given a finite set of functions $\mathcal{H}$, and a sample space $\mathcal{X}$. A function in $\mathcal{H}$ assigns a label to a sample in $\mathcal{X}$, and the result of a label query on a sample is corrupted by independent noise. The goal is to identify the function in $\mathcal{H}$ that generates the labels with high confidence using as few label queries as possible, by selecting the queries adaptively in a strategic manner. Previous work in Bayesian active learning considers Generalized Binary Search, and its variants for the noisy case, and analyzes the number of queries required by these sampling strategies. In this paper, we show that these schemes are, in general, suboptimal. Instead we propose and analyze an alternative strategy for sample collection. Our sampling strategy is motivated by a connection between Bayesian active learning and active hypothesis testing, and is based on querying the label of a sample which maximizes the Extrinsic Jensen--Shannon Divergence at each step. We provide upper and lower bounds on the performance of this sampling strategy, and show that these bounds are better than previous bounds."
Noisy Bayesian Active Learning,"We consider the problem of noisy Bayesian active learning, where we are given a finite set of functions $\mathcal{H}$, and a sample space $\mathcal{X}$. A function in $\mathcal{H}$ assigns a label to a sample in $\mathcal{X}$, and the result of a label query on a sample is corrupted by independent noise. The goal is to identify the function in $\mathcal{H}$ that generates the labels with high confidence using as few label queries as possible, by selecting the queries adaptively in a strategic manner. Previous work in Bayesian active learning considers Generalized Binary Search, and its variants for the noisy case, and analyzes the number of queries required by these sampling strategies. In this paper, we show that these schemes are, in general, suboptimal. Instead we propose and analyze an alternative strategy for sample collection. Our sampling strategy is motivated by a connection between Bayesian active learning and active hypothesis testing, and is based on querying the label of a sample which maximizes the Extrinsic Jensen--Shannon Divergence at each step. We provide upper and lower bounds on the performance of this sampling strategy, and show that these bounds are better than previous bounds."
Noisy Bayesian Active Learning,"We consider the problem of noisy Bayesian active learning, where we are given a finite set of functions $\mathcal{H}$, and a sample space $\mathcal{X}$. A function in $\mathcal{H}$ assigns a label to a sample in $\mathcal{X}$, and the result of a label query on a sample is corrupted by independent noise. The goal is to identify the function in $\mathcal{H}$ that generates the labels with high confidence using as few label queries as possible, by selecting the queries adaptively in a strategic manner. Previous work in Bayesian active learning considers Generalized Binary Search, and its variants for the noisy case, and analyzes the number of queries required by these sampling strategies. In this paper, we show that these schemes are, in general, suboptimal. Instead we propose and analyze an alternative strategy for sample collection. Our sampling strategy is motivated by a connection between Bayesian active learning and active hypothesis testing, and is based on querying the label of a sample which maximizes the Extrinsic Jensen--Shannon Divergence at each step. We provide upper and lower bounds on the performance of this sampling strategy, and show that these bounds are better than previous bounds."
Human memory search as a random walk in a semantic network,"The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single, undirected process rather than one process for exploring a cluster and one process for switching between clusters."
Human memory search as a random walk in a semantic network,"The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single, undirected process rather than one process for exploring a cluster and one process for switching between clusters."
Manifold Regularization and Embedding Through Laplacian Low-Rank Correction,"This paper introduces Laplacian low-rank correction (LLRC), a method for manifold regularization  that is generally applicable to arbitrary data sets and learning problems.  Unlike popular embedding algorithms such as Laplacian eigenmaps, locally linear embedding, and ISOMAP, our approach does not explicitly seek a low-dimensional representation of data.  Rather, LLRC leverages the graph Laplacian of a local similarity graph to construct a low-rank linear correction that maintains the original dimension of the feature space. This linear correction eliminates dominant off-manifold noise directions, such that the principal components of the corrected space represent the data manifold.LLRC can be kernelized to account for nonlinear structure. A primary advantage of such a linear approach is that the correction can be easily extended out-of-sample.A low-dimensional embedding may be obtained from LLRC by extracting the (kernel) principal components from the corrected feature representation, and can significantly outperform standard (kernel) PCA at recovering manifold structure. The modified representation may also be applied in a number of other learning problems, including classification, regression, and clustering. As an example, we apply LLRC to SVM classification in a semi-supervised setting. In particular, we develop a low-rank approximation to the standard Laplacian SVM. This approximation offers greatly reduced computation for large data sets and, in some cases, improved performance."
Manifold Regularization and Embedding Through Laplacian Low-Rank Correction,"This paper introduces Laplacian low-rank correction (LLRC), a method for manifold regularization  that is generally applicable to arbitrary data sets and learning problems.  Unlike popular embedding algorithms such as Laplacian eigenmaps, locally linear embedding, and ISOMAP, our approach does not explicitly seek a low-dimensional representation of data.  Rather, LLRC leverages the graph Laplacian of a local similarity graph to construct a low-rank linear correction that maintains the original dimension of the feature space. This linear correction eliminates dominant off-manifold noise directions, such that the principal components of the corrected space represent the data manifold.LLRC can be kernelized to account for nonlinear structure. A primary advantage of such a linear approach is that the correction can be easily extended out-of-sample.A low-dimensional embedding may be obtained from LLRC by extracting the (kernel) principal components from the corrected feature representation, and can significantly outperform standard (kernel) PCA at recovering manifold structure. The modified representation may also be applied in a number of other learning problems, including classification, regression, and clustering. As an example, we apply LLRC to SVM classification in a semi-supervised setting. In particular, we develop a low-rank approximation to the standard Laplacian SVM. This approximation offers greatly reduced computation for large data sets and, in some cases, improved performance."
Coarse-to-fine video segmentation using supervoxel trees,"Image and video segmentation is a task of immense importance in the field of computer vision. Existing algorithms like graph cuts solve this problem by considering the possibility of every adjoining pixel (superpixel) or voxel (supervoxel) getting different labels. However, real images tend to have spatial continuity and videos have additional temporal continuity. In this paper, we consider a hierarchical tree of supervoxels.We propose a coarse-to-fine video segmentation scheme whereby larger supervoxels belonging to the same label, need not be refined into finer supervoxels. For videos with significant spatio-temporal continuity, such a scheme can lead to significant computational savings. By using admissible heuristic estimates of the unary and binary potentials, we can show that this scheme leads to the exact segmentation that would have been obtained by considering the finest layer of supervoxels."
Instance Level Multiple Instance Learning Using Similarity Preserving Quasi Cliques,In this paper we introduce an instance-level approach to multiple instance learning. Our bottom-up approach learns a discriminative notion of similarity between instances in positive bags and use it to form a discriminative similarity graph. We then introduce the notion of similarity preserving quasi-cliques that aims at discovering large quasi-cliques with high scores of within-clique similarities. We argue that such large cliques provide clue to infer the underlying structure between positive instances. We use a ranking function that takes into account pairwise similarities coupled with prospectiveness of edges to score all positive instances. We show that these scores yield to positive instance discovery. Our experimental evaluations show that our method outperforms state-of-the-art MIL methods both at the bag-level and instance-level predictions in standard benchmarks and image and text datasets.
Diagnose and Decide: An Optimal Bayesian Approach 004,"Many real-world scenarios require making informed choices after some sequence of actions that yield noisy information about a latent state.  Prior research has mostly focused on generic methods that struggle to scale to large domains. We focus on a subclass of such problems with two particular characteristics. First, though information gathering actions or tests only provide noisy information about the hidden state, once performed a test will always yield the same result. This means it is sufficient to perform each test once. Second, we assume that test costs can be expressed in the same units as costs of the final decisions made. We call such scenarios diagnose-and-decide problems. We prove diagnose-and-decide problems are a special subclass of POMDPs for which the optimal policy can be computed in time polynomial in the set of possible tests' outcomes. We develop a new simple algorithm which is able to take advantage of the unique structure in our problem while guaranteeing optimality. We demonstrate the advantages of our approach over greedy and traditional POMDP methods in two simulations based on real-world data (colon cancer screening and object recognition) as well as a large synthetic domain. "
Extending generalized delta rules for efficient Hessian calculations through backpropagation,"Recent extensions of first-order backpropagation (BP), also known asgeneralized delta rules, of Rumelhart et al. (1986) lead to the development of efficient Hessian calculations for second-orderoptimization (e.g., Levenberg-Marquardt methods).  Consider, for instance, the evaluation of  the so-called Gauss-Newton Hessian matrix J'*J of size n x n when optimizing a multi-layer neural network that has multipleZ outputs (Z > 1).  Fairbank & Alonso~(2012) described how to use first-order BPfor (Z+n) times per data pattern in forming Z rows of J and then J'*Jexplicitly column by column.  Their claim is thatthe proposed method works faster than the ``standard'' algebraic method bya factor of Z. Yet, their analysis totally ignores several key factors thatare already discussed individually in other computational techniques.Even under their assumption 1 << Z <= square root of n,the standard method can work faster in some situations.By combining the strengths of existing algorithms,we have derived an efficient algorithm that performs backward passes only forB times, followed by some algebraic manipulations, where B denotes the total number of hidden nodes.Since B is approximately equal to sqaure root of n, our improvement would be significant.We also show its further extensions and an efficient matrix-freealgorithm that combines BP with a forward mode of automatic differentiation."
Learning mixture models with the hierarchical expectation maximization algorithm,"Driven by the need for computationally efficient parameter estimation from large, web-scale data sets, the hierarchical EM (HEM) algorithm has been proposed and proven effective for a variety of modeling tasks and applications. In this paper, we investigate the benefits of HEM as a general-purpose algorithm for parameter estimation in mixture models, compared to regular EM. First, we re-derive the algorithm in more generality, for generic exponential family distributions, with and without unobserved variables. Second, we discuss and experimentally verify its benefits across a broad spectrum of model classes and applications. Besides scalability, HEM's implicit regularization and adaptation for multiple instance learning make it an appealing alternative to standard EM, for practitioners."
Learning mixture models with the hierarchical expectation maximization algorithm,"Driven by the need for computationally efficient parameter estimation from large, web-scale data sets, the hierarchical EM (HEM) algorithm has been proposed and proven effective for a variety of modeling tasks and applications. In this paper, we investigate the benefits of HEM as a general-purpose algorithm for parameter estimation in mixture models, compared to regular EM. First, we re-derive the algorithm in more generality, for generic exponential family distributions, with and without unobserved variables. Second, we discuss and experimentally verify its benefits across a broad spectrum of model classes and applications. Besides scalability, HEM's implicit regularization and adaptation for multiple instance learning make it an appealing alternative to standard EM, for practitioners."
Modeling Salient Object-Object Interactions to Generate Textual Descriptions for Natural Images,"In this paper we propose a new method for automatically generating textual descriptions of images. Our method consists of two main steps. Using saliency maps, it detects the areas of interests in the image and then creates the description by recognizing the interactions between detected objects within those areas. These interactions are modeled using the pose(body parts configuration) of the objects. To create sentences a syntactic model is used that builds subtrees around the detected objects and then combines those subtrees using recognized interaction. Our Results show the improved accuracy of the descriptions generated by our algorithm."
Probabilistic Event Cascades for Alzheimer's disease," Accurate and detailed models of the progression of neurodegenerative diseases such as  Alzheimer's (AD) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments. In this paper, we introduce the ALPACA (Alzheimer's disease Probabilistic Cascades) model, a generative model linking latent Alzheimer's progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a fixed ordering of events, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed disease progression models. We describe efficient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer's patients from the  Alzheimer's Disease Neuroimaging Initiative."
Spectral Learning of Latent-Variable HMMs ,We derive a spectral algorithm for learning the parameters of a latent-variable HMM. This method avoids the problem of local optima and provides a consistent estimate of the parameters. We demonstrate the method on a phoneme recognition task and show that it performs competitively with EM. 
Richer Dynamic Textures: Optimized Hierarchical Bases and Sparse Updates,"Dynamic textures are used to create perceptually reasonable continuations of repetitive video sequences and as a tool for segmentation or classification of video sequences. We develop three extensions which have the goal of allowing Dynamic Textures to accurately represent richer, more complex natural video sequences. First, we jointly solve for the representation and the dynamics model, optimizing for the ability to best predict future appearance. Second, we derive the dynamic textures formulation using a hierarchical basis that captures both global correlations and local variations. Third, we derive an approach to drive the dynamic texture model to fit known data at a sparse set of known locations. Collectively, these improvements show quantitative and qualitative improvements on a variety of textures drawn from the dyntex database."
Richer Dynamic Textures: Optimized Hierarchical Bases and Sparse Updates,"Dynamic textures are used to create perceptually reasonable continuations of repetitive video sequences and as a tool for segmentation or classification of video sequences. We develop three extensions which have the goal of allowing Dynamic Textures to accurately represent richer, more complex natural video sequences. First, we jointly solve for the representation and the dynamics model, optimizing for the ability to best predict future appearance. Second, we derive the dynamic textures formulation using a hierarchical basis that captures both global correlations and local variations. Third, we derive an approach to drive the dynamic texture model to fit known data at a sparse set of known locations. Collectively, these improvements show quantitative and qualitative improvements on a variety of textures drawn from the dyntex database."
Richer Dynamic Textures: Optimized Hierarchical Bases and Sparse Updates,"Dynamic textures are used to create perceptually reasonable continuations of repetitive video sequences and as a tool for segmentation or classification of video sequences. We develop three extensions which have the goal of allowing Dynamic Textures to accurately represent richer, more complex natural video sequences. First, we jointly solve for the representation and the dynamics model, optimizing for the ability to best predict future appearance. Second, we derive the dynamic textures formulation using a hierarchical basis that captures both global correlations and local variations. Third, we derive an approach to drive the dynamic texture model to fit known data at a sparse set of known locations. Collectively, these improvements show quantitative and qualitative improvements on a variety of textures drawn from the dyntex database."
One Permutation Hashing,"While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.In this paper, we develop a simple \textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing $k$ permutations to just one  would be much more \textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \& logistic regression also confirm the theoretical results."
One Permutation Hashing,"While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.In this paper, we develop a simple \textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing $k$ permutations to just one  would be much more \textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \& logistic regression also confirm the theoretical results."
Getting the First Page Right: Bayesian Active Retrieval under Uncertainty,"Triggered by the idea of an information retrieval system for objects with noisy and missing features, we investigate the general problem of actively learning a similarity function of complex objects when the inputs to this function are not known exactly. To reduce the uncertainty in the inputs, and in turn improve the similarity function, we are interested in acquiring more information about the input objects. As gathering clean and complete information is costly or even impossible, it is important to carefully select the information needed and to be able to deal with uncertainty in order to retrieve meaningful results fast and with low total cost. Hence, we propose a Bayesian active learning approach to efficiently learn the most similar objects to a given query object in the setting where only partial and noisy information about entities is available. In our information retrieval case this corresponds to the task of getting the first page (of retrieval results) right. We evaluate the proposed Bayesian decision theoretic framework to actively acquire information on several retrieval problems, including a real-world document retrieval task."
Getting the First Page Right: Bayesian Active Retrieval under Uncertainty,"Triggered by the idea of an information retrieval system for objects with noisy and missing features, we investigate the general problem of actively learning a similarity function of complex objects when the inputs to this function are not known exactly. To reduce the uncertainty in the inputs, and in turn improve the similarity function, we are interested in acquiring more information about the input objects. As gathering clean and complete information is costly or even impossible, it is important to carefully select the information needed and to be able to deal with uncertainty in order to retrieve meaningful results fast and with low total cost. Hence, we propose a Bayesian active learning approach to efficiently learn the most similar objects to a given query object in the setting where only partial and noisy information about entities is available. In our information retrieval case this corresponds to the task of getting the first page (of retrieval results) right. We evaluate the proposed Bayesian decision theoretic framework to actively acquire information on several retrieval problems, including a real-world document retrieval task."
Near Optimal Chernoff Bounds for Markov Decision Processes,"The expected return is a widely used objective in decision making under uncertainty.  Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize.  We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw connections to previously proposed objectives for risk-aware planing: minmax, exponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded.  Additionally, we present an efficient algorithm for optimizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale."
Near Optimal Chernoff Bounds for Markov Decision Processes,"The expected return is a widely used objective in decision making under uncertainty.  Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize.  We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw connections to previously proposed objectives for risk-aware planing: minmax, exponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded.  Additionally, we present an efficient algorithm for optimizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale."
Is Matching Pursuit Solving Convex Problems?,"Matching pursuit  (\texttt{MP}) algorithms have been successfullyapplied in signal processing and pattern recognition areas. However,as far as we know, it is still not clear whether any \texttt{MP}algorithm can solve a convex problem or not. In this paper, a novelconvex relaxation is proposed for a class of matching pursuitalgorithms, which includes the classical orthogonal matching pursuit(\texttt{OMP}) as a special case.  Based on the proposed scheme, ageneral matching pursuit (\texttt{GMP}) algorithm can be naturallyobtained. As it solves a convex problem, \texttt{GMP}  guarantees toconverge globally. In addition, a subspace exploratory search canfurther improve the performance. Finally, we show that \texttt{GMP}with an $\ell_1$ regularization term can recover the $k$-sparsesignals if the restricted isometry constant $\sigma_k\leq 0.307-\nu$,where $\nu$ can be arbitrarily close to 0. The proposed method can beeasily parallelized and the efficiency can be further improved.Simulations on an 8-core machine show that the proposed method cansuccessfully decode the problems of scale $2^{13} \times 2^{17}$within 10 seconds and scale $2^{10} \times 2^{20}$ within 2 seconds."
Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models,"Links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics, i.e., letting the variance of particular distributions in a graphical model go to zero. For instance, in the context of clustering, such an approach yields precise connections between the k-means and EM algorithms.  In thispaper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models.  Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models.  We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis."
Entropy Estimations Using Correlated Symmetric Stable Random Projections,"Methods for efficiently estimating  the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of {\em Compressed Counting (CC)}~\cite{Proc:Li_Zhang_COLT11}  based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two \textbf{correlated} frequency moments estimated from  correlated samples of \textbf{symmetric} stable random variables. Interestingly, the  estimator for the moment we recommend for entropy estimation  barely has bounded variance itself, whereas the  geometric mean estimator (which has bounded higher-order moments) is not sufficient for entropy estimation. Our experiments confirm that this method is able to  well approximate the Shannon entropy using small storage (e.g., $100\sim 1000$ samples). A prior study~\cite{Proc:Zhao_IMC07}  approximated the Shannon entropy using symmetric stable random projections with (e.g.,) $10^5\sim1.6\times10^6$ \textbf{independent} samples."
Behavior segmentation and labeling using structured SVM,"We explore representing, segmenting and classifying behavior using bout-widestatistics, rather than local descriptors. To this effect we develop a structuredsupport vector machine (sSVM) where the loss function is computed at the boutlevelrather than at frame level, and bouts of different length are treated equally.We test our ideas on a set of videos of pairs of flies engaged in social behavior(exploration, aggression and courtship); four actions and a fifth grab-bag categoryin the videos are annotated by expert fly biologists. Our experiments show thatwith limited amount of data, we are able to obtain detection rate of 64%-93% foreach of the actions, while maintaining a precision of 64%-91%."
Deep Learning of invariant features via tracked video sequences,"We use video sequences produced by tracking as training data to learn invariant features. These features are spatial instead of temporal, and well suited to extract from still images. With a temporal coherence objective, a multi-layer neural network encodes invariance that grow increasingly complex with layer hierarchy. Without fine-tuning with labels, we achieve competitive performance on five non-temporal image datasets and state-of-the-art classification accuracy 61% on STL-10 object recognition dataset."
Deep Learning of invariant features via tracked video sequences,"We use video sequences produced by tracking as training data to learn invariant features. These features are spatial instead of temporal, and well suited to extract from still images. With a temporal coherence objective, a multi-layer neural network encodes invariance that grow increasingly complex with layer hierarchy. Without fine-tuning with labels, we achieve competitive performance on five non-temporal image datasets and state-of-the-art classification accuracy 61% on STL-10 object recognition dataset."
Structure Learning for Weakly Dependent Observations,"We consider the problem of estimating the graph structure of a certain class of stochastic processes defined on an Ising model. This class contains as special cases the i.i.d. observation model, the geometric $\alpha$-mixing process, and the rapidly-mixing Glauber dynamics. We analyze thenode-based neighborhood estimation algorithm of \cite{RavWaiLaf09}, which reduces to $\ell_1$-regularized logistic regression. Our main result is to provide sufficient conditions on the triple $(\numobs, \pdim, \ddim)$ for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously."
On the Sample Complexity of Robust PCA,"We estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix.This estimator is used in a convex algorithm for robust subspace recovery (i.e., robust PCA).Our model assumes a sub-Gaussian underlying distribution and an i.i.d.~sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $N$ is of order $O(N^{-0.5+\eps})$ for arbitrarily small $\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation, i.e., $O(N^{-0.5})$. Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is $O(D^{2+\delta})$ for arbitrarily small $\delta>0$ (whereas the sample complexity of direct covariance estimation with Frobenius norm is $O(D^{2})$).These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm, which are close to those of PCA. To the best of our knowledge, this is the only work analyzing the sample complexity of any robust PCA algorithm."
View-invariance and mirror-symmetric tuning in a model of the macaque face-processing system,"Recent experimental results characterizing the face-processing network in macaque visual cortex pose a major puzzle. View-tuned units (found in patches ML/MF) are a natural step to a view-tolerant representation (found in patch AM), as predicted by several models. However, the observation that cells in patch AL are tuned to faces and their mirror reflections remains unexplained (cf. Freiwald and Tsao (2010)). We show that a model based on the hypothesis that the ventral stream implements a memory-based approach to transformation invariance predicts the main properties of ML/MF, AL and AM. In this view, a major computational goal of the ventral stream is to compute invariant ?signatures? that can be used to recognize novel objects under previously-seen transformations of arbitrary ``templates''.  These invariant signatures can be regarded as encodings of a novel object relative to a compressed memory of the transformation of familiar objects (PCA). "
View-invariance and mirror-symmetric tuning in a model of the macaque face-processing system,"Recent experimental results characterizing the face-processing network in macaque visual cortex pose a major puzzle. View-tuned units (found in patches ML/MF) are a natural step to a view-tolerant representation (found in patch AM), as predicted by several models. However, the observation that cells in patch AL are tuned to faces and their mirror reflections remains unexplained (cf. Freiwald and Tsao (2010)). We show that a model based on the hypothesis that the ventral stream implements a memory-based approach to transformation invariance predicts the main properties of ML/MF, AL and AM. In this view, a major computational goal of the ventral stream is to compute invariant ?signatures? that can be used to recognize novel objects under previously-seen transformations of arbitrary ``templates''.  These invariant signatures can be regarded as encodings of a novel object relative to a compressed memory of the transformation of familiar objects (PCA). "
View-invariance and mirror-symmetric tuning in a model of the macaque face-processing system,"Recent experimental results characterizing the face-processing network in macaque visual cortex pose a major puzzle. View-tuned units (found in patches ML/MF) are a natural step to a view-tolerant representation (found in patch AM), as predicted by several models. However, the observation that cells in patch AL are tuned to faces and their mirror reflections remains unexplained (cf. Freiwald and Tsao (2010)). We show that a model based on the hypothesis that the ventral stream implements a memory-based approach to transformation invariance predicts the main properties of ML/MF, AL and AM. In this view, a major computational goal of the ventral stream is to compute invariant ?signatures? that can be used to recognize novel objects under previously-seen transformations of arbitrary ``templates''.  These invariant signatures can be regarded as encodings of a novel object relative to a compressed memory of the transformation of familiar objects (PCA). "
View-invariance and mirror-symmetric tuning in a model of the macaque face-processing system,"Recent experimental results characterizing the face-processing network in macaque visual cortex pose a major puzzle. View-tuned units (found in patches ML/MF) are a natural step to a view-tolerant representation (found in patch AM), as predicted by several models. However, the observation that cells in patch AL are tuned to faces and their mirror reflections remains unexplained (cf. Freiwald and Tsao (2010)). We show that a model based on the hypothesis that the ventral stream implements a memory-based approach to transformation invariance predicts the main properties of ML/MF, AL and AM. In this view, a major computational goal of the ventral stream is to compute invariant ?signatures? that can be used to recognize novel objects under previously-seen transformations of arbitrary ``templates''.  These invariant signatures can be regarded as encodings of a novel object relative to a compressed memory of the transformation of familiar objects (PCA). "
Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning,"We describe a latent variable model for supervised dimensionality reduction and distance metric learning. The model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones. The model?s continuous latent variables locate pairs of examples in a latent space of lower dimensionality. The model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate Gaussian. Nevertheless we show that inference is completely tractable and derive an Expectation-Maximization (EM) algorithm for parameter estimation. We also compare the model to other approaches in distance metric learning. The model?s main advantage is its simplicity: at each iteration of the EM algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem. Experiments show that these simple updates are highly effective."
Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning,"We describe a latent variable model for supervised dimensionality reduction and distance metric learning. The model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones. The model?s continuous latent variables locate pairs of examples in a latent space of lower dimensionality. The model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate Gaussian. Nevertheless we show that inference is completely tractable and derive an Expectation-Maximization (EM) algorithm for parameter estimation. We also compare the model to other approaches in distance metric learning. The model?s main advantage is its simplicity: at each iteration of the EM algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem. Experiments show that these simple updates are highly effective."
Out-of-Sample Extensions for Manifold Learning Using Sparse Kernel Ridge Regression,"Many manifold learning algorithms do not provide a mapping from the input space to the low-dimensional manifold, requiring an out-of-sample extension to project new points onto the low-dimensional manifold. We propose an out-of-sample extension approach based on a sparse approximation to kernel ridge regression to achieve significant computational savings. In particular, we present a convex program for approximating kernel ridge regression where given a set of points in $\mathbb{R}^d$ and corresponding points in $\mathbb{R}^p$, we find a mapping from $\mathbb{R}^d$ to $\mathbb{R}^p$ that depends only on a subset of the input data points acting as support vectors. Our construction uses group sparsity and guarantees an upper bound on the average squared Euclidean distance between the predicted points of the sparsified mapping and those of kernel ridge regression. We present two medical imaging applications that necessitate a fast projection to a low-dimensional space. The first is respiratory gating in ultrasound, where we assign the breathing state to each ultrasound frame during the acquisition in real-time. The second is the detection of the position of a patient in an MRI scanner while the bed the patient lies on is moving to a target location."
Out-of-Sample Extensions for Manifold Learning Using Sparse Kernel Ridge Regression,"Many manifold learning algorithms do not provide a mapping from the input space to the low-dimensional manifold, requiring an out-of-sample extension to project new points onto the low-dimensional manifold. We propose an out-of-sample extension approach based on a sparse approximation to kernel ridge regression to achieve significant computational savings. In particular, we present a convex program for approximating kernel ridge regression where given a set of points in $\mathbb{R}^d$ and corresponding points in $\mathbb{R}^p$, we find a mapping from $\mathbb{R}^d$ to $\mathbb{R}^p$ that depends only on a subset of the input data points acting as support vectors. Our construction uses group sparsity and guarantees an upper bound on the average squared Euclidean distance between the predicted points of the sparsified mapping and those of kernel ridge regression. We present two medical imaging applications that necessitate a fast projection to a low-dimensional space. The first is respiratory gating in ultrasound, where we assign the breathing state to each ultrasound frame during the acquisition in real-time. The second is the detection of the position of a patient in an MRI scanner while the bed the patient lies on is moving to a target location."
Out-of-Sample Extensions for Manifold Learning Using Sparse Kernel Ridge Regression,"Many manifold learning algorithms do not provide a mapping from the input space to the low-dimensional manifold, requiring an out-of-sample extension to project new points onto the low-dimensional manifold. We propose an out-of-sample extension approach based on a sparse approximation to kernel ridge regression to achieve significant computational savings. In particular, we present a convex program for approximating kernel ridge regression where given a set of points in $\mathbb{R}^d$ and corresponding points in $\mathbb{R}^p$, we find a mapping from $\mathbb{R}^d$ to $\mathbb{R}^p$ that depends only on a subset of the input data points acting as support vectors. Our construction uses group sparsity and guarantees an upper bound on the average squared Euclidean distance between the predicted points of the sparsified mapping and those of kernel ridge regression. We present two medical imaging applications that necessitate a fast projection to a low-dimensional space. The first is respiratory gating in ultrasound, where we assign the breathing state to each ultrasound frame during the acquisition in real-time. The second is the detection of the position of a patient in an MRI scanner while the bed the patient lies on is moving to a target location."
Trajectory-Based Short-Sighted Probabilistic Planning,"Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [ref] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs.  We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately $10^{70}$ states."
Trajectory-Based Short-Sighted Probabilistic Planning,"Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [ref] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs.  We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately $10^{70}$ states."
Jointly Learning and Selecting Features via Conditional Point-wise Mixture RBMs,"Feature selection is an important technique for ?nding relevant features from high-dimensional data. However, the performance of feature selection methods is often limited by the raw feature representation. On the other hand, unsupervised feature learning has recently emerged as promising tools for extracting useful features from data. Although supervised information can be exploited in the process of supervised ?ne-tuning (preceded by unsupervised pre-training), the training becomes challenging when the unlabeled data contain signi?cant amounts of irrelevant information. To address these issues, we propose a new unsupervised feature learning algorithm, the conditional point-wise mixture restricted Boltzmann machine, which attempts to perform feature grouping while learning the features. Our model represents each input coordinate as a mixture model when conditioned on the hidden units, where each group of hidden units can generate the corresponding mixture component. Furthermore, we present an extension of our method that combines bottom-up feature learning and top-down feature selection in a coherent way, which can effectively handle irrelevant input patterns by focusing on relevant signals and thus learn more informative features. Our experiments show that our model is effective in learning separate groups of hidden units (e.g., that correspond to informative signals vs. irrelevant patterns) from complex, noisy data."
Jointly Learning and Selecting Features via Conditional Point-wise Mixture RBMs,"Feature selection is an important technique for ?nding relevant features from high-dimensional data. However, the performance of feature selection methods is often limited by the raw feature representation. On the other hand, unsupervised feature learning has recently emerged as promising tools for extracting useful features from data. Although supervised information can be exploited in the process of supervised ?ne-tuning (preceded by unsupervised pre-training), the training becomes challenging when the unlabeled data contain signi?cant amounts of irrelevant information. To address these issues, we propose a new unsupervised feature learning algorithm, the conditional point-wise mixture restricted Boltzmann machine, which attempts to perform feature grouping while learning the features. Our model represents each input coordinate as a mixture model when conditioned on the hidden units, where each group of hidden units can generate the corresponding mixture component. Furthermore, we present an extension of our method that combines bottom-up feature learning and top-down feature selection in a coherent way, which can effectively handle irrelevant input patterns by focusing on relevant signals and thus learn more informative features. Our experiments show that our model is effective in learning separate groups of hidden units (e.g., that correspond to informative signals vs. irrelevant patterns) from complex, noisy data."
Jointly Learning and Selecting Features via Conditional Point-wise Mixture RBMs,"Feature selection is an important technique for ?nding relevant features from high-dimensional data. However, the performance of feature selection methods is often limited by the raw feature representation. On the other hand, unsupervised feature learning has recently emerged as promising tools for extracting useful features from data. Although supervised information can be exploited in the process of supervised ?ne-tuning (preceded by unsupervised pre-training), the training becomes challenging when the unlabeled data contain signi?cant amounts of irrelevant information. To address these issues, we propose a new unsupervised feature learning algorithm, the conditional point-wise mixture restricted Boltzmann machine, which attempts to perform feature grouping while learning the features. Our model represents each input coordinate as a mixture model when conditioned on the hidden units, where each group of hidden units can generate the corresponding mixture component. Furthermore, we present an extension of our method that combines bottom-up feature learning and top-down feature selection in a coherent way, which can effectively handle irrelevant input patterns by focusing on relevant signals and thus learn more informative features. Our experiments show that our model is effective in learning separate groups of hidden units (e.g., that correspond to informative signals vs. irrelevant patterns) from complex, noisy data."
Tight Bounds on Redundancy and Distinguishability of Label-Invariant Distributions,"The minimax KL-divergence of any distribution from alldistributions in a given collection has several practicalimplications. In compression, it is the least additional number of bitsover the entropy needed in the worst case to encode theoutput of a distribution in the collection. In onlineestimation and learning, it is the lowestexpected log-loss regret when guessing a sequence of randomvalues. In hypothesis testing, it upper bounds the largestnumber of distinguishable distributions in the collection.Motivated by problems ranging from population estimation totext classification and speech recognition, severalmachine-learning and information-theory researchers haverecently considered label-invariant distributions and propertiesof \iid-drawn samples.Using techniques that reveal and exploit the structure of these distributions,we improve on a sequence of previous works and show that theminimax KL-divergence of the collection of label-invariantdistributions over length-$n$ \iid sequencesis between $0.3\cdot n^{1/3}$ and $n^{1/3}\log^2n$."
Tight Bounds on Redundancy and Distinguishability of Label-Invariant Distributions,"The minimax KL-divergence of any distribution from alldistributions in a given collection has several practicalimplications. In compression, it is the least additional number of bitsover the entropy needed in the worst case to encode theoutput of a distribution in the collection. In onlineestimation and learning, it is the lowestexpected log-loss regret when guessing a sequence of randomvalues. In hypothesis testing, it upper bounds the largestnumber of distinguishable distributions in the collection.Motivated by problems ranging from population estimation totext classification and speech recognition, severalmachine-learning and information-theory researchers haverecently considered label-invariant distributions and propertiesof \iid-drawn samples.Using techniques that reveal and exploit the structure of these distributions,we improve on a sequence of previous works and show that theminimax KL-divergence of the collection of label-invariantdistributions over length-$n$ \iid sequencesis between $0.3\cdot n^{1/3}$ and $n^{1/3}\log^2n$."
Tight Bounds on Redundancy and Distinguishability of Label-Invariant Distributions,"The minimax KL-divergence of any distribution from alldistributions in a given collection has several practicalimplications. In compression, it is the least additional number of bitsover the entropy needed in the worst case to encode theoutput of a distribution in the collection. In onlineestimation and learning, it is the lowestexpected log-loss regret when guessing a sequence of randomvalues. In hypothesis testing, it upper bounds the largestnumber of distinguishable distributions in the collection.Motivated by problems ranging from population estimation totext classification and speech recognition, severalmachine-learning and information-theory researchers haverecently considered label-invariant distributions and propertiesof \iid-drawn samples.Using techniques that reveal and exploit the structure of these distributions,we improve on a sequence of previous works and show that theminimax KL-divergence of the collection of label-invariantdistributions over length-$n$ \iid sequencesis between $0.3\cdot n^{1/3}$ and $n^{1/3}\log^2n$."
Direct Optimization of Ranking Measures for Learning to Rank Models,"We present a novel learning algorithm that directly optimizes the ranking measures without resorting to any upper bounds or approximations. Our appraoch is essentially an iterative greedy coordinate descent method in optimization. For each iteration, we only update one parameter along one coordinate with all others fixed. Since the ranking measure is a stepwise functionof a single parameter, we exploit an exhaustive line search algorithmto locate the interval with the best ranking measure along each coordinate.We pick the coordinates that lead to the largest improvement of ranking measures. In order to determine  the optimal value of the parameter for the selected coordinates, we construct a probabilistic framework for the permutation, and maximize the likelihood of top-$m$ ranked documents. This iterative procedure is continued until convergence.We conduct experiments on five datasets selected from Microsoft LETOR datasets, our experimental results show that the proposed DirectRank algorithm outperforms severalwell-known state-of-the-art ranking algorithms."
Direct Optimization of Ranking Measures for Learning to Rank Models,"We present a novel learning algorithm that directly optimizes the ranking measures without resorting to any upper bounds or approximations. Our appraoch is essentially an iterative greedy coordinate descent method in optimization. For each iteration, we only update one parameter along one coordinate with all others fixed. Since the ranking measure is a stepwise functionof a single parameter, we exploit an exhaustive line search algorithmto locate the interval with the best ranking measure along each coordinate.We pick the coordinates that lead to the largest improvement of ranking measures. In order to determine  the optimal value of the parameter for the selected coordinates, we construct a probabilistic framework for the permutation, and maximize the likelihood of top-$m$ ranked documents. This iterative procedure is continued until convergence.We conduct experiments on five datasets selected from Microsoft LETOR datasets, our experimental results show that the proposed DirectRank algorithm outperforms severalwell-known state-of-the-art ranking algorithms."
Direct Optimization of Ranking Measures for Learning to Rank Models,"We present a novel learning algorithm that directly optimizes the ranking measures without resorting to any upper bounds or approximations. Our appraoch is essentially an iterative greedy coordinate descent method in optimization. For each iteration, we only update one parameter along one coordinate with all others fixed. Since the ranking measure is a stepwise functionof a single parameter, we exploit an exhaustive line search algorithmto locate the interval with the best ranking measure along each coordinate.We pick the coordinates that lead to the largest improvement of ranking measures. In order to determine  the optimal value of the parameter for the selected coordinates, we construct a probabilistic framework for the permutation, and maximize the likelihood of top-$m$ ranked documents. This iterative procedure is continued until convergence.We conduct experiments on five datasets selected from Microsoft LETOR datasets, our experimental results show that the proposed DirectRank algorithm outperforms severalwell-known state-of-the-art ranking algorithms."
Direct Optimization of Ranking Measures for Learning to Rank Models,"We present a novel learning algorithm that directly optimizes the ranking measures without resorting to any upper bounds or approximations. Our appraoch is essentially an iterative greedy coordinate descent method in optimization. For each iteration, we only update one parameter along one coordinate with all others fixed. Since the ranking measure is a stepwise functionof a single parameter, we exploit an exhaustive line search algorithmto locate the interval with the best ranking measure along each coordinate.We pick the coordinates that lead to the largest improvement of ranking measures. In order to determine  the optimal value of the parameter for the selected coordinates, we construct a probabilistic framework for the permutation, and maximize the likelihood of top-$m$ ranked documents. This iterative procedure is continued until convergence.We conduct experiments on five datasets selected from Microsoft LETOR datasets, our experimental results show that the proposed DirectRank algorithm outperforms severalwell-known state-of-the-art ranking algorithms."
Minimax vs. UCT: A Comparative Study Using Synthetic Games,"Upper Confidence bounds for Trees (UCT) and Minimax are two of themost prominent tree-search based adversarial reasoning strategies fora variety of challenging domains, such as Chess and Go. Theircomplementary strengths in different domains have been the motivationfor several works attempting to achieve a better understanding oftheir behaviors. In this paper, rather than using complex games as atestbed for deriving indirect insights into UCT and Minimax, wepropose the study of relatively simple synthetic trees that permitanalysis and afford a greater degree of experimental freedom. Using anovel tree model that does not suffer from the shortcomings ofpreviously studied models, we provide a relatively straightforwardcharacterization of the kinds of games where UCT is superior toMinimax, and vice versa --- to the best of our knowledge, this is thefirst time such an effort has been successful. In particular, we showthat UCT shines in games where heuristics are accurately modeled usingadditive Gaussian noise, that contrary to previous work, earlyterminal states by themselves do not necessarily hurt UCT, and that intrees with heuristic dispersion lag, UCT is outperformed byMinimax."
Minimax vs. UCT: A Comparative Study Using Synthetic Games,"Upper Confidence bounds for Trees (UCT) and Minimax are two of themost prominent tree-search based adversarial reasoning strategies fora variety of challenging domains, such as Chess and Go. Theircomplementary strengths in different domains have been the motivationfor several works attempting to achieve a better understanding oftheir behaviors. In this paper, rather than using complex games as atestbed for deriving indirect insights into UCT and Minimax, wepropose the study of relatively simple synthetic trees that permitanalysis and afford a greater degree of experimental freedom. Using anovel tree model that does not suffer from the shortcomings ofpreviously studied models, we provide a relatively straightforwardcharacterization of the kinds of games where UCT is superior toMinimax, and vice versa --- to the best of our knowledge, this is thefirst time such an effort has been successful. In particular, we showthat UCT shines in games where heuristics are accurately modeled usingadditive Gaussian noise, that contrary to previous work, earlyterminal states by themselves do not necessarily hurt UCT, and that intrees with heuristic dispersion lag, UCT is outperformed byMinimax."
Interpreting prediction markets: a stochastic approach,"We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution.This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved.In particular, we show that the stationary point of the stochastic process of prices generated by the market is equal to the market's Walrasian equilibrium of classic market analysis.Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour."
MCMC algorithms for near deterministic systems,"Markov Chain Monte Carlo (MCMC) methods are used ubiquitously to generate samples from a probability distribution where exact sampling is not feasible. However, in the presence of near-deterministic components in a joint distribution, it is well-known that mixing is very slow and jumping from one mode to another can take very large amount of time.In this work, we explore one possible fix to this problem by visiting the deterministic problem corresponding to the near-deterministic components. If there exists an efficient algorithm to identify the solutions to this deterministic problem, then we show that it is often possible to design an MCMC algorithm where the proposal distribution is shaped by the efficient algorithm to the deterministic problem."
Generalized Least Squares for Principled Complex Backups in Temporal Difference Learning,"We derive the form of an estimator that uses generalized least squares to obtaina principled form of the eligibility trace. We show that both the $\lambda$-return and $\gamma$-return can be thought of as assuming that the inverse covariance matrix of $n$-step returns has specific values on its diagonal, and is zero elsewhere.  The new weighting scheme has a single parameter that can easily be set from data,closely matches the empirical covariance weights, and performs very well across several settings of $\gamma$ and $\epsilon$. "
Branch-and-Bound Prediction for Large Data,"For the problems requiring instance-based learning, nearest neighbor is one of the most popular learning methods. While a linear search for the nearest neighbor can be improved upon by building an efficient structure such as a kd-tree, nearest neighbor method suffers from several shortcomings. These shortcomings include lack of generalization, issue of overfitting, sensitivity to outliers, and inability to triage inputs appropriately. Assuming we have probabilistic equivalence of kd-tree, this paper describes how efficiently we are able to find the most likely prediction through our branch-and-bound approach. The experiments show that branch-and-bound becomes more effective as tree size grows and works significantly faster in big trees. In order to argue this, we implicitly build a tree containing more than 30 billion training examples, and launch experiments to find the most likely example among those 30 billion subclasses. The results show that the branch-and-bound approach allows searches that are roughly 1.41 times deeper than possible with linear search for large binary trees. Considering that the branch-and-bound approach we employed here is parallelizable, these results open a possibility to develop a new class of machine learning algorithms for large data that delay some parts of training process to prediction time while efficiently solving critical issues such as overfitting."
Module propagation: probabilistic frequent subgraph discovery,"A central task in graph data analysis is to discover subgraphs recurring in a single or multiple graphs. Although many graph mining algorithms have been proposed to identify identical subgraphs recurring in multiple clean graphs, it remains an open problem to discover frequent subgraphs that appear in a {\em single} or multiple {\em noisy} graphs. Solving this problem is critical because most real-world graph data is noisy and many subgraphs are repeated only in a single graph. In this paper, we propose a new approach, Module Propagation, as a principled and practical solution to this open problem. Instead of relying on deterministic search as previous graph mining algorithms do, we reformulate the problem in a probabilistic framework.By maximizing the probability of a new Markov random field model via a fast message passing algorithm, we decompose a single or multiple noisy graphs into recurring {densely connected} and possibly overlapped subgraphs efficiently.The procedure is done efficiently  based on a message passing algorithm that explores the sparsity of our new model.We not only demonstrate the advantage of \mp on synthetic data over alternative graph mining algorithms, but also successfully apply it to a novel application of graph mining --- the discovery of modules in chemically similar crystal structures. This application can pave the way for predicting unknown crystal structures, an important yet challenging task in computational materials science."
Module propagation: probabilistic frequent subgraph discovery,"A central task in graph data analysis is to discover subgraphs recurring in a single or multiple graphs. Although many graph mining algorithms have been proposed to identify identical subgraphs recurring in multiple clean graphs, it remains an open problem to discover frequent subgraphs that appear in a {\em single} or multiple {\em noisy} graphs. Solving this problem is critical because most real-world graph data is noisy and many subgraphs are repeated only in a single graph. In this paper, we propose a new approach, Module Propagation, as a principled and practical solution to this open problem. Instead of relying on deterministic search as previous graph mining algorithms do, we reformulate the problem in a probabilistic framework.By maximizing the probability of a new Markov random field model via a fast message passing algorithm, we decompose a single or multiple noisy graphs into recurring {densely connected} and possibly overlapped subgraphs efficiently.The procedure is done efficiently  based on a message passing algorithm that explores the sparsity of our new model.We not only demonstrate the advantage of \mp on synthetic data over alternative graph mining algorithms, but also successfully apply it to a novel application of graph mining --- the discovery of modules in chemically similar crystal structures. This application can pave the way for predicting unknown crystal structures, an important yet challenging task in computational materials science."
Learning Hierarchical Compositional Models in the Presence of Clutter,"Our goal is to identify hierarchical compositional models from highlycluttered data. The data to learn from are assumed to be imperfect intwo respects. Firstly, large portion of the data is coming frombackground clutter. Secondly, data generated by a recursivecompositional model are subject to random replacements of correctdescendants by randomly chosen ones at every level of the hierarchy.In this paper, we show the limits and capabilities of an approachwhich is based on likelihood maximization. The algorithm makesexplicit probabilistic assignments of individual data to compositionalmodel and background clutter. It uses these assignments to effectivelyfocus on the data coming from the compositional model and iterativelyestimate their compositional structure."
