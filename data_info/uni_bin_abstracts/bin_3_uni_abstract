title,abstract
Statistical Computations Underlying the Dynamics of Memory Updating,"Psychophysical and neurophysiological studies have suggested that the formation of memory traces is sensitive to the temporal structure of the environment. We present a statistical theory of memory formation in a dynamic environment, based on a nonparametric generalization of the switching Kalman filter. We show that this theory can account for existing data on the dynamics of memory updating, as well as the results of a new behavioral experiment. Our behavioral findings suggest that humans use temporal discontinuities in the structure of the environment to determine when to form new memory traces. The statistical perspective provides a coherent account of the conditions under which old memories are modified and new memories are formed."
Statistical Computations Underlying the Dynamics of Memory Updating,"Psychophysical and neurophysiological studies have suggested that the formation of memory traces is sensitive to the temporal structure of the environment. We present a statistical theory of memory formation in a dynamic environment, based on a nonparametric generalization of the switching Kalman filter. We show that this theory can account for existing data on the dynamics of memory updating, as well as the results of a new behavioral experiment. Our behavioral findings suggest that humans use temporal discontinuities in the structure of the environment to determine when to form new memory traces. The statistical perspective provides a coherent account of the conditions under which old memories are modified and new memories are formed."
Statistical Computations Underlying the Dynamics of Memory Updating,"Psychophysical and neurophysiological studies have suggested that the formation of memory traces is sensitive to the temporal structure of the environment. We present a statistical theory of memory formation in a dynamic environment, based on a nonparametric generalization of the switching Kalman filter. We show that this theory can account for existing data on the dynamics of memory updating, as well as the results of a new behavioral experiment. Our behavioral findings suggest that humans use temporal discontinuities in the structure of the environment to determine when to form new memory traces. The statistical perspective provides a coherent account of the conditions under which old memories are modified and new memories are formed."
Statistical Computations Underlying the Dynamics of Memory Updating,"Psychophysical and neurophysiological studies have suggested that the formation of memory traces is sensitive to the temporal structure of the environment. We present a statistical theory of memory formation in a dynamic environment, based on a nonparametric generalization of the switching Kalman filter. We show that this theory can account for existing data on the dynamics of memory updating, as well as the results of a new behavioral experiment. Our behavioral findings suggest that humans use temporal discontinuities in the structure of the environment to determine when to form new memory traces. The statistical perspective provides a coherent account of the conditions under which old memories are modified and new memories are formed."
Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery,"Given pairwise dissimilarities between data points, we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection. We formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming. The solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives. We obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives. When data points are distributed around multiple clusters according to the dissimilarities, we show that the data in each cluster select only representatives from that cluster. Unlike metric-based methods, our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality. We demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text."
Active Learning of Model Evidence Using Bayesian Quadrature ,"Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy."
Active Learning of Model Evidence Using Bayesian Quadrature ,"Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy."
Local Supervised Learning through Space Partitioning,"We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers. We formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error. We train locally linear classifiers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also show improved robustness to label noise."
Local Supervised Learning through Space Partitioning,"We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers. We formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error. We train locally linear classifiers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also show improved robustness to label noise."
Localized Gaussian Process Kernel Combining,"This paper investigates learning to combine multiple kernels in the context of Gaussian Process modeling. The fusing of kernels empowers the learner to take advantage of multiple heterogeneous data sources and views but poses the problem of how to best combine them. Unlike many existing algorithms where kernels are linearly combined at the matrix level, we propose an element-wise kernel combining approach with the former being a special case. The lower-level combining scheme is not limited to more flexible data integration, it also motivates new problem settings. We explore one such setting, rejecting noisy instances of MRI data to improve the learning performance. We propose EM and efficient gradient based optimization methods which make it possible to handle large scale problems. The promising experimental results demonstrate the performance of our model, and validate the effectiveness of element-level kernel combining."
A Constraint Boosting Approach for Matching Problems,"In matching problems, we have two goals: $1$) maximizing the compatibility function, and $2$) satisfying the matching constraints. Since matching constraints, such as one-to-one or many-to-one, fragment the feasible space, the matching problems usually have large numbers of local optima. Existing methods are vulnerable to these local optima and easily get stuck in poor local optima. In this paper, we propose a \textbf{constraint boosting algorithm}, where matching constraints are expressed as a penalty term in the objective function, and the weight of penalty term adaptively increases. When the weight of penalty term is small, the optimization process mainly depends on the compatibility function, and thus approaches regions with large compatible values; when the weight of penalty term is large, the penalty term dominates the optimization process and force it to reach a nearby point satisfying the matching constraints. Empirically, this optimization procedure can escape from poor local optima and finally reach a good optimum. Moreover, we devise dependent optimization processes which utilize the best known optimum to escape from worse optima and reach a better optimum. In this way, an optimal or close-to-optimal solution can be quickly obtained. The experiments on various matching problems clearly demonstrate the superiority of our proposed method."
A Constraint Boosting Approach for Matching Problems,"In matching problems, we have two goals: $1$) maximizing the compatibility function, and $2$) satisfying the matching constraints. Since matching constraints, such as one-to-one or many-to-one, fragment the feasible space, the matching problems usually have large numbers of local optima. Existing methods are vulnerable to these local optima and easily get stuck in poor local optima. In this paper, we propose a \textbf{constraint boosting algorithm}, where matching constraints are expressed as a penalty term in the objective function, and the weight of penalty term adaptively increases. When the weight of penalty term is small, the optimization process mainly depends on the compatibility function, and thus approaches regions with large compatible values; when the weight of penalty term is large, the penalty term dominates the optimization process and force it to reach a nearby point satisfying the matching constraints. Empirically, this optimization procedure can escape from poor local optima and finally reach a good optimum. Moreover, we devise dependent optimization processes which utilize the best known optimum to escape from worse optima and reach a better optimum. In this way, an optimal or close-to-optimal solution can be quickly obtained. The experiments on various matching problems clearly demonstrate the superiority of our proposed method."
Random Utility Theory for Social Choice: Theory and Algorithms,"Random utility theory models an agent's preferences onalternatives by drawing a real-valued score on each alternative(typically independently) from a parameterized distribution, and thenranking according to scores. A special case that has receivedsignificant attention is the Plackett-Luce model, for which fastinference methods for maximum likelihood estimators areavailable. This paper develops conditions on general,random utility models that enable fast inference withina Bayesian framework through MC-EM, providing unimodal log-likelihoodfunctions. Results on both real-world and simulated data provide supportfor the scalability of the approach, despite its flexibility."
Random Utility Theory for Social Choice: Theory and Algorithms,"Random utility theory models an agent's preferences onalternatives by drawing a real-valued score on each alternative(typically independently) from a parameterized distribution, and thenranking according to scores. A special case that has receivedsignificant attention is the Plackett-Luce model, for which fastinference methods for maximum likelihood estimators areavailable. This paper develops conditions on general,random utility models that enable fast inference withina Bayesian framework through MC-EM, providing unimodal log-likelihoodfunctions. Results on both real-world and simulated data provide supportfor the scalability of the approach, despite its flexibility."
Variational Inference in Nonconjugate Models," Mean-field variational inference is a powerful algorithm for approximate posterior inference, but is difficult to derive for nonconjugate probabilistic models. We develop two variational strategies for nonconjugate priors---Laplace variational inference and delta method variational inference---which place minimal conditions on the model. These strategies extend and unify existing methods that were derived for specific models.  We illustrate our approach on the correlated topic models, Bayesian logistic regression, and hierarchical Bayesian logistic regression.  Our experimental results show that our methods work well on real-world datasets."
Variational Inference in Nonconjugate Models," Mean-field variational inference is a powerful algorithm for approximate posterior inference, but is difficult to derive for nonconjugate probabilistic models. We develop two variational strategies for nonconjugate priors---Laplace variational inference and delta method variational inference---which place minimal conditions on the model. These strategies extend and unify existing methods that were derived for specific models.  We illustrate our approach on the correlated topic models, Bayesian logistic regression, and hierarchical Bayesian logistic regression.  Our experimental results show that our methods work well on real-world datasets."
Venue Discovery,Didn't finish writing the abstract yet...
Topical Structural Analysis on Social Communications,"The popularity of online social networks has lowered the barrier of online communications, which results in massive number of users using the networks for interaction and friendship making.To characterize the user positions and message content generated by users of different positions, we propose the Dirichlet Allocation Blockmodels (DABM) for topical structural analysis.DABM model allows each pair of users to generate message content following the topic distribution conditioned on the social positions of the two users.Compared with the earlier model, DABM allows users of the same positions to have some variability in their message topic distribution.We evaluate both DABM and the earlier model on tweets generated by a set of Twitter users connected by follow links, and show that DABM achieves better likelihood and perplexity than the earlier model."
Topical Structural Analysis on Social Communications,"The popularity of online social networks has lowered the barrier of online communications, which results in massive number of users using the networks for interaction and friendship making.To characterize the user positions and message content generated by users of different positions, we propose the Dirichlet Allocation Blockmodels (DABM) for topical structural analysis.DABM model allows each pair of users to generate message content following the topic distribution conditioned on the social positions of the two users.Compared with the earlier model, DABM allows users of the same positions to have some variability in their message topic distribution.We evaluate both DABM and the earlier model on tweets generated by a set of Twitter users connected by follow links, and show that DABM achieves better likelihood and perplexity than the earlier model."
An Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks,"We present an axiomatic construction of hierarchical clustering in asymmetric networks where the dissimilarity from node $a$ to node $b$ is not necessarily equal to the dissimilarity from node $b$ to node $a$. The theory is built on the axioms of value, influence, and transformation. The Axiom of Value says that in a two-node network the nodes cluster at resolution equal to the maximum dissimilarity between them. The Axiom of Influence says that no clusters are formed at resolutions that do not allow bidirectional paths to be formed. The Axiom of Transformation states that if we consider a network and do not increase any pairwise dissimilarity, the level at which two nodes become part of the same cluster is not larger than the level at which they were clustered together in the original network. Two asymmetric hierarchical clustering methods that abide to these axioms are derived. Reciprocal clustering requires clusters to form through arcs that are similar in both directions. Nonreciprocal clustering allows clusters to form through cycles of small dissimilarity. We further show that any clustering method that satisfies the axioms of value, influence, and transformation lies between reciprocal and nonreciprocal clustering in the sense that all other methods cluster two points together at resolutions larger than nonreciprocal clustering and smaller than reciprocal clustering. To conclude, we apply this theory to the formation of circles of trust in social networks."
State-Based Hierarchical Clustering,"Once data points are grouped together into a cluster, standard complete linkage hierarchical agglomerative clustering does not allow them to migrate into separate clusters at higher strata in the hierarchy.  On the other hand, once data points are assigned to separate clusters, standard complete linkage hierarchical divisive clustering does not allow them to recombine within a single cluster at lower strata in the hierarchy.  Further, alternative dendrograms are used to resolve ties between the inter-cluster distances that determine which two clusters (cluster) will be combined (subdivided) next.  These problems make these methods difficult to use where correctness and relatively precise mathematical models are required.  The notion of finding sets of clusters based solely on the distances between the data points, as opposed to inter-cluster distances, is used to design a basic algorithm that overcomes these problems.  The state of a data set and the degrees of the data points as of a variable threshold distance are used to find the sets of clusters.  The algorithm was successfully tested on numerous test patterns and several real world data sets."
Distributed Non-Stochastic Experts,"We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator. At each time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon T, while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal O(\sqrt{log(n)T}) regret bound at the cost of T communication. (ii) No communication: Each site runs an independent copy ? the regret is O(\sqrt{log(n)kT}) and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than \sqrt{kT} and communication better than T. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret O(\sqrt{k^{5(1+\epsilon)/6} T}) and communication O(T/k^\epsilon), for any value of \epsilon in (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off."
Distributed Non-Stochastic Experts,"We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator. At each time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon T, while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal O(\sqrt{log(n)T}) regret bound at the cost of T communication. (ii) No communication: Each site runs an independent copy ? the regret is O(\sqrt{log(n)kT}) and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than \sqrt{kT} and communication better than T. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret O(\sqrt{k^{5(1+\epsilon)/6} T}) and communication O(T/k^\epsilon), for any value of \epsilon in (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off."
Multi-task Vector Field Learning,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach."
Multi-task Vector Field Learning,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach."
Multidimensional Artificial Field Embedding,"Embedding data nonlinearly has generated a surge on techniques that compute low dimensional representations of high dimensional observations. This paper examines and exploits the force field interpretation from mechanics to devise a general nonlinear embedding framework with properties for developing new dimension reduction models. In its simplified nature, the unifying framework yields formulations of several related existing techniques and yet with a fast optimization strategy. As an example, we propose a new dimension reduction model based on intuitive superposition of pair-dependent local functions of attraction and repulsion fields. Experiments on standard data sets suggest that the proposed approach offers models for better dimension reduction and visualization with strong capabilities to preserve local distances compared to other methods. Additional illustrations and a main theoretical result are provided to present new design insights for developing new nonlinear embedding methods."
Multidimensional Artificial Field Embedding,"Embedding data nonlinearly has generated a surge on techniques that compute low dimensional representations of high dimensional observations. This paper examines and exploits the force field interpretation from mechanics to devise a general nonlinear embedding framework with properties for developing new dimension reduction models. In its simplified nature, the unifying framework yields formulations of several related existing techniques and yet with a fast optimization strategy. As an example, we propose a new dimension reduction model based on intuitive superposition of pair-dependent local functions of attraction and repulsion fields. Experiments on standard data sets suggest that the proposed approach offers models for better dimension reduction and visualization with strong capabilities to preserve local distances compared to other methods. Additional illustrations and a main theoretical result are provided to present new design insights for developing new nonlinear embedding methods."
MILEAGE: Multiple Instance LEArning with Global Embedding," Multiple Instance Learning (MIL) methods generally represent each example as a collection of  individual instances, whereastraditional learning methods typically extract a global featurevector for the whole content of each example. Substantial priorresearch work has been proposed to solve  MIL problems. However, MILmethods do not always perform better than traditional learningmethods in all the cases. Limited research work has studied thisissue. This paper proposes a novel framework -- \emph{MultipleInstance LEArning with Global Embedding (MILEAGE)}, in which theglobal feature vectors for traditional learning methods areintegrated into the MIL setting. MILEAGE can leverage the benefitsderived from both learning settings. Within the proposed framework,a large margin method is formulated. In particular,  the proposedmethod adaptively tunes the weights on the two different kinds offeature representations (i.e., global and multiple instance) foreach example and trains the classifier simultaneously. Analternative algorithm is proposed to solve the resultingoptimization problem, which extends the bundle method to thenon-convex case. Some important properties of the proposed method,such as the convergence rate and the generalization error rate, areanalyzed. A series of experiments on both the image and textclassification tasks have been conducted to demonstrate theadvantages of the proposed method over several state-of-the-artmultiple instance and traditional learning methods."
Learning Heteroscedastic Models via SOCP under Group Sparsity,"Sparse estimation methods based on  $\ell_1$  relaxation, such as the Lasso and the Dantzig selector,are powerful tools for estimating high dimensional linear models. However, in order to properly tune these methods, the variance ofthe noise is often required. This constitutes a major obstacle in applying these methods in several frameworks---such astime series, random fields, inverse problems---for which noise is rarely homoscedastic and the noise level is hard to know in advance.In this paper, we propose a new approach to the joint estimation of the conditional mean andthe conditional variance in a high-dimensional (auto-)regression setting. An attractive feature of the proposed estimator isthat it is efficiently computable even for very large scale problems by solving a second-order cone program (SOCP). We present numericalresults assessing the performance of the proposed procedure. We also establish non-asymptotic risk bounds which are nearly asstrong as those for original $\ell_1$-penalized estimators: the Lasso, the Dantzig selector and their grouped counterparts."
Multiclass Active Learning with Hierarchical-Structured Embedded Variance,"  We consider the problem of multiclass active learning where the relationship of the labels are represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to exploit the hierarchical structure of the label tree as well as the characteristics of the training data to select the most informative data for human labeling.  This goal can be achieved by a novel embedding-based approach called hierarchical-structured embedded variance, which learns an embedding of the labels that both preserves the structure of the label tree and reflects the characteristics of the training data. We show that the proposed approach is a generalization of entropy-based and cost-based uncertainty measure. We also demonstrate that notable improvement on the performance can be achieved with the proposed approach on synthetic and benchmark datasets."
Truncation-free Online Variational Inference for Bayesian Nonparametric Models," We present a truncation-free online variational inference algorithm for Bayesian nonparametric models. Unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly. Our experiments for Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms."
Truncation-free Online Variational Inference for Bayesian Nonparametric Models," We present a truncation-free online variational inference algorithm for Bayesian nonparametric models. Unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly. Our experiments for Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms."
ROST: Realtime Online Spatiotemporal Topics for Navigation Summaries and Surprise Detection," We describe a novel online topic modeling framework to compute a low dimension descriptor of visual observations made by a mobile robot, which is sensitive to the structural and thematic changes in the environment. Our approach is designed to run in realtime, and is suitable for long term execution on a robotic platform. Using this image descriptor we build online anytime summaries consisting of surprising observations experienced by a robot thus far. The observations in the summary are chosen such that they cover the set of all observations in topic space, while minimizing the cover radius. Like almost any summarization method, the technique is meant to produce data for human consumption. Thus, we assess our approach on 307 human subjects and compare it to the classic bag-of-words description based summaries, and find it superior."
ROST: Realtime Online Spatiotemporal Topics for Navigation Summaries and Surprise Detection," We describe a novel online topic modeling framework to compute a low dimension descriptor of visual observations made by a mobile robot, which is sensitive to the structural and thematic changes in the environment. Our approach is designed to run in realtime, and is suitable for long term execution on a robotic platform. Using this image descriptor we build online anytime summaries consisting of surprising observations experienced by a robot thus far. The observations in the summary are chosen such that they cover the set of all observations in topic space, while minimizing the cover radius. Like almost any summarization method, the technique is meant to produce data for human consumption. Thus, we assess our approach on 307 human subjects and compare it to the classic bag-of-words description based summaries, and find it superior."
ROST: Realtime Online Spatiotemporal Topics for Navigation Summaries and Surprise Detection," We describe a novel online topic modeling framework to compute a low dimension descriptor of visual observations made by a mobile robot, which is sensitive to the structural and thematic changes in the environment. Our approach is designed to run in realtime, and is suitable for long term execution on a robotic platform. Using this image descriptor we build online anytime summaries consisting of surprising observations experienced by a robot thus far. The observations in the summary are chosen such that they cover the set of all observations in topic space, while minimizing the cover radius. Like almost any summarization method, the technique is meant to produce data for human consumption. Thus, we assess our approach on 307 human subjects and compare it to the classic bag-of-words description based summaries, and find it superior."
On Consistent Classification with Imbalanced Classes,"We consider the problem of imbalanced classes in binary classification, where one class is rare compared to the other. This problem arises frequently in practice and has been widely studied. However very little is understood in terms of the theoretical properties of the problem or of the algorithms proposed: what performance measures are appropriate, how these affect the learning process, and whether the algorithms are statistically consistent with respect to the desired performance measures. In this paper, we initiate a formal study of these issues, focusing on the balanced 0-1 error that evaluates errors on the majority and minority classes separately and effectively balances the two. The underlying balanced 0-1 loss bears similarity to cost-sensitive losses; however a critical difference between the two is that the balanced loss depends on the underlying distribution, while cost-sensitive losses are defined independent of the distribution. We establish statistical consistency of two types of algorithms with respect to the balanced 0-1 error: plug-in rules that use an empirically determined threshold, and certain types of empirically balanced risk minimization algorithms. Our experiments support our theoretical results, showing that both these approaches perform as well as (or better than) under-/over-sampling methods that are currently viewed as the state of the art."
Multiplicative Forests for Continuous-Time Processes,"Learning temporal dependencies between variables over continuous time is an important and challenging task. Continuous-time Bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices, which grows exponentially in the number of parents per variable. We develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits. Using a multiplicative assumption we show how to update the forest likelihood in closed form, producing efficient model updates. Our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability."
Multiplicative Forests for Continuous-Time Processes,"Learning temporal dependencies between variables over continuous time is an important and challenging task. Continuous-time Bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices, which grows exponentially in the number of parents per variable. We develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits. Using a multiplicative assumption we show how to update the forest likelihood in closed form, producing efficient model updates. Our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability."
Multivariate Convex Regression with Adaptive Partitioning,"We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function.  Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is computationally feasible for more than a few hundred observations.  We introduce Convex Adaptive Partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is computationally efficient, in stark contrast to current methods. We show that CAP has a computational complexity of O(n log(n)^2) and also give consistency results. Empirically, CAP shows dramatic improvement over existing methods in terms of runtime and predictive error."
Multivariate Convex Regression with Adaptive Partitioning,"We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function.  Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is computationally feasible for more than a few hundred observations.  We introduce Convex Adaptive Partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is computationally efficient, in stark contrast to current methods. We show that CAP has a computational complexity of O(n log(n)^2) and also give consistency results. Empirically, CAP shows dramatic improvement over existing methods in terms of runtime and predictive error."
Stochastic Gradient Descent with Only One Projection,"Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $O(1/\sqrt{T})$ convergence rate for general convex optimization, and an $O(\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function."
Stochastic Gradient Descent with Only One Projection,"Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $O(1/\sqrt{T})$ convergence rate for general convex optimization, and an $O(\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function."
Stochastic Gradient Descent with Only One Projection,"Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $O(1/\sqrt{T})$ convergence rate for general convex optimization, and an $O(\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function."
Probabilistic Topic Coding for Superset Label Learning,"In the superset label learning problem, each training instance provides a set of candidate labels of which one is the true label of the instance.  Most approaches learn a discriminative classifier that tries to minimize an upper bound of the unobserved 0/1 loss. In this work, we propose a probabilistic model, Probabilistic Topic Coding (PTC), for the superset label learning problem. The PTC model is derived from logistic stick breaking process. It first maps the data to ``topics'', and then assigns to each topic a label drawn from a multinomial distribution.  The layer of topics can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art.  The discovered underlying structures also provide improved explanations of the classification predictions."
Weighted Online Learning,"We consider an unconventional online learning problem which we  call Weighted Online Learning (WOL), where each training example has associated with it a (non-uniform) non-negative weight. WOL problems occur in many real life applications including banking business, medical diagnosis and visual tracking, where different samples are of differing value to the learning process. We propose several algorithms for WOL and show these algorithms have similar regret bounds and convergence rates to Pegasos. Applications in bank credit estimation, medical diagnosis and visual tracking show a significant improvement over state-of-the-art methods using traditional online learning."
Efficient deep learning through novel sparsity constraints and heterogeneous maps,"Deep convolutional neural networks (CNNs) have been successfully applied to a range of tasks spanning object recognition to sparse basis transformation. However, these very high dimensional networks are computationally expensive and have not realized their potential for real time applications. How can we minimize run time and yet retain state of the art results? To answer this question, we introduce two key ideas not yet fully explored for CNNs: (i) True Sparsity and (ii) Heterogeneous Feature Mappings. True sparsity comes to us from biological networks where neurons have a binary activation scheme. Although in machine learning a penalty norm is usually used to impose network sparsity, we achieve sparsity by modifying the activation function for a discrete response and introduce a 'burn-in' optimization scheme. Heterogeneous feature mappings are inspired by mixed selectivity -- a recent neuroscience term for diverse neuronal behavior. These two ideas lead to deep sparse networks that are more computationally feasible for high dimensional classification and extend naturally to unsupervised basis extraction. We achieve very competitive results on benchmark datasets with a speed-up factor over conventional methods. "
Fast Multiple Kernel Learning With Multiplicative Weight Updates,"In this work, we propose a fast algorithm for multiple kernel learning (MKL). Our proposed approach builds on the original QCQP formulation of Lanckriet et al. It uses a multiplicative weight update based approximation for the underlying SDP, exploiting a careful reformulation of the MKL problem as well as a novel fast matrix exponentiation routine for QCQP constraints that might be of independent interest. Our method avoids the use of commercial nonlinear solvers, and scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with an uniform unweighted combination of kernels."
Fast Multiple Kernel Learning With Multiplicative Weight Updates,"In this work, we propose a fast algorithm for multiple kernel learning (MKL). Our proposed approach builds on the original QCQP formulation of Lanckriet et al. It uses a multiplicative weight update based approximation for the underlying SDP, exploiting a careful reformulation of the MKL problem as well as a novel fast matrix exponentiation routine for QCQP constraints that might be of independent interest. Our method avoids the use of commercial nonlinear solvers, and scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with an uniform unweighted combination of kernels."
Fast Multiple Kernel Learning With Multiplicative Weight Updates,"In this work, we propose a fast algorithm for multiple kernel learning (MKL). Our proposed approach builds on the original QCQP formulation of Lanckriet et al. It uses a multiplicative weight update based approximation for the underlying SDP, exploiting a careful reformulation of the MKL problem as well as a novel fast matrix exponentiation routine for QCQP constraints that might be of independent interest. Our method avoids the use of commercial nonlinear solvers, and scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with an uniform unweighted combination of kernels."
Fast Multiple Kernel Learning With Multiplicative Weight Updates,"In this work, we propose a fast algorithm for multiple kernel learning (MKL). Our proposed approach builds on the original QCQP formulation of Lanckriet et al. It uses a multiplicative weight update based approximation for the underlying SDP, exploiting a careful reformulation of the MKL problem as well as a novel fast matrix exponentiation routine for QCQP constraints that might be of independent interest. Our method avoids the use of commercial nonlinear solvers, and scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with an uniform unweighted combination of kernels."
Semi-Supervised Domain Adaptation with Non-parametric Copulas,"A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model across different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques."
Identification of Recurrent Patterns in the Activation of Brain Networks,"Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks."
Identification of Recurrent Patterns in the Activation of Brain Networks,"Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks."
Multiresolution Gaussian Processes,"We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity."
Search-Based Understanding of Hierarchical Dirichlet Process Topic Models,"Bayesian nonparametric models are widely used to allow unsupervised learning of statistical model structure from data.  Conventional learning algorithms, based on Gibbs sampling or variational Bayesian approximations, can be undesirably prone to local optima and sensitive to initialization.  We explore this issue in the context of the hierarchical Dirichlet process, which with combined with Dirichlet-multinomial likelihoods provides a nonparametric topic model.  We present a more efficient learning algorithm based on the Maximization-Expectation (ME) algorithm, based on a novel combinatorial formulation of the HDP marginal likelihood and implemented with carefully crafted search moves.  Experiments on synthetic data with statistics similar to real text corpora, and four real document collections, show consistent improvement over sampling algorithms in both recovery of the true number of topics, and predictive likelihood of test data."
Newton-Like Methods for Sparse Inverse Covariance Estimation,"We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimationproblem. Comparisons with the method implemented in the QUIC software package are presented."
Newton-Like Methods for Sparse Inverse Covariance Estimation,"We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimationproblem. Comparisons with the method implemented in the QUIC software package are presented."
Topology Constraints in Graphical Models,"Graphical models are a very useful tool to describe and understand natural phenomena, from gene expression to climate change and social interactions. The topological structure of these graphs/networks is a fundamental part of the analysis, and in many cases the main goal of the study. However, little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data.In this work we propose extensions to the basic joint regression model for network estimation, which explicitly incorporate graph-topological constraints into the corresponding optimization approach. The first proposed extension includes an eigenvector centrality constraint, thereby promoting this important prior topological property. The second developed extension promotes the formation of certain motifs, triangle-shaped ones in particular, which are known to exist for example in genetic regulatory networks. The presentation of the underlying formulations, which serve as examples of the introduction of topological constraints in network estimation, is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge."
Cosegmentation with Subspace Constraints ? Identifying Shared Structure in Related Image Sets,"We develop new algorithms to analyze and exploit the joint subspace structure of a set of related images to facilitate the process of concurrent segmentation of a large set of images. Most existing approaches for this problem are either limited to extracting a single similar object across the given image set or do not scale wellto a large number of images containing multiple objects varying at different scales. One of the goals of this paper is to show that various desirable properties of such an algorithm (ability to handle multiple images with multiple objects showing arbitary scale variations) can be cast elegantly using simple constructs from linearalgebra: this signi?cantly extends the operating range of such methods. While intuitive, this formulation leads to a hard optimization problem where one must perform the image segmentation task together with appropriate constraints which enforce desired algebraic regularity (e.g., common subspace structure). We propose ef?cient iterative algorithms (with small computational requirements) whose key steps reduce to objective functions solvable by max-?ow and/or nearly closed form identities. We study the qualitative, theoretical, and empirical properties of the method,and present results on benchmark datasets."
Cosegmentation with Subspace Constraints ? Identifying Shared Structure in Related Image Sets,"We develop new algorithms to analyze and exploit the joint subspace structure of a set of related images to facilitate the process of concurrent segmentation of a large set of images. Most existing approaches for this problem are either limited to extracting a single similar object across the given image set or do not scale wellto a large number of images containing multiple objects varying at different scales. One of the goals of this paper is to show that various desirable properties of such an algorithm (ability to handle multiple images with multiple objects showing arbitary scale variations) can be cast elegantly using simple constructs from linearalgebra: this signi?cantly extends the operating range of such methods. While intuitive, this formulation leads to a hard optimization problem where one must perform the image segmentation task together with appropriate constraints which enforce desired algebraic regularity (e.g., common subspace structure). We propose ef?cient iterative algorithms (with small computational requirements) whose key steps reduce to objective functions solvable by max-?ow and/or nearly closed form identities. We study the qualitative, theoretical, and empirical properties of the method,and present results on benchmark datasets."
Cosegmentation with Subspace Constraints ? Identifying Shared Structure in Related Image Sets,"We develop new algorithms to analyze and exploit the joint subspace structure of a set of related images to facilitate the process of concurrent segmentation of a large set of images. Most existing approaches for this problem are either limited to extracting a single similar object across the given image set or do not scale wellto a large number of images containing multiple objects varying at different scales. One of the goals of this paper is to show that various desirable properties of such an algorithm (ability to handle multiple images with multiple objects showing arbitary scale variations) can be cast elegantly using simple constructs from linearalgebra: this signi?cantly extends the operating range of such methods. While intuitive, this formulation leads to a hard optimization problem where one must perform the image segmentation task together with appropriate constraints which enforce desired algebraic regularity (e.g., common subspace structure). We propose ef?cient iterative algorithms (with small computational requirements) whose key steps reduce to objective functions solvable by max-?ow and/or nearly closed form identities. We study the qualitative, theoretical, and empirical properties of the method,and present results on benchmark datasets."
Multiview Spectral Clustering via Pareto Optimization,"Traditionally, the input of spectral clustering is limited to single-view data. However, many real-world datasets come with multiple heterogeneous feature sets, which provide multiple views of the same data. Such datasets include scientific data (fMRI scans of different individuals), social data (different types of connections between people), web data (multi-type data), and so on. How to optimally combine knowledge from multiple views to help spectral clustering find a better partition remains a developing area. Previous work formulates the problem as a single objective function to optimize, typically by combining the views under a compatibility assumption and requiring the users to decide the importance of each view a priori. In this work, we propose a multi-objective formulation and show how to solve it using Pareto optimization. The Pareto frontier captures all possible good cuts without requiring the users to set the ``correct'' parameter. The effectiveness of our approach is justified by both theoretical analysis and empirical results on benchmark datasets."
Efficient Sparse Group Feature Selection via Nonconvex Optimization,"Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2)statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance."
Efficient Sparse Group Feature Selection via Nonconvex Optimization,"Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2)statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance."
Efficient Sparse Group Feature Selection via Nonconvex Optimization,"Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2)statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance."
A scalable direct formulation for multi-class boosting,"We present a scalable and effective classification model for multi-class boosting classification. In [15], a direct formulation for multi-class boosting was introduced. Unlike many existing multi-class boosting algorithms, which rely on output coding matrices, the approach in [15] directly maximizes the multi-class margin. The major problem of [15] is its extremely high computational complexity, which hampers its application on real-world problems. In this work, we propose a scalable and simple stage-wise boosting method, which also directly maximizes the multi-class margin. Our approach has several advantages: (1) it is simple and computationally efficient to train. The approach can speed up the training time by more than two orders of magnitude without sacrificing the classification accuracy. (2) Like traditional AdaBoost, it is parameter free and empirically demonstrates excellent generalization performance. In contrast, one has to tune the regularization parameter for the multi-class boosting of [15]. Experimental results on challenging multi-class machine learning and vision tasks demonstrate that the proposed approach substantially improves the convergence rate and accuracy of the final visual detector at no additional computational cost."
A scalable direct formulation for multi-class boosting,"We present a scalable and effective classification model for multi-class boosting classification. In [15], a direct formulation for multi-class boosting was introduced. Unlike many existing multi-class boosting algorithms, which rely on output coding matrices, the approach in [15] directly maximizes the multi-class margin. The major problem of [15] is its extremely high computational complexity, which hampers its application on real-world problems. In this work, we propose a scalable and simple stage-wise boosting method, which also directly maximizes the multi-class margin. Our approach has several advantages: (1) it is simple and computationally efficient to train. The approach can speed up the training time by more than two orders of magnitude without sacrificing the classification accuracy. (2) Like traditional AdaBoost, it is parameter free and empirically demonstrates excellent generalization performance. In contrast, one has to tune the regularization parameter for the multi-class boosting of [15]. Experimental results on challenging multi-class machine learning and vision tasks demonstrate that the proposed approach substantially improves the convergence rate and accuracy of the final visual detector at no additional computational cost."
Regularized Off-Policy TD-Learning,"We present a novel $l_1$ regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization.A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm."
Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes,"To learn reliable rules that can generalize to novel situations, the brain must be capable of imposing some form of regularization. Here we suggest, through theoretical and computational arguments, that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system. The functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise. Noise on the inputs is shown to impose regularization, and when synchronization upstream induces time-varying correlations across noise variables, the degree of regularization can be calibrated over time. The resulting qualitative behavior matches experimental data from visual cortex."
Parameter Learning for Submodular Quadratic Pseudo-Boolean Functions,"Submodular quadratic pseudo-Boolean functions play an important role in many inference tasks, particularly in computer vision. Their importance in recent years has grown due to their modeling capability and the development of very fast inference techniques based on max-flow/min-cut algorithms. However, learning the parameters of these models remains a challenge. In this work, we present a simple and efficient algorithm for learning the parameters of a pseudo-Boolean function from training data. Our method directly adjusts edge capacities in the max-flow/min-cut graph so as to minimize the discrepancy between the minimum cut in the graph and the cut induced by the ground truth assignment. We show how our algorithm relates to the subgradient method for structured support vector machines and perform extensive experiments to evaluate variants of our approach."
Parameter Learning for Submodular Quadratic Pseudo-Boolean Functions,"Submodular quadratic pseudo-Boolean functions play an important role in many inference tasks, particularly in computer vision. Their importance in recent years has grown due to their modeling capability and the development of very fast inference techniques based on max-flow/min-cut algorithms. However, learning the parameters of these models remains a challenge. In this work, we present a simple and efficient algorithm for learning the parameters of a pseudo-Boolean function from training data. Our method directly adjusts edge capacities in the max-flow/min-cut graph so as to minimize the discrepancy between the minimum cut in the graph and the cut induced by the ground truth assignment. We show how our algorithm relates to the subgradient method for structured support vector machines and perform extensive experiments to evaluate variants of our approach."
Fast Convolutional Sparse Coding,"Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many signals (e.g. visual and acoustic) however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimisation problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and augmented Lagrange multipliers (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence. "
The Smoothness of The Stationary Distribution of Linear Predictive State Representations,"In this paper we consider linear predictive state representations (PSR), a modeling framework that has been proposed as an alternative to finite state partially observable Markov decision processes (POMDP), which is also capable of representing a strictly larger class of systems. Our main result is the proof of smoothness of the stationary distribution of a linear PSR with respect to the parameters of any finite memory policy. This property is important in cases when it is difficult to model the system accurately, while estimating expectations of different quantities from data is easy. Such result suggests that these estimates are robust with respect to slight changes in a policy. This result can also be seen as the first step in developing a perturbation theory for linear PSRs.In addition, we propose a new representation for the reward process, termed (linear) predictive-state reward process (PRP), which naturally extends a well known (linear) predictive state representations to capture a possibly continuous reward signal. Linear PRPs are, by design, suited for planning approaches that optimize average reward criterion. We then show that for a certain class of policies the average reward in linear PRPs is a smooth function of policy parameters. The result suggests that in fact the same should hold for any finite memory policies, remaining a subject of future work."
The Smoothness of The Stationary Distribution of Linear Predictive State Representations,"In this paper we consider linear predictive state representations (PSR), a modeling framework that has been proposed as an alternative to finite state partially observable Markov decision processes (POMDP), which is also capable of representing a strictly larger class of systems. Our main result is the proof of smoothness of the stationary distribution of a linear PSR with respect to the parameters of any finite memory policy. This property is important in cases when it is difficult to model the system accurately, while estimating expectations of different quantities from data is easy. Such result suggests that these estimates are robust with respect to slight changes in a policy. This result can also be seen as the first step in developing a perturbation theory for linear PSRs.In addition, we propose a new representation for the reward process, termed (linear) predictive-state reward process (PRP), which naturally extends a well known (linear) predictive state representations to capture a possibly continuous reward signal. Linear PRPs are, by design, suited for planning approaches that optimize average reward criterion. We then show that for a certain class of policies the average reward in linear PRPs is a smooth function of policy parameters. The result suggests that in fact the same should hold for any finite memory policies, remaining a subject of future work."
Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states."
Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states."
Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states."
Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states."
Bayesian Conditional Tensor Factorizations for High-Dimensional Classification,"In many application areas, data are collected on a categorical response and highdimensional categorical features, with the goals being to build a parsimoniousmodel for classification while doing inferences on the important features. By using a carefully-structured Tucker factorization, we define a model that can characterizeany classification function, while facilitating variable selection and modeling of higher-order interactions. Following a Bayesian approach, we propose a Markov chain Monte Carlo algorithm for posterior computation accommodating uncertainty in the features to be included. Under near sparsity assumptions, the posterior distribution for the classification function is shown to achieve close tothe parametric rate of contraction even in ultra high-dimensional settings in which the number of candidate features increases exponentially with sample size. Themethods are illustrated through several applications."
Learning Grouped Parameters in Undirected Graphical Models,"In large-scale applications of undirected graphical models, similar kinds of relations can occur frequently and give rise to repeated occurrences of similar parameters. Therefore, it is often beneficial to group the parameters for more effective learning. In cases when the grouping is unknown, allowing the parameter learner to automatically identify these groups can lead to a more accurate estimate and a better understanding of dependence among the variables. In this paper, we provide such a method to learn groups during parameter learning. Specifically, we place a Dirichlet process prior on the parameters in the graphical models, which can avoid the model selection problem, and is useful when we do not know the number of groups in advance. We solve the posterior inference problem with a Gibbs sampling algorithm integrated with classical parameter learning methods for undirected graphical models. Our parameter learning method does not only estimate parameters for undirected graphical models, but can also identify the possible latent groups among the parameters. We evaluate our grouping-aware parameter learning method and the classical grouping-blind parameter learning method on different undirected graphical models, and our method significantly outperforms the grouping-blind method when there indeed exist groups. When there are no groups among the parameters in the ground truth, our method does not lose much estimation accuracy. "
Learning Grouped Parameters in Undirected Graphical Models,"In large-scale applications of undirected graphical models, similar kinds of relations can occur frequently and give rise to repeated occurrences of similar parameters. Therefore, it is often beneficial to group the parameters for more effective learning. In cases when the grouping is unknown, allowing the parameter learner to automatically identify these groups can lead to a more accurate estimate and a better understanding of dependence among the variables. In this paper, we provide such a method to learn groups during parameter learning. Specifically, we place a Dirichlet process prior on the parameters in the graphical models, which can avoid the model selection problem, and is useful when we do not know the number of groups in advance. We solve the posterior inference problem with a Gibbs sampling algorithm integrated with classical parameter learning methods for undirected graphical models. Our parameter learning method does not only estimate parameters for undirected graphical models, but can also identify the possible latent groups among the parameters. We evaluate our grouping-aware parameter learning method and the classical grouping-blind parameter learning method on different undirected graphical models, and our method significantly outperforms the grouping-blind method when there indeed exist groups. When there are no groups among the parameters in the ground truth, our method does not lose much estimation accuracy. "
Indexed Optimization: Learning Ramp-Loss SVM in Sublinear Time,"Multidimensional indexing has been frequently used for sublinear-time nearest neighbor search in various applications. In this paper, we demonstrate how this technique can be integrated into learning problem with sublinear sparsity like ramp-loss SVM. We propose an outlier-free convex-relaxation for ramp-loss SVM and an indexed optimization algorithm which solves large-scale problem in sublinear-time even when data cannot fit into memory. We compare our algorithm with state-of-the-art linear hinge-loss solver and ramp-loss solver in both sufficient and limited memory conditions, where our algorithm not only learns several times faster but achieves more accurate result on noisy and large-scale datasets."
Random function priors for exchangeable graphs and arrays,"A fundamental problem in the analysis of relational data---graphs, matrices or higher-dimensional arrays---is to extract a summary of the common structure underlying relations between individual entities. A successful approach is latent variable modeling, which summarizes this structure as an embedding into a suitable latent space. Results in probability theory, due to Aldous, Hoover and Kallenberg, show that relational data satisfying an exchangeability property can be represented in terms of a random measurable function. In a Bayesian model, this function constitutes the natural model parameter, and we discuss how available latent variable models can be classified according to how they implicitly approximate this parameter. We obtain a flexible yet simple model for relational data by representing the  parameter function as a Gaussian process. Efficient inference draws on the large available arsenal of Gaussian process algorithms; sparse approximations prove particularly useful. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases."
A Bayesian Boosting Model,"We offer a novel view of AdaBoost in a statistical setting. We propose a Bayesian model for binary classification in which label noise is modeled hierarchically. Using variational inference to optimize a dynamic evidence lower bound, we derive a new boosting-like algorithm called VIBoost. We show its close connections to AdaBoost and give experimental results from four datasets."
A Bayesian Boosting Model,"We offer a novel view of AdaBoost in a statistical setting. We propose a Bayesian model for binary classification in which label noise is modeled hierarchically. Using variational inference to optimize a dynamic evidence lower bound, we derive a new boosting-like algorithm called VIBoost. We show its close connections to AdaBoost and give experimental results from four datasets."
A Bayesian Boosting Model,"We offer a novel view of AdaBoost in a statistical setting. We propose a Bayesian model for binary classification in which label noise is modeled hierarchically. Using variational inference to optimize a dynamic evidence lower bound, we derive a new boosting-like algorithm called VIBoost. We show its close connections to AdaBoost and give experimental results from four datasets."
A Bayesian Boosting Model,"We offer a novel view of AdaBoost in a statistical setting. We propose a Bayesian model for binary classification in which label noise is modeled hierarchically. Using variational inference to optimize a dynamic evidence lower bound, we derive a new boosting-like algorithm called VIBoost. We show its close connections to AdaBoost and give experimental results from four datasets."
Unifying Common Multi-Task Multi-Kernel Learning Problems,"Over the past few years, Multi-Kernel Learning (MKL) has received significant attention among data-driven feature selection techniques. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including Multi-Task Learning (MTL). Soving different Multi-Task Multi-Kernel Learning (Multi-Task MKL) formulations usually involves designing algorithms that are tailored to the problem at hand, which is a non-trivial accomplishment. In this paper we present a general Multi-Task MKL framework that subsumes well-known Multi-Task MKL formulations, as well as several important MKL approaches on single-task problems. We then derive a simple algorithm that can solve the unifying framework. To furthermore underline the generality of the proposed framework, we formulate a new learning problem, namely Partially-Shared Common Space (PSCS) Multi-Task MKL, and demonstrate its merits through experimentation."
Bayesian Nonparametric Image  Super-resolution,"Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data.  We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior.  However, this algorithm is not feasible for large-scale data.  To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries  in a fraction of the time needed by the Gibbs sampler. "
Bayesian Nonparametric Image  Super-resolution,"Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data.  We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior.  However, this algorithm is not feasible for large-scale data.  To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries  in a fraction of the time needed by the Gibbs sampler. "
Bayesian Nonparametric Image  Super-resolution,"Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data.  We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior.  However, this algorithm is not feasible for large-scale data.  To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries  in a fraction of the time needed by the Gibbs sampler. "
Bayesian Nonparametric Image  Super-resolution,"Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data.  We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior.  However, this algorithm is not feasible for large-scale data.  To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries  in a fraction of the time needed by the Gibbs sampler. "
Learning Robust Low-Rank Representations,"In this paper we present a comprehensive framework for learning robustlow-rank representations by combining and extending recent ideas forlearning fast sparse coding regressors with structured non-convexoptimization techniques. This approach connects robust principalcomponent analysis (RPCA) with dictionary learning techniques andallows its approximation via trainable encoders. We propose anefficient feed-forward architecture derived from an optimizationalgorithm designed to exactly solve robust low dimensionalprojections. This architecture, in combination with different trainingobjective functions, allows the regressors to be used as onlineapproximants of the exact offline RPCA problem or as RPCA-based neuralnetworks. Simple modifications of these encoders can handlechallenging extensions, such as the inclusion of geometric data transformations.We present several examples with real data from image, audio, and videoprocessing. When used to approximate RPCA, our basic implementationshows several orders of magnitude speedup compared to the exactsolvers with almost no performance degradation. We show the strengthof the inclusion of learning to the RPCA approach on a music source separationapplication, where the encoders outperform the exact RPCA algorithms,which are already reported to produce state-of-the-art results on a benchmarkdatabase. Our preliminary implementation on an iPad showsfaster-than-real-time performancewith minimal latency."
Learning Robust Low-Rank Representations,"In this paper we present a comprehensive framework for learning robustlow-rank representations by combining and extending recent ideas forlearning fast sparse coding regressors with structured non-convexoptimization techniques. This approach connects robust principalcomponent analysis (RPCA) with dictionary learning techniques andallows its approximation via trainable encoders. We propose anefficient feed-forward architecture derived from an optimizationalgorithm designed to exactly solve robust low dimensionalprojections. This architecture, in combination with different trainingobjective functions, allows the regressors to be used as onlineapproximants of the exact offline RPCA problem or as RPCA-based neuralnetworks. Simple modifications of these encoders can handlechallenging extensions, such as the inclusion of geometric data transformations.We present several examples with real data from image, audio, and videoprocessing. When used to approximate RPCA, our basic implementationshows several orders of magnitude speedup compared to the exactsolvers with almost no performance degradation. We show the strengthof the inclusion of learning to the RPCA approach on a music source separationapplication, where the encoders outperform the exact RPCA algorithms,which are already reported to produce state-of-the-art results on a benchmarkdatabase. Our preliminary implementation on an iPad showsfaster-than-real-time performancewith minimal latency."
Sampling based approximation schemes for Normalized Cuts,"The Normalized Cuts (NCut) objective seeks to partition a graph into roughly balanced clusters, and forms the cornerstone of a wide variety of applications in computer vision and machine learning. Finding the optimal normalized cut is NP-hard, and only few results are known in terms of approximation guarantees for various solution strategies. Relaxations like spectral clustering perform quite well in practice but are difficult to analyze in terms of approximation ratios. This paper provides a sampling based approximation scheme for the Normalized Cuts problem.  We derive a polynomial-time algorithm which yields an approximation ratio of $(1+\epsilon)$ with constant probability."
Sampling based approximation schemes for Normalized Cuts,"The Normalized Cuts (NCut) objective seeks to partition a graph into roughly balanced clusters, and forms the cornerstone of a wide variety of applications in computer vision and machine learning. Finding the optimal normalized cut is NP-hard, and only few results are known in terms of approximation guarantees for various solution strategies. Relaxations like spectral clustering perform quite well in practice but are difficult to analyze in terms of approximation ratios. This paper provides a sampling based approximation scheme for the Normalized Cuts problem.  We derive a polynomial-time algorithm which yields an approximation ratio of $(1+\epsilon)$ with constant probability."
Nonparametric Bayesian Clustering via Infinite Warped Mixture Models,"We introduce a flexible class of mixture models for clustering and density estimation. Our model allows clustering of non-linearly-separable data, produces a potentially low-dimensional latent representation, automatically infers the number of clusters, and produces a density estimate. Our approach makes use of two tools from Bayesian nonparametrics: a Dirichlet process mixture model to allow an unbounded number of clusters, and a Gaussian process warping function to allow each cluster to have a complex shape. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, and performs much better than infinite Gaussian mixture models at discovering meaningful clusters."
Real-time online denoising and speaker identification by learning low-rank non-negative sparse models,"In this paper we present a new framework for real time speech denoising under non-stationary noise. We first propose a regularized version of nonnnegative matrix factorization (NMF) for modeling time-frequency representations of speech signals in which the spectral frames are decomposed as sparse linear combinations of atoms of an undercomplete dictionary. This regularization minimizes an upper-bound of the nuclear norm of the reconstruction. The proposed model outperforms standard NMF in speech denoising experiments and reduces the sensitivity of the obtained results with respect to the size of the dictionary. The main contribution consists of combining this model with recent developments in fast regressors for approximating sparse codes, to produce efficient feed-forward architectures that approximate the output of the exact algorithms with low latency and a fraction of the complexity. Incorporating dictionary update and elements of discriminative learning makes the proposed architecture full-featured low-rank non-negative sparse models, significantly outperforming exact NMF algorithms. We present several experiments in speech denoising and speaker identification in the presence of non-stationary noise that show successful results and the potential of the framework."
Real-time online denoising and speaker identification by learning low-rank non-negative sparse models,"In this paper we present a new framework for real time speech denoising under non-stationary noise. We first propose a regularized version of nonnnegative matrix factorization (NMF) for modeling time-frequency representations of speech signals in which the spectral frames are decomposed as sparse linear combinations of atoms of an undercomplete dictionary. This regularization minimizes an upper-bound of the nuclear norm of the reconstruction. The proposed model outperforms standard NMF in speech denoising experiments and reduces the sensitivity of the obtained results with respect to the size of the dictionary. The main contribution consists of combining this model with recent developments in fast regressors for approximating sparse codes, to produce efficient feed-forward architectures that approximate the output of the exact algorithms with low latency and a fraction of the complexity. Incorporating dictionary update and elements of discriminative learning makes the proposed architecture full-featured low-rank non-negative sparse models, significantly outperforming exact NMF algorithms. We present several experiments in speech denoising and speaker identification in the presence of non-stationary noise that show successful results and the potential of the framework."
A Bayesian Approach for Policy Learning from Trajectory Preference Queries,"We consider the problem of learning control policies via trajectory preference queries to an expert. In particular,the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates thepreferred trajectory. The agent's goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problemwe propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries.Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and thatactive query selection can be substantially more efficient than random selection."
A Bayesian Approach for Policy Learning from Trajectory Preference Queries,"We consider the problem of learning control policies via trajectory preference queries to an expert. In particular,the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates thepreferred trajectory. The agent's goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problemwe propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries.Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and thatactive query selection can be substantially more efficient than random selection."
Efficient Dimensionality Reduction for  Canonical Correlation Analysis,"We present the first sub-cubic time algorithm for Canonical Correlation Analysis. Given a pair of tall-and-thin matrices, our algorithm first employs a randomized dimensionality reduction transform to reduce the size of the input matrices, and then applies a standard SVD-based method to compute the canonical correlations. We prove that our algorithm computes an accurate approximation to the canonical correlations with high probability, and with asymptotic running times much better than the standard algorithm. We also show that our algorithm beats the standard algorithm in practice by 30-40% even on fairly small matrices."
Dictionary Training with Side Information,"Recently, learning with side information, which incorporates information contained in the training data but not available in the testing phase to guide the learning process, attracts great attention in machine learning field. In this work, we propose a discriminative dictionary learning method that involves side information. In particular, we introduce a new soft constraint derived from side information and combine it with the reconstruction error and the classification error to form a unified objective function. The optimal solution to the objective function is efficiently obtained using the K-SVD algorithm. Our algorithm learns the dictionary and an optimal linear classifier jointly. We apply the proposed method to two pattern recognition problems, namely low resolution expression recognition and face recognition, where it demonstrates the effectiveness of the proposed method in classification performance."
Correlations strike back (again): the case of associative memory retrieval,"It has long been recognised that statistical dependencies in neuronalactivity need to be taken into account when decoding stimuli encoded ina neural population. Less studied, though equally pernicious, is theneed to take account of dependencies between synaptic weights whendecoding stimuli previously encoded in an auto-associative memory. Weshow that activity-dependent learning generically produces suchcorrelations, and failing to take them into account in the dynamics ofmemory retrieval leads to catastrophically poor recall. We deriveoptimal network dynamics for recall in the face of synaptic correlationscaused by a range of synaptic plasticity rules. These dynamics involvewell-studied circuit motifs, such as forms of feedback inhibition andexperimentally observed dendritic nonlinearities. We therefore show howassuaging an old enemy leads to a novel functional account of keybiophysical features of the neural substrate."
Efficient Inference for Robust GP Regression on Highly Contaminated Data,"We consider robust non-parametric regression on data where the scale of the outlier errors is larger than the amplitude of the oscillation of the inlier manifold. This phenomenon appears frequently in warp estimation for scattered data interpolation, where outliers are incorrect data assignments, and the scale of the outlier errors is usually larger than the warping effects of the underlying interpolant. In this context, we propose a fast inference algorithm for robust Gaussian Process regression which takes advantage of the above condition. Our idea is to putatively fit a simple parametric model to capture the global trend of the inlier distribution \emph{relative} to the outliers, and then guide the Gaussian Process inference using statistics collected from the fitted parametric model. Compared to standard inference algorithms, this combination of parametric and non-parametric techniques achieves significantly higher efficiency with less tendency to converge to local minima."
Efficient Inference for Robust GP Regression on Highly Contaminated Data,"We consider robust non-parametric regression on data where the scale of the outlier errors is larger than the amplitude of the oscillation of the inlier manifold. This phenomenon appears frequently in warp estimation for scattered data interpolation, where outliers are incorrect data assignments, and the scale of the outlier errors is usually larger than the warping effects of the underlying interpolant. In this context, we propose a fast inference algorithm for robust Gaussian Process regression which takes advantage of the above condition. Our idea is to putatively fit a simple parametric model to capture the global trend of the inlier distribution \emph{relative} to the outliers, and then guide the Gaussian Process inference using statistics collected from the fitted parametric model. Compared to standard inference algorithms, this combination of parametric and non-parametric techniques achieves significantly higher efficiency with less tendency to converge to local minima."
Efficient Inference for Robust GP Regression on Highly Contaminated Data,"We consider robust non-parametric regression on data where the scale of the outlier errors is larger than the amplitude of the oscillation of the inlier manifold. This phenomenon appears frequently in warp estimation for scattered data interpolation, where outliers are incorrect data assignments, and the scale of the outlier errors is usually larger than the warping effects of the underlying interpolant. In this context, we propose a fast inference algorithm for robust Gaussian Process regression which takes advantage of the above condition. Our idea is to putatively fit a simple parametric model to capture the global trend of the inlier distribution \emph{relative} to the outliers, and then guide the Gaussian Process inference using statistics collected from the fitted parametric model. Compared to standard inference algorithms, this combination of parametric and non-parametric techniques achieves significantly higher efficiency with less tendency to converge to local minima."
Efficient Inference for Robust GP Regression on Highly Contaminated Data,"We consider robust non-parametric regression on data where the scale of the outlier errors is larger than the amplitude of the oscillation of the inlier manifold. This phenomenon appears frequently in warp estimation for scattered data interpolation, where outliers are incorrect data assignments, and the scale of the outlier errors is usually larger than the warping effects of the underlying interpolant. In this context, we propose a fast inference algorithm for robust Gaussian Process regression which takes advantage of the above condition. Our idea is to putatively fit a simple parametric model to capture the global trend of the inlier distribution \emph{relative} to the outliers, and then guide the Gaussian Process inference using statistics collected from the fitted parametric model. Compared to standard inference algorithms, this combination of parametric and non-parametric techniques achieves significantly higher efficiency with less tendency to converge to local minima."
Generalization Bounds for Domain Adaptation,"In this paper, we provide a new framework to study the generalization bound of thelearning process for domain adaptation. Without loss of generality, we considertwo kinds of representative domain adaptation settings: one is domain adaptationwith multiple sources and the other is domain adaptation combining sourceand target data. In particular, we introduce two quantities that capture the inherentcharacteristics of domains. For either kind of domain adaptation, basedon the two quantities, we then develop the specific Hoeffding-type deviation inequalityand symmetrization inequality to achieve the corresponding generalizationbound based on the uniform entropy number. By using the resultant generalizationbound, we analyze the asymptotic convergence and the rate of convergenceof the learning process for such kind of domain adaptation. Meanwhile, we discussthe factors that affect the asymptotic behavior of the learning process. Thenumerical experiments support our results."
Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning,"One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called \textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency."
Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning,"One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called \textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency."
Transformed Poisson-Dirichlet Processes for Differential Topic Modeling,"We want to compare topics from a number of different document collections:some of these topics capture shared content, others capture the different andunique aspects that the collections may contain. We propose the transformedPoisson-Dirichlet process (TPDP), which is defined to be a class of hierarchicalPoisson-Dirichlet processes (HPDP) with transformed base measures, to build differential topic model among different groups of data. The main challenge of using the TPDP is the non-conjugacy between the prior and likelihood. We propose an efficient sampling algorithm by introducing auxiliary variables, which effectively resolve this problem. Experiment results show a dramatic reduced test perplexity compared to existing approximating methods on a variety of text and image collections. The model also gives an insightful analysis of the Democrat versus Republican blogs leading up to the 2008 USA election."
Kernel Hyperalignment,"We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We conducted experiments using real-world, multi-subject fMRI data."
Kernel Hyperalignment,"We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We conducted experiments using real-world, multi-subject fMRI data."
Dynamical models and tracking regret in online convex programming,"This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that capture a comparator's deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network."
Dynamical models and tracking regret in online convex programming,"This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that capture a comparator's deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network."
Ants Crawling to Discover the Community Structure in Networks,"We cast the problem of discovering the community structure in networks as the composition of community candidates, obtained from several community detection base algorithms, into a coherent structure. In turn, this composition can be cast into a maximum-weight clique problem, and we propose an ant colony optimization algorithm to solve it. Our results show that the proposed method is able to discover better community structures, according to several evaluation criteria, than the ones obtained with the base algorithms. It also outperforms, both in quality and in speed, the recently introduced FG-Tiling algorithm."
Kernel Clustering: Speedup via Efficient and Accurate Eigenfunction Approximation,"Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data and achieve state-of-the-art clustering performance. However, they suffer from two major drawbacks: (i) Their runtimecomplexity and memory requirements increase quadratically with the number of data points, rendering them unsuitable for large data sets, and (ii) the resulting partitions cannot be used to efficiently handle out-of-sample data points. We address these limitations by developing an efficient kernel clustering algorithm based on approximate eigenfunctions. The eigenfunctions are found using a small number of randomly sampled data points, thereby avoiding the computation of the full kernel matrix. The eigenfunctions are then used to compute a low-dimensional representation ofthe cluster centers, which can later be used to assign any out-of-sample point to a cluster. Unlike the existing approaches that compute the eigenfunctions using either thekernel similarity between all the points in the data set or only the similarity between the sampled points, we use the similarity between the sampled points and all the data points, leading to a better trade-off between efficiency and accuracy in eigenfunction approximation, as supported by ouranalysis. Our empirical studies demonstrate that, while the performance of the proposed kernel clustering technique is similar to the performance of state-of-the-art algorithms for large-scale data clustering, its runtime is significantly lower. This allows us to perform kernel clustering efficiently on data sets with millions of points. "
Kernel Clustering: Speedup via Efficient and Accurate Eigenfunction Approximation,"Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data and achieve state-of-the-art clustering performance. However, they suffer from two major drawbacks: (i) Their runtimecomplexity and memory requirements increase quadratically with the number of data points, rendering them unsuitable for large data sets, and (ii) the resulting partitions cannot be used to efficiently handle out-of-sample data points. We address these limitations by developing an efficient kernel clustering algorithm based on approximate eigenfunctions. The eigenfunctions are found using a small number of randomly sampled data points, thereby avoiding the computation of the full kernel matrix. The eigenfunctions are then used to compute a low-dimensional representation ofthe cluster centers, which can later be used to assign any out-of-sample point to a cluster. Unlike the existing approaches that compute the eigenfunctions using either thekernel similarity between all the points in the data set or only the similarity between the sampled points, we use the similarity between the sampled points and all the data points, leading to a better trade-off between efficiency and accuracy in eigenfunction approximation, as supported by ouranalysis. Our empirical studies demonstrate that, while the performance of the proposed kernel clustering technique is similar to the performance of state-of-the-art algorithms for large-scale data clustering, its runtime is significantly lower. This allows us to perform kernel clustering efficiently on data sets with millions of points. "
Kernel Clustering: Speedup via Efficient and Accurate Eigenfunction Approximation,"Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data and achieve state-of-the-art clustering performance. However, they suffer from two major drawbacks: (i) Their runtimecomplexity and memory requirements increase quadratically with the number of data points, rendering them unsuitable for large data sets, and (ii) the resulting partitions cannot be used to efficiently handle out-of-sample data points. We address these limitations by developing an efficient kernel clustering algorithm based on approximate eigenfunctions. The eigenfunctions are found using a small number of randomly sampled data points, thereby avoiding the computation of the full kernel matrix. The eigenfunctions are then used to compute a low-dimensional representation ofthe cluster centers, which can later be used to assign any out-of-sample point to a cluster. Unlike the existing approaches that compute the eigenfunctions using either thekernel similarity between all the points in the data set or only the similarity between the sampled points, we use the similarity between the sampled points and all the data points, leading to a better trade-off between efficiency and accuracy in eigenfunction approximation, as supported by ouranalysis. Our empirical studies demonstrate that, while the performance of the proposed kernel clustering technique is similar to the performance of state-of-the-art algorithms for large-scale data clustering, its runtime is significantly lower. This allows us to perform kernel clustering efficiently on data sets with millions of points. "
Repulsive Mixtures,"Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.  Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set."
Repulsive Mixtures,"Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.  Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set."
Slice Normalized Dynamic Markov Logic Networks,"Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the domain of time points typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional randomfield for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues.It supports efficient online inference, and can directly model influencesbetween variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks."
Slice Normalized Dynamic Markov Logic Networks,"Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the domain of time points typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional randomfield for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues.It supports efficient online inference, and can directly model influencesbetween variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks."
Slice Normalized Dynamic Markov Logic Networks,"Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the domain of time points typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional randomfield for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues.It supports efficient online inference, and can directly model influencesbetween variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks."
EigenGP: KL-expansion based Gaussian process learning,"Gaussian processes (GPs) provide a nonparametric representation of functions. Given N training points, the exact GP inference incurs high computational cost. In this paper, we propose a sparse Gaussian process model, EigenGP, based on Karhunen-Lo`eve (KL) expansions of a GP prior. We use the Nystr?om approximation to obtain eigenfunctions of the covariance function and use an empirical Bayesian approach to select these eigenfunctions. To handle nonlinear likelihoods, we develop an efficient expectation propagation inference algorithm, and couple it with expectation maximization for evidence maximization. By selecting eigenfunctions of Gaussian kernels that are associated with data clusters, EigenGP is also suitable for semi-supervised learning. Our experimental results demonstrate improved predictive performance of EigenGP over alternative state-of-the-art sparse GP and semisupervised learning methods for regression, classification, and semisupervised classification."
EigenGP: KL-expansion based Gaussian process learning,"Gaussian processes (GPs) provide a nonparametric representation of functions. Given N training points, the exact GP inference incurs high computational cost. In this paper, we propose a sparse Gaussian process model, EigenGP, based on Karhunen-Lo`eve (KL) expansions of a GP prior. We use the Nystr?om approximation to obtain eigenfunctions of the covariance function and use an empirical Bayesian approach to select these eigenfunctions. To handle nonlinear likelihoods, we develop an efficient expectation propagation inference algorithm, and couple it with expectation maximization for evidence maximization. By selecting eigenfunctions of Gaussian kernels that are associated with data clusters, EigenGP is also suitable for semi-supervised learning. Our experimental results demonstrate improved predictive performance of EigenGP over alternative state-of-the-art sparse GP and semisupervised learning methods for regression, classification, and semisupervised classification."
Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification,"This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification.  We show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation.  Applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification.  Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose five different criteria for adaptively acquiring nearest neighbors.  Experiments with both synthetic and real datasets demonstrate the effectivness of our classification criteria."
Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization,"We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\cite{kRegExpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice. "
Developmental Stage Annotation of Drosophila Embryos using Transfer-Active Learning,"Drosophila melanogaster is the major model organism for explicating the function and interconnection of animal genes and for establishing a better understanding of human diseases. Today, images capturing gene expression have unprecedented spatial resolution, resulting in high quality maps (expression images) in model organisms. Based on morphological landmarks, the continuous process of Drosophila embryo genesis is traditionally divided into a series of consecutive stages (e.g., 1-16). The manual annotation of developmental stage of an image of an embryo is done by an expert. This manual annotation process is very expensive and time consuming. It is, therefore, tempting to design computational methods for the automated annotation of gene expression patterns. Images of geneexpression patterns vary between different image databases based on their imaging techniques and resolutions, leading to distribution difference across databases.Hence, it is a challenge to directly use an available annotated image database for building a classifier for a new database of images. In this paper, we propose a transfer and active learning based computational method to develop a classification system for annotating the gene expression patterns. Transfer learning or domain adaptation is performed on the labeled database and active sampling is performed on the new set of images.The proposed framework performs domain adaptation and active sampling simultaneously by  minimizing a common objective of reducing distribution difference between the domain adapted source, the queried samples and the rest of the unlabeled target domain data. Our empirical studies on synthetic and the Drosophila image databases demonstrate the potential of the proposed approach."
Developmental Stage Annotation of Drosophila Embryos using Transfer-Active Learning,"Drosophila melanogaster is the major model organism for explicating the function and interconnection of animal genes and for establishing a better understanding of human diseases. Today, images capturing gene expression have unprecedented spatial resolution, resulting in high quality maps (expression images) in model organisms. Based on morphological landmarks, the continuous process of Drosophila embryo genesis is traditionally divided into a series of consecutive stages (e.g., 1-16). The manual annotation of developmental stage of an image of an embryo is done by an expert. This manual annotation process is very expensive and time consuming. It is, therefore, tempting to design computational methods for the automated annotation of gene expression patterns. Images of geneexpression patterns vary between different image databases based on their imaging techniques and resolutions, leading to distribution difference across databases.Hence, it is a challenge to directly use an available annotated image database for building a classifier for a new database of images. In this paper, we propose a transfer and active learning based computational method to develop a classification system for annotating the gene expression patterns. Transfer learning or domain adaptation is performed on the labeled database and active sampling is performed on the new set of images.The proposed framework performs domain adaptation and active sampling simultaneously by  minimizing a common objective of reducing distribution difference between the domain adapted source, the queried samples and the rest of the unlabeled target domain data. Our empirical studies on synthetic and the Drosophila image databases demonstrate the potential of the proposed approach."
Developmental Stage Annotation of Drosophila Embryos using Transfer-Active Learning,"Drosophila melanogaster is the major model organism for explicating the function and interconnection of animal genes and for establishing a better understanding of human diseases. Today, images capturing gene expression have unprecedented spatial resolution, resulting in high quality maps (expression images) in model organisms. Based on morphological landmarks, the continuous process of Drosophila embryo genesis is traditionally divided into a series of consecutive stages (e.g., 1-16). The manual annotation of developmental stage of an image of an embryo is done by an expert. This manual annotation process is very expensive and time consuming. It is, therefore, tempting to design computational methods for the automated annotation of gene expression patterns. Images of geneexpression patterns vary between different image databases based on their imaging techniques and resolutions, leading to distribution difference across databases.Hence, it is a challenge to directly use an available annotated image database for building a classifier for a new database of images. In this paper, we propose a transfer and active learning based computational method to develop a classification system for annotating the gene expression patterns. Transfer learning or domain adaptation is performed on the labeled database and active sampling is performed on the new set of images.The proposed framework performs domain adaptation and active sampling simultaneously by  minimizing a common objective of reducing distribution difference between the domain adapted source, the queried samples and the rest of the unlabeled target domain data. Our empirical studies on synthetic and the Drosophila image databases demonstrate the potential of the proposed approach."
Analytic Tuning of the Elastic Net With an Application to Text Mining,"The elastic net is a proposed compromise between the stability of ridge regression and the model selection properties of the lasso. It is particularly useful for conservative model selection in the presence of data with highly correlated covariates, where the lasso is known to behave poorly. Tuning the regularization parameter in the elastic net via cross-validation, however, greatly reduces its model selection properties. We provide novel analytic tuning values for the elastic net estimator along with finite sample guarantees for parameter estimation. Our tuning method is applied to age prediction in a standard text corpus of internet blogs."
Analytic Tuning of the Elastic Net With an Application to Text Mining,"The elastic net is a proposed compromise between the stability of ridge regression and the model selection properties of the lasso. It is particularly useful for conservative model selection in the presence of data with highly correlated covariates, where the lasso is known to behave poorly. Tuning the regularization parameter in the elastic net via cross-validation, however, greatly reduces its model selection properties. We provide novel analytic tuning values for the elastic net estimator along with finite sample guarantees for parameter estimation. Our tuning method is applied to age prediction in a standard text corpus of internet blogs."
Multi-Stage Multi-Task Feature Learning,"Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex regularization problem, which is usually suboptimal, due to its looseness for approximating an $\ell_0$-type regularizer. In this paper, we propose a non-convex regularized formulation for multi-task sparse feature learning; we propose to solve the non-convex optimization problem by employing a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms."
From Deformations to Parts: Motion-based Segmentation of 3D Objects,"We develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3D) poses.  We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.  To allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.  By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty.  Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods."
From Deformations to Parts: Motion-based Segmentation of 3D Objects,"We develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3D) poses.  We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.  To allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.  By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty.  Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods."
A Geometric take on Metric Learning,"Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way.We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structuregives us a principled way to perform dimensionality reduction and regression according to the learned metrics.Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Combined these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data."
Low-Rank Modeling via Capped-Trace Norm,"The problem of low-rank modeling has recently received increasing attentions in machine learning. Most problems that directly tackle the rank function are known to be NP-hard and thus many heuristics such as those based on the trace norm have been proposed for low-rank modeling. The convex relaxation based on the trace norm admits a global solution and has theoretical guarantees under certain assumptions. However, the trace norm may not be a good approximation of the rank function in practice, resulting in estimation bias. In this paper, we consider low-rank modeling via the capped trace norm (CTRN) which provides a better approximation of the rank function than the trace norm. The basic idea of the CTRN is to perform thresholding on singular values and reduce the dominating impact of the top singular values. Although the capped $\ell_1$ norm has been well studied in the vector case, to our best knowledge the extension to the matrix case has not been studied in the literature. The practical challenge lies in the efficient optimization associated with the CTRN which is non-convex. We employ the difference-of-convex (DC) programming by decomposing the non-convex CTRN into the difference of two convex functions. By applying the concave-convex procedure, the problem can be iteratively computed via solving convex optimization problems. We present a block coordinate descend algorithm for general problems where the objective and/or the constraint can be a difference-of-convex function, and we present the convergence property of the algorithm. Our convergence proof is much simpler and requires weaker assumptions than existing work. We perform extensive experiments using both synthetic and real data. Our results show that the proposed algorithms outperform many popular heuristics for low-rank modeling, including the trace norm and the Schatten-$p$ norm."
Low-Rank Modeling via Capped-Trace Norm,"The problem of low-rank modeling has recently received increasing attentions in machine learning. Most problems that directly tackle the rank function are known to be NP-hard and thus many heuristics such as those based on the trace norm have been proposed for low-rank modeling. The convex relaxation based on the trace norm admits a global solution and has theoretical guarantees under certain assumptions. However, the trace norm may not be a good approximation of the rank function in practice, resulting in estimation bias. In this paper, we consider low-rank modeling via the capped trace norm (CTRN) which provides a better approximation of the rank function than the trace norm. The basic idea of the CTRN is to perform thresholding on singular values and reduce the dominating impact of the top singular values. Although the capped $\ell_1$ norm has been well studied in the vector case, to our best knowledge the extension to the matrix case has not been studied in the literature. The practical challenge lies in the efficient optimization associated with the CTRN which is non-convex. We employ the difference-of-convex (DC) programming by decomposing the non-convex CTRN into the difference of two convex functions. By applying the concave-convex procedure, the problem can be iteratively computed via solving convex optimization problems. We present a block coordinate descend algorithm for general problems where the objective and/or the constraint can be a difference-of-convex function, and we present the convergence property of the algorithm. Our convergence proof is much simpler and requires weaker assumptions than existing work. We perform extensive experiments using both synthetic and real data. Our results show that the proposed algorithms outperform many popular heuristics for low-rank modeling, including the trace norm and the Schatten-$p$ norm."
Online Convex-Concave Optimization,"We study online convex optimization (OCO) and online convex-concave optimization(OCC) under a unified framework of variational inequalities (VI). The OCCyields projection-free online learning algorithms, breaking the bottlenecks in OCOwhere the projection onto the constraints set is challenging. We define a new typeof regret based on VI named VI-regret, which includes standard regret in OCO asa special case. We show the prox method has a variation-based VI-regret boundand online alternating direction method has a sublinear regret bound in OCC. Wealso give two examples to show the projection-free OCC."
Online Convex-Concave Optimization,"We study online convex optimization (OCO) and online convex-concave optimization(OCC) under a unified framework of variational inequalities (VI). The OCCyields projection-free online learning algorithms, breaking the bottlenecks in OCOwhere the projection onto the constraints set is challenging. We define a new typeof regret based on VI named VI-regret, which includes standard regret in OCO asa special case. We show the prox method has a variation-based VI-regret boundand online alternating direction method has a sublinear regret bound in OCC. Wealso give two examples to show the projection-free OCC."
Spike triggered covariance for strongly correlated Gaussian stimuli,"Characterizing feature selectivity is an important problem because it can shed light on how neurons process their inputs. The spike triggered covariance method (STCM) is a very commonly used method to extract the relevant set of stimulus features to which a neuron responds. One of the main advantages of STCM is that it can determine the dimensionality of the cell's relevant subspace. The method has been previously thought to be applicable when stimuli are drawn from a Gaussian ensemble, with or without stimulus correlations.  Here we use random matrix theory to show that when STCM is used with strongly correlated Gaussian stimuli, the null distribution of eigenvalues has a large outstanding mode. As a result, STCM can either yield an extra feature, which often corresponds to the strongest eigenvalue, or fail to yield any significant dimensions. We present a simple correction scheme that removes this artifact and illustrate its effectiveness by analyzing model neurons and recordings from retinal ganglion cells probed with correlated Gaussian stimuli whose second-order statistics was matched to natural stimuli. Our results can serve as guidelines for design of reverse correlation experiments that can help illuminate how neurons are optimized to code natural stimuli."
Collaborative Gaussian Processes for Preference Learning,"We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms."
Collaborative Gaussian Processes for Preference Learning,"We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms."
Collaborative Gaussian Processes for Preference Learning,"We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms."
Lasso Screening Rules via Dual Polytope Projection,"Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. Bytransforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose a fast and efficient screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. We have evaluated our screening rule using both synthetic and real data sets. Results show that our rule is more effective to identifyinactive predictors than existing state-of-art screening rules. We also extend our screening rule to identify inactive groups in group Lasso."
Lasso Screening Rules via Dual Polytope Projection,"Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. Bytransforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose a fast and efficient screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. We have evaluated our screening rule using both synthetic and real data sets. Results show that our rule is more effective to identifyinactive predictors than existing state-of-art screening rules. We also extend our screening rule to identify inactive groups in group Lasso."
Lasso Screening Rules via Dual Polytope Projection,"Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. Bytransforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose a fast and efficient screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. We have evaluated our screening rule using both synthetic and real data sets. Results show that our rule is more effective to identifyinactive predictors than existing state-of-art screening rules. We also extend our screening rule to identify inactive groups in group Lasso."
Domain-Unifying Embedding,"We propose Domain Unifying Embedding, together with its kernelized version, as a consolidated framework for domain adaptation and cross-domain recognition. Our approach embeds samples from one or more source domains and a target domain into a single latent shared domain, with the embedding represented by a linear or kernel transformation. In addition to allowing an arbitrary number of source domains, the approach allows these sources to be heterogeneous in the sense of having different dimensions. It also allows simultaneously exploiting a variety of types of semi-supervision, including target samples with explicit class labels when available, and instances for which corresponding, but unlabeled, samples are available in two or more domains."
Domain-Unifying Embedding,"We propose Domain Unifying Embedding, together with its kernelized version, as a consolidated framework for domain adaptation and cross-domain recognition. Our approach embeds samples from one or more source domains and a target domain into a single latent shared domain, with the embedding represented by a linear or kernel transformation. In addition to allowing an arbitrary number of source domains, the approach allows these sources to be heterogeneous in the sense of having different dimensions. It also allows simultaneously exploiting a variety of types of semi-supervision, including target samples with explicit class labels when available, and instances for which corresponding, but unlabeled, samples are available in two or more domains."
Hypergraph-based Gaussian Process Models with Qualitative and Quantitative Input Variables,"Most existing Gaussian process models assume that all the input variables are quantitative, which makes them fall short in many applications that involve both qualitative and quantitative inputs. The fundamental challenge for enabling GP models on these applications is the design of a proper correlation function. We develop such a correlation function based on hypergraph and the associated Laplacian matrix. Compared with existing models, the proposed model requires much fewer free parameters and shows better performance on benchmark examples."
Hypergraph-based Gaussian Process Models with Qualitative and Quantitative Input Variables,"Most existing Gaussian process models assume that all the input variables are quantitative, which makes them fall short in many applications that involve both qualitative and quantitative inputs. The fundamental challenge for enabling GP models on these applications is the design of a proper correlation function. We develop such a correlation function based on hypergraph and the associated Laplacian matrix. Compared with existing models, the proposed model requires much fewer free parameters and shows better performance on benchmark examples."
Exact inference in flipper graphs and their use for approximate inference using dual decomposition,"We show exact inference is possible for a subclass of binary labeled graphs that we call flipper graphs. These are graphs where the submodular and non-submodular edges follow a specific pattern-- essentially that an isomorphism exists to a fully submodular graph. Examples of flipper graphs include tree-structured graphs, submodular graphs, and bipartite graphs with all non-submodular edges. We investigate the use of flipper graphs in dual decomposition, based on two different decomposition strategies. Experimentally, we find that decomposition using flipper graphs outperform traditional tree-based dual decomposition."
Exact inference in flipper graphs and their use for approximate inference using dual decomposition,"We show exact inference is possible for a subclass of binary labeled graphs that we call flipper graphs. These are graphs where the submodular and non-submodular edges follow a specific pattern-- essentially that an isomorphism exists to a fully submodular graph. Examples of flipper graphs include tree-structured graphs, submodular graphs, and bipartite graphs with all non-submodular edges. We investigate the use of flipper graphs in dual decomposition, based on two different decomposition strategies. Experimentally, we find that decomposition using flipper graphs outperform traditional tree-based dual decomposition."
Classi?er Calibration: A Bayesian Non-Parametric Approach,"A set of probabilistic predictions is well calibrated if the events that are predicted to occur with probability p do in fact occur about p fraction of the time. Well calibrated predictions are particularly important when machine learning models are used in decision analysis. This paper presents two new non-parametric methods for calibrating outputs of binary classi?cation models: a method based on the Bayes optimal selection and a method based on the Bayesian model averaging. The advantage of these methods is that they are independent of the algorithm used to learn a predictive model, and they can be applied in a post-processing step, after the model is learned. This makes them applicableto a wide variety of machine learning models and methods. These calibration methods, as well as other methods, are tested on a variety of datasets in terms of both discrimination and calibration performance. The results show the methods either outperform or are comparable in performance to the state-of-the-art calibration methods."
Classi?er Calibration: A Bayesian Non-Parametric Approach,"A set of probabilistic predictions is well calibrated if the events that are predicted to occur with probability p do in fact occur about p fraction of the time. Well calibrated predictions are particularly important when machine learning models are used in decision analysis. This paper presents two new non-parametric methods for calibrating outputs of binary classi?cation models: a method based on the Bayes optimal selection and a method based on the Bayesian model averaging. The advantage of these methods is that they are independent of the algorithm used to learn a predictive model, and they can be applied in a post-processing step, after the model is learned. This makes them applicableto a wide variety of machine learning models and methods. These calibration methods, as well as other methods, are tested on a variety of datasets in terms of both discrimination and calibration performance. The results show the methods either outperform or are comparable in performance to the state-of-the-art calibration methods."
Classi?er Calibration: A Bayesian Non-Parametric Approach,"A set of probabilistic predictions is well calibrated if the events that are predicted to occur with probability p do in fact occur about p fraction of the time. Well calibrated predictions are particularly important when machine learning models are used in decision analysis. This paper presents two new non-parametric methods for calibrating outputs of binary classi?cation models: a method based on the Bayes optimal selection and a method based on the Bayesian model averaging. The advantage of these methods is that they are independent of the algorithm used to learn a predictive model, and they can be applied in a post-processing step, after the model is learned. This makes them applicableto a wide variety of machine learning models and methods. These calibration methods, as well as other methods, are tested on a variety of datasets in terms of both discrimination and calibration performance. The results show the methods either outperform or are comparable in performance to the state-of-the-art calibration methods."
Gaussian Process Model Predictive Control: A Comparison with Conventional Techniques,"Gaussian processes are gaining increased popularity in the area of system identification for control. In this paper a new Model Predictive Control (MPC) algorithm based on a Gaussian process internal model is defined. The framework can be used for modelling and control of arbitrary nonlinear state-space systems, an extension of previous work which only considered modelling of ARMAX models. The performance of the algorithm is then compared with standard MPC and an adaptive Linear Quadratic Regulator (LQR), which use linearised models of the real system, on a benchmark industrial process control problem."
Gaussian Process Model Predictive Control: A Comparison with Conventional Techniques,"Gaussian processes are gaining increased popularity in the area of system identification for control. In this paper a new Model Predictive Control (MPC) algorithm based on a Gaussian process internal model is defined. The framework can be used for modelling and control of arbitrary nonlinear state-space systems, an extension of previous work which only considered modelling of ARMAX models. The performance of the algorithm is then compared with standard MPC and an adaptive Linear Quadratic Regulator (LQR), which use linearised models of the real system, on a benchmark industrial process control problem."
Generic Active Appearance Models Revisited: The Active Orientation Models Paradigm ,"The proposed Active Orientation Models (AOMs) are generative models of facial shape and appearance. Their main differences with the well-known paradigm of Active Appearance Models (AAMs) are (i) they use a different statistical model of appearance (ii) they are accompanied by a robust algorithm for model fitting and parameter estimation and (iii) and, most importantly, they generalize well to unseen faces and variations. Their main similarity is computational complexity. The project-out version of AOMs is as computationally efficient as the standard project-out inverse compositional algorithm which is admittedly the fastest algorithm for fitting AAMs. We show that not only does the AOM generalize well to unseen identities, but also it outperforms state-of-the-art algorithms for the same task by a large margin. Finally, we prove our claims by providing Matlab code for reproducing our experiments."
Which classifiers are worth learning?,"Despite advances in model selection techniques,  choosing among different statistical models to select the ``correct'' one for the problem at hand remains a fairly subjective and even personal decision.  This short note suggests that so long as one is only interested inpolynomial-time algorithms then fairly mild assumptions about the learning problem ---namely, that the inputs are noisy---can lead to a simple understanding of what concepts are learnable even in principle.  In the case of uniformly distributed inputs and iid noise, there is a simple characterization of learnable concepts as well as a  simple universal learning algorithm that runs in polynomial time. The main technical observation involves the theory of {\em noise stable} functions (considered earlier in context of Fourier learning)."
Schizophrenia Detection and Classification by Advanced Analysis of EEG Recordings using a Single Electrode Approach ,"Electroencephalographic (EEG) analysis has emerged as a powerful tool for brain state interpretation and diagnosis. However, it has not emerged as a powerful tool in diagnosis of mental disorders. This may be explained by the low spatial resolution or depth sensitivity of EEG.  This paper concerns diagnosis of Schizophrenia using EEG, which currently suffers from few cardinal problems: it heavily depends on assumptions, conditions and prior knowledge of the patient. Additionally, the diagnostic experiments take hours, and accuracy of the analysis is low or unreliable.This article presents a novel approach for Schizophrenia detection showing great success in classification accuracy. The methodology is built for a single electrode recording attempting to make the data acquisition process feasible and quick for most patients."
Schizophrenia Detection and Classification by Advanced Analysis of EEG Recordings using a Single Electrode Approach ,"Electroencephalographic (EEG) analysis has emerged as a powerful tool for brain state interpretation and diagnosis. However, it has not emerged as a powerful tool in diagnosis of mental disorders. This may be explained by the low spatial resolution or depth sensitivity of EEG.  This paper concerns diagnosis of Schizophrenia using EEG, which currently suffers from few cardinal problems: it heavily depends on assumptions, conditions and prior knowledge of the patient. Additionally, the diagnostic experiments take hours, and accuracy of the analysis is low or unreliable.This article presents a novel approach for Schizophrenia detection showing great success in classification accuracy. The methodology is built for a single electrode recording attempting to make the data acquisition process feasible and quick for most patients."
Robust Dictionary Learning by Source Decomposition,"It is now well established that sparse coding is well suited to many applications such as image restoration, denoising and classification. Especially, adaptive sparsemodels learned from data, or ?dictionary?, outperformfixed basis such as Discrete Cosine Basis and Fourier Basis. This paper extends this line of research to consider outlier elimination, proposing two methods to decompose the reconstructive residual into two components: a non-sparse component for small universal noises as well as a sparse one for the outliers respectively. In addition, further analysis reveals the connection between our model and the ?partial? dictionary learning approach, updating only part of the codewords (informative codewords DInfo)with remaining (noisy one DNoisy) fixed. We validate and evaluate our new approach on synthetic data as well as real applications and achieved satisfactory performance."
Fused Multiple Graphical Lasso,"In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which  encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a blockwise coordinate descent method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."
Fused Multiple Graphical Lasso,"In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which  encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a blockwise coordinate descent method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."
Fused Multiple Graphical Lasso,"In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which  encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a blockwise coordinate descent method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."
Fused Multiple Graphical Lasso,"In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which  encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a blockwise coordinate descent method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."
Structured Label Propagation in an Ensemble of Directed Acyclic Graphs,"This paper proposes a new approach to semi-supervised structured learning. Our structured learning formulation is based on energy minimization using graphic models that consist of overlapping local cliques. Local label patterns of cliques are propagated from training data to testing data, using an ensemble of directed acyclic graphs (DAGs). The key innovation in our approach is that the label propagation does not make the label smoothness assumption that  if two feature vectors are similar, then so should be their corresponding output labels. We argue that this assumption introduces a bias toward labels with dominant population because local cliques in a graphical model may and often exhibit similar weak local features but take different labels. In contrast, our label propagation makes a weaker label repetitiveness assumption that if one feature is similar to a few other features that may have different labels, then the label of this feature is one of the other features'; which label to use is determined by inferring the energy model. We present algorithms for structured label estimation marginalized over an ensemble of sampled DAGs. Our method compares favorably with the conventional approach that assumes label smoothness.  "
Structured Label Propagation in an Ensemble of Directed Acyclic Graphs,"This paper proposes a new approach to semi-supervised structured learning. Our structured learning formulation is based on energy minimization using graphic models that consist of overlapping local cliques. Local label patterns of cliques are propagated from training data to testing data, using an ensemble of directed acyclic graphs (DAGs). The key innovation in our approach is that the label propagation does not make the label smoothness assumption that  if two feature vectors are similar, then so should be their corresponding output labels. We argue that this assumption introduces a bias toward labels with dominant population because local cliques in a graphical model may and often exhibit similar weak local features but take different labels. In contrast, our label propagation makes a weaker label repetitiveness assumption that if one feature is similar to a few other features that may have different labels, then the label of this feature is one of the other features'; which label to use is determined by inferring the energy model. We present algorithms for structured label estimation marginalized over an ensemble of sampled DAGs. Our method compares favorably with the conventional approach that assumes label smoothness.  "
Structured Label Propagation in an Ensemble of Directed Acyclic Graphs,"This paper proposes a new approach to semi-supervised structured learning. Our structured learning formulation is based on energy minimization using graphic models that consist of overlapping local cliques. Local label patterns of cliques are propagated from training data to testing data, using an ensemble of directed acyclic graphs (DAGs). The key innovation in our approach is that the label propagation does not make the label smoothness assumption that  if two feature vectors are similar, then so should be their corresponding output labels. We argue that this assumption introduces a bias toward labels with dominant population because local cliques in a graphical model may and often exhibit similar weak local features but take different labels. In contrast, our label propagation makes a weaker label repetitiveness assumption that if one feature is similar to a few other features that may have different labels, then the label of this feature is one of the other features'; which label to use is determined by inferring the energy model. We present algorithms for structured label estimation marginalized over an ensemble of sampled DAGs. Our method compares favorably with the conventional approach that assumes label smoothness.  "
Learning to Classify From Multiple Experts,"Label is a critical component of the classification learning framework. However in many practical applications when labels are based on human  assessments, it is infeasible to assume one can obtain a perfect set of labels everybody agrees on. A solution that has been recently explored by the machine learning community is learning from multiple annotators: instead of collecting labels from a single expert/annotator, we collect labels from a number of annotators/experts. Since there may be substantial disagreements among labels of multiple annotators, some kind of a consensus model, that incorporates the characteristics (e.g. reliability) of each annotator, is sought.In this work, we study and develop a new approach for learning classification models from labels provided by multiple annotators. Our approach explicitly models and learns annotator-specific  model, reliability, and bias and incorporates them into the learning of the consensus model. Experimental results show that our approach outperforms commonly used multiple annotators baselines on multiple UCI-based datasets and a real-world medical data set."
Learning to Classify From Multiple Experts,"Label is a critical component of the classification learning framework. However in many practical applications when labels are based on human  assessments, it is infeasible to assume one can obtain a perfect set of labels everybody agrees on. A solution that has been recently explored by the machine learning community is learning from multiple annotators: instead of collecting labels from a single expert/annotator, we collect labels from a number of annotators/experts. Since there may be substantial disagreements among labels of multiple annotators, some kind of a consensus model, that incorporates the characteristics (e.g. reliability) of each annotator, is sought.In this work, we study and develop a new approach for learning classification models from labels provided by multiple annotators. Our approach explicitly models and learns annotator-specific  model, reliability, and bias and incorporates them into the learning of the consensus model. Experimental results show that our approach outperforms commonly used multiple annotators baselines on multiple UCI-based datasets and a real-world medical data set."
Learning to Classify From Multiple Experts,"Label is a critical component of the classification learning framework. However in many practical applications when labels are based on human  assessments, it is infeasible to assume one can obtain a perfect set of labels everybody agrees on. A solution that has been recently explored by the machine learning community is learning from multiple annotators: instead of collecting labels from a single expert/annotator, we collect labels from a number of annotators/experts. Since there may be substantial disagreements among labels of multiple annotators, some kind of a consensus model, that incorporates the characteristics (e.g. reliability) of each annotator, is sought.In this work, we study and develop a new approach for learning classification models from labels provided by multiple annotators. Our approach explicitly models and learns annotator-specific  model, reliability, and bias and incorporates them into the learning of the consensus model. Experimental results show that our approach outperforms commonly used multiple annotators baselines on multiple UCI-based datasets and a real-world medical data set."
Factoring nonnegative matrices with linear programs,"This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes."
Factoring nonnegative matrices with linear programs,"This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes."
Factoring nonnegative matrices with linear programs,"This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes."
Optical FLow Estimation by Adaptive Data Fusion,"Many state-of-the-art optical flow estimation algorithms are based on the variational method, which optimizes regularity and data terms simultaneously. This study presents a novel approach that provides weights to various data terms adaptively against a single data term in the conventional variational framework. In this study, a new optical flow estimation model with weighted sum of multiple data terms is introduced, and its optimization procedure is proposed. Competitive experimental results on the Middlebury optical flow benchmark show that the proposed method outperforms conventional methods with the aid of complementary data terms. In particular, this study is of importance for cases that incorporate various data terms into a unified variational framework."
Optical FLow Estimation by Adaptive Data Fusion,"Many state-of-the-art optical flow estimation algorithms are based on the variational method, which optimizes regularity and data terms simultaneously. This study presents a novel approach that provides weights to various data terms adaptively against a single data term in the conventional variational framework. In this study, a new optical flow estimation model with weighted sum of multiple data terms is introduced, and its optimization procedure is proposed. Competitive experimental results on the Middlebury optical flow benchmark show that the proposed method outperforms conventional methods with the aid of complementary data terms. In particular, this study is of importance for cases that incorporate various data terms into a unified variational framework."
Optical FLow Estimation by Adaptive Data Fusion,"Many state-of-the-art optical flow estimation algorithms are based on the variational method, which optimizes regularity and data terms simultaneously. This study presents a novel approach that provides weights to various data terms adaptively against a single data term in the conventional variational framework. In this study, a new optical flow estimation model with weighted sum of multiple data terms is introduced, and its optimization procedure is proposed. Competitive experimental results on the Middlebury optical flow benchmark show that the proposed method outperforms conventional methods with the aid of complementary data terms. In particular, this study is of importance for cases that incorporate various data terms into a unified variational framework."
Minimum Uncertainty Gap for Robust Visual Tracking,"We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at that state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the bounds, our method finds the confident state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood score, especially when there are severe illumination changes, occlusions, and pose variations.A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that minimizes the gap between the bounds. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods."
Minimum Uncertainty Gap for Robust Visual Tracking,"We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at that state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the bounds, our method finds the confident state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood score, especially when there are severe illumination changes, occlusions, and pose variations.A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that minimizes the gap between the bounds. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods."
Soft Bounding Box Representation for Visual Tracking ,"A new tracking algorithm that tracks highly non-rigid targets robustly is proposed using a new bounding box representation called a soft bounding box (SBB).In the soft bounding box representation, the target is described as a range of the bounding box, which is bounded by an inner bounding box and an outer bounding box.In the paper, the inner and outer bounding boxes are theoretically constructed based on the theory of evidence.With these bounding boxes, the proposed method can solve the inherent ambiguity in a single bounding box representation for highly non-rigid targets. In addition, the method does not deal with the ambiguous region directly, which includes the foreground and the background at the same time. Hence, it robustly tracks highly non-rigid targets. In the soft bounding box representation, the best state of the target is efficiently found using a new Constrained Markov Chain Monte Carlo sampling method,which uses the constraint in which the outer bounding box must include the inner bounding box. Experimental results show that our method can track non-rigid targets accurately and robustly, and outperform even state-of-the-art methods."
Soft Bounding Box Representation for Visual Tracking ,"A new tracking algorithm that tracks highly non-rigid targets robustly is proposed using a new bounding box representation called a soft bounding box (SBB).In the soft bounding box representation, the target is described as a range of the bounding box, which is bounded by an inner bounding box and an outer bounding box.In the paper, the inner and outer bounding boxes are theoretically constructed based on the theory of evidence.With these bounding boxes, the proposed method can solve the inherent ambiguity in a single bounding box representation for highly non-rigid targets. In addition, the method does not deal with the ambiguous region directly, which includes the foreground and the background at the same time. Hence, it robustly tracks highly non-rigid targets. In the soft bounding box representation, the best state of the target is efficiently found using a new Constrained Markov Chain Monte Carlo sampling method,which uses the constraint in which the outer bounding box must include the inner bounding box. Experimental results show that our method can track non-rigid targets accurately and robustly, and outperform even state-of-the-art methods."
Soft Bounding Box Representation for Visual Tracking ,"A new tracking algorithm that tracks highly non-rigid targets robustly is proposed using a new bounding box representation called a soft bounding box (SBB).In the soft bounding box representation, the target is described as a range of the bounding box, which is bounded by an inner bounding box and an outer bounding box.In the paper, the inner and outer bounding boxes are theoretically constructed based on the theory of evidence.With these bounding boxes, the proposed method can solve the inherent ambiguity in a single bounding box representation for highly non-rigid targets. In addition, the method does not deal with the ambiguous region directly, which includes the foreground and the background at the same time. Hence, it robustly tracks highly non-rigid targets. In the soft bounding box representation, the best state of the target is efficiently found using a new Constrained Markov Chain Monte Carlo sampling method,which uses the constraint in which the outer bounding box must include the inner bounding box. Experimental results show that our method can track non-rigid targets accurately and robustly, and outperform even state-of-the-art methods."
Learning-based Stereo Method using MMSE Estimation,"We model the stereo problem using a product of Gaussian mixture models(PGMM). This enables efficient sampling and optimization, which makes general parameter learning possible. The learning procedure, along with the strong modeling power of PGMM,  helps us to find prior model for stereo problem using training data. Another important contribution of this work is that the proposed method computes its solution via minimum-mean-squared-error(MMSE) estimation instead of maximum-a-posteriori(MAP) estimation. The benefits of this approach is two-fold: it utilizes the learned characteristics of our model better than MAP and the result is more robust to subtle errors in stereo model itself. Experimental results show that the performance of our method based on MMSE estimation is far better than that of MAP estimation with the same model, while achieving competitive quantitative evaluation score in comparison with other learning-based stereo methods."
Learning-based Stereo Method using MMSE Estimation,"We model the stereo problem using a product of Gaussian mixture models(PGMM). This enables efficient sampling and optimization, which makes general parameter learning possible. The learning procedure, along with the strong modeling power of PGMM,  helps us to find prior model for stereo problem using training data. Another important contribution of this work is that the proposed method computes its solution via minimum-mean-squared-error(MMSE) estimation instead of maximum-a-posteriori(MAP) estimation. The benefits of this approach is two-fold: it utilizes the learned characteristics of our model better than MAP and the result is more robust to subtle errors in stereo model itself. Experimental results show that the performance of our method based on MMSE estimation is far better than that of MAP estimation with the same model, while achieving competitive quantitative evaluation score in comparison with other learning-based stereo methods."
Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination ,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) isa fundamental component of a variety of studies in Neuroscience. The goal hereis to identify regions that exhibit changes as a function of the clinical conditionunder study. As the clinical questions of interest move towards identifying veryearly signs of diseases, the corresponding statistical differences at the group levelinvariably become weaker and increasingly hard to identify. Indeed, after a mul-tiple comparisons correction is adopted (to account for correlated statistical testsover all surface points), very few regions may survive. In contrast to hypothesistests on point-wise measurements, in this paper, we make the case for perform-ing statistical analysis on multi-scale shape descriptors that characterize the localtopological context of the signal around each surface vertex. Our descriptors arebased on recent results from harmonic analysis, that show how wavelet theoryextends to non-Euclidean settings (i.e., irregular weighted graphs). We providestrong evidence that these descriptors successfully pick up group-wise differences,where traditional methods either fail or yield unsatisfactory results. Other than thisprimary application, we show how the framework (i) allows performing corticalsurface smoothing in the native space (without a unit sphere mapping) and (ii)yields an efficient algorithm for perceptual shape segmentation of 3-D model thatcompares very favorably with the state of the art."
Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination ,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) isa fundamental component of a variety of studies in Neuroscience. The goal hereis to identify regions that exhibit changes as a function of the clinical conditionunder study. As the clinical questions of interest move towards identifying veryearly signs of diseases, the corresponding statistical differences at the group levelinvariably become weaker and increasingly hard to identify. Indeed, after a mul-tiple comparisons correction is adopted (to account for correlated statistical testsover all surface points), very few regions may survive. In contrast to hypothesistests on point-wise measurements, in this paper, we make the case for perform-ing statistical analysis on multi-scale shape descriptors that characterize the localtopological context of the signal around each surface vertex. Our descriptors arebased on recent results from harmonic analysis, that show how wavelet theoryextends to non-Euclidean settings (i.e., irregular weighted graphs). We providestrong evidence that these descriptors successfully pick up group-wise differences,where traditional methods either fail or yield unsatisfactory results. Other than thisprimary application, we show how the framework (i) allows performing corticalsurface smoothing in the native space (without a unit sphere mapping) and (ii)yields an efficient algorithm for perceptual shape segmentation of 3-D model thatcompares very favorably with the state of the art."
Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination ,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) isa fundamental component of a variety of studies in Neuroscience. The goal hereis to identify regions that exhibit changes as a function of the clinical conditionunder study. As the clinical questions of interest move towards identifying veryearly signs of diseases, the corresponding statistical differences at the group levelinvariably become weaker and increasingly hard to identify. Indeed, after a mul-tiple comparisons correction is adopted (to account for correlated statistical testsover all surface points), very few regions may survive. In contrast to hypothesistests on point-wise measurements, in this paper, we make the case for perform-ing statistical analysis on multi-scale shape descriptors that characterize the localtopological context of the signal around each surface vertex. Our descriptors arebased on recent results from harmonic analysis, that show how wavelet theoryextends to non-Euclidean settings (i.e., irregular weighted graphs). We providestrong evidence that these descriptors successfully pick up group-wise differences,where traditional methods either fail or yield unsatisfactory results. Other than thisprimary application, we show how the framework (i) allows performing corticalsurface smoothing in the native space (without a unit sphere mapping) and (ii)yields an efficient algorithm for perceptual shape segmentation of 3-D model thatcompares very favorably with the state of the art."
Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination ,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) isa fundamental component of a variety of studies in Neuroscience. The goal hereis to identify regions that exhibit changes as a function of the clinical conditionunder study. As the clinical questions of interest move towards identifying veryearly signs of diseases, the corresponding statistical differences at the group levelinvariably become weaker and increasingly hard to identify. Indeed, after a mul-tiple comparisons correction is adopted (to account for correlated statistical testsover all surface points), very few regions may survive. In contrast to hypothesistests on point-wise measurements, in this paper, we make the case for perform-ing statistical analysis on multi-scale shape descriptors that characterize the localtopological context of the signal around each surface vertex. Our descriptors arebased on recent results from harmonic analysis, that show how wavelet theoryextends to non-Euclidean settings (i.e., irregular weighted graphs). We providestrong evidence that these descriptors successfully pick up group-wise differences,where traditional methods either fail or yield unsatisfactory results. Other than thisprimary application, we show how the framework (i) allows performing corticalsurface smoothing in the native space (without a unit sphere mapping) and (ii)yields an efficient algorithm for perceptual shape segmentation of 3-D model thatcompares very favorably with the state of the art."
Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination ,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) isa fundamental component of a variety of studies in Neuroscience. The goal hereis to identify regions that exhibit changes as a function of the clinical conditionunder study. As the clinical questions of interest move towards identifying veryearly signs of diseases, the corresponding statistical differences at the group levelinvariably become weaker and increasingly hard to identify. Indeed, after a mul-tiple comparisons correction is adopted (to account for correlated statistical testsover all surface points), very few regions may survive. In contrast to hypothesistests on point-wise measurements, in this paper, we make the case for perform-ing statistical analysis on multi-scale shape descriptors that characterize the localtopological context of the signal around each surface vertex. Our descriptors arebased on recent results from harmonic analysis, that show how wavelet theoryextends to non-Euclidean settings (i.e., irregular weighted graphs). We providestrong evidence that these descriptors successfully pick up group-wise differences,where traditional methods either fail or yield unsatisfactory results. Other than thisprimary application, we show how the framework (i) allows performing corticalsurface smoothing in the native space (without a unit sphere mapping) and (ii)yields an efficient algorithm for perceptual shape segmentation of 3-D model thatcompares very favorably with the state of the art."
Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination ,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) isa fundamental component of a variety of studies in Neuroscience. The goal hereis to identify regions that exhibit changes as a function of the clinical conditionunder study. As the clinical questions of interest move towards identifying veryearly signs of diseases, the corresponding statistical differences at the group levelinvariably become weaker and increasingly hard to identify. Indeed, after a mul-tiple comparisons correction is adopted (to account for correlated statistical testsover all surface points), very few regions may survive. In contrast to hypothesistests on point-wise measurements, in this paper, we make the case for perform-ing statistical analysis on multi-scale shape descriptors that characterize the localtopological context of the signal around each surface vertex. Our descriptors arebased on recent results from harmonic analysis, that show how wavelet theoryextends to non-Euclidean settings (i.e., irregular weighted graphs). We providestrong evidence that these descriptors successfully pick up group-wise differences,where traditional methods either fail or yield unsatisfactory results. Other than thisprimary application, we show how the framework (i) allows performing corticalsurface smoothing in the native space (without a unit sphere mapping) and (ii)yields an efficient algorithm for perceptual shape segmentation of 3-D model thatcompares very favorably with the state of the art."
Pareto-Path Multi-Task Multiple Kernel Learning,"Traditional Multi-Task Multiple Kernel Learning (MT-MKL) methods routinely optimize the sum (thus, the average) of objective functions to simultaneously improve performances for all tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO) problem, which considers the concurrent optimization of all task objectives involved in the Multi-Task Learning problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel Support Vector Machine (SVM) MT-MKL framework, that considers an implicitly-defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of improving performances uniformly over tasks, when compared to the traditional MTL approach."
A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation,"A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lovasz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset."
How Prior Probability Influences Decision Making: A Unifying Probabilistic Model,"How does the brain combine prior knowledge with sensory evidence whenmaking decisions under uncertainty? Two competing descriptive modelshave been proposed based on experimental data.  The first posits anadditive offset to a decision variable, implying a static effect ofthe prior. However, this model is inconsistent with recent data from amotion discrimination task involving temporal integration of uncertainsensory evidence. To explain this data, a second model has beenproposed which assumes a time-varying influence of the prior. Here wepresent a normative model of decision making that incorporates priorknowledge in a principled way.  We show that the additive offset modeland the time-varying prior model emerge naturally when decision makingis viewed within the framework of partially observable Markov decisionprocesses (POMDPs).  Decision making in the model reduces to (1)computing beliefs given observations and prior information in a Bayesianmanner, and (2) selecting actions based on these beliefs to maximize the expected sum of future rewards. We show that the model can explain both data previously explained using the additive offset model as well as more recent data on the time-varying influence of prior knowledge on decision making."
Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data,"Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences."
Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data,"Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences."
Generalized Classification-based Approximate Policy Iteration,"Classification-based approximate policy iteration allows us to benefit from the regularities of the optimal policy by explicitly controlling the complexity of the policy space. This leads to considerable improvements whenever the optimal policy is easy to represent. The conventional classification-based methods, however, do not benefit from the regularities of the value function as they often use a rollout-based estimate of the action-value function, which is rather data-inefficient and cannot generalize the estimate of the action-value function over states. In this paper, we introduce a general framework for classification-based approximate policy iteration, CAPI, that lets us benefit from the present regularities of both the policy and the value.Our theoretical analysis extends existing work by allowing the policy evaluation to be performed by any reinforcement learning algorithm, by handling nonparametric representations of policies, and by providing tighter convergence bounds on the estimation error of policy learning.A small illustration shows that this approach can be faster than purely value-based methods."
Value Pursuit Iteration,"Value Pursuit Iteration (VPI) is an approximate value iteration algorithm that finds a close to optimal policy for  reinforcement learning and planning problems with large state spaces.VPI has two main features: First, it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features. The algorithm is almost insensitive to the number of irrelevant features. Second, after each iteration of VPI, the algorithm adds a set of functions based on the currently learned value function to the dictionary. This increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function.We theoretically study VPI and provide a finite-sample error upper bound for it."
"Compressive neural representation of sparse, high-dimensional probabilities","This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a high-dimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain."
Optimization of non-metric MRFs with QPBO via graph approximation,"Markov random field (MRF) has been used for many areas in computer vision. Many optimization methods were proposed to achieve good solutions on MRFs. Among them, graph cuts have gained widespread popularity. They achieve good approximated solutions when the energy function is metric using an $\alpha$-expansion scheme. However, if the energy function is non-metric, the conventional $\alpha$-expansion cannot solve the problem. In this case, the possible choice so far is the truncation, partial labeling with unlabeled nodes, and fusion move with heuristic proposals. In this paper, we propose a general way to handle non-metric MRFs with graph cuts using graph approximations. Extensive experiments support our claims and show that the proposed algorithm obtains better solutions than others both on synthetic and real problems."
Optimization of non-metric MRFs with QPBO via graph approximation,"Markov random field (MRF) has been used for many areas in computer vision. Many optimization methods were proposed to achieve good solutions on MRFs. Among them, graph cuts have gained widespread popularity. They achieve good approximated solutions when the energy function is metric using an $\alpha$-expansion scheme. However, if the energy function is non-metric, the conventional $\alpha$-expansion cannot solve the problem. In this case, the possible choice so far is the truncation, partial labeling with unlabeled nodes, and fusion move with heuristic proposals. In this paper, we propose a general way to handle non-metric MRFs with graph cuts using graph approximations. Extensive experiments support our claims and show that the proposed algorithm obtains better solutions than others both on synthetic and real problems."
Symbolic Dynamic Programming for Continuous State and Observation POMDPs,"Partially-observable Markov decision processes (POMDPs) provide a powerfulmodel for real-world sequential decision-making problems. In recent years, point-based value iteration methods have proven to be extremely effective techniquesfor ?nding (approximately) optimal dynamic programming solutions to POMDPswhen an initial set of belief states is known. However, no point-based work hasprovided exact point-based backups for both continuous state and observationspaces, which we tackle in this paper. Our key insight is that while there maybe an in?nite number of possible observations, there are only a ?nite number ofobservation partitionings that are relevant for optimal decision-making when a?nite, ?xed set of reachable belief states is known. To this end, we make twoimportant contributions: (1) we show how previous exact symbolic dynamic pro-gramming solutions for continuous state MDPs can be generalized to continu-ous state POMDPs with discrete observations, and (2) we show how this solutioncan be further extended via recently developed symbolic methods to continuousstate and observations to derive the minimal relevant observation partitioning forpotentially correlated, multivariate observation spaces. We demonstrate proof-of-concept results on uni- and multi-variate state and observation steam plant control."
Scaled Gradients on Grassmann Manifolds for Matrix Completion,This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices while maintaining established global convegence and exact recovery guarantees. We also show the connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure. Our conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods.
Scaled Gradients on Grassmann Manifolds for Matrix Completion,This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices while maintaining established global convegence and exact recovery guarantees. We also show the connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure. Our conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods.
Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging ,"Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model?s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject?s conversion to Alzheimer?s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here, our new model outperforms the state of the art (p-values << 10?3 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity)."
Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging ,"Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model?s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject?s conversion to Alzheimer?s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here, our new model outperforms the state of the art (p-values << 10?3 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity)."
Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging ,"Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model?s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject?s conversion to Alzheimer?s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here, our new model outperforms the state of the art (p-values << 10?3 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity)."
Near optimal policy decomposition for tasks with multiple goals explains human behavior,"The natural environment presents people with multiple potential goals that compete for action selection. Recent studies suggest that the brain generates several concurrent and partially prepared actions associated with alternative goals and use perceptual information to drive the goal competition, until a single action is selected. In the current study, we propose a near-optimal policy decomposition that the brain may use in visuomotor tasks with multiple competing goals. We show how human and animal strategies in the presence of competing goals can be expressed as a weighted mixture of multiple control policies, each of which produces a sequence of actions associated with a specific goal. We evaluate the performance of the proposed framework in a series of simulated reaching and saccade tasks with multiple targets in environments with and without presence of obstacles. The results show that the proposed model can qualitatively predict many aspects of human/animal strategies in goal-directed movements."
Unidimensionality of sequential effects in human response times,Evidence is present showing that sequential effects occurring in 2-alternative forced-choice tasks are in fact a unidimensional phenomenon. Individual differences data from four different experiments was analyzed a multidimensional scaling analysis was performed on the distances between individual results. We found that the sequential effects described previously in the literature fit well in single dimension space. A dynamic belief model fit well to data from individual subjects and its parameters correlated strongly with sequential effects measures and distances within the one-dimensional space identified.
Unidimensionality of sequential effects in human response times,Evidence is present showing that sequential effects occurring in 2-alternative forced-choice tasks are in fact a unidimensional phenomenon. Individual differences data from four different experiments was analyzed a multidimensional scaling analysis was performed on the distances between individual results. We found that the sequential effects described previously in the literature fit well in single dimension space. A dynamic belief model fit well to data from individual subjects and its parameters correlated strongly with sequential effects measures and distances within the one-dimensional space identified.
Unidimensionality of sequential effects in human response times,Evidence is present showing that sequential effects occurring in 2-alternative forced-choice tasks are in fact a unidimensional phenomenon. Individual differences data from four different experiments was analyzed a multidimensional scaling analysis was performed on the distances between individual results. We found that the sequential effects described previously in the literature fit well in single dimension space. A dynamic belief model fit well to data from individual subjects and its parameters correlated strongly with sequential effects measures and distances within the one-dimensional space identified.
Unidimensionality of sequential effects in human response times,Evidence is present showing that sequential effects occurring in 2-alternative forced-choice tasks are in fact a unidimensional phenomenon. Individual differences data from four different experiments was analyzed a multidimensional scaling analysis was performed on the distances between individual results. We found that the sequential effects described previously in the literature fit well in single dimension space. A dynamic belief model fit well to data from individual subjects and its parameters correlated strongly with sequential effects measures and distances within the one-dimensional space identified.
Graphical Model Selection Using Junction Trees: Decompositions and Active Learning,"This paper proposes a framework for decomposing the undirected graphical model selection (UGMS) problem into multiple subproblems over clusters and separators of a junction tree.  Under certain conditions, we show that the junction tree framework significantly weakens the sufficient conditions on the number of observations required for high-dimensional consistent graph estimation.  When the conditions on the graphical model do not hold, we recover the standard conditions for high-dimensional consistency.  This motivates the use of our framework as a wrapper around algorithms for more accurate graph estimation.  Further, we show that the subproblems over the clusters and separators can be solved independently, which allows for using different regularization parameters or different UGMS algorithms to learn different parts of the graph.  Finally, the junction tree framework motivates active learning for UGMS that sequentially draws observations from the graphical model based on prior observations.  In the high-dimensional setting, we identify conditions under which the sufficient conditions on the number of scalar observations needed for an active algorithm is significantly less than that needed for a non-active algorithm.  Intuitively, the active learning algorithm draws more observations from parts of the graph that are difficult to learn and less measurements from parts of the graph that are easy to learn.  Our numerical results clearly identify the advantages of using the junction tree framework for both non-active and active learning for UGMS."
Graphical Model Selection Using Junction Trees: Decompositions and Active Learning,"This paper proposes a framework for decomposing the undirected graphical model selection (UGMS) problem into multiple subproblems over clusters and separators of a junction tree.  Under certain conditions, we show that the junction tree framework significantly weakens the sufficient conditions on the number of observations required for high-dimensional consistent graph estimation.  When the conditions on the graphical model do not hold, we recover the standard conditions for high-dimensional consistency.  This motivates the use of our framework as a wrapper around algorithms for more accurate graph estimation.  Further, we show that the subproblems over the clusters and separators can be solved independently, which allows for using different regularization parameters or different UGMS algorithms to learn different parts of the graph.  Finally, the junction tree framework motivates active learning for UGMS that sequentially draws observations from the graphical model based on prior observations.  In the high-dimensional setting, we identify conditions under which the sufficient conditions on the number of scalar observations needed for an active algorithm is significantly less than that needed for a non-active algorithm.  Intuitively, the active learning algorithm draws more observations from parts of the graph that are difficult to learn and less measurements from parts of the graph that are easy to learn.  Our numerical results clearly identify the advantages of using the junction tree framework for both non-active and active learning for UGMS."
Optimally Learning Hashing Functions Using Column Generation,"Fast nearest neighbor search is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learningdata-dependent hashing functions using machine learning techniques have been developed. In this work, we propose a column generation based method for learninghashing functions on the basis of proximity comparison information. Given a set of examples of proximity comparisons among triples of data points the methodlearns hashing functions which preserve the relative distances between them as well as possible within the large-margin learning framework. The learning procedureis implemented using column generation and hence is named CGHash. At each iteration of column generation procedure the best hashing function is selected. Unlike other recent hashing methods, our method generalizes to newpoints naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposedmethod learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on publicly available datasets."
Optimally Learning Hashing Functions Using Column Generation,"Fast nearest neighbor search is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learningdata-dependent hashing functions using machine learning techniques have been developed. In this work, we propose a column generation based method for learninghashing functions on the basis of proximity comparison information. Given a set of examples of proximity comparisons among triples of data points the methodlearns hashing functions which preserve the relative distances between them as well as possible within the large-margin learning framework. The learning procedureis implemented using column generation and hence is named CGHash. At each iteration of column generation procedure the best hashing function is selected. Unlike other recent hashing methods, our method generalizes to newpoints naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposedmethod learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on publicly available datasets."
Optimally Learning Hashing Functions Using Column Generation,"Fast nearest neighbor search is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learningdata-dependent hashing functions using machine learning techniques have been developed. In this work, we propose a column generation based method for learninghashing functions on the basis of proximity comparison information. Given a set of examples of proximity comparisons among triples of data points the methodlearns hashing functions which preserve the relative distances between them as well as possible within the large-margin learning framework. The learning procedureis implemented using column generation and hence is named CGHash. At each iteration of column generation procedure the best hashing function is selected. Unlike other recent hashing methods, our method generalizes to newpoints naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposedmethod learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on publicly available datasets."
Optimally Learning Hashing Functions Using Column Generation,"Fast nearest neighbor search is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learningdata-dependent hashing functions using machine learning techniques have been developed. In this work, we propose a column generation based method for learninghashing functions on the basis of proximity comparison information. Given a set of examples of proximity comparisons among triples of data points the methodlearns hashing functions which preserve the relative distances between them as well as possible within the large-margin learning framework. The learning procedureis implemented using column generation and hence is named CGHash. At each iteration of column generation procedure the best hashing function is selected. Unlike other recent hashing methods, our method generalizes to newpoints naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposedmethod learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on publicly available datasets."
On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization,"The ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge. This paper presents a new reinforcement-learning algorithm, called iKBSF, which extends the benefits of kernel-based learning to the on-line scenario. As a kernel-based method, the proposed algorithm is stable and has good convergence properties. However, unlike other similar algorithms,iKBSF's space complexity is independent of the number of sample transitions, and as a result it can process an arbitrary amount of data. We present theoretical results showing that iKBSF can approximate (to any level of accuracy) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator. In order to show the effectiveness of the proposed algorithm in practice, we apply iKBSF to the challenging three-pole balancing task, where the ability to process a large number of transitions is crucial for achieving a high success rate."
On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization,"The ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge. This paper presents a new reinforcement-learning algorithm, called iKBSF, which extends the benefits of kernel-based learning to the on-line scenario. As a kernel-based method, the proposed algorithm is stable and has good convergence properties. However, unlike other similar algorithms,iKBSF's space complexity is independent of the number of sample transitions, and as a result it can process an arbitrary amount of data. We present theoretical results showing that iKBSF can approximate (to any level of accuracy) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator. In order to show the effectiveness of the proposed algorithm in practice, we apply iKBSF to the challenging three-pole balancing task, where the ability to process a large number of transitions is crucial for achieving a high success rate."
Recovery Guarantees of Augmented Trace Norm Models in Tensor Recovery,"This paper studies the recovery guarantees of the models of minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ where $\mathcal{X}$ is a tensor and $\|\mathcal{X}\|_*$ and $\|\mathcal{X}\|_F$ are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors.In particular, they enjoy exact guarantees similar to those known for minimizing$\|\mathcal{X}\|_*$ under the conditions on the sensing operator such as its null-space property, restrictedisometry property, or spherical section property. To recover a low-rank tensor$\mathcal{X}^0$, minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ returns the same solution as minimizing $\|\mathcal{X}\|_*$ almost whenever$\alpha\geq10\mathop {\max}\limits_{i}\|X^0_{(i)}\|_2$."
Recovery Guarantees of Augmented Trace Norm Models in Tensor Recovery,"This paper studies the recovery guarantees of the models of minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ where $\mathcal{X}$ is a tensor and $\|\mathcal{X}\|_*$ and $\|\mathcal{X}\|_F$ are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors.In particular, they enjoy exact guarantees similar to those known for minimizing$\|\mathcal{X}\|_*$ under the conditions on the sensing operator such as its null-space property, restrictedisometry property, or spherical section property. To recover a low-rank tensor$\mathcal{X}^0$, minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ returns the same solution as minimizing $\|\mathcal{X}\|_*$ almost whenever$\alpha\geq10\mathop {\max}\limits_{i}\|X^0_{(i)}\|_2$."
Recovery Guarantees of Augmented Trace Norm Models in Tensor Recovery,"This paper studies the recovery guarantees of the models of minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ where $\mathcal{X}$ is a tensor and $\|\mathcal{X}\|_*$ and $\|\mathcal{X}\|_F$ are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors.In particular, they enjoy exact guarantees similar to those known for minimizing$\|\mathcal{X}\|_*$ under the conditions on the sensing operator such as its null-space property, restrictedisometry property, or spherical section property. To recover a low-rank tensor$\mathcal{X}^0$, minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ returns the same solution as minimizing $\|\mathcal{X}\|_*$ almost whenever$\alpha\geq10\mathop {\max}\limits_{i}\|X^0_{(i)}\|_2$."
Recovery Guarantees of Augmented Trace Norm Models in Tensor Recovery,"This paper studies the recovery guarantees of the models of minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ where $\mathcal{X}$ is a tensor and $\|\mathcal{X}\|_*$ and $\|\mathcal{X}\|_F$ are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors.In particular, they enjoy exact guarantees similar to those known for minimizing$\|\mathcal{X}\|_*$ under the conditions on the sensing operator such as its null-space property, restrictedisometry property, or spherical section property. To recover a low-rank tensor$\mathcal{X}^0$, minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ returns the same solution as minimizing $\|\mathcal{X}\|_*$ almost whenever$\alpha\geq10\mathop {\max}\limits_{i}\|X^0_{(i)}\|_2$."
Joint Modeling of a Matrix with Associated Text via Latent Binary Features,"A new methodology is developed for joint analysis of a matrix and accompanyingdocuments, with the documents associated with the matrix rows/columns. Thedocuments are modeled with a focused topic model, inferring latent binary features(topics) for each document. A new matrix decomposition is developed, withlatent binary features associated with the rows/columns, and with imposition of alow-rank constraint. The matrix decomposition and topic model are coupled bysharing the latent binary feature vectors associated with each. The model is appliedto roll-call data, with the associated documents defined by the legislation.State-of-the-art results are manifested for prediction of votes on a new piece oflegislation, based only on the observed text legislation. The coupling of the textand legislation is also demonstrated to yield insight into the properties of the matrixdecomposition for roll-call data."
Joint Modeling of a Matrix with Associated Text via Latent Binary Features,"A new methodology is developed for joint analysis of a matrix and accompanyingdocuments, with the documents associated with the matrix rows/columns. Thedocuments are modeled with a focused topic model, inferring latent binary features(topics) for each document. A new matrix decomposition is developed, withlatent binary features associated with the rows/columns, and with imposition of alow-rank constraint. The matrix decomposition and topic model are coupled bysharing the latent binary feature vectors associated with each. The model is appliedto roll-call data, with the associated documents defined by the legislation.State-of-the-art results are manifested for prediction of votes on a new piece oflegislation, based only on the observed text legislation. The coupling of the textand legislation is also demonstrated to yield insight into the properties of the matrixdecomposition for roll-call data."
Optimally fuzzy scale-free memory buffer,"Any system with the ability to learn patterns  from a time series of stimuli and predict the subsequent stimulus at each moment, should have  a buffer storing the stimuli from the recent past. In cases where the external environment generating the time series has a fixed scale, the buffer can be a simple shift register---a moving window of finite width extending into the past. However, such a traditional buffer is inappropriate for signals with scale-free long range correlations, which are found in many physical environments. We argue  for a scale-free fuzzy buffer that optimally sacrifices accuracy  in favor of  capacity to represent long time scales. Here we describe a neuro-cognitive model of internal time that satisfies these constraints."
Optimally fuzzy scale-free memory buffer,"Any system with the ability to learn patterns  from a time series of stimuli and predict the subsequent stimulus at each moment, should have  a buffer storing the stimuli from the recent past. In cases where the external environment generating the time series has a fixed scale, the buffer can be a simple shift register---a moving window of finite width extending into the past. However, such a traditional buffer is inappropriate for signals with scale-free long range correlations, which are found in many physical environments. We argue  for a scale-free fuzzy buffer that optimally sacrifices accuracy  in favor of  capacity to represent long time scales. Here we describe a neuro-cognitive model of internal time that satisfies these constraints."
Solving Relational MDPs with Exogenous Events and Additive Rewards,"We formalize a simple but natural subclass of service domains for relational planning problems with object-centered independent exogenous events and additive rewards, capturing, for example, problems in inventory control and fire and rescue operations. Focusing on this subclass, we then present the first complete symbolic solution for stochastic planning problems in relational domains that is able to handle exogenous events and additive rewards, and is independent of domain size.  Our planning algorithm provides a lower bound approximation on the optimal solution given by the true value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation that can represent and manipulate real-valued functions over relational world states. A preliminary experimental evaluation demonstrates the validity and potential of our approach.  "
Solving Relational MDPs with Exogenous Events and Additive Rewards,"We formalize a simple but natural subclass of service domains for relational planning problems with object-centered independent exogenous events and additive rewards, capturing, for example, problems in inventory control and fire and rescue operations. Focusing on this subclass, we then present the first complete symbolic solution for stochastic planning problems in relational domains that is able to handle exogenous events and additive rewards, and is independent of domain size.  Our planning algorithm provides a lower bound approximation on the optimal solution given by the true value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation that can represent and manipulate real-valued functions over relational world states. A preliminary experimental evaluation demonstrates the validity and potential of our approach.  "
Solving Relational MDPs with Exogenous Events and Additive Rewards,"We formalize a simple but natural subclass of service domains for relational planning problems with object-centered independent exogenous events and additive rewards, capturing, for example, problems in inventory control and fire and rescue operations. Focusing on this subclass, we then present the first complete symbolic solution for stochastic planning problems in relational domains that is able to handle exogenous events and additive rewards, and is independent of domain size.  Our planning algorithm provides a lower bound approximation on the optimal solution given by the true value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation that can represent and manipulate real-valued functions over relational world states. A preliminary experimental evaluation demonstrates the validity and potential of our approach.  "
Solving Relational MDPs with Exogenous Events and Additive Rewards,"We formalize a simple but natural subclass of service domains for relational planning problems with object-centered independent exogenous events and additive rewards, capturing, for example, problems in inventory control and fire and rescue operations. Focusing on this subclass, we then present the first complete symbolic solution for stochastic planning problems in relational domains that is able to handle exogenous events and additive rewards, and is independent of domain size.  Our planning algorithm provides a lower bound approximation on the optimal solution given by the true value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation that can represent and manipulate real-valued functions over relational world states. A preliminary experimental evaluation demonstrates the validity and potential of our approach.  "
Bayesian Meta-classifier Learning from Biased Multiple Predictions,"We propose a probabilistic generative model for combining the predictions of multiple classifiers to form a meta-classifier that provides high classification accuracy.  The key feature of the model is the introduction of a latent variable that can identify whether the classifier in the ensemble is {\it related or unrelated} to each of the classes.  Our modeling is motivated by the idea that classifiers which provide incorrect predictions but are informative in terms of discriminating one class from others can also be effectively utilized for learning a meta-classifier.  The proposed meta-classifier learningscheme is particularly useful when the performance of each classifier is biased toward some specific class.  We perform empirical evaluations using both synthetic and real data. As a real case study of a combination of biased classifiers, we show its application to the high-level recognition of actual nursing activity by using accelerometers."
Optimized dictionary based sparse representation for robust speaker recognition,"The mismatch between the training and the testing environments greatly degrades the performance of speaker recognition. Although many robust techniques have been proposed, speaker recognition in mismatch condition is still a challenge. To solve this problem, we propose an optimized dictionary based sparse representation for robust speaker recognition. To this end, we first train a speech dictionary and a noise dictionary, and concatenate them for sparse representation; then design an optimization algorithm to reduce the mutual coherence between the two learned dictionaries; after that, utilize mixture k-means to model speaker corresponding to sparse feature; and finally, present a distance divergence to measure the similarity. Compared with the standard Universal Background Model and Gaussian Mixture Models based speaker recognition, our preliminary experiments show that the proposed recognition framework consistently improve the robustness in mismatched condition."
Optimized dictionary based sparse representation for robust speaker recognition,"The mismatch between the training and the testing environments greatly degrades the performance of speaker recognition. Although many robust techniques have been proposed, speaker recognition in mismatch condition is still a challenge. To solve this problem, we propose an optimized dictionary based sparse representation for robust speaker recognition. To this end, we first train a speech dictionary and a noise dictionary, and concatenate them for sparse representation; then design an optimization algorithm to reduce the mutual coherence between the two learned dictionaries; after that, utilize mixture k-means to model speaker corresponding to sparse feature; and finally, present a distance divergence to measure the similarity. Compared with the standard Universal Background Model and Gaussian Mixture Models based speaker recognition, our preliminary experiments show that the proposed recognition framework consistently improve the robustness in mismatched condition."
Optimized dictionary based sparse representation for robust speaker recognition,"The mismatch between the training and the testing environments greatly degrades the performance of speaker recognition. Although many robust techniques have been proposed, speaker recognition in mismatch condition is still a challenge. To solve this problem, we propose an optimized dictionary based sparse representation for robust speaker recognition. To this end, we first train a speech dictionary and a noise dictionary, and concatenate them for sparse representation; then design an optimization algorithm to reduce the mutual coherence between the two learned dictionaries; after that, utilize mixture k-means to model speaker corresponding to sparse feature; and finally, present a distance divergence to measure the similarity. Compared with the standard Universal Background Model and Gaussian Mixture Models based speaker recognition, our preliminary experiments show that the proposed recognition framework consistently improve the robustness in mismatched condition."
Ellipsoidal Multiple Instance Learning,"We propose a large margin method for learning with ellipsoids that isparticularly suited to asymmetric detection tasks such as multipleinstance learning (MIL). In contrast to current approaches for solving MILthat involve complex computationally expensive algorithms, ourapproach is a direct geometric one.We consider the distance between ellipsoidsand the hyperplane, generalising the standard support vector machine.Negative bags in MIL contain all negativeinstances, and we treat them akin to the robust optimisationframework. However, our method allows positive bags to cross themargin, since it is not known which instances within are positive. We derive agradient descent approach to solve the resulting bilevel program, andapply it to several MIL datasets. Surprisingly, our geometric approachresults in state of the art performance."
Thompson Sampling for Complex Online Problems,"We study stochastic multi-armed bandit settings with complex actionsover the basic arms, where the decision maker has to select a subsetof the basic arms or a partition of the basic arms at every round(rather than only selecting a single basic arm). The reward of thecomplex action is some function of the basic arms' rewards, and thefeedback observed may not necessarily be the reward per-arm. Forexample, when the complex action is a subset of the arms, we may onlyobserve the total reward or the maximum reward over the chosensubset. We use Thompson sampling to decide which complex action toselect. We prove a general theorem showing that a variant of Thompsonsampling with uniform exploration obtains logarithmic regret, and weshow how the regret depends explicitly on the information gain fromthe observations. As applications, we obtain several corollaries forspecific complex bandit problem setups with improved rates. Usingparticle filters for computing posterior distributions, we devise andsimulate Thompson-sampling algorithms for subset selection andjob-scheduling problems."
Mixability in Statistical Learning,"Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability."
Mixability in Statistical Learning,"Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability."
Gradient-Boosted Adaptive Codes for Classification and Embedding,"Discriminative classifiers pursue simultaneous embeddings of observations and classes in a shared space such that the embedding of each observation is more similar to the embedding of its associated class than to that of any other class. Boosting-based classifiers are often partitioned into two sets: those which assume a fixed embedding of the classes and those which concurrently learn to embed both the observations and classes. The classifier we introduce falls into the latter camp; it learns all dimensions of the observation and class embeddings concurrently. With L1 regularization applied to the class embedding, our method outperforms existing boosted classifiers on standard benchmarks. Using Euclidean distance to measure class-observation similarity, rather than the typical dot-product, and with additional regularization controlling the spread of each class' embedded representation, our method also produces meaningful embeddings of labeled data. We begin our presentation by recapitulating boosting as functional gradient descent and then examining a weakness in one frequently cited theorem concerning the convergence of gradient-based boosting."
Bandit Market Makers,"We propose a flexible framework for profit-seeking market-making, using a sequence of cost-function based automated market-makers with bandit learning algorithms. We do this by considering the magnitude to which a cost-function extends beyond the simplex as a bandit arm, and the minimum-expected profits consistent with a no-arbitrage condition as the rewards. This allows for the creation of market-makers that can adjust bid-asks spreads dynamically, maximising worst-case-expected profits. "
Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Hamiltonian Monte Carlo Sampling,"This paper proposes a novel framework for multigroup shape analysis relying on a hierarchical graphical statistical model on shapes within a population. Under the proposed hierarchical model, individual shapes are represented as pointsets, derived from their group shape model. Similarly, each group shape model is derived from a single population shape model. The hierarchical model follows the natural organization of population data and enables comparison of shape models between groups, via hypothesis testing, by proving a common frame of reference for the shape models. Unlike typical approaches for shape modeling, the proposed model is a generative statistical model that defines a joint distribution of objectboundary data and the model variables in the hierarchical model. Furthermore, it naturally enforces optimal correspondences during the process of model fitting and thereby subsumes the correspondence problem. The proposed optimization framework employs an expectation maximization (EM) algorithm that treats the individual and group shape variables as hidden random variables and integrates them out before estimating the parameters (population mean and variance and the group variances). The underpinning of the EM algorithm is the sampling of shapes from their posterior distribution, for which the paper exploits a highly efficient scheme based on Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted hierarchical model to perform hypothesis testing for comparison between pairs of groups using permutation testing. The paper validates the proposed framework on simulated data and demonstrates results on real data."
Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Hamiltonian Monte Carlo Sampling,"This paper proposes a novel framework for multigroup shape analysis relying on a hierarchical graphical statistical model on shapes within a population. Under the proposed hierarchical model, individual shapes are represented as pointsets, derived from their group shape model. Similarly, each group shape model is derived from a single population shape model. The hierarchical model follows the natural organization of population data and enables comparison of shape models between groups, via hypothesis testing, by proving a common frame of reference for the shape models. Unlike typical approaches for shape modeling, the proposed model is a generative statistical model that defines a joint distribution of objectboundary data and the model variables in the hierarchical model. Furthermore, it naturally enforces optimal correspondences during the process of model fitting and thereby subsumes the correspondence problem. The proposed optimization framework employs an expectation maximization (EM) algorithm that treats the individual and group shape variables as hidden random variables and integrates them out before estimating the parameters (population mean and variance and the group variances). The underpinning of the EM algorithm is the sampling of shapes from their posterior distribution, for which the paper exploits a highly efficient scheme based on Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted hierarchical model to perform hypothesis testing for comparison between pairs of groups using permutation testing. The paper validates the proposed framework on simulated data and demonstrates results on real data."
Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Hamiltonian Monte Carlo Sampling,"This paper proposes a novel framework for multigroup shape analysis relying on a hierarchical graphical statistical model on shapes within a population. Under the proposed hierarchical model, individual shapes are represented as pointsets, derived from their group shape model. Similarly, each group shape model is derived from a single population shape model. The hierarchical model follows the natural organization of population data and enables comparison of shape models between groups, via hypothesis testing, by proving a common frame of reference for the shape models. Unlike typical approaches for shape modeling, the proposed model is a generative statistical model that defines a joint distribution of objectboundary data and the model variables in the hierarchical model. Furthermore, it naturally enforces optimal correspondences during the process of model fitting and thereby subsumes the correspondence problem. The proposed optimization framework employs an expectation maximization (EM) algorithm that treats the individual and group shape variables as hidden random variables and integrates them out before estimating the parameters (population mean and variance and the group variances). The underpinning of the EM algorithm is the sampling of shapes from their posterior distribution, for which the paper exploits a highly efficient scheme based on Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted hierarchical model to perform hypothesis testing for comparison between pairs of groups using permutation testing. The paper validates the proposed framework on simulated data and demonstrates results on real data."
Sparsest Combination under Linear Transformation,"We consider the following signal recovery problem: given a measurement matrix $\Phi\in \mathbb{R}^{n\times p}$ and a noisy observation vector $c\in \mathbb{R}^{n}$ constructed from $c = \Phi \theta^* + \epsilon$ where $\epsilon\in \mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal $\theta^*$ if $D\theta^*$ is sparse where $D\in\mathbb{R}^{m\times p}$? One natural method using convex optimization is to solve the following problem: $$\min_{\theta}~{1\over 2}\|\Phi\theta - c\|^2 + \lambda\|D\theta\|_1.$$ This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix $\Phi$ is a Gaussian random matrix:1) in the noiseless case, if the condition number of $D$ is bounded and the measurement number $n\geq \Omega(s\log(p))$ where $s$ is the sparsity number then the true solution can be recovered with high probability; and2) in the noisy case if the condition number of $D$ is bounded and the measurement increases faster than $s\log (p)$ (that is, $s\log(p)=o(n)$), the estimate error converges to zero with probability 1 when $p$ and $s$ go to infinity. Our results are consistent with those for the special case $D=\bold{I}_{p\times p}$ (equivalently LASSO) and improve the existing analysis for the same formulation. The condition number of $D$ plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if $m/p$ is larger than a certain constant. Numerical simulations are consistent with our theoretical results."
Sparsest Combination under Linear Transformation,"We consider the following signal recovery problem: given a measurement matrix $\Phi\in \mathbb{R}^{n\times p}$ and a noisy observation vector $c\in \mathbb{R}^{n}$ constructed from $c = \Phi \theta^* + \epsilon$ where $\epsilon\in \mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal $\theta^*$ if $D\theta^*$ is sparse where $D\in\mathbb{R}^{m\times p}$? One natural method using convex optimization is to solve the following problem: $$\min_{\theta}~{1\over 2}\|\Phi\theta - c\|^2 + \lambda\|D\theta\|_1.$$ This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix $\Phi$ is a Gaussian random matrix:1) in the noiseless case, if the condition number of $D$ is bounded and the measurement number $n\geq \Omega(s\log(p))$ where $s$ is the sparsity number then the true solution can be recovered with high probability; and2) in the noisy case if the condition number of $D$ is bounded and the measurement increases faster than $s\log (p)$ (that is, $s\log(p)=o(n)$), the estimate error converges to zero with probability 1 when $p$ and $s$ go to infinity. Our results are consistent with those for the special case $D=\bold{I}_{p\times p}$ (equivalently LASSO) and improve the existing analysis for the same formulation. The condition number of $D$ plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if $m/p$ is larger than a certain constant. Numerical simulations are consistent with our theoretical results."
Sparsest Combination under Linear Transformation,"We consider the following signal recovery problem: given a measurement matrix $\Phi\in \mathbb{R}^{n\times p}$ and a noisy observation vector $c\in \mathbb{R}^{n}$ constructed from $c = \Phi \theta^* + \epsilon$ where $\epsilon\in \mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal $\theta^*$ if $D\theta^*$ is sparse where $D\in\mathbb{R}^{m\times p}$? One natural method using convex optimization is to solve the following problem: $$\min_{\theta}~{1\over 2}\|\Phi\theta - c\|^2 + \lambda\|D\theta\|_1.$$ This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix $\Phi$ is a Gaussian random matrix:1) in the noiseless case, if the condition number of $D$ is bounded and the measurement number $n\geq \Omega(s\log(p))$ where $s$ is the sparsity number then the true solution can be recovered with high probability; and2) in the noisy case if the condition number of $D$ is bounded and the measurement increases faster than $s\log (p)$ (that is, $s\log(p)=o(n)$), the estimate error converges to zero with probability 1 when $p$ and $s$ go to infinity. Our results are consistent with those for the special case $D=\bold{I}_{p\times p}$ (equivalently LASSO) and improve the existing analysis for the same formulation. The condition number of $D$ plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if $m/p$ is larger than a certain constant. Numerical simulations are consistent with our theoretical results."
Semi-Supervised Learning with Probabilistic Smoothness on Graphs,"We study graph-based semi-supervised learning by exploiting the smoothness constraint with respect to the intrinsic structure that exists among labeled and unlabeled data. Unlike previous works that define the smoothness constraint by different cost functions, we formulate it under a probabilistic framework instead. Interestingly, our probabilistic smoothness constraint can be tied to and hence justifies a new cost function. Based on this probabilistic smoothness constraint, we further derive an algorithm PSmooth for semi-supervised learning, which can also be interpreted as a random walk on graphs. Finally, our experiments show that PSmooth consistently outperforms existing state-of-the-art algorithms on various public benchmark datasets."
Scalable Inference of Overlapping Communities,"We develop a scalable algorithm for posterior inference of overlappingcommunities in large networks.  Our algorithm is based on stochasticvariational inference in the mixed-membership stochastic blockmodel.It naturally interleaves subsampling the network with estimating itscommunity structure.  We apply our algorithm on ten large, real-worldnetworks with up to 60,000 nodes. It converges several orders ofmagnitude faster than the state-of-the-art algorithm for MMSB, findshundreds of communities in large real-world networks, and detects thetrue communities in 280 benchmark networks with equal or betteraccuracy compared to other scalable algorithms."
Scalable Inference of Overlapping Communities,"We develop a scalable algorithm for posterior inference of overlappingcommunities in large networks.  Our algorithm is based on stochasticvariational inference in the mixed-membership stochastic blockmodel.It naturally interleaves subsampling the network with estimating itscommunity structure.  We apply our algorithm on ten large, real-worldnetworks with up to 60,000 nodes. It converges several orders ofmagnitude faster than the state-of-the-art algorithm for MMSB, findshundreds of communities in large real-world networks, and detects thetrue communities in 280 benchmark networks with equal or betteraccuracy compared to other scalable algorithms."
Scalable Inference of Overlapping Communities,"We develop a scalable algorithm for posterior inference of overlappingcommunities in large networks.  Our algorithm is based on stochasticvariational inference in the mixed-membership stochastic blockmodel.It naturally interleaves subsampling the network with estimating itscommunity structure.  We apply our algorithm on ten large, real-worldnetworks with up to 60,000 nodes. It converges several orders ofmagnitude faster than the state-of-the-art algorithm for MMSB, findshundreds of communities in large real-world networks, and detects thetrue communities in 280 benchmark networks with equal or betteraccuracy compared to other scalable algorithms."
Scalable Inference of Overlapping Communities,"We develop a scalable algorithm for posterior inference of overlappingcommunities in large networks.  Our algorithm is based on stochasticvariational inference in the mixed-membership stochastic blockmodel.It naturally interleaves subsampling the network with estimating itscommunity structure.  We apply our algorithm on ten large, real-worldnetworks with up to 60,000 nodes. It converges several orders ofmagnitude faster than the state-of-the-art algorithm for MMSB, findshundreds of communities in large real-world networks, and detects thetrue communities in 280 benchmark networks with equal or betteraccuracy compared to other scalable algorithms."
Scalable Inference of Overlapping Communities,"We develop a scalable algorithm for posterior inference of overlappingcommunities in large networks.  Our algorithm is based on stochasticvariational inference in the mixed-membership stochastic blockmodel.It naturally interleaves subsampling the network with estimating itscommunity structure.  We apply our algorithm on ten large, real-worldnetworks with up to 60,000 nodes. It converges several orders ofmagnitude faster than the state-of-the-art algorithm for MMSB, findshundreds of communities in large real-world networks, and detects thetrue communities in 280 benchmark networks with equal or betteraccuracy compared to other scalable algorithms."
Online L1-Dictionary Learning with Application to Novel Document Detection,"Given their pervasive use, social media, such as Twitter, have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge, we introduce the problem of online L1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the L1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data, shows that this online L1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results. Our algorithm for online L1-dictionary learning could be of independent interest."
Online L1-Dictionary Learning with Application to Novel Document Detection,"Given their pervasive use, social media, such as Twitter, have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge, we introduce the problem of online L1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the L1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data, shows that this online L1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results. Our algorithm for online L1-dictionary learning could be of independent interest."
A systematic approach to extracting semantic information from functional MRI data,"This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure."
Rational inference of relative preferences,"Statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions. However, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented. In this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects. Complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function. "
Recognizing Human Activities from Incompletely Observed Videos,"In this paper, we present a novel method for handling the problem of recognizing human activities from incompletely observed videos. Compared with the similar problem of human activity prediction from unfinished activities [12], in an incompletely observed video an un-observed subsequence of frames may occur any time with any duration and yield a temporal gap in the video. In practice, incompletely observed videos may occur when the video signal drops off, when camera or objects of interest are occluded, or when videos are composited from multiple sources. In this paper, we formulate the problem of human activity recognition from incompletely observed videos in a probabilistic framework. In this framework, we take a set of training video samples (completely observed) of each activity class as the basis, and then use sparse coding to derive the likelihood that an incompletely observed test video belongs to a certain activity class. Furthermore, we propose to divide each activity into multiple temporal stages, apply sparse coding to derive the activity likelihood at each stage, and finally combine the likelihoods at each stage to achieve a global posterior for the activity. We evaluate the proposed method on both the widely used UT-Interaction human activity dataset and a new human activity dataset selected from the Year-1 corpus of the DARPA Mind's Eye program [4]. For the new DARPA dataset, both the activities and the videos show very large within-class temporal, spatial, and background variation. Our results demonstrate that the proposed method performs substantially better than several competing methods on both datasets."
Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand,"In many large economic markets, goods are sold through sequential auctions. Such domains include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions. In this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown."
Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand,"In many large economic markets, goods are sold through sequential auctions. Such domains include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions. In this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown."
Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand,"In many large economic markets, goods are sold through sequential auctions. Such domains include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions. In this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown."
A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation,"In this paper, we consider the $\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables. We further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure. As an example, a recent state-of-the-art method, QUICrequires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem."
"Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders","We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ is chosen uniformly at random from $\{+1, -1\}^n$, $\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers $A$ and $\Sigma$ up to an additive $\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \epsilon$. To accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search."
"Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders","We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ is chosen uniformly at random from $\{+1, -1\}^n$, $\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers $A$ and $\Sigma$ up to an additive $\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \epsilon$. To accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search."
"Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders","We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ is chosen uniformly at random from $\{+1, -1\}^n$, $\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers $A$ and $\Sigma$ up to an additive $\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \epsilon$. To accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search."
Approximate Factored Real-time Dynamic Programming,"Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when there is information about the initial state. Traditional RTDP approaches work best on problems with sparse transition matrices where they can often efficiently achieve e-convergence without visiting all states; however, the advantages of traditional RTDP are often lost on problems with dense transition matrices where most states are reachable in one step (which is the case of a variety of control problems with exogenous events), as we demonstrate in this paper. One approach to overcome this caveat is to exploit the regularities in the domain dynamics, reward and value function throughout factored representation and calculations. In this paper, we propose a new  factored RTDP algorithm, called FactRTDP, and its approximate version, called aFactRTDP, which is the first straight forward factored version of enumerative RTDP, i.e., without performing generalized updates. Experiments show that these new algorithms can deal with dense transition matrices and have good online behavior when compared to the best probabilistic planning systems, without engineering optimizations, but by simply exploiting factored backups."
MAP Inference on Million Node Graphical Models: KL-divergence based Alternating Direction Method,"We consider the problem of maximum a posteriori (MAP) inference in graphical models with millions of nodes. We present a parallel primal MAP inference algorithm called KL-ADM based on two ideas: tree-decomposition of a graph, and the alternating direction method (ADM). However, unlike the standard ADM, we use an inexact ADM augmented with a Kullback-Leibler (KL) divergence based regularization. Theunusual modification leads to an efficient iterative algorithm while avoiding double-loops. We rigorously prove global convergence of the KL-ADM algorithm. The proposed algorithm is extensively evaluated on simulated datasets and compares favorably to existing approximate MAP inference algorithms. We also implement parallel KL-ADM using Open MPI and the experimental results on a drought detection problem withmore than 7 million variables demonstrate that the algorithm scales nicely in the multicore setting."
MAP Inference on Million Node Graphical Models: KL-divergence based Alternating Direction Method,"We consider the problem of maximum a posteriori (MAP) inference in graphical models with millions of nodes. We present a parallel primal MAP inference algorithm called KL-ADM based on two ideas: tree-decomposition of a graph, and the alternating direction method (ADM). However, unlike the standard ADM, we use an inexact ADM augmented with a Kullback-Leibler (KL) divergence based regularization. Theunusual modification leads to an efficient iterative algorithm while avoiding double-loops. We rigorously prove global convergence of the KL-ADM algorithm. The proposed algorithm is extensively evaluated on simulated datasets and compares favorably to existing approximate MAP inference algorithms. We also implement parallel KL-ADM using Open MPI and the experimental results on a drought detection problem withmore than 7 million variables demonstrate that the algorithm scales nicely in the multicore setting."
MAP Inference on Million Node Graphical Models: KL-divergence based Alternating Direction Method,"We consider the problem of maximum a posteriori (MAP) inference in graphical models with millions of nodes. We present a parallel primal MAP inference algorithm called KL-ADM based on two ideas: tree-decomposition of a graph, and the alternating direction method (ADM). However, unlike the standard ADM, we use an inexact ADM augmented with a Kullback-Leibler (KL) divergence based regularization. Theunusual modification leads to an efficient iterative algorithm while avoiding double-loops. We rigorously prove global convergence of the KL-ADM algorithm. The proposed algorithm is extensively evaluated on simulated datasets and compares favorably to existing approximate MAP inference algorithms. We also implement parallel KL-ADM using Open MPI and the experimental results on a drought detection problem withmore than 7 million variables demonstrate that the algorithm scales nicely in the multicore setting."
Augment-and-Conquer Negative Binomial Processes,"By developing augment-and-conquer methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters."
Augment-and-Conquer Negative Binomial Processes,"By developing augment-and-conquer methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters."
Minimization of Continuous Bethe Approximations: A Positive Variation,"We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions.  While existing message passing algorithms define fixed point iterations corresponding to stationary points of the Bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge. For continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as Gaussians with negative variance. Conversely, our approach leverages multiplier methods with well-understood convergence properties,and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random fields, showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation."
Minimization of Continuous Bethe Approximations: A Positive Variation,"We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions.  While existing message passing algorithms define fixed point iterations corresponding to stationary points of the Bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge. For continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as Gaussians with negative variance. Conversely, our approach leverages multiplier methods with well-understood convergence properties,and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random fields, showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation."
Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential ?1-Minimization,"We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where $A_k$ and $e_k$ denote the measurement matrix and noise, respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ \emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity."
Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential ?1-Minimization,"We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where $A_k$ and $e_k$ denote the measurement matrix and noise, respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ \emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity."
A Bias-Variance Analysis of Model-Based Estimation in Reinforcement Learning ,"This paper provides the first bias and variance characterization of the model-based value function estimator in finite-horizon reinforcement learning (RL) problems with discrete state spaces. The closed-formed formulas we derive to estimate the bias and variance rely on an approximation that is exact if the estimates of the transition model, reward model, and value function are normally distributed. These results can be used to characterize performance of RL systems in a wide range of application domains. We are particularly interested in applications concerning resource management domains, and therefore we include experiments demonstrating the use of our estimators to evaluate strategies for population management of animal species. We find the bias/variance estimates produced by our method to be more accurate than those produced by the well-known bootstrap or jackknife estimators. We also compare our results to the bias-variance analysis of Mannor et al. [2007], and show that even in their setting (infinite-horizon, discounted problems), our approach may be preferable."
A Bias-Variance Analysis of Model-Based Estimation in Reinforcement Learning ,"This paper provides the first bias and variance characterization of the model-based value function estimator in finite-horizon reinforcement learning (RL) problems with discrete state spaces. The closed-formed formulas we derive to estimate the bias and variance rely on an approximation that is exact if the estimates of the transition model, reward model, and value function are normally distributed. These results can be used to characterize performance of RL systems in a wide range of application domains. We are particularly interested in applications concerning resource management domains, and therefore we include experiments demonstrating the use of our estimators to evaluate strategies for population management of animal species. We find the bias/variance estimates produced by our method to be more accurate than those produced by the well-known bootstrap or jackknife estimators. We also compare our results to the bias-variance analysis of Mannor et al. [2007], and show that even in their setting (infinite-horizon, discounted problems), our approach may be preferable."
A Bias-Variance Analysis of Model-Based Estimation in Reinforcement Learning ,"This paper provides the first bias and variance characterization of the model-based value function estimator in finite-horizon reinforcement learning (RL) problems with discrete state spaces. The closed-formed formulas we derive to estimate the bias and variance rely on an approximation that is exact if the estimates of the transition model, reward model, and value function are normally distributed. These results can be used to characterize performance of RL systems in a wide range of application domains. We are particularly interested in applications concerning resource management domains, and therefore we include experiments demonstrating the use of our estimators to evaluate strategies for population management of animal species. We find the bias/variance estimates produced by our method to be more accurate than those produced by the well-known bootstrap or jackknife estimators. We also compare our results to the bias-variance analysis of Mannor et al. [2007], and show that even in their setting (infinite-horizon, discounted problems), our approach may be preferable."
Query Complexity of Derivative-Free Optimization,"Derivative Free Optimization (DFO) is attractive when the objective function's derivatives are not available and evaluations are costly.   Moreover, if the function evaluations are noisy, then approximating gradients by finite differences is difficult.  This paper gives quantitative lower bounds on the performance of DFO with noisy function evaluations, exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients. This challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient.  However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions.  A distinctive feature of the algorithm is that it only uses Boolean-valued function comparisons, rather than evaluations.  This makes the algorithm useful in an even wider range of applications, including optimization based on paired comparisons from human subjects, for example.  Remarkably, we show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same."
Query Complexity of Derivative-Free Optimization,"Derivative Free Optimization (DFO) is attractive when the objective function's derivatives are not available and evaluations are costly.   Moreover, if the function evaluations are noisy, then approximating gradients by finite differences is difficult.  This paper gives quantitative lower bounds on the performance of DFO with noisy function evaluations, exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients. This challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient.  However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions.  A distinctive feature of the algorithm is that it only uses Boolean-valued function comparisons, rather than evaluations.  This makes the algorithm useful in an even wider range of applications, including optimization based on paired comparisons from human subjects, for example.  Remarkably, we show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same."
Query Complexity of Derivative-Free Optimization,"Derivative Free Optimization (DFO) is attractive when the objective function's derivatives are not available and evaluations are costly.   Moreover, if the function evaluations are noisy, then approximating gradients by finite differences is difficult.  This paper gives quantitative lower bounds on the performance of DFO with noisy function evaluations, exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients. This challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient.  However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions.  A distinctive feature of the algorithm is that it only uses Boolean-valued function comparisons, rather than evaluations.  This makes the algorithm useful in an even wider range of applications, including optimization based on paired comparisons from human subjects, for example.  Remarkably, we show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same."
Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes,"Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning.  In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics."
Stochastic Optimization of the Variational Bound,"We present an algorithm for performing posterior inference by stochastically optimizing a variational lower bound.  Importantly, this algorithm circumvents symbolic evaluation of the variational lower bound and requires only an unnormalized likelihood function $p(x, y)$, making the benefits of variational inference accessible to casual practitioners.  We compare this algorithm with MCMC and demonstrate that it provides a fast, simple alternative.  We also demonstrate that this opens the door to a variety of variational posteriors which were previously unexplored."
Stochastic Optimization of the Variational Bound,"We present an algorithm for performing posterior inference by stochastically optimizing a variational lower bound.  Importantly, this algorithm circumvents symbolic evaluation of the variational lower bound and requires only an unnormalized likelihood function $p(x, y)$, making the benefits of variational inference accessible to casual practitioners.  We compare this algorithm with MCMC and demonstrate that it provides a fast, simple alternative.  We also demonstrate that this opens the door to a variety of variational posteriors which were previously unexplored."
Active Batch Selection via Convex Relaxations with Guaranteed Performance Bounds,"Batch mode active learning (BMAL) effectively reduces human annotation effort in training a reliable classifier by selecting batches of promising and exemplar instances from large quantities of unlabeled data. In this paper, we propose two novel BMAL algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP hard optimization problem; we then propose two convex relaxations, one based on linear programming (LP) and the other based on semi-definite programming (SDP) to solve the batch selection problem. Finally, a deterministic performance bound is derived for the first relaxation and a probabilistic bound for the second. Our extensive empirical studies on the UCI datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques and also deliver high quality solutions."
Active Batch Selection via Convex Relaxations with Guaranteed Performance Bounds,"Batch mode active learning (BMAL) effectively reduces human annotation effort in training a reliable classifier by selecting batches of promising and exemplar instances from large quantities of unlabeled data. In this paper, we propose two novel BMAL algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP hard optimization problem; we then propose two convex relaxations, one based on linear programming (LP) and the other based on semi-definite programming (SDP) to solve the batch selection problem. Finally, a deterministic performance bound is derived for the first relaxation and a probabilistic bound for the second. Our extensive empirical studies on the UCI datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques and also deliver high quality solutions."
Active Batch Selection via Convex Relaxations with Guaranteed Performance Bounds,"Batch mode active learning (BMAL) effectively reduces human annotation effort in training a reliable classifier by selecting batches of promising and exemplar instances from large quantities of unlabeled data. In this paper, we propose two novel BMAL algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP hard optimization problem; we then propose two convex relaxations, one based on linear programming (LP) and the other based on semi-definite programming (SDP) to solve the batch selection problem. Finally, a deterministic performance bound is derived for the first relaxation and a probabilistic bound for the second. Our extensive empirical studies on the UCI datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques and also deliver high quality solutions."
Active Batch Selection via Convex Relaxations with Guaranteed Performance Bounds,"Batch mode active learning (BMAL) effectively reduces human annotation effort in training a reliable classifier by selecting batches of promising and exemplar instances from large quantities of unlabeled data. In this paper, we propose two novel BMAL algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP hard optimization problem; we then propose two convex relaxations, one based on linear programming (LP) and the other based on semi-definite programming (SDP) to solve the batch selection problem. Finally, a deterministic performance bound is derived for the first relaxation and a probabilistic bound for the second. Our extensive empirical studies on the UCI datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques and also deliver high quality solutions."
Active Batch Selection via Convex Relaxations with Guaranteed Performance Bounds,"Batch mode active learning (BMAL) effectively reduces human annotation effort in training a reliable classifier by selecting batches of promising and exemplar instances from large quantities of unlabeled data. In this paper, we propose two novel BMAL algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP hard optimization problem; we then propose two convex relaxations, one based on linear programming (LP) and the other based on semi-definite programming (SDP) to solve the batch selection problem. Finally, a deterministic performance bound is derived for the first relaxation and a probabilistic bound for the second. Our extensive empirical studies on the UCI datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques and also deliver high quality solutions."
How They Vote: Issue-Adjusted Models of Legislative Behavior,"We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues.  Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space."
How They Vote: Issue-Adjusted Models of Legislative Behavior,"We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues.  Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space."
Towards Sparse Coding on Riemannian Manifolds via K-Geodesic Clustering,"In this paper we study the problem of sparse coding and dictionary learning on Riemannian manifolds. The extension of these concepts from Euclidean spaces is not straightforward. We first present a model comprised of geodesic submanifolds to represent manifold valued data. We present an algorithm called K-geodesic clustering to learn the model from the data. Then, we argue that this lends itself naturally to be considered as a dictionary model for sparse coding on manifolds. Then we propose an intrinsic and extrinsic approach for manifold sparse coding. Our experiments on human activity data show that our model fits the data more accurately compared to other classical approaches such as K-means clustering. We demonstrate the discriminatory power of the sparse codes in an activity recognition experiment and obtain accuracies that compare well with recent approaches proposed in the literature."
Towards Sparse Coding on Riemannian Manifolds via K-Geodesic Clustering,"In this paper we study the problem of sparse coding and dictionary learning on Riemannian manifolds. The extension of these concepts from Euclidean spaces is not straightforward. We first present a model comprised of geodesic submanifolds to represent manifold valued data. We present an algorithm called K-geodesic clustering to learn the model from the data. Then, we argue that this lends itself naturally to be considered as a dictionary model for sparse coding on manifolds. Then we propose an intrinsic and extrinsic approach for manifold sparse coding. Our experiments on human activity data show that our model fits the data more accurately compared to other classical approaches such as K-means clustering. We demonstrate the discriminatory power of the sparse codes in an activity recognition experiment and obtain accuracies that compare well with recent approaches proposed in the literature."
Towards Sparse Coding on Riemannian Manifolds via K-Geodesic Clustering,"In this paper we study the problem of sparse coding and dictionary learning on Riemannian manifolds. The extension of these concepts from Euclidean spaces is not straightforward. We first present a model comprised of geodesic submanifolds to represent manifold valued data. We present an algorithm called K-geodesic clustering to learn the model from the data. Then, we argue that this lends itself naturally to be considered as a dictionary model for sparse coding on manifolds. Then we propose an intrinsic and extrinsic approach for manifold sparse coding. Our experiments on human activity data show that our model fits the data more accurately compared to other classical approaches such as K-means clustering. We demonstrate the discriminatory power of the sparse codes in an activity recognition experiment and obtain accuracies that compare well with recent approaches proposed in the literature."
Towards Sparse Coding on Riemannian Manifolds via K-Geodesic Clustering,"In this paper we study the problem of sparse coding and dictionary learning on Riemannian manifolds. The extension of these concepts from Euclidean spaces is not straightforward. We first present a model comprised of geodesic submanifolds to represent manifold valued data. We present an algorithm called K-geodesic clustering to learn the model from the data. Then, we argue that this lends itself naturally to be considered as a dictionary model for sparse coding on manifolds. Then we propose an intrinsic and extrinsic approach for manifold sparse coding. Our experiments on human activity data show that our model fits the data more accurately compared to other classical approaches such as K-means clustering. We demonstrate the discriminatory power of the sparse codes in an activity recognition experiment and obtain accuracies that compare well with recent approaches proposed in the literature."
Towards Sparse Coding on Riemannian Manifolds via K-Geodesic Clustering,"In this paper we study the problem of sparse coding and dictionary learning on Riemannian manifolds. The extension of these concepts from Euclidean spaces is not straightforward. We first present a model comprised of geodesic submanifolds to represent manifold valued data. We present an algorithm called K-geodesic clustering to learn the model from the data. Then, we argue that this lends itself naturally to be considered as a dictionary model for sparse coding on manifolds. Then we propose an intrinsic and extrinsic approach for manifold sparse coding. Our experiments on human activity data show that our model fits the data more accurately compared to other classical approaches such as K-means clustering. We demonstrate the discriminatory power of the sparse codes in an activity recognition experiment and obtain accuracies that compare well with recent approaches proposed in the literature."
Bellman Error Based Feature Generation using Random Projections on Sparse Spaces,"We address the problem of automatic generation of features for value function approximation.Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error.  Empirical results demonstrate the strength of this method."
Bellman Error Based Feature Generation using Random Projections on Sparse Spaces,"We address the problem of automatic generation of features for value function approximation.Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error.  Empirical results demonstrate the strength of this method."
Bellman Error Based Feature Generation using Random Projections on Sparse Spaces,"We address the problem of automatic generation of features for value function approximation.Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error.  Empirical results demonstrate the strength of this method."
Bellman Error Based Feature Generation using Random Projections on Sparse Spaces,"We address the problem of automatic generation of features for value function approximation.Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error.  Empirical results demonstrate the strength of this method."
Max-Product Particle Belief Propagation,"Belief Propagation (BP) is a popular message passing algorithm for inference in factored probabilistic graphical models. Sum-Product BP computes marginal distributions for node variables, while Max-Product BP outputs a solution that maximizes the joint distribution of all variables, and is used for MAP (maximum a posteriori) inference.BP is commonly applied to problems that assume a discrete (or discretized) state space. When the state space of a variables cannot be enumerated in practice, and the messages cannot be computed in closed form, methods based on sampling are considered. Several nonparametric Sum-Product approximations exist, however many inference problems of scientific interest require MAP inference for high-dimensional, continuous and multimodal distributed random variables, calling for nonparametric algorithms for Max-Product BP. In this paper we formulate a Max-Product version of the PBP algorithm, and analyze its behavior in performing inference for a model with continuous variables that do not easily admit a discrete representation. "
Max-Product Particle Belief Propagation,"Belief Propagation (BP) is a popular message passing algorithm for inference in factored probabilistic graphical models. Sum-Product BP computes marginal distributions for node variables, while Max-Product BP outputs a solution that maximizes the joint distribution of all variables, and is used for MAP (maximum a posteriori) inference.BP is commonly applied to problems that assume a discrete (or discretized) state space. When the state space of a variables cannot be enumerated in practice, and the messages cannot be computed in closed form, methods based on sampling are considered. Several nonparametric Sum-Product approximations exist, however many inference problems of scientific interest require MAP inference for high-dimensional, continuous and multimodal distributed random variables, calling for nonparametric algorithms for Max-Product BP. In this paper we formulate a Max-Product version of the PBP algorithm, and analyze its behavior in performing inference for a model with continuous variables that do not easily admit a discrete representation. "
Max-Product Particle Belief Propagation,"Belief Propagation (BP) is a popular message passing algorithm for inference in factored probabilistic graphical models. Sum-Product BP computes marginal distributions for node variables, while Max-Product BP outputs a solution that maximizes the joint distribution of all variables, and is used for MAP (maximum a posteriori) inference.BP is commonly applied to problems that assume a discrete (or discretized) state space. When the state space of a variables cannot be enumerated in practice, and the messages cannot be computed in closed form, methods based on sampling are considered. Several nonparametric Sum-Product approximations exist, however many inference problems of scientific interest require MAP inference for high-dimensional, continuous and multimodal distributed random variables, calling for nonparametric algorithms for Max-Product BP. In this paper we formulate a Max-Product version of the PBP algorithm, and analyze its behavior in performing inference for a model with continuous variables that do not easily admit a discrete representation. "
Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks,"Many social network datasets consist of a sequence of relational observations through time.  Latent variable models for these networks are popular Bayesian approaches to handle our uncertainty over the unobserved dynamics in the network and how these dynamics influence the observed data.  However, current approaches in this Bayesian modeling framework have not been able to capture the reciprocal influence of past observations on future latent representations.  In this paper, we introduce a new probabilistic model for dynamic social network data based on a semi-hidden Markov model, which assumes that the evolution of latent features are influenced by the local topology of the network in past observations.  We show that the particular form of our model is very interpretable in the context of social networks and describe how it captures a phenomenon which we call latent feature propagation.  We present a Markov-Chain Monte-Carlo inference procedure and experimentally show that our model improves upon current methods on link pediction performance using synthetic and real datasets of social networks."
Kernel-based Distance Metric Learning in the Output Space,"In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2- or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches."
Efficient and Optimal Active Learning on Graphs,"We investigate the problem of active label prediction on a given graph. Towards this end, we propose an extremely efficient algorithm S^2 that adaptively selects a sequence of nodes that it would like to see the labels of. We present a theoretical analysis of the performance of S2 that is based on a novel refinement of standard measures of label complexity. We also show that this algorithm is information theoretically optimal in a certain regime and demonstrate its performance on real world data. "
Efficient and Optimal Active Learning on Graphs,"We investigate the problem of active label prediction on a given graph. Towards this end, we propose an extremely efficient algorithm S^2 that adaptively selects a sequence of nodes that it would like to see the labels of. We present a theoretical analysis of the performance of S2 that is based on a novel refinement of standard measures of label complexity. We also show that this algorithm is information theoretically optimal in a certain regime and demonstrate its performance on real world data. "
Efficient and Optimal Active Learning on Graphs,"We investigate the problem of active label prediction on a given graph. Towards this end, we propose an extremely efficient algorithm S^2 that adaptively selects a sequence of nodes that it would like to see the labels of. We present a theoretical analysis of the performance of S2 that is based on a novel refinement of standard measures of label complexity. We also show that this algorithm is information theoretically optimal in a certain regime and demonstrate its performance on real world data. "
Discovering Sparse Networks In Spiking Data,"  In order to reason about the interacting processes giving rise to a given set of data, we must understand the causal relationships between those latent processes. One way to uncover these relationships is to discover a directed network structure from the data. Often, these data have the form of discrete events --- for example, neural spikes --- which can be modeled effectively via interacting point processes; the Hawkes process is the classical example of such a joint process. Here we provide a fully-Bayesian treatment of the Hawkes process, introducing 1)~convenient conjugate priors for the model parameters and 2)~a sparsity-promoting  spike-and-slab prior on the elements of the interaction matrix.  We demonstrate how to perform efficient inference in this model with Markov chain Monte Carlo.  This enables us to both recover posterior samples of the network structure and infer the characteristics of the underlying temporal dynamics.  We validate our approach on a simulated dataset with known ground truth before examining two real-world data sets --- neural spike train data and financial tick streams.  In each case, we uncover sparse networks with meaningful parameters, suggesting that this method is widely applicable for network discovery."
Discovering Sparse Networks In Spiking Data,"  In order to reason about the interacting processes giving rise to a given set of data, we must understand the causal relationships between those latent processes. One way to uncover these relationships is to discover a directed network structure from the data. Often, these data have the form of discrete events --- for example, neural spikes --- which can be modeled effectively via interacting point processes; the Hawkes process is the classical example of such a joint process. Here we provide a fully-Bayesian treatment of the Hawkes process, introducing 1)~convenient conjugate priors for the model parameters and 2)~a sparsity-promoting  spike-and-slab prior on the elements of the interaction matrix.  We demonstrate how to perform efficient inference in this model with Markov chain Monte Carlo.  This enables us to both recover posterior samples of the network structure and infer the characteristics of the underlying temporal dynamics.  We validate our approach on a simulated dataset with known ground truth before examining two real-world data sets --- neural spike train data and financial tick streams.  In each case, we uncover sparse networks with meaningful parameters, suggesting that this method is widely applicable for network discovery."
Sequential Inference with Compact Posterior Representations for the Indian Buffet Process,"Infinite latent feature models, such as those based on the Indian Buffet Process (IBP), provide a flexible way to learn latent features underlying observed data, without having to specify their number a priori. Scalable and accurate posterior inference in these models however remains a challenge. Moreover, in many settings, data arrives sequentially and batch methods such as Gibbs sampling are no longer an option. We present a scalable, sequential MCMC inference method for the IBP that can process one observation at a time. As opposed to the standard particle filter for the IBP, our method incorporates the current observation in the proposal distribution and in the computation of the particle weights. This leads to our method achieving better or comparable inference quality as compared to the standard particle filter for the IBP, while requiring far fewer number of particles (therefore yielding compact posterior representations) and being comparable in terms of inference speed.  Moreover, our method also yields competitive accuracies as compared to the state-of-the-art batch methods based on MCMC and variational inference, while being considerably faster."
The Dual Regularization Path of Lasso,"The Lasso regularization path has been extensively studied in the literature. It is now well understood that the path is well defined, unique and continuous piecewise linear under certain conditions. However, when the covariates in the active set (covaraites with greatest absolute correlations with the residual) are dependent, the regularization path is not unique and existing path following algorithms such as LARS may break down. In this paper, we systematically analyze the dual regularization path of Lasso, i.e., the path of the dual optimal solution. In contrast to the primal regularization path, the dual regularization path is well defined, unique and continuous piecewise linear without the independence assumption. We then propose how to compute the sequence of breakpoints of the dual regularization path with or without dependent covariates in the active set. Under the independence assumption, when the sign restriction is violated, the covariate which violates the sign restriction should be dropped and a new direction of the current line segment is updated accordingly. However, when the independence assumption fails, we show it is not necessary to drop the covariate which violates the sign restriction and change the direction of the current line segment, overcoming one of the major issues in deriving the primal regularization Lasso path. We systematically study different possibilities and show how to find the correct solution. Once we get the dual regularization path, we obtain the primal regularization path by the KKT conditions. Our simulation studies validate the correctness of the path generated by the proposed algorithm."
The Dual Regularization Path of Lasso,"The Lasso regularization path has been extensively studied in the literature. It is now well understood that the path is well defined, unique and continuous piecewise linear under certain conditions. However, when the covariates in the active set (covaraites with greatest absolute correlations with the residual) are dependent, the regularization path is not unique and existing path following algorithms such as LARS may break down. In this paper, we systematically analyze the dual regularization path of Lasso, i.e., the path of the dual optimal solution. In contrast to the primal regularization path, the dual regularization path is well defined, unique and continuous piecewise linear without the independence assumption. We then propose how to compute the sequence of breakpoints of the dual regularization path with or without dependent covariates in the active set. Under the independence assumption, when the sign restriction is violated, the covariate which violates the sign restriction should be dropped and a new direction of the current line segment is updated accordingly. However, when the independence assumption fails, we show it is not necessary to drop the covariate which violates the sign restriction and change the direction of the current line segment, overcoming one of the major issues in deriving the primal regularization Lasso path. We systematically study different possibilities and show how to find the correct solution. Once we get the dual regularization path, we obtain the primal regularization path by the KKT conditions. Our simulation studies validate the correctness of the path generated by the proposed algorithm."
The Dual Regularization Path of Lasso,"The Lasso regularization path has been extensively studied in the literature. It is now well understood that the path is well defined, unique and continuous piecewise linear under certain conditions. However, when the covariates in the active set (covaraites with greatest absolute correlations with the residual) are dependent, the regularization path is not unique and existing path following algorithms such as LARS may break down. In this paper, we systematically analyze the dual regularization path of Lasso, i.e., the path of the dual optimal solution. In contrast to the primal regularization path, the dual regularization path is well defined, unique and continuous piecewise linear without the independence assumption. We then propose how to compute the sequence of breakpoints of the dual regularization path with or without dependent covariates in the active set. Under the independence assumption, when the sign restriction is violated, the covariate which violates the sign restriction should be dropped and a new direction of the current line segment is updated accordingly. However, when the independence assumption fails, we show it is not necessary to drop the covariate which violates the sign restriction and change the direction of the current line segment, overcoming one of the major issues in deriving the primal regularization Lasso path. We systematically study different possibilities and show how to find the correct solution. Once we get the dual regularization path, we obtain the primal regularization path by the KKT conditions. Our simulation studies validate the correctness of the path generated by the proposed algorithm."
Practical Bayesian Optimization of Machine Learning Algorithms ,"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ?black art? requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm?s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieveexpert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including LatentDirichlet Allocation, Structured SVMs and convolutional neural networks."
A nonparametric variable clustering model,"Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a clustering, of observed variables so that variables in a cluster are highly correlated. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. "
A nonparametric variable clustering model,"Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a clustering, of observed variables so that variables in a cluster are highly correlated. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. "
Priors for Diversity in Generative Latent Variable Models,"Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization, for building density models of data, and for  providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions  on internal parameters.  For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the  underlying i.i.d.\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions, we are able to define a DPP on probability measures.  We show how to perform MAP inference  with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model."
Priors for Diversity in Generative Latent Variable Models,"Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization, for building density models of data, and for  providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions  on internal parameters.  For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the  underlying i.i.d.\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions, we are able to define a DPP on probability measures.  We show how to perform MAP inference  with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model."
Bias-corrected Q-learning to Control Max-operator Bias in Q-learning,We identify a class of stochastic control problems with random rewards which induce high levels of statistical error in the estimate of Q-factors.  This produces significant levels of max-operator bias in Q-learning algorithms which can induce the algorithm to diverge for millions of iterations.  We present a bias-corrected Q-learning with asymptotically unbiased resistance against max-operator bias and show that the algorithm asymptotically converges to the optimal policy as Q-learning does.  We demonstrate that bias-corrected Q-learning shows practical resistance against max-operator bias and performs well in select problems with highly random rewards where Q-learning and other provably convergent algorithms rooted on Q-learning suffer max-operator bias.
Bias-corrected Q-learning to Control Max-operator Bias in Q-learning,We identify a class of stochastic control problems with random rewards which induce high levels of statistical error in the estimate of Q-factors.  This produces significant levels of max-operator bias in Q-learning algorithms which can induce the algorithm to diverge for millions of iterations.  We present a bias-corrected Q-learning with asymptotically unbiased resistance against max-operator bias and show that the algorithm asymptotically converges to the optimal policy as Q-learning does.  We demonstrate that bias-corrected Q-learning shows practical resistance against max-operator bias and performs well in select problems with highly random rewards where Q-learning and other provably convergent algorithms rooted on Q-learning suffer max-operator bias.
Bayesian Inference Reveals Synapse-Specific Short-Term Plasticity in Neocortical Microcircuits,"Short-term synaptic plasticity is highly diverse and varies according to brain area, cortical layer, and developmental stage. Since this form of plasticity shapes neural dynamics, its diversity suggests a specific and essential role in neural information processing. Therefore, a correct identification of short-term plasticity is an important step towards understanding and modeling neural systems. Although accurate phenomenological models have been developed, they are usually fitted to experimental data using least-mean square methods. We demonstrate that, for typical synaptic dynamics, such fitting gives unreliable results. Instead, we introduce a Bayesian approach based on a Markov Chain Monte Carlo method, which provides the full posterior distribution over the parameters of the model. We test the approach on simulated data over different regimes and show that common short-term plasticity protocols yield broad distributions over some of the parameters. Finally, we infer the model parameters using experimental data from three different neocortical excitatory connection types, revealing novel synapse-specific distributions and synaptic transfer functions, while the approach yields more robust clustering results. We conclude that ? because short-term plasticity presumably provides key computational features ? our approach to demarcate synapse-specific synaptic dynamics is an important improvement on the state of the art."
Novelty Detection in Multi-Instance Multi-Label Learning,"Novelty detection plays an important role in machine learning and signal processing. However, this problem has not been previously studied in multi-instance multi-label learning (MIML). In MIML, the training dataset consists of bags of instances associated with sets of labels. It is a common assumption that every instance in a bag is associated with one of the labels in the set. In this paper, we focus on the scenario where a bag may contain novel class instances, which are not associated with the bag-level label set. The goal is to determine for any given instance in a new bag whether it belongs to a known class or not. Detecting novelty in the MIML setting captures manyreal-world phenomena and has many potential applications. For example, in a collection of tagged images not all objects in a given image are represented in the image tag set. Discovering an object which has not been taggedbefore can be useful for the purpose of soliciting a label for the new object. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed methods, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in unseen bags."
Novelty Detection in Multi-Instance Multi-Label Learning,"Novelty detection plays an important role in machine learning and signal processing. However, this problem has not been previously studied in multi-instance multi-label learning (MIML). In MIML, the training dataset consists of bags of instances associated with sets of labels. It is a common assumption that every instance in a bag is associated with one of the labels in the set. In this paper, we focus on the scenario where a bag may contain novel class instances, which are not associated with the bag-level label set. The goal is to determine for any given instance in a new bag whether it belongs to a known class or not. Detecting novelty in the MIML setting captures manyreal-world phenomena and has many potential applications. For example, in a collection of tagged images not all objects in a given image are represented in the image tag set. Discovering an object which has not been taggedbefore can be useful for the purpose of soliciting a label for the new object. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed methods, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in unseen bags."
Novelty Detection in Multi-Instance Multi-Label Learning,"Novelty detection plays an important role in machine learning and signal processing. However, this problem has not been previously studied in multi-instance multi-label learning (MIML). In MIML, the training dataset consists of bags of instances associated with sets of labels. It is a common assumption that every instance in a bag is associated with one of the labels in the set. In this paper, we focus on the scenario where a bag may contain novel class instances, which are not associated with the bag-level label set. The goal is to determine for any given instance in a new bag whether it belongs to a known class or not. Detecting novelty in the MIML setting captures manyreal-world phenomena and has many potential applications. For example, in a collection of tagged images not all objects in a given image are represented in the image tag set. Discovering an object which has not been taggedbefore can be useful for the purpose of soliciting a label for the new object. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed methods, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in unseen bags."
Infinite Tensor Factorization Priors,"There is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. We consider a general framework for nonparametric Bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure.  The mixing measure is assigned a novel infinite tensor factorization (ITF) prior that allows flexible dependence in cluster allocation across data types.  The ITF prior is formulated as a tensor product of stick-breaking processes.  Focusing on a convenient special case corresponding to a Parafac factorization, we provide basic theory justifying the flexibility of the proposed prior and resulting asymptotic properties.  Focusing on ITF mixtures of product kernels, we develop a  new Gibbs sampling algorithm for routine implementation relying on slice sampling. The methods are compared with alternative joint mixture models based on Dirichlet processes and related approaches through simulations and real data applications."
Infinite Tensor Factorization Priors,"There is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. We consider a general framework for nonparametric Bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure.  The mixing measure is assigned a novel infinite tensor factorization (ITF) prior that allows flexible dependence in cluster allocation across data types.  The ITF prior is formulated as a tensor product of stick-breaking processes.  Focusing on a convenient special case corresponding to a Parafac factorization, we provide basic theory justifying the flexibility of the proposed prior and resulting asymptotic properties.  Focusing on ITF mixtures of product kernels, we develop a  new Gibbs sampling algorithm for routine implementation relying on slice sampling. The methods are compared with alternative joint mixture models based on Dirichlet processes and related approaches through simulations and real data applications."
Infinite Tensor Factorization Priors,"There is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. We consider a general framework for nonparametric Bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure.  The mixing measure is assigned a novel infinite tensor factorization (ITF) prior that allows flexible dependence in cluster allocation across data types.  The ITF prior is formulated as a tensor product of stick-breaking processes.  Focusing on a convenient special case corresponding to a Parafac factorization, we provide basic theory justifying the flexibility of the proposed prior and resulting asymptotic properties.  Focusing on ITF mixtures of product kernels, we develop a  new Gibbs sampling algorithm for routine implementation relying on slice sampling. The methods are compared with alternative joint mixture models based on Dirichlet processes and related approaches through simulations and real data applications."
Bayesian n-Choose-k Models for Classification and Ranking,"In categorical data there is often structure in the number ofvariables that take on each label.  For example, the total number of objects in an image and the number of highlyrelevant documents per query in web search both tend to follow a structured distribution.In this paper, we study a probabilistic model that explicitly includes a priordistribution over such counts, along with a count-conditional likelihood thatdefines probabilities over all subsets of a given size.When labels are binary and the prior over counts is a Poisson-Binomialdistribution, a standard logistic regression model is recovered, but for othercount distributions, such priors induce global dependencies and combinatoricsthat appear to complicate learning and inference. However, we demonstrate that simple, efficient learning procedures can bederived for more general forms of this model.We show the utility of the formulation by exploring multi-object classification asmaximum likelihood learning, and ranking and top-K classification asloss-sensitive learning. "
Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models,"Recent experiments have demonstrated that humans and animals typically reasonprobabilistically about their environment. This ability requires a neural codethat represents probability distributions and neural circuits that are capable ofimplementing the operations of probabilistic inference. The proposed probabilisticpopulation coding (PPC) framework provides a statistically efficient neuralrepresentation of probability distributions that is both broadly consistent withphysiological measurements and capable of implementing some of the basic operationsof probabilistic inference in a biologically plausible way. However, theseexperiments and the corresponding neural models have largely focused on simple(tractable) probabilistic computations such as cue combination, coordinate transformations,and decision making. As a result it remains unclear how to generalizethis framework to more complex probabilistic computations. Here we addressthis short coming by showing that a very general approximate inference algorithmknown as Variational Bayesian Expectation Maximization can be implementedwithin the linear PPC framework. We apply this approach to a generic problemfaced by any given layer of cortex, namely the identification of latent causes ofcomplex mixtures of spikes. We identify a formal equivalent between this spikepattern demixing problem and topic models used for document classification, inparticular Latent Dirichlet Allocation (LDA). We then construct a neural networkimplementation of variational inference and learning for LDA that utilizes a linearPPC. This network relies critically on two non-linear operations: divisive normalizationand super-linear facilitation, both of which are ubiquitously observed inneural circuits. We also demonstrate how online learning can be achieved using avariation of Hebb?s rule and describe an extesion of this work which allows us todeal with time varying and correlated latent causes."
Hierarchical Estimation of Locomotion Mode and Gait Cycle using Switching Unscented Kalman Filters,"As we walk, the state of our limbs varies cyclically, while other variables such as walking speed vary along continuous axes and all are nonlinearly related to one another and to potentially observed aspects of gait. Moreover, our locomotion may switch between modes such as walking and standing. Here we present an efficient solution to nonlinear estimation problems with both cyclical and contin- uous state variables with dynamics that undergo switches. This solution is based on a Hidden Markov Model (HMM) over Unscented Kalman Filters (UKF). The resulting algorithm captures the total variability of the gait parameters with a to- tal variance accounted for (R-squared) of 0.99, outperforming existing linear regression estimators (R-squared = 0.92)"
Hierarchical Estimation of Locomotion Mode and Gait Cycle using Switching Unscented Kalman Filters,"As we walk, the state of our limbs varies cyclically, while other variables such as walking speed vary along continuous axes and all are nonlinearly related to one another and to potentially observed aspects of gait. Moreover, our locomotion may switch between modes such as walking and standing. Here we present an efficient solution to nonlinear estimation problems with both cyclical and contin- uous state variables with dynamics that undergo switches. This solution is based on a Hidden Markov Model (HMM) over Unscented Kalman Filters (UKF). The resulting algorithm captures the total variability of the gait parameters with a to- tal variance accounted for (R-squared) of 0.99, outperforming existing linear regression estimators (R-squared = 0.92)"
Hierarchical Estimation of Locomotion Mode and Gait Cycle using Switching Unscented Kalman Filters,"As we walk, the state of our limbs varies cyclically, while other variables such as walking speed vary along continuous axes and all are nonlinearly related to one another and to potentially observed aspects of gait. Moreover, our locomotion may switch between modes such as walking and standing. Here we present an efficient solution to nonlinear estimation problems with both cyclical and contin- uous state variables with dynamics that undergo switches. This solution is based on a Hidden Markov Model (HMM) over Unscented Kalman Filters (UKF). The resulting algorithm captures the total variability of the gait parameters with a to- tal variance accounted for (R-squared) of 0.99, outperforming existing linear regression estimators (R-squared = 0.92)"
Hierarchical Estimation of Locomotion Mode and Gait Cycle using Switching Unscented Kalman Filters,"As we walk, the state of our limbs varies cyclically, while other variables such as walking speed vary along continuous axes and all are nonlinearly related to one another and to potentially observed aspects of gait. Moreover, our locomotion may switch between modes such as walking and standing. Here we present an efficient solution to nonlinear estimation problems with both cyclical and contin- uous state variables with dynamics that undergo switches. This solution is based on a Hidden Markov Model (HMM) over Unscented Kalman Filters (UKF). The resulting algorithm captures the total variability of the gait parameters with a to- tal variance accounted for (R-squared) of 0.99, outperforming existing linear regression estimators (R-squared = 0.92)"
Cost-Sensitive Exploration in Bayesian Reinforcement Learning,"In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems."
Relating Structural MRI and Behavioral Measure to Functional MRI Measures,"Structural magnetic resonance imaging (MRI) and behavioral measures in the elderly have been shown to be associated in someway or another with functional MRI measures in past studies. However, the goal of this study is to analyze the ability of structural MRI and behavioral measures to predict functional measures in relationship to one another. This study uses both linear regression and artificial neural networks to achieve this goal. The results show that linear regression performs better and the features that best relate to functional measures include age, mini-mental state examination scores, total regional volume, number of tracks connecting regions, and regional white matter hyperintensities volume."
Subset Selection for Gaussian Processes,"Given a Gaussian Process on a graph (see, e.g., [RW06]), we consider the problem of selecting a subset of variables to observe which minimizes the total expected squared prediction error of the unobserved variables. We focus on Gaussian Processes on bounded tree-width graphs and on a restricted class of Gaussian Processes, called Gaussian Free Fields (GFF), which arise in semi-supervised learning and computer vision. We ?rst show that ?nding an exact solution is NP-hard even for GFFs. We give a simple greedy approximately optimal algorithm for GFFs on arbitrary graphs. We then give a message passing algorithm for Gaussian Processes on bounded tree-width graphs (bounded tree-width graphs have been widely studied in the context of inference, see, e.g., [Sud02])."
Subset Selection for Gaussian Processes,"Given a Gaussian Process on a graph (see, e.g., [RW06]), we consider the problem of selecting a subset of variables to observe which minimizes the total expected squared prediction error of the unobserved variables. We focus on Gaussian Processes on bounded tree-width graphs and on a restricted class of Gaussian Processes, called Gaussian Free Fields (GFF), which arise in semi-supervised learning and computer vision. We ?rst show that ?nding an exact solution is NP-hard even for GFFs. We give a simple greedy approximately optimal algorithm for GFFs on arbitrary graphs. We then give a message passing algorithm for Gaussian Processes on bounded tree-width graphs (bounded tree-width graphs have been widely studied in the context of inference, see, e.g., [Sud02])."
A new edge selection method for preserving the topology of persistent brain network,"The functional brain connectivity at the macro-scale studies how the localized areas of brain, i.e., the regions of interest (ROIs), work together during the specific mental functions.Their inter-regional connections estimated by the dissimilarity measure between observations in ROIs are usually too dense to visualize and interpret.So, we need to select the important edges in the network.Thresholding the edge weight matrix is most popular way to select the edges because it is assumed that only the strongly connected edges are important.However, there is no widely accepted rule for determining the threshold as well as the weakly connected edges also have the information which discriminates networks.In this paper, we propose a new edge selection method which preserves the topological structures of brain network based on the persistent homology.The persistent homology scans the topological structures by increasing the threshold and transforms the topological invariants to the algebraic form, known as Betti numbers.We seek the edges which affect the changes of the zeroth and first Betti numbers, i.e., connected components and holes.We applied the proposed method to the functional brain network based on FDG-PET data consisting of 24 attention deficit hyperactivity disorder (ADHD), 26 autism spectrum disorder (ASD) children and 11 pediatric control (PedCon) subjects.We showed that our edge selection method finds the minimum number of edges preserving the origianl geodesic distance of network."
A new edge selection method for preserving the topology of persistent brain network,"The functional brain connectivity at the macro-scale studies how the localized areas of brain, i.e., the regions of interest (ROIs), work together during the specific mental functions.Their inter-regional connections estimated by the dissimilarity measure between observations in ROIs are usually too dense to visualize and interpret.So, we need to select the important edges in the network.Thresholding the edge weight matrix is most popular way to select the edges because it is assumed that only the strongly connected edges are important.However, there is no widely accepted rule for determining the threshold as well as the weakly connected edges also have the information which discriminates networks.In this paper, we propose a new edge selection method which preserves the topological structures of brain network based on the persistent homology.The persistent homology scans the topological structures by increasing the threshold and transforms the topological invariants to the algebraic form, known as Betti numbers.We seek the edges which affect the changes of the zeroth and first Betti numbers, i.e., connected components and holes.We applied the proposed method to the functional brain network based on FDG-PET data consisting of 24 attention deficit hyperactivity disorder (ADHD), 26 autism spectrum disorder (ASD) children and 11 pediatric control (PedCon) subjects.We showed that our edge selection method finds the minimum number of edges preserving the origianl geodesic distance of network."
A new edge selection method for preserving the topology of persistent brain network,"The functional brain connectivity at the macro-scale studies how the localized areas of brain, i.e., the regions of interest (ROIs), work together during the specific mental functions.Their inter-regional connections estimated by the dissimilarity measure between observations in ROIs are usually too dense to visualize and interpret.So, we need to select the important edges in the network.Thresholding the edge weight matrix is most popular way to select the edges because it is assumed that only the strongly connected edges are important.However, there is no widely accepted rule for determining the threshold as well as the weakly connected edges also have the information which discriminates networks.In this paper, we propose a new edge selection method which preserves the topological structures of brain network based on the persistent homology.The persistent homology scans the topological structures by increasing the threshold and transforms the topological invariants to the algebraic form, known as Betti numbers.We seek the edges which affect the changes of the zeroth and first Betti numbers, i.e., connected components and holes.We applied the proposed method to the functional brain network based on FDG-PET data consisting of 24 attention deficit hyperactivity disorder (ADHD), 26 autism spectrum disorder (ASD) children and 11 pediatric control (PedCon) subjects.We showed that our edge selection method finds the minimum number of edges preserving the origianl geodesic distance of network."
Stochastically Emerging Medoids: Application of Classical Problems in Probability Theory for Clustering Massive Data Sets,"K-medoid methods for clustering data have many desirable properties such as robustness and the ability to use non-numerical values, but their typically high computational complexity has made their application to large data sets difficult. In this paper, we present AGORAS, a novel stochastic algorithm for the k-medoids problem that is especially well-suited to clustering massive data sets. Our approach involves taking a sequence of uniform sample sets and a heuristic for determining the sample size and identifying cluster medoids from the sampled items. As a result, computing the final solution only involves solving k trivial sub-problems of centrality, which can be done much more efficiently on large data sets than searching a combinatorial space for the optimal value of an objective function. As a result, the complexity of AGORAS is effectively independent of the full data size, and it can scale to arbitrarily large data sets.  We evaluate AGORAS experimentally against PAM and CLARANS, the best-known existing algorithms for the k- medoids problem, across a variety of published and synthetic data sets.  We find that AGORAS outperforms PAM by up to four orders of magnitude for data sets with less than 10,000 points, and it outperforms CLARANS by two orders of magnitude on a data set of just 64,000 points.  Moreover, we find in some cases that AGORAS also outperforms these algorithms in terms of cluster quality. "
Stochastically Emerging Medoids: Application of Classical Problems in Probability Theory for Clustering Massive Data Sets,"K-medoid methods for clustering data have many desirable properties such as robustness and the ability to use non-numerical values, but their typically high computational complexity has made their application to large data sets difficult. In this paper, we present AGORAS, a novel stochastic algorithm for the k-medoids problem that is especially well-suited to clustering massive data sets. Our approach involves taking a sequence of uniform sample sets and a heuristic for determining the sample size and identifying cluster medoids from the sampled items. As a result, computing the final solution only involves solving k trivial sub-problems of centrality, which can be done much more efficiently on large data sets than searching a combinatorial space for the optimal value of an objective function. As a result, the complexity of AGORAS is effectively independent of the full data size, and it can scale to arbitrarily large data sets.  We evaluate AGORAS experimentally against PAM and CLARANS, the best-known existing algorithms for the k- medoids problem, across a variety of published and synthetic data sets.  We find that AGORAS outperforms PAM by up to four orders of magnitude for data sets with less than 10,000 points, and it outperforms CLARANS by two orders of magnitude on a data set of just 64,000 points.  Moreover, we find in some cases that AGORAS also outperforms these algorithms in terms of cluster quality. "
Stochastically Emerging Medoids: Application of Classical Problems in Probability Theory for Clustering Massive Data Sets,"K-medoid methods for clustering data have many desirable properties such as robustness and the ability to use non-numerical values, but their typically high computational complexity has made their application to large data sets difficult. In this paper, we present AGORAS, a novel stochastic algorithm for the k-medoids problem that is especially well-suited to clustering massive data sets. Our approach involves taking a sequence of uniform sample sets and a heuristic for determining the sample size and identifying cluster medoids from the sampled items. As a result, computing the final solution only involves solving k trivial sub-problems of centrality, which can be done much more efficiently on large data sets than searching a combinatorial space for the optimal value of an objective function. As a result, the complexity of AGORAS is effectively independent of the full data size, and it can scale to arbitrarily large data sets.  We evaluate AGORAS experimentally against PAM and CLARANS, the best-known existing algorithms for the k- medoids problem, across a variety of published and synthetic data sets.  We find that AGORAS outperforms PAM by up to four orders of magnitude for data sets with less than 10,000 points, and it outperforms CLARANS by two orders of magnitude on a data set of just 64,000 points.  Moreover, we find in some cases that AGORAS also outperforms these algorithms in terms of cluster quality. "
Stochastically Emerging Medoids: Application of Classical Problems in Probability Theory for Clustering Massive Data Sets,"K-medoid methods for clustering data have many desirable properties such as robustness and the ability to use non-numerical values, but their typically high computational complexity has made their application to large data sets difficult. In this paper, we present AGORAS, a novel stochastic algorithm for the k-medoids problem that is especially well-suited to clustering massive data sets. Our approach involves taking a sequence of uniform sample sets and a heuristic for determining the sample size and identifying cluster medoids from the sampled items. As a result, computing the final solution only involves solving k trivial sub-problems of centrality, which can be done much more efficiently on large data sets than searching a combinatorial space for the optimal value of an objective function. As a result, the complexity of AGORAS is effectively independent of the full data size, and it can scale to arbitrarily large data sets.  We evaluate AGORAS experimentally against PAM and CLARANS, the best-known existing algorithms for the k- medoids problem, across a variety of published and synthetic data sets.  We find that AGORAS outperforms PAM by up to four orders of magnitude for data sets with less than 10,000 points, and it outperforms CLARANS by two orders of magnitude on a data set of just 64,000 points.  Moreover, we find in some cases that AGORAS also outperforms these algorithms in terms of cluster quality. "
Simultaneously Leveraging Output and Task Structures  for Multiple-Output Regression,"Multiple-output regression models require estimating multiple functions, one for each output. To improve parameter estimation in such models, methods based on structural regularization of the model parameters are usually needed. In this paper, we present a multiple-output regression model that leverages the covariance structure of the functions (i.e., how the multiple functions are related with each other) as well as the conditional covariance structure of the outputs. This is in contrast with existing methods that usually take into account only one of these structures. More importantly, unlike most of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data. Several previously proposed structural regularization based  multiple-output regression models turn out to be special cases of our model. Moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs). Experimental results on both synthetic and real datasets demonstrate the effectiveness of our method."
Adaptive Sparsity in Gaussian Graphical Models,"An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffrey's hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation."
Adaptive Sparsity in Gaussian Graphical Models,"An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffrey's hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation."
Adaptive Sparsity in Gaussian Graphical Models,"An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffrey's hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation."
On the Sample Complexity of Robust PCA,"We estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix.This estimator is used in a convex algorithm for robust subspace recovery (i.e., robust PCA).Our model assumes a sub-Gaussian underlying distribution and an i.i.d.~sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $N$ is of order $O(N^{-0.5+\eps})$ for arbitrarily small $\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation, i.e., $O(N^{-0.5})$. Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is $O(D^{2+\delta})$ for arbitrarily small $\delta>0$ (whereas the sample complexity of direct covariance estimation with Frobenius norm is $O(D^{2})$).These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm, which are close to those of PCA. To the best of our knowledge, this is the only work analyzing the sample complexity of any robust PCA algorithm."
Semi-blind Source Separation via Sparse Representations and Online Dictionary Learning,"This work examines a semi-blind source separation problem having applications in audio, image, and video processing.  Our essential aim is to separate one source whose local structure is partially or approximately known from another a priori unspecified but structured source, given only a single linear combination of the two sources.  We propose a novel separation technique based on local sparse approximations; a key feature of our procedure is the online learning of dictionaries (using only the data itself) which sparsely model the unknown structured background source.  We demonstrate the performance of our proposed approach via simulation on two stylized applications -one entailing audio source separation, and another where the goal is to separate two spatially multipliexed images."
Semi-blind Source Separation via Sparse Representations and Online Dictionary Learning,"This work examines a semi-blind source separation problem having applications in audio, image, and video processing.  Our essential aim is to separate one source whose local structure is partially or approximately known from another a priori unspecified but structured source, given only a single linear combination of the two sources.  We propose a novel separation technique based on local sparse approximations; a key feature of our procedure is the online learning of dictionaries (using only the data itself) which sparsely model the unknown structured background source.  We demonstrate the performance of our proposed approach via simulation on two stylized applications -one entailing audio source separation, and another where the goal is to separate two spatially multipliexed images."
Causal Inference on Spillover Effects by Assignment Strategies and Linear Models,"Estimating causal effects of treatment on a network is challenging because the potential outcome of one unit is affected by the treatment on others (spillovers). To estimate the spillover effects, we introduce a novel estimand and propose two estimation approaches.In a randomization-based approach, we characterize a bias-manipulability tradeoff.Randomizations that reveal more causal information tend to have more bias and vice versa.We propose a novel randomization, namely $\inrx$, which fixes the treatment of $x\%$ of common neighbors in order to control these two competing factors.In model-based approach, assuming additivity of spillover effects results in a linear model, allowing Bayesian inference on the causal estimands. The model not only accounts for network uncertainty but also gives insight on optimal assignment through Fisher information analysis.Empirical results demonstrate the strength of the model-based approach under linear-additive spillover effects,and the strength of randomization under non-linear effects. "
Causal Inference on Spillover Effects by Assignment Strategies and Linear Models,"Estimating causal effects of treatment on a network is challenging because the potential outcome of one unit is affected by the treatment on others (spillovers). To estimate the spillover effects, we introduce a novel estimand and propose two estimation approaches.In a randomization-based approach, we characterize a bias-manipulability tradeoff.Randomizations that reveal more causal information tend to have more bias and vice versa.We propose a novel randomization, namely $\inrx$, which fixes the treatment of $x\%$ of common neighbors in order to control these two competing factors.In model-based approach, assuming additivity of spillover effects results in a linear model, allowing Bayesian inference on the causal estimands. The model not only accounts for network uncertainty but also gives insight on optimal assignment through Fisher information analysis.Empirical results demonstrate the strength of the model-based approach under linear-additive spillover effects,and the strength of randomization under non-linear effects. "
Causal Inference on Spillover Effects by Assignment Strategies and Linear Models,"Estimating causal effects of treatment on a network is challenging because the potential outcome of one unit is affected by the treatment on others (spillovers). To estimate the spillover effects, we introduce a novel estimand and propose two estimation approaches.In a randomization-based approach, we characterize a bias-manipulability tradeoff.Randomizations that reveal more causal information tend to have more bias and vice versa.We propose a novel randomization, namely $\inrx$, which fixes the treatment of $x\%$ of common neighbors in order to control these two competing factors.In model-based approach, assuming additivity of spillover effects results in a linear model, allowing Bayesian inference on the causal estimands. The model not only accounts for network uncertainty but also gives insight on optimal assignment through Fisher information analysis.Empirical results demonstrate the strength of the model-based approach under linear-additive spillover effects,and the strength of randomization under non-linear effects. "
Message passing with relaxed moment matching,"Bayesian learning is often hampered by large computational expense. As a powerful generalization of popular belief propagation,  expectation propagation (EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP can be sensitive to outliers and suffer from divergence for difficult cases.  To address this issue, we propose a new approximate inference approach, relaxed expectation propagation (REP). It relaxes the moment matching requirement of expectation propagation by adding a relaxation factor into the KL minimization. We penalize this relaxation with a $l_1$ penalty. With this penalty, when two distributions in the relaxed KL divergence are similar, we obtain the exact moment matching; in the presence of outliers, the relaxation factor will used to relax the moment matching constraint. Based on this penalized KL minimization, REP is robust to outliers and can greatly improve the posterior approximation quality over EP. To examine the effectiveness of REP,  we apply it to Gaussian process classification, a task known to be suitable to EP. Our classification results on synthetic and UCI benchmark datasets demonstrate significant improvement of REP over EP and Power EP---in terms of algorithmic stability, estimation accuracy and predictive performance."
Message passing with relaxed moment matching,"Bayesian learning is often hampered by large computational expense. As a powerful generalization of popular belief propagation,  expectation propagation (EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP can be sensitive to outliers and suffer from divergence for difficult cases.  To address this issue, we propose a new approximate inference approach, relaxed expectation propagation (REP). It relaxes the moment matching requirement of expectation propagation by adding a relaxation factor into the KL minimization. We penalize this relaxation with a $l_1$ penalty. With this penalty, when two distributions in the relaxed KL divergence are similar, we obtain the exact moment matching; in the presence of outliers, the relaxation factor will used to relax the moment matching constraint. Based on this penalized KL minimization, REP is robust to outliers and can greatly improve the posterior approximation quality over EP. To examine the effectiveness of REP,  we apply it to Gaussian process classification, a task known to be suitable to EP. Our classification results on synthetic and UCI benchmark datasets demonstrate significant improvement of REP over EP and Power EP---in terms of algorithmic stability, estimation accuracy and predictive performance."
Interpreting prediction markets: a stochastic approach,"We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution.This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved.In particular, we show that the stationary point of the stochastic process of prices generated by the market is equal to the market's Walrasian equilibrium of classic market analysis.Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour."
Bayesian Inference from Non-Ignorable Network Sampling Designs,"Consider individuals interacting in a social network and a response that can be measured on each individual. We are interested in making inferences on a population quantity that is a function of both the response and the social interactions. In this paper, working within Rubin's inferential framework, we introduce a new notion of non-ignorable sampling design for the case of missing covariates. This notion is the key element for developing valid inferences in applications to epidemiology and healthcare in which hard-to-reach populations are sampled using link-tracing designs, including respondent-driven sampling, that carry information about the quantify of interest."
Module propagation: probabilistic frequent subgraph discovery,"A central task in graph data analysis is to discover subgraphs recurring in a single or multiple graphs. Although many graph mining algorithms have been proposed to identify identical subgraphs recurring in multiple clean graphs, it remains an open problem to discover frequent subgraphs that appear in a {\em single} or multiple {\em noisy} graphs. Solving this problem is critical because most real-world graph data is noisy and many subgraphs are repeated only in a single graph. In this paper, we propose a new approach, Module Propagation, as a principled and practical solution to this open problem. Instead of relying on deterministic search as previous graph mining algorithms do, we reformulate the problem in a probabilistic framework.By maximizing the probability of a new Markov random field model via a fast message passing algorithm, we decompose a single or multiple noisy graphs into recurring {densely connected} and possibly overlapped subgraphs efficiently.The procedure is done efficiently  based on a message passing algorithm that explores the sparsity of our new model.We not only demonstrate the advantage of \mp on synthetic data over alternative graph mining algorithms, but also successfully apply it to a novel application of graph mining --- the discovery of modules in chemically similar crystal structures. This application can pave the way for predicting unknown crystal structures, an important yet challenging task in computational materials science."
Module propagation: probabilistic frequent subgraph discovery,"A central task in graph data analysis is to discover subgraphs recurring in a single or multiple graphs. Although many graph mining algorithms have been proposed to identify identical subgraphs recurring in multiple clean graphs, it remains an open problem to discover frequent subgraphs that appear in a {\em single} or multiple {\em noisy} graphs. Solving this problem is critical because most real-world graph data is noisy and many subgraphs are repeated only in a single graph. In this paper, we propose a new approach, Module Propagation, as a principled and practical solution to this open problem. Instead of relying on deterministic search as previous graph mining algorithms do, we reformulate the problem in a probabilistic framework.By maximizing the probability of a new Markov random field model via a fast message passing algorithm, we decompose a single or multiple noisy graphs into recurring {densely connected} and possibly overlapped subgraphs efficiently.The procedure is done efficiently  based on a message passing algorithm that explores the sparsity of our new model.We not only demonstrate the advantage of \mp on synthetic data over alternative graph mining algorithms, but also successfully apply it to a novel application of graph mining --- the discovery of modules in chemically similar crystal structures. This application can pave the way for predicting unknown crystal structures, an important yet challenging task in computational materials science."
Cardinality Restricted Boltzmann Machines ,"The Restricted Boltzmann Machine (RBM) is a popular density model thatis also good for extracting features.A main source of tractability in RBM models is the model's assumptionthat given an input, hidden units activate independently from one another.Sparsity and competition in the hidden representation is believed tobe beneficial,and while an RBM with competition among its hidden units would acquire someof the attractive properties of sparse coding, such constraints are not added due to the widespread belief that the resulting model would become intractable.In this work, we show how a dynamic programming algorithm developed in 1981can be used to implement exact sparsity in the RBM's hidden units.We then expand on this and show how to pass derivatives through alayer of exact sparsity, which makes it possible to fine-tune adeep belief network (DBN) consisting of RBMs with sparse hiddenlayers.  We show that sparsity in the RBM's hidden layer improves theperformance of both the pre-trained representations and of thefine-tuned model."
