title,abstract
High-Dimensional Feature Selection by Kernel-Based Feature-Wise Non-Linear Lasso,"The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features."
High-Dimensional Feature Selection by Kernel-Based Feature-Wise Non-Linear Lasso,"The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features."
High-Dimensional Feature Selection by Kernel-Based Feature-Wise Non-Linear Lasso,"The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features."
Context-Dependent Topic Modeling Using Multinomial Probit Random Effect Regression (MPR),"This paper presents the multinomial probit random effect regression (MPR) topic model that can incorporate document meta-data for topic inference. Document meta-data enter the topic model through regression covariates of a multinomial-probit-like setting that influence the prior of topic distribution. Both discrete and continuous variables can be included. The MPR contains document-specific random effects to allow flexible inference. We developed Gibbs sampling for the non-Dirichlet-conjugate setting. We demonstrate the utility of MPR by incorporating intra-day, weekly, and monthly cyclical patterns into topic models. Experimental results show that MPR can capture interesting short-term cyclical patterns in investor forum postings, newswires, and newspaper articles. Likelihood evaluation shows that MPR outperforms LDA in the three testbeds considered in this study."
Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information,"The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high non-linearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets."
Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information,"The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high non-linearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets."
Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information,"The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high non-linearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets."
A Covariate dependent HMM for learning hybrid dynamical models,"Our main contribution is a learning system that acquires an hybrid representation of a dynamical system. For this we extend hidden markov models by including a covariate in the transition model. We propose an efficient Gibbs sampler and show that we can reliably estimate such models compute the full posterior model thus having access also to the confidence on our model. In the work of \cite{ghahramani2000variational} we can see a general discussion of many different graphical models with different structures. Our model adds some extra complexity due to the dependency of the switch state transitions on the continuous variables.  This model does not only provide better prediction quality but it is easier to interpret and is suitable for control, filtering and planning that is grounded on the physical system dynamics and on the task goal. We will show a simple example of how planning can be achieved with such model."
A Covariate dependent HMM for learning hybrid dynamical models,"Our main contribution is a learning system that acquires an hybrid representation of a dynamical system. For this we extend hidden markov models by including a covariate in the transition model. We propose an efficient Gibbs sampler and show that we can reliably estimate such models compute the full posterior model thus having access also to the confidence on our model. In the work of \cite{ghahramani2000variational} we can see a general discussion of many different graphical models with different structures. Our model adds some extra complexity due to the dependency of the switch state transitions on the continuous variables.  This model does not only provide better prediction quality but it is easier to interpret and is suitable for control, filtering and planning that is grounded on the physical system dynamics and on the task goal. We will show a simple example of how planning can be achieved with such model."
Can We Recognize Tiger by Bus Images? ?Robust and Discriminative Self-Taught Image Categorization,"The lack of training data is a common challenge in many real-world image categorization problems, which is often tackled by semi-supervised learning or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught image categorization approach to utilize any unlabeled images (e.g., those randomly downloaded from Internet) without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled images. Because our new objective involves non-smooth terms in both the loss function and the regularization, it is difficult to solve in general. Thus, we derive an efficient iterative algorithm to solve the optimization problem, and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.  "
Can We Recognize Tiger by Bus Images? ?Robust and Discriminative Self-Taught Image Categorization,"The lack of training data is a common challenge in many real-world image categorization problems, which is often tackled by semi-supervised learning or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught image categorization approach to utilize any unlabeled images (e.g., those randomly downloaded from Internet) without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled images. Because our new objective involves non-smooth terms in both the loss function and the regularization, it is difficult to solve in general. Thus, we derive an efficient iterative algorithm to solve the optimization problem, and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.  "
"Robust Linear Discriminant Analysis Using Ratio Minimization of $l_{1,2}$-Norms","Traditional Linear Discriminant Analysis (LDA) minimizes the ratio of squared $l_2$-norms, which is sensitive to outliers. In recent research, many $l_1$-norm based robust learning models were proposed. However, so far there is no existing work to utilize $l_1$-norm based objective for LDA, due to the difficulty of $l_1$-norm ratio optimization. Meanwhile, trivially replacing $l_2$-norms by $l_1$-norms in LDA objective introduces the $l_1$-norm maximization problem and doesn't provide the robustness. In this paper, we propose a novel robust LDA formulation based on the $l_{1,2}$-norm ratio minimization. Minimizing the $l_{1,2}$-norm ratio is a much more challenging problem than the traditional methods, and existing optimization algorithms cannot solve such a non-smooth terms ratio problem. We derive a new efficient algorithm to solve this challenging problem, and provide the theoretical analysis on the algorithm convergence. Our algorithm is easy to be implemented, and converges fast in practice with the same computational complexity as the trace ratio LDA. Extensive experiments on both synthetic data and nine real benchmark data sets show the effectiveness of the proposed robust LDA method."
"Robust Linear Discriminant Analysis Using Ratio Minimization of $l_{1,2}$-Norms","Traditional Linear Discriminant Analysis (LDA) minimizes the ratio of squared $l_2$-norms, which is sensitive to outliers. In recent research, many $l_1$-norm based robust learning models were proposed. However, so far there is no existing work to utilize $l_1$-norm based objective for LDA, due to the difficulty of $l_1$-norm ratio optimization. Meanwhile, trivially replacing $l_2$-norms by $l_1$-norms in LDA objective introduces the $l_1$-norm maximization problem and doesn't provide the robustness. In this paper, we propose a novel robust LDA formulation based on the $l_{1,2}$-norm ratio minimization. Minimizing the $l_{1,2}$-norm ratio is a much more challenging problem than the traditional methods, and existing optimization algorithms cannot solve such a non-smooth terms ratio problem. We derive a new efficient algorithm to solve this challenging problem, and provide the theoretical analysis on the algorithm convergence. Our algorithm is easy to be implemented, and converges fast in practice with the same computational complexity as the trace ratio LDA. Extensive experiments on both synthetic data and nine real benchmark data sets show the effectiveness of the proposed robust LDA method."
A Near Neighbor Scoring Function Improves Error Bounds Based on Worst Likely Assignments,"Error bounds based on worst likely assignments operate in a transductive classification setting, where there are training examples with known class labels and the goal is to classify a set of working examples with known inputs and unknown class labels. These bounds have proven effective even for small data sets. This note describes a method to improve these bounds. The method evaluates potential assignments of class labels to working examples based on whether the assigned labels agree with labels on nearby training examples. The method shows the greatest improvement for validating accurate classifiers."
Learning from Distributions via Support Measure Machines,"This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework."
Learning from Distributions via Support Measure Machines,"This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework."
Learning from Distributions via Support Measure Machines,"This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework."
Learning from Distributions via Support Measure Machines,"This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework."
Community Detection in the Labelled Stochastic Block Model,"We consider the problem of community detection from observed interactions between individuals, in the context where multiple types of interaction are possible. We use labelled stochastic block models to represent the observed data, where labels correspond to interaction types. Focusing on a two-community scenario, we formulate a conjecture that the detection task goes from infeasible to feasible as the average degree in the model crosses a particular threshold. To substantiate the conjecture, we prove that the given threshold correctly identifies a transition on the behaviour of belief propagation from insensitive to sensitive. We further prove that the same threshold corresponds to the transition in a related inference problem on a tree model from infeasible to feasible. Finally, numerical results using belief propagation for community detection give further support to the conjecture."
Community Detection in the Labelled Stochastic Block Model,"We consider the problem of community detection from observed interactions between individuals, in the context where multiple types of interaction are possible. We use labelled stochastic block models to represent the observed data, where labels correspond to interaction types. Focusing on a two-community scenario, we formulate a conjecture that the detection task goes from infeasible to feasible as the average degree in the model crosses a particular threshold. To substantiate the conjecture, we prove that the given threshold correctly identifies a transition on the behaviour of belief propagation from insensitive to sensitive. We further prove that the same threshold corresponds to the transition in a related inference problem on a tree model from infeasible to feasible. Finally, numerical results using belief propagation for community detection give further support to the conjecture."
Community Detection in the Labelled Stochastic Block Model,"We consider the problem of community detection from observed interactions between individuals, in the context where multiple types of interaction are possible. We use labelled stochastic block models to represent the observed data, where labels correspond to interaction types. Focusing on a two-community scenario, we formulate a conjecture that the detection task goes from infeasible to feasible as the average degree in the model crosses a particular threshold. To substantiate the conjecture, we prove that the given threshold correctly identifies a transition on the behaviour of belief propagation from insensitive to sensitive. We further prove that the same threshold corresponds to the transition in a related inference problem on a tree model from infeasible to feasible. Finally, numerical results using belief propagation for community detection give further support to the conjecture."
Feature Clustering for Accelerating Parallel Coordinate Descent,"Large scale $\ell_1$-regularized loss minimization problemsarise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\ell_1$ regularizedproblems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\ell_1$-regularization problems."
Feature Clustering for Accelerating Parallel Coordinate Descent,"Large scale $\ell_1$-regularized loss minimization problemsarise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\ell_1$ regularizedproblems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\ell_1$-regularization problems."
Feature Clustering for Accelerating Parallel Coordinate Descent,"Large scale $\ell_1$-regularized loss minimization problemsarise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\ell_1$ regularizedproblems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\ell_1$-regularization problems."
Feature Clustering for Accelerating Parallel Coordinate Descent,"Large scale $\ell_1$-regularized loss minimization problemsarise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\ell_1$ regularizedproblems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\ell_1$-regularization problems."
Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA,"Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules in correspondence to components of a monosynaptic spinal loop. Results of emulated activities are shown. The paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions. "
Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA,"Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules in correspondence to components of a monosynaptic spinal loop. Results of emulated activities are shown. The paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions. "
Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA,"Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules in correspondence to components of a monosynaptic spinal loop. Results of emulated activities are shown. The paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions. "
Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition,We present data-driven techniques to augment Bag of Word (BoW) models within a Vector Space Model framework. This approach allows for more robust modeling and recognition of complex activities for instances where the structure and topology of the activities are not known a priori. Our approach addresses the limitation of standard BoW approaches that fail to represent the temporal information inherent in activity streams. We also introduce the use of randomly sampled Regular Expressions to capture the global structure of activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in three complex data-sets. 
From Sparsity to Compositionality: Unsupervised Learning of Active Basis Models for Image Representation,"Sparsity and compositionality are two fundamental concepts in image representation and vision. In this paper, we explore the transition between these two concepts. Specifically, we adopt the Olshausen-Field model where images are represented by linear superpositions of small sets of wavelet elements selected from a large dictionary. We seek to learn recurring compositional patterns of the wavelet elements in the sparse representations. We represent each compositional pattern by an active basis model, which is a composition of a small number of Gabor wavelets automatically selected from a dictionary of such wavelets. The selected wavelets are allowed to perturb their locations and orientations so that the linear basis formed by the selected wavelets become active and the active basis forms a deformable template. For a given set of training images, our method learns a vocabulary of such active basis templates, so that each training image can be represented by a small number of templates that are translated, rotated, scaled and deformed versions of the learned templates in the vocabulary. "
From Sparsity to Compositionality: Unsupervised Learning of Active Basis Models for Image Representation,"Sparsity and compositionality are two fundamental concepts in image representation and vision. In this paper, we explore the transition between these two concepts. Specifically, we adopt the Olshausen-Field model where images are represented by linear superpositions of small sets of wavelet elements selected from a large dictionary. We seek to learn recurring compositional patterns of the wavelet elements in the sparse representations. We represent each compositional pattern by an active basis model, which is a composition of a small number of Gabor wavelets automatically selected from a dictionary of such wavelets. The selected wavelets are allowed to perturb their locations and orientations so that the linear basis formed by the selected wavelets become active and the active basis forms a deformable template. For a given set of training images, our method learns a vocabulary of such active basis templates, so that each training image can be represented by a small number of templates that are translated, rotated, scaled and deformed versions of the learned templates in the vocabulary. "
From Sparsity to Compositionality: Unsupervised Learning of Active Basis Models for Image Representation,"Sparsity and compositionality are two fundamental concepts in image representation and vision. In this paper, we explore the transition between these two concepts. Specifically, we adopt the Olshausen-Field model where images are represented by linear superpositions of small sets of wavelet elements selected from a large dictionary. We seek to learn recurring compositional patterns of the wavelet elements in the sparse representations. We represent each compositional pattern by an active basis model, which is a composition of a small number of Gabor wavelets automatically selected from a dictionary of such wavelets. The selected wavelets are allowed to perturb their locations and orientations so that the linear basis formed by the selected wavelets become active and the active basis forms a deformable template. For a given set of training images, our method learns a vocabulary of such active basis templates, so that each training image can be represented by a small number of templates that are translated, rotated, scaled and deformed versions of the learned templates in the vocabulary. "
From Sparsity to Compositionality: Unsupervised Learning of Active Basis Models for Image Representation,"Sparsity and compositionality are two fundamental concepts in image representation and vision. In this paper, we explore the transition between these two concepts. Specifically, we adopt the Olshausen-Field model where images are represented by linear superpositions of small sets of wavelet elements selected from a large dictionary. We seek to learn recurring compositional patterns of the wavelet elements in the sparse representations. We represent each compositional pattern by an active basis model, which is a composition of a small number of Gabor wavelets automatically selected from a dictionary of such wavelets. The selected wavelets are allowed to perturb their locations and orientations so that the linear basis formed by the selected wavelets become active and the active basis forms a deformable template. For a given set of training images, our method learns a vocabulary of such active basis templates, so that each training image can be represented by a small number of templates that are translated, rotated, scaled and deformed versions of the learned templates in the vocabulary. "
From Sparsity to Compositionality: Unsupervised Learning of Active Basis Models for Image Representation,"Sparsity and compositionality are two fundamental concepts in image representation and vision. In this paper, we explore the transition between these two concepts. Specifically, we adopt the Olshausen-Field model where images are represented by linear superpositions of small sets of wavelet elements selected from a large dictionary. We seek to learn recurring compositional patterns of the wavelet elements in the sparse representations. We represent each compositional pattern by an active basis model, which is a composition of a small number of Gabor wavelets automatically selected from a dictionary of such wavelets. The selected wavelets are allowed to perturb their locations and orientations so that the linear basis formed by the selected wavelets become active and the active basis forms a deformable template. For a given set of training images, our method learns a vocabulary of such active basis templates, so that each training image can be represented by a small number of templates that are translated, rotated, scaled and deformed versions of the learned templates in the vocabulary. "
Stratified Sensor Network Calibration from TDOA Measurements,"This paper presents a study of the sensor network calibrationthat arise in TOA and TDOA measurements.Such calibration arise in several applications such ascalibration of (acoustic) microphone arrays, calibration ofwifi-transmittor arraysand radio antenna networks. There are at present no solution methodsfor the minimal cases. In the paper we improve on the currentstate-of-the-artby solving several new cases that are 'closer' to the minimal ones. We apply a three-step stratification process, (i) using a novel set ofrank constraintsto determine the unknown offsets, (ii) applying factorization techniquesto determinesenders and receivers up to unknown affine transformation and (iii) determiningthe affine stratification using the remaining constraints. For thetime-of-arrivalcase only steps (ii) and (iii) are needed.Experiments are shown both for simulated and real data with promising results.For simulated data we explore how sensitive the estimated parameters are withrespect to different degrees noise in the data and show that the proposed methods are numerically similar to previous methods. For real data, we test the method on several microphone and sound measurements and verify the results using computer vision based methods. "
Stratified Sensor Network Calibration from TDOA Measurements,"This paper presents a study of the sensor network calibrationthat arise in TOA and TDOA measurements.Such calibration arise in several applications such ascalibration of (acoustic) microphone arrays, calibration ofwifi-transmittor arraysand radio antenna networks. There are at present no solution methodsfor the minimal cases. In the paper we improve on the currentstate-of-the-artby solving several new cases that are 'closer' to the minimal ones. We apply a three-step stratification process, (i) using a novel set ofrank constraintsto determine the unknown offsets, (ii) applying factorization techniquesto determinesenders and receivers up to unknown affine transformation and (iii) determiningthe affine stratification using the remaining constraints. For thetime-of-arrivalcase only steps (ii) and (iii) are needed.Experiments are shown both for simulated and real data with promising results.For simulated data we explore how sensitive the estimated parameters are withrespect to different degrees noise in the data and show that the proposed methods are numerically similar to previous methods. For real data, we test the method on several microphone and sound measurements and verify the results using computer vision based methods. "
Active Learning of Model Evidence Using Bayesian Quadrature ,"Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy."
Active Learning of Model Evidence Using Bayesian Quadrature ,"Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy."
Active Learning of Model Evidence Using Bayesian Quadrature ,"Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy."
Bayesian Nonparametric Maximum Margin Matrix Factorization for Collaborative Prediction,"We present a probabilistic formulation to max-margin matrix factorization and build accordingly an infinite nonparametric Bayesian model to automatically resolve the unknown number of latent factors. Our work demonstrates a successful example that integrates Bayesian nonparametrics and max-margin learning, which are conventionally two separate paradigms and enjoy complementary advantages. We develop an efficient variational learning algorithm for posterior inference, and our extensive empirical studies on large-scale MovieLens and EachMovie data sets appear to demonstrate the advantages inherited from both max-margin matrix factorization and Bayesian nonparametrics."
The Subspace Intersection Problem,"This paper introduces a novel and very general problem termed the subspace intersection problem. The goal is to infer an unknown subspace from a set of given subspaces which are known to each intersect this unknown subspace. The intersections however are unknown, as well. As an example, an intuitive instance of such a subspace intersection problem is given by 4 lines in 3D space where an unknown additional line has to be determined such that the known 4 lines intersect with it. A general algebraic formulation based on the Laplace expansion for determinants is presented enabling the computation of the unknown subspace in closed-form, given sufficiently many constraining known subspaces. Moreover, an efficient numerical scheme based on a partial reduced row-echelon form is introduced. The theory and algorithms for subspace intersection problems are showcased on a structure-from-sound problem. The theory enables the computation of an unknown synchronization of sound sources in closed-form. Furthermore, improving upon previous work, new cases such as missing observations can be handled in closed-form. "
The Subspace Intersection Problem,"This paper introduces a novel and very general problem termed the subspace intersection problem. The goal is to infer an unknown subspace from a set of given subspaces which are known to each intersect this unknown subspace. The intersections however are unknown, as well. As an example, an intuitive instance of such a subspace intersection problem is given by 4 lines in 3D space where an unknown additional line has to be determined such that the known 4 lines intersect with it. A general algebraic formulation based on the Laplace expansion for determinants is presented enabling the computation of the unknown subspace in closed-form, given sufficiently many constraining known subspaces. Moreover, an efficient numerical scheme based on a partial reduced row-echelon form is introduced. The theory and algorithms for subspace intersection problems are showcased on a structure-from-sound problem. The theory enables the computation of an unknown synchronization of sound sources in closed-form. Furthermore, improving upon previous work, new cases such as missing observations can be handled in closed-form. "
A Bayesian Framework for Low-Rank and Sparse Patterns Inference in Multi-Task Learning,"In this paper we study the low-rank modeling for task relatedness in multi-task learning paradigm. We propose a Bayesian framework to infer the underlying low-rank and sparse patterns from multiple tasks. With the assumption of a shared low-rank hypothesis subspace, the framework treats the low-rank and sparse components as hidden variables which are constrained by certain sparsity inducing priors. We present a principled framework to infer the low-rank and sparse parts alternatively. Unlike optimization based methods, our method doesn't suffer from convex constraints and provides a novel perspective for multi-task learning from Bayesian standpoint. Experimental results on real-world data demonstrate the competitive capability of our method in dealing with multi-task learning problem."
A Bayesian Framework for Low-Rank and Sparse Patterns Inference in Multi-Task Learning,"In this paper we study the low-rank modeling for task relatedness in multi-task learning paradigm. We propose a Bayesian framework to infer the underlying low-rank and sparse patterns from multiple tasks. With the assumption of a shared low-rank hypothesis subspace, the framework treats the low-rank and sparse components as hidden variables which are constrained by certain sparsity inducing priors. We present a principled framework to infer the low-rank and sparse parts alternatively. Unlike optimization based methods, our method doesn't suffer from convex constraints and provides a novel perspective for multi-task learning from Bayesian standpoint. Experimental results on real-world data demonstrate the competitive capability of our method in dealing with multi-task learning problem."
Visual Topics Without Visual Words,The computer vision community has greatly benefitted from transferring techniques originally developed in the document processing domain to the visual domain by means of discretizing the features space into visual words.This paper reinvestigates the necessity of this artificially discretization of the continuous space of visual features and consequently proposes an alternative formulation of the popular topic models that is based on kernel density estimates.Results indicate the benefits of our model in terms of decreased perplexity as well as improved performance on object discovery and classification tasks.
Visual Topics Without Visual Words,The computer vision community has greatly benefitted from transferring techniques originally developed in the document processing domain to the visual domain by means of discretizing the features space into visual words.This paper reinvestigates the necessity of this artificially discretization of the continuous space of visual features and consequently proposes an alternative formulation of the popular topic models that is based on kernel density estimates.Results indicate the benefits of our model in terms of decreased perplexity as well as improved performance on object discovery and classification tasks.
Visual Topics Without Visual Words,The computer vision community has greatly benefitted from transferring techniques originally developed in the document processing domain to the visual domain by means of discretizing the features space into visual words.This paper reinvestigates the necessity of this artificially discretization of the continuous space of visual features and consequently proposes an alternative formulation of the popular topic models that is based on kernel density estimates.Results indicate the benefits of our model in terms of decreased perplexity as well as improved performance on object discovery and classification tasks.
"Greedy Bilateral Sketch, Completion & Smoothing","Recovering a large low-rank matrix from highly corrupted, incomplete or sparse outlier overwhelmed observations is the crux of various intriguing statistical problems. We explore the power of ``greedy bilateral (GreB)'' paradigm in reducing both time and sample complexities for solving these problems. GreB models a low-rank variable as a bilateral factorization, and updates the left and right factors in a mutually adaptive and greedy incremental manner. We detail how to model and solve low-rank approximation, matrix completion and robust PCA in GreB's paradigm. On their MATLAB implementations, approximating a noisy $10^4\times 10^4$ matrix of rank $500$ with SVD accuracy takes $6$s; MovieLens10M matrix can be completed in $10$s from $30\%$ of $10^7$ ratings with RMSE $0.86$ on the rest $70\%$; the low-rank background and sparse moving outliers in a $120\times 160$ video of $500$ frames are accurately separated in $1$s."
Stereoscopic Tracking with Neural Network Hardware,"A stereoscopic learning and tracking system is built using off-the-shelf parts: a pair of Cognimem V1KU neural network boards with onboard CMOS cameras; four HiTec servo motors; a Phidgets servo controller; and a laptop. A simple acrylic mount was constructed. Object learning and recognition occurs primarily within the Cognimem neural network chips, with tracking and triangulation done in software. A Kalman filter is used for predictive tracking. The resulting prototype can learn a new object quickly, usually within a second. It then can track the object?s motion in 3D space with depth-perception, at fairly fast speeds even in the presence of noisy background, without the use of structured light. The software is designed for extension to a variety of practical applications."
Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction,"We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable's marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufficiently certain about any individual decision. Whenever this is the case, we dynamically prune the variable we are confident about from the underlying factor graph. Consequently, at any time only samples of variable whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classification and image inpainting, shows that adaptive sampling can drastically accelerate MMP without sacrificing prediction accuracy."
Multi-View Clustering and Feature Learning via Structured Sparsity-Inducing Norms,"Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which make the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods."
Multi-View Clustering and Feature Learning via Structured Sparsity-Inducing Norms,"Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which make the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods."
Localized Gaussian Process Kernel Combining,"This paper investigates learning to combine multiple kernels in the context of Gaussian Process modeling. The fusing of kernels empowers the learner to take advantage of multiple heterogeneous data sources and views but poses the problem of how to best combine them. Unlike many existing algorithms where kernels are linearly combined at the matrix level, we propose an element-wise kernel combining approach with the former being a special case. The lower-level combining scheme is not limited to more flexible data integration, it also motivates new problem settings. We explore one such setting, rejecting noisy instances of MRI data to improve the learning performance. We propose EM and efficient gradient based optimization methods which make it possible to handle large scale problems. The promising experimental results demonstrate the performance of our model, and validate the effectiveness of element-level kernel combining."
Localized Gaussian Process Kernel Combining,"This paper investigates learning to combine multiple kernels in the context of Gaussian Process modeling. The fusing of kernels empowers the learner to take advantage of multiple heterogeneous data sources and views but poses the problem of how to best combine them. Unlike many existing algorithms where kernels are linearly combined at the matrix level, we propose an element-wise kernel combining approach with the former being a special case. The lower-level combining scheme is not limited to more flexible data integration, it also motivates new problem settings. We explore one such setting, rejecting noisy instances of MRI data to improve the learning performance. We propose EM and efficient gradient based optimization methods which make it possible to handle large scale problems. The promising experimental results demonstrate the performance of our model, and validate the effectiveness of element-level kernel combining."
Localized Gaussian Process Kernel Combining,"This paper investigates learning to combine multiple kernels in the context of Gaussian Process modeling. The fusing of kernels empowers the learner to take advantage of multiple heterogeneous data sources and views but poses the problem of how to best combine them. Unlike many existing algorithms where kernels are linearly combined at the matrix level, we propose an element-wise kernel combining approach with the former being a special case. The lower-level combining scheme is not limited to more flexible data integration, it also motivates new problem settings. We explore one such setting, rejecting noisy instances of MRI data to improve the learning performance. We propose EM and efficient gradient based optimization methods which make it possible to handle large scale problems. The promising experimental results demonstrate the performance of our model, and validate the effectiveness of element-level kernel combining."
A Generative Model for Parts-based Object Segmentation,"The Shape Boltzmann Machine (SBM) has recently been introduced as a state-of-the-art model of foreground/background object shape. We extend the SBM to account for the foreground object's parts. Our model, the Multinomial SBM (MSBM), can capture both local and global statistics of part shapes accurately. We combine the MSBM with an appearance model to form a fully generative model of images of objects. Parts-based image segmentations are obtained simply by performing probabilistic inference in the model. We apply the model to two challenging datasets which exhibit significant shape and appearance variability, and find that it obtains results that are comparable to the state-of-the-art."
A Generative Model for Parts-based Object Segmentation,"The Shape Boltzmann Machine (SBM) has recently been introduced as a state-of-the-art model of foreground/background object shape. We extend the SBM to account for the foreground object's parts. Our model, the Multinomial SBM (MSBM), can capture both local and global statistics of part shapes accurately. We combine the MSBM with an appearance model to form a fully generative model of images of objects. Parts-based image segmentations are obtained simply by performing probabilistic inference in the model. We apply the model to two challenging datasets which exhibit significant shape and appearance variability, and find that it obtains results that are comparable to the state-of-the-art."
Contextual-?-greedy for Context-aware recommender System,"Most existing approaches in Mobile Context-Aware Recommender Systems focus on recommending relevant items to users taking into account contextual information, such as time, location, or social aspects. However, none of them has considered the problem of user?s content dynamicity. We introduce in this paper an algorithm that tackles this dynamicity. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which user?s situation is most relevant to exploration or exploitation. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms."
Super-Bit Locality-Sensitive Hashing,"Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method towards scalable nearest neighbor search in high dimensional data space, which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments."
Super-Bit Locality-Sensitive Hashing,"Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method towards scalable nearest neighbor search in high dimensional data space, which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments."
Fisher?s Discriminant with Natual Image Priors,"We suggest a Bayesian framework to combine Fisher's discriminant and natural image statistics. The probability structure of Fisher's discriminant is expressed, and the idea of natural image statistics is utilized as prior probabilities (\emph{Natural Image Priors}). Previous methods which directly employed the spatial smoothness assumption of images can be shown as special cases within this framework, but with potentially improper priors which are different from the natural image prior. We also propose a novel method which is a \emph{maximum a posteriori probability} (MAP) estimate with the natural image prior. Experimental results on the Yale face database and the ETH-80 object categorization dataset show that the proposed method significantly outperforms the state-of-the-art methods for general image data."
Fisher?s Discriminant with Natual Image Priors,"We suggest a Bayesian framework to combine Fisher's discriminant and natural image statistics. The probability structure of Fisher's discriminant is expressed, and the idea of natural image statistics is utilized as prior probabilities (\emph{Natural Image Priors}). Previous methods which directly employed the spatial smoothness assumption of images can be shown as special cases within this framework, but with potentially improper priors which are different from the natural image prior. We also propose a novel method which is a \emph{maximum a posteriori probability} (MAP) estimate with the natural image prior. Experimental results on the Yale face database and the ETH-80 object categorization dataset show that the proposed method significantly outperforms the state-of-the-art methods for general image data."
Fisher?s Discriminant with Natual Image Priors,"We suggest a Bayesian framework to combine Fisher's discriminant and natural image statistics. The probability structure of Fisher's discriminant is expressed, and the idea of natural image statistics is utilized as prior probabilities (\emph{Natural Image Priors}). Previous methods which directly employed the spatial smoothness assumption of images can be shown as special cases within this framework, but with potentially improper priors which are different from the natural image prior. We also propose a novel method which is a \emph{maximum a posteriori probability} (MAP) estimate with the natural image prior. Experimental results on the Yale face database and the ETH-80 object categorization dataset show that the proposed method significantly outperforms the state-of-the-art methods for general image data."
Accelerated Training of Linear Object Detectors,"We describe a general and exact method to speed up the training of linear object detection systems operating in a sliding, multi-scale window fashion, such as deformable part-based models.Our approach consists of reformulating the computation of the gradient as a convolution, and making use of properties of the Fourier transform to obtain a speedup factor proportional to the linear filters' sizes. This technique does not rely on the sparsity induced by a specific loss, nor on a stochastic sub-sampling of the training examples.Experiments on the PASCAL VOC benchmark show a speedup factor of more than one order of magnitude compared to a standard exact generic method."
Accelerated Training of Linear Object Detectors,"We describe a general and exact method to speed up the training of linear object detection systems operating in a sliding, multi-scale window fashion, such as deformable part-based models.Our approach consists of reformulating the computation of the gradient as a convolution, and making use of properties of the Fourier transform to obtain a speedup factor proportional to the linear filters' sizes. This technique does not rely on the sparsity induced by a specific loss, nor on a stochastic sub-sampling of the training examples.Experiments on the PASCAL VOC benchmark show a speedup factor of more than one order of magnitude compared to a standard exact generic method."
The Bethe Partition Function of Log-supermodular Graphical Models,"Sudderth, Wainwright, and Willsky conjectured that the Bethe approximation corresponding to any fixed point of the belief propagation algorithm over an attractive, pairwise binary graphical model provides a lower bound on the true partition function. In this work, we resolve this conjecture in the affirmative by demonstrating that, for any graphical model with binary variables whose potential functions (not necessarily pairwise) are all log-supermodular, the Bethe partition function always lower bounds the true partition function.  The proof of this result follows from a new variant of the ?four functions? theorem that may be of independent interest."
A Tree Based Classifier for Non-Disjoint Classification,"Traditional approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that best describes it, and that all other labels are equally bad. In this paper we investigate the non-disjoint classification problem. In this scenario, each output label is instead a vector representing a data point's affinity for each of the classes. At test time, the class(es) with the highest affinity can be compared probabilistically. To this end, we propose a new supervised learning classifier underpinned by ensembles of decision trees. We show that by exploiting a multidimensional label vector, we can create less complex classifiers that generalize better at test time, even when a single label is sought. We compare our classifier to ensembles of classification and regression trees on both synthetic and real image data, and show the advantages of our model in terms of probabilistic interpretability and accuracy."
A Tree Based Classifier for Non-Disjoint Classification,"Traditional approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that best describes it, and that all other labels are equally bad. In this paper we investigate the non-disjoint classification problem. In this scenario, each output label is instead a vector representing a data point's affinity for each of the classes. At test time, the class(es) with the highest affinity can be compared probabilistically. To this end, we propose a new supervised learning classifier underpinned by ensembles of decision trees. We show that by exploiting a multidimensional label vector, we can create less complex classifiers that generalize better at test time, even when a single label is sought. We compare our classifier to ensembles of classification and regression trees on both synthetic and real image data, and show the advantages of our model in terms of probabilistic interpretability and accuracy."
Convergence Analysis and Ef?cient Algorithms for Hyper-Graph Matching,"This paper focuses on the theoretical and algorithmic design for hyper graph matching where the affinity is represented by a tensor. We start with the simple Gradient Assignment (HGA) that works in the discrete domain iteratively, and ?nd its m-point loop convergence property under rather weak conditions, where m is the tensor order. Then Hyper Constrained Gradient Assignment (HCGA) is proposed to avoid such unwanted iteration track, and we show this algorithm will ensure to converge to a unique discrete point. Then we further extend HCGA to the continuous domain: HSCGA which plays the role to the classical Graduate Assignment (HGAGM) as HCGA to HGA. Then we explore the underlying connection between HCGA and its continuous counterpart both theoretically and empirically: under weak conditions, we first prove HGAGM will converge to m-discrete point like HGA, then illustrate HSCGA have the same convergence property with HCGA. These findings build the theoretical connection between the proposed two algorithms. Experimental results on both synthetic and real data consent our theoretical analysis: both algorithms perform competitively to state-of-the-arts. While HCGA outstands due to its working in the discrete space, able to handle ill cases when Hungarian method return multiple solutions, and being an ef?cient and anytime algorithm."
Convergence Analysis and Ef?cient Algorithms for Hyper-Graph Matching,"This paper focuses on the theoretical and algorithmic design for hyper graph matching where the affinity is represented by a tensor. We start with the simple Gradient Assignment (HGA) that works in the discrete domain iteratively, and ?nd its m-point loop convergence property under rather weak conditions, where m is the tensor order. Then Hyper Constrained Gradient Assignment (HCGA) is proposed to avoid such unwanted iteration track, and we show this algorithm will ensure to converge to a unique discrete point. Then we further extend HCGA to the continuous domain: HSCGA which plays the role to the classical Graduate Assignment (HGAGM) as HCGA to HGA. Then we explore the underlying connection between HCGA and its continuous counterpart both theoretically and empirically: under weak conditions, we first prove HGAGM will converge to m-discrete point like HGA, then illustrate HSCGA have the same convergence property with HCGA. These findings build the theoretical connection between the proposed two algorithms. Experimental results on both synthetic and real data consent our theoretical analysis: both algorithms perform competitively to state-of-the-arts. While HCGA outstands due to its working in the discrete space, able to handle ill cases when Hungarian method return multiple solutions, and being an ef?cient and anytime algorithm."
Learning Compositional Model with Attributes,"We present a framework for unsupervised learning of a compositional model of human figures from raw images and assigning attributes - text labels associated with images - to the parts of the model. The objectives of our approach are 1) to learn a meaningful part dictionary and unknown structure that can explain the images well, and 2) to assign given set of attributes onto the parts (nodes) of themodel based on three criteria: uniqueness, precision and consistency. The learning process is structure learning and associated parameter estimation driven by images and attributes. For evaluation, we propose a new dataset containing 1000 images of upper bodies of people with attribute annotations. On this dataset, we show that our learning algorithm discovers meaningful compositional parts of human bodies and that the final model captures meaningful information about the given attributes."
Learning Compositional Model with Attributes,"We present a framework for unsupervised learning of a compositional model of human figures from raw images and assigning attributes - text labels associated with images - to the parts of the model. The objectives of our approach are 1) to learn a meaningful part dictionary and unknown structure that can explain the images well, and 2) to assign given set of attributes onto the parts (nodes) of themodel based on three criteria: uniqueness, precision and consistency. The learning process is structure learning and associated parameter estimation driven by images and attributes. For evaluation, we propose a new dataset containing 1000 images of upper bodies of people with attribute annotations. On this dataset, we show that our learning algorithm discovers meaningful compositional parts of human bodies and that the final model captures meaningful information about the given attributes."
A Comparative Study of Evolutionary Clustering Algorithm for Predicting User Preference ,"Recommender systems are the tools that predict user preferences and thus help a na?ve user in finding useful information on the world wide web. They have become a necessary agent in the information bombardment arena of World Wide Web. A number of algorithms are implemented to predict the preference of user and thereby give them recommendation. Majority of these algorithm use data mining techniques. In this paper, we present a comparative analysis of various classification algorithm and there integration with various clustering algorithm that could effectively and accurately predict temporal changes in user preferences. The paper also presents a newly developed evolutionary clustering approach and its comparative analysis. Several experiments were conducted using these algorithms based upon various parameters using WEKA (Waikato Environment for Knowledge Analysis), a Data Mining tool. The results of the experiment show that integration of clustering and classification gives promising results with higher accuracy rate and lower error rates when compared over temporal parameters."
A Comparative Study of Evolutionary Clustering Algorithm for Predicting User Preference ,"Recommender systems are the tools that predict user preferences and thus help a na?ve user in finding useful information on the world wide web. They have become a necessary agent in the information bombardment arena of World Wide Web. A number of algorithms are implemented to predict the preference of user and thereby give them recommendation. Majority of these algorithm use data mining techniques. In this paper, we present a comparative analysis of various classification algorithm and there integration with various clustering algorithm that could effectively and accurately predict temporal changes in user preferences. The paper also presents a newly developed evolutionary clustering approach and its comparative analysis. Several experiments were conducted using these algorithms based upon various parameters using WEKA (Waikato Environment for Knowledge Analysis), a Data Mining tool. The results of the experiment show that integration of clustering and classification gives promising results with higher accuracy rate and lower error rates when compared over temporal parameters."
A Constraint Boosting Approach for Matching Problems,"In matching problems, we have two goals: $1$) maximizing the compatibility function, and $2$) satisfying the matching constraints. Since matching constraints, such as one-to-one or many-to-one, fragment the feasible space, the matching problems usually have large numbers of local optima. Existing methods are vulnerable to these local optima and easily get stuck in poor local optima. In this paper, we propose a \textbf{constraint boosting algorithm}, where matching constraints are expressed as a penalty term in the objective function, and the weight of penalty term adaptively increases. When the weight of penalty term is small, the optimization process mainly depends on the compatibility function, and thus approaches regions with large compatible values; when the weight of penalty term is large, the penalty term dominates the optimization process and force it to reach a nearby point satisfying the matching constraints. Empirically, this optimization procedure can escape from poor local optima and finally reach a good optimum. Moreover, we devise dependent optimization processes which utilize the best known optimum to escape from worse optima and reach a better optimum. In this way, an optimal or close-to-optimal solution can be quickly obtained. The experiments on various matching problems clearly demonstrate the superiority of our proposed method."
Feature Selection using Partial Least Squares Regression and Optimal Experiment Design,We introduce a supervised feature selection criterion based on linear Partial Least Squares (PLS) regression. We show that an optimal feature subset can be identified by employing the optimal experiment design criterion on the loadings covariance matrix obtained from PLS. Our feature selection criterion can be derived in two different ways. We first derive it using the properties of maximum relevance and minimum redundancy on PLS models and then obtain the same criterion by applying optimal experiment design to PLS. To overcome the computational challenges in evaluating this criterion we use an approximate criterion and still obtain superior results. In our experiments we use the D-optimality criterion which maximizes the determinant of loadings covariance matrix. Experimental evaluation on four datasets demonstrates a consistent and better performance of our Optimal Loadings criterion when compared to other supervised feature selection techniques.
Feature Selection using Partial Least Squares Regression and Optimal Experiment Design,We introduce a supervised feature selection criterion based on linear Partial Least Squares (PLS) regression. We show that an optimal feature subset can be identified by employing the optimal experiment design criterion on the loadings covariance matrix obtained from PLS. Our feature selection criterion can be derived in two different ways. We first derive it using the properties of maximum relevance and minimum redundancy on PLS models and then obtain the same criterion by applying optimal experiment design to PLS. To overcome the computational challenges in evaluating this criterion we use an approximate criterion and still obtain superior results. In our experiments we use the D-optimality criterion which maximizes the determinant of loadings covariance matrix. Experimental evaluation on four datasets demonstrates a consistent and better performance of our Optimal Loadings criterion when compared to other supervised feature selection techniques.
"Towards Bridging the Gaps Between Pattern Recognition, Symbolic Representations, and Online Learning","In neural networks, and more specifically perceptrons that can perform pattern recognition, underlying associations are opaque: the weights are sub-symbolic.  This complicates the ability to use symbolic representations, reuse networks, learn, and learn online.  Methods have been proposed to address these difficulties separately, with various degrees of success.  This work shows that by implementing network dynamics differently, during the testing phase instead of the training phase, connection weights can represent the fixed points or solutions of the network.  This allows the weights to be symbolically relevant: by looking at the weights, fixed-points and symbolic-like components can be inferred.  Moreover, it is easier to learn and modify existing representations, and localizes changes required for online learning.  Although this method is functionally analogous to a single layer linear perceptron and has similar limitations, it is an important step towards realizing neural-symbolic representations and online learning using methods optimized for pattern recognition."
Graduated Non-Convexity and Graduated Concavity Procedure (GNCGCP),"In this paper we propose the graduated nonconvexity and graduated concavity procedure (GNCGCP) as a general optimization framework to approximately solve the discrete optimization problems (usually NP-hard) over the set of partial permutation matrices. As implied by its name, the GNCGCP comprises two sub-procedures, the graduated nonconvexity (GNC) which realizes a convex relaxation and graduated concavity which realizes a concave relaxation. It is proved that the GNCGCP is a type of convex-concave relaxation procedure (CCRP), but with a much simpler formulation which does not involve the convex or concave relaxation in an explicit way. Two typical NP-hard problems, (sub)graph matching and quadratic assignment problem (QAP), are employed to demonstrate the simplicity as well as state-of-the-art performance of the GNCGCP."
Random Utility Theory for Social Choice: Theory and Algorithms,"Random utility theory models an agent's preferences onalternatives by drawing a real-valued score on each alternative(typically independently) from a parameterized distribution, and thenranking according to scores. A special case that has receivedsignificant attention is the Plackett-Luce model, for which fastinference methods for maximum likelihood estimators areavailable. This paper develops conditions on general,random utility models that enable fast inference withina Bayesian framework through MC-EM, providing unimodal log-likelihoodfunctions. Results on both real-world and simulated data provide supportfor the scalability of the approach, despite its flexibility."
Local and Global Manifold Preserving Embedding,"Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional Euclidean space. In this paper, we propose a novel linear subspace learning algorithm called Local and Global Manifold Preserving Embedding (LGMPE). LGMPE can explicitly preserve both the local and global manifold structures which respectively describe local linear reconstruction structure and global geodesic distance structure, and can balance the contributions of the two parts. Therefore, our algorithm has the merit of handling complex data space. Several experiments on synthetic and real face datasets demonstrate the effectiveness of the proposed algorithm."
Local and Global Manifold Preserving Embedding,"Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional Euclidean space. In this paper, we propose a novel linear subspace learning algorithm called Local and Global Manifold Preserving Embedding (LGMPE). LGMPE can explicitly preserve both the local and global manifold structures which respectively describe local linear reconstruction structure and global geodesic distance structure, and can balance the contributions of the two parts. Therefore, our algorithm has the merit of handling complex data space. Several experiments on synthetic and real face datasets demonstrate the effectiveness of the proposed algorithm."
Local and Global Manifold Preserving Embedding,"Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional Euclidean space. In this paper, we propose a novel linear subspace learning algorithm called Local and Global Manifold Preserving Embedding (LGMPE). LGMPE can explicitly preserve both the local and global manifold structures which respectively describe local linear reconstruction structure and global geodesic distance structure, and can balance the contributions of the two parts. Therefore, our algorithm has the merit of handling complex data space. Several experiments on synthetic and real face datasets demonstrate the effectiveness of the proposed algorithm."
Local and Global Manifold Preserving Embedding,"Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional Euclidean space. In this paper, we propose a novel linear subspace learning algorithm called Local and Global Manifold Preserving Embedding (LGMPE). LGMPE can explicitly preserve both the local and global manifold structures which respectively describe local linear reconstruction structure and global geodesic distance structure, and can balance the contributions of the two parts. Therefore, our algorithm has the merit of handling complex data space. Several experiments on synthetic and real face datasets demonstrate the effectiveness of the proposed algorithm."
Exponential weight algorithms for the Exploration and Exploitation of Finite Sequences,"The adversarial multi-armed bandit algorithms are efficient, easy to implement and useful to solve online optimization problems. This paper considers a new setting, which we call scratch-game, where the sequences of rewards are finite. Is it possible to adapt standard adversarial bandit algorithms to take advantage of the proposed setting? We propose two new algorithms for finite sequences of rewards. In order to tune the exploration factor ?, we provide a lower bound of expected gain for each algorithm. We have compared favorably these algorithms with Exp3 on synthetic problems, on an ad serving problem and on emailing campaigns."
Implicit Collaborative Filtering with Random Graphs,"Implicit collaborative filtering harnesses the co-occurrence of edges between user and item vertices in a graph, to interpolate the presence of other edges. We advocate a distribution over random graphs as a novel foundation to collaborative filtering. By mimicking the power law properties of real world networks in a model, we achieve state of the art results on large scale problems. Inference is performed with a mean field approximation, and we show how a tractable procedure for the inclusion of the graph-based prior can be derived by drawing on Monte Carlo samples and stochastic optimization."
Implicit Collaborative Filtering with Random Graphs,"Implicit collaborative filtering harnesses the co-occurrence of edges between user and item vertices in a graph, to interpolate the presence of other edges. We advocate a distribution over random graphs as a novel foundation to collaborative filtering. By mimicking the power law properties of real world networks in a model, we achieve state of the art results on large scale problems. Inference is performed with a mean field approximation, and we show how a tractable procedure for the inclusion of the graph-based prior can be derived by drawing on Monte Carlo samples and stochastic optimization."
Venue Discovery,Didn't finish writing the abstract yet...
Venue Discovery,Didn't finish writing the abstract yet...
Stock Clustering through Equity Analyst Hypergraph Partitioning,"Use of industry classifications in the finance community is pervasive. They are critical to deriving a balanced portfolio of stocks and, more broadly, to risk management. Businesses, academics and government agencies have all researched and developed various schemes with mixed success. Recognizing major brokerages and research firms tend to assign their analysts to cover highly similar companies, we propose a scheme that makes use of stock analyst coverage assignments. Although creating coverage groups of highly similar stocks is not the direct goal of research firms, it may be imperative to their success because increasing similarity in coverage helps maximize synergy and derive the most value per analyst. To create our industry scheme, we construct a hypergraph where vertices represent stocks and hyperedges represent analyst coverage, connecting his/her similar companies. Using no additional information, we perform hypergraph partitioning to form clusters of stocks. Our scheme can produce any number of clusters and can dynamically update as research firms change analyst coverage as opposed to  today's leading industry schemes which have only fixed numbers of industries and require periodic expert review. Can our dynamic scheme match the quality of stock groups from the expert-driven schemes? We make head-to-head comparisons to a leading academic and a leading commercial scheme using a methodology from the finance community that measures the coincidence of stock price movements. We also compare our scheme against a clusterer that creates groups based on past return correlations. Our results rival and often exceed all 3 schemes."
Stock Clustering through Equity Analyst Hypergraph Partitioning,"Use of industry classifications in the finance community is pervasive. They are critical to deriving a balanced portfolio of stocks and, more broadly, to risk management. Businesses, academics and government agencies have all researched and developed various schemes with mixed success. Recognizing major brokerages and research firms tend to assign their analysts to cover highly similar companies, we propose a scheme that makes use of stock analyst coverage assignments. Although creating coverage groups of highly similar stocks is not the direct goal of research firms, it may be imperative to their success because increasing similarity in coverage helps maximize synergy and derive the most value per analyst. To create our industry scheme, we construct a hypergraph where vertices represent stocks and hyperedges represent analyst coverage, connecting his/her similar companies. Using no additional information, we perform hypergraph partitioning to form clusters of stocks. Our scheme can produce any number of clusters and can dynamically update as research firms change analyst coverage as opposed to  today's leading industry schemes which have only fixed numbers of industries and require periodic expert review. Can our dynamic scheme match the quality of stock groups from the expert-driven schemes? We make head-to-head comparisons to a leading academic and a leading commercial scheme using a methodology from the finance community that measures the coincidence of stock price movements. We also compare our scheme against a clusterer that creates groups based on past return correlations. Our results rival and often exceed all 3 schemes."
Experimental Proposal on Simulating Artificial Neural Netwworks Using Local Area Networks.,"An experiment for the simulation of artificial neural networks using a Local Area Network (LAN) is proposed in order to see if fundamental questions can eventually be answered such as: what is thinking made of (visual initially, but it may be possible to generalize later)? Is it possible to simulate brains using the arrangement here proposed? The paper is intended to propose the experiments in order to gather feedback concerning its nature before it is actually implemented."
Locality-Sensitive Hashing With Margin Based Feature Selection,"This paper proposes a method for locality-sensitive hashing with margin based feature selection for large-scale, high dimension data.The basic concept is to generate a hash function and importance for selection that targets a much larger size than the final targeted bits, and to select the importance in descending order until the final targeted bits are reached. The new algorithm was applied to biometric, speech, and image datasets and compared with other methods, and the effects for each data application were verified."
A Growing Technique for Construction of Conlitron and Multiconlitron,"Based on the concepts of conlitron and multiconlitron, we propose a growing construction technique for improving the performance of piecewise linear classifiers on two-class problems. This growing technique consists of two basic operations: SQUEEZE and INFLATE, it can make the classification boundary adjusted to improve the generalization ability. Experimental evaluation shows that the growing technique can simplify the structure of a conlitron/multiconlitron effectively by reducing the number of linear functions, largely keeping and even greatly improving the level of classification performances. It would come to play an important role in the subsequent development of piecewise linear learning."
A Growing Technique for Construction of Conlitron and Multiconlitron,"Based on the concepts of conlitron and multiconlitron, we propose a growing construction technique for improving the performance of piecewise linear classifiers on two-class problems. This growing technique consists of two basic operations: SQUEEZE and INFLATE, it can make the classification boundary adjusted to improve the generalization ability. Experimental evaluation shows that the growing technique can simplify the structure of a conlitron/multiconlitron effectively by reducing the number of linear functions, largely keeping and even greatly improving the level of classification performances. It would come to play an important role in the subsequent development of piecewise linear learning."
A Growing Technique for Construction of Conlitron and Multiconlitron,"Based on the concepts of conlitron and multiconlitron, we propose a growing construction technique for improving the performance of piecewise linear classifiers on two-class problems. This growing technique consists of two basic operations: SQUEEZE and INFLATE, it can make the classification boundary adjusted to improve the generalization ability. Experimental evaluation shows that the growing technique can simplify the structure of a conlitron/multiconlitron effectively by reducing the number of linear functions, largely keeping and even greatly improving the level of classification performances. It would come to play an important role in the subsequent development of piecewise linear learning."
A Growing Technique for Construction of Conlitron and Multiconlitron,"Based on the concepts of conlitron and multiconlitron, we propose a growing construction technique for improving the performance of piecewise linear classifiers on two-class problems. This growing technique consists of two basic operations: SQUEEZE and INFLATE, it can make the classification boundary adjusted to improve the generalization ability. Experimental evaluation shows that the growing technique can simplify the structure of a conlitron/multiconlitron effectively by reducing the number of linear functions, largely keeping and even greatly improving the level of classification performances. It would come to play an important role in the subsequent development of piecewise linear learning."
Coordinate Descent Optimization for L1 Norm Low Rank Tensor Factorization,"The L1 norm low-rank tensor factorization (LRTF) is recently attracting attention due to its intrinsic robustness to outliers and missing data. However, this problem is difficult to be resolved due to its non-convexity and non-smoothness. In this paper, we propose a coordinate descent (CoD) method to solve this problem. The main idea is to coordinate-wisely optimize each scalar parameter involved in the L1 LRTF model with all the others fixed. Each of these one-scalar subproblems is proved to have a closed-form (global) solution, and thus by recursively solving these small problems, an efficient algorithm can be readily constructed to tackle the original problem. The specific advantage of the proposed CoD strategy is that it provides a unified framework of solving L1 LRTF problems on both non-missing and missing data cases, and also can be easily extended to multiple other important tensor factorization tasks, such as nonnegative tensor factorization and sparse tensor factorization. Based on a series of TensorFace experiments, it is verified that our method performs significantly more robust than previous methods in the presence of outliers and/or missing data."
A new metric on the manifold of kernel matrices with application to matrix geometric means,"Symmetric positive definite (spd) matrices are remarkably pervasive in a multitude of scientific disciplines, including machine learning and optimization. We consider the fundamental task of measuring distances between two spd matrices; a task that is often nontrivial whenever an application demands the distance function to respect the non-Euclidean geometry of spd matrices. Unfortunately, typical non-Euclidean distance measures such as the Riemannian metric $\riem(X,Y)=\frob{\log(X\inv{Y})}$, are computationally demanding and also complicated to use. To allay some of these difficulties, we introduce a new metric on spd matrices: this metric not only respects non-Euclidean geometry, it also offers faster computation than $\riem$ while being less complicated to use. We support our claims theoretically via a series of theorems that relate our metric to $\riem(X,Y)$, and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances."
Multi-Label Multi-View Laplacian Hashing,"With the advent of the Internet, large scale datasets are available.The data may be high dimensional,  represented by multiple features,and associated with more than one concepts. Hashing is an effectivestrategy for dimensionality reduction and efficient nearest neighborsearch in massive datasets. We propose a novel method to seekcompact hash code that allows efficient retrieval with multi-labelmulti-view data. Based on multi-graph Laplacian, we learn theoptimal combination of heterogeneous features to effectivelydescribe multi-view data, which exploit the feature correlationsbetween different views. We obtain  the hash embedding whichpreserves the neighborhood context in the original spaces, and thesemantic embedding (i.e., multi-label vectors) at the same time.Both labeled and unlabeled data are employed for learning, andinter-label correlations are sufficiently captured to improve theperformance of hash learning with multi-label data. The experimentalevaluation on real-world datasets demonstrates promising resultsthat validate our method."
Learning a Discriminative Isotropic Space from Labeled and Unlabeled Data,"Euclidean distance measure is computationally simple and commonlyused in the task of classification. However, it does not capitalizeon any discriminant information from training samples, which iscrucial to classification performance. Moreover, Euclidean distanceis invalid when the input space is not isotropic, which often occursin many practical applications. In this paper, we learn aDiscriminative Isotropic Space (DIS) from both labeled and unlabeldata to improve classification accuracy as well as generalizationability. Intra-class compactness and inter-class separability areachieved simultaneously in the learned space, so it isdiscriminative and benefits the task of classification. The learnedspace looks as the same in every direction as possible, so Euclideandistance is suitable to measure dissimilarities between samples insuch space. Our regularized objective function implicitly minimizesmutual information between input space and transformed space, whichis reasonable from the perspective of information theory. Our methodis scalable and can be applicable to large dataset. There is nolocal optimum problem in our algorithm since the objective functionis convex and its closed form solution can be easily obtained,therefore the proposed method is more effective and more efficientthan the alternative. Experiments on real data sets demonstrate theefficacy of our method."
Shortest Path Gaussians For State Action Graphs: An Empirical Study,"We approximate action-value functions, defined on state graphs and state-action graphs derived from the Markov decision problem (MDP) to be solved, by a linear combination of shortest path Gaussian kernels. An empirical comparison on a testbed of 3 MDPs shows that this works better than using other basis functions derived from the state or state-action graph. Examples of such other basis functions are the smoothest eigenfunctions of the combinatorial and normalized Laplacian, eigenfunctions of random walk operator and diffusion wavelet bases."
Shortest Path Gaussians For State Action Graphs: An Empirical Study,"We approximate action-value functions, defined on state graphs and state-action graphs derived from the Markov decision problem (MDP) to be solved, by a linear combination of shortest path Gaussian kernels. An empirical comparison on a testbed of 3 MDPs shows that this works better than using other basis functions derived from the state or state-action graph. Examples of such other basis functions are the smoothest eigenfunctions of the combinatorial and normalized Laplacian, eigenfunctions of random walk operator and diffusion wavelet bases."
Efficient Regularized Isotonic Regression,"Isotonic regression is a nonparametric approach that fits the model subject to a set of isotonic constraints. In structured variable selection and estimation, isotonic constraints can also be used to capture the hierarchical relationships among variables according to the heredity principle. However, isotonic regression solvers cannot handle regularizers (e.g., sparsity regularizers) and constraints (e.g., non-negative constraints) on the parameters, which on the other hand are often important ingredients in learning with structured sparsity. In this paper, we propose a general optimization formulation that addresses these limitations. Efficient solvers are proposed for regression with both tree-ordered and DAG-ordered isotonic constraints. Experiments on a number of large data sets show that they are fast and accurate. Using together with proximal gradient methods, hierarchy information can now be flexibly incorporated, leading to better structured sparse models."
Efficient Regularized Isotonic Regression,"Isotonic regression is a nonparametric approach that fits the model subject to a set of isotonic constraints. In structured variable selection and estimation, isotonic constraints can also be used to capture the hierarchical relationships among variables according to the heredity principle. However, isotonic regression solvers cannot handle regularizers (e.g., sparsity regularizers) and constraints (e.g., non-negative constraints) on the parameters, which on the other hand are often important ingredients in learning with structured sparsity. In this paper, we propose a general optimization formulation that addresses these limitations. Efficient solvers are proposed for regression with both tree-ordered and DAG-ordered isotonic constraints. Experiments on a number of large data sets show that they are fast and accurate. Using together with proximal gradient methods, hierarchy information can now be flexibly incorporated, leading to better structured sparse models."
Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification,"In hierarchical classification, the prediction paths may be required to always end at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes. However, while there have been a lot of MLNP methods in hierarchical multiclass classification, performing MLNP in hierarchical multilabel classification is much more difficult. In this paper, we propose a novel MLNP algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and DAGs. We show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm. Moreover, this can be further extended to the minimization of the expected symmetric loss. Experiments are performed on a number of real-world data sets with tree- and DAG-structured label hierarchies. The  proposed method consistently outperforms other hierarchical and flat multilabel classification methods."
Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification,"In hierarchical classification, the prediction paths may be required to always end at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes. However, while there have been a lot of MLNP methods in hierarchical multiclass classification, performing MLNP in hierarchical multilabel classification is much more difficult. In this paper, we propose a novel MLNP algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and DAGs. We show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm. Moreover, this can be further extended to the minimization of the expected symmetric loss. Experiments are performed on a number of real-world data sets with tree- and DAG-structured label hierarchies. The  proposed method consistently outperforms other hierarchical and flat multilabel classification methods."
How informative is your algorithm?,"This paper elaborates a new framework for the analysis of algorithms. The generalization capability of algorithms w.r.t. their task of processing input data to meaningful outputs is introduced. The trade-off between the informativeness of algorithmic procedures and their stability against noise is attacked by generalizing Shannon's channel capacity to an arbitrary learning problem.The concept of a generalization capacity (GC) is proposedto characterize the sensitivity of an algorithm to noisy input data. GC optimally measures the generalization ability of algorithms to extract context-sensitive information w.r.t. a given output space. For the first time, different algorithms for the same data processing task, e.g. the various clustering methods, can now be objectively compared based on the bit rate of their respective capacities.The problem of grouping data is used to demonstrate this validation principle for clustering algorithms, e.g. K-means, pairwise clustering, DBSCAN and dominant set clustering. Our new validation approach selects the most informative clustering algorithm, i.e. the procedure which filters out the maximal number of stable, task-related bits relative to the hypothesis class."
How informative is your algorithm?,"This paper elaborates a new framework for the analysis of algorithms. The generalization capability of algorithms w.r.t. their task of processing input data to meaningful outputs is introduced. The trade-off between the informativeness of algorithmic procedures and their stability against noise is attacked by generalizing Shannon's channel capacity to an arbitrary learning problem.The concept of a generalization capacity (GC) is proposedto characterize the sensitivity of an algorithm to noisy input data. GC optimally measures the generalization ability of algorithms to extract context-sensitive information w.r.t. a given output space. For the first time, different algorithms for the same data processing task, e.g. the various clustering methods, can now be objectively compared based on the bit rate of their respective capacities.The problem of grouping data is used to demonstrate this validation principle for clustering algorithms, e.g. K-means, pairwise clustering, DBSCAN and dominant set clustering. Our new validation approach selects the most informative clustering algorithm, i.e. the procedure which filters out the maximal number of stable, task-related bits relative to the hypothesis class."
How informative is your algorithm?,"This paper elaborates a new framework for the analysis of algorithms. The generalization capability of algorithms w.r.t. their task of processing input data to meaningful outputs is introduced. The trade-off between the informativeness of algorithmic procedures and their stability against noise is attacked by generalizing Shannon's channel capacity to an arbitrary learning problem.The concept of a generalization capacity (GC) is proposedto characterize the sensitivity of an algorithm to noisy input data. GC optimally measures the generalization ability of algorithms to extract context-sensitive information w.r.t. a given output space. For the first time, different algorithms for the same data processing task, e.g. the various clustering methods, can now be objectively compared based on the bit rate of their respective capacities.The problem of grouping data is used to demonstrate this validation principle for clustering algorithms, e.g. K-means, pairwise clustering, DBSCAN and dominant set clustering. Our new validation approach selects the most informative clustering algorithm, i.e. the procedure which filters out the maximal number of stable, task-related bits relative to the hypothesis class."
Topical Structural Analysis on Social Communications,"The popularity of online social networks has lowered the barrier of online communications, which results in massive number of users using the networks for interaction and friendship making.To characterize the user positions and message content generated by users of different positions, we propose the Dirichlet Allocation Blockmodels (DABM) for topical structural analysis.DABM model allows each pair of users to generate message content following the topic distribution conditioned on the social positions of the two users.Compared with the earlier model, DABM allows users of the same positions to have some variability in their message topic distribution.We evaluate both DABM and the earlier model on tweets generated by a set of Twitter users connected by follow links, and show that DABM achieves better likelihood and perplexity than the earlier model."
Learning Separable Dictionaries,"Many techniques in neuroscience, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are better adapted to the considered class of signals. However, high dimensional signals and the numerical costs for applying the learned dictionary in reconstruction tasks pose enormous computational challenges.In this paper, we combine the advantages of fast implementation and of capturing the global structure of a signal. This is achieved by enforcing a separable structure on the dictionary throughout the learning process. Depending on the dimension of the signal, we propose two algorithms based on geometric optimization on the product of spheres. For signals of moderate dimension, we suggest a geometric conjugate gradient method, while for learning large scale dictionaries we use an adaption of stochastic gradient descent to the geometric setting. "
Learning Separable Dictionaries,"Many techniques in neuroscience, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are better adapted to the considered class of signals. However, high dimensional signals and the numerical costs for applying the learned dictionary in reconstruction tasks pose enormous computational challenges.In this paper, we combine the advantages of fast implementation and of capturing the global structure of a signal. This is achieved by enforcing a separable structure on the dictionary throughout the learning process. Depending on the dimension of the signal, we propose two algorithms based on geometric optimization on the product of spheres. For signals of moderate dimension, we suggest a geometric conjugate gradient method, while for learning large scale dictionaries we use an adaption of stochastic gradient descent to the geometric setting. "
Learning Separable Dictionaries,"Many techniques in neuroscience, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are better adapted to the considered class of signals. However, high dimensional signals and the numerical costs for applying the learned dictionary in reconstruction tasks pose enormous computational challenges.In this paper, we combine the advantages of fast implementation and of capturing the global structure of a signal. This is achieved by enforcing a separable structure on the dictionary throughout the learning process. Depending on the dimension of the signal, we propose two algorithms based on geometric optimization on the product of spheres. For signals of moderate dimension, we suggest a geometric conjugate gradient method, while for learning large scale dictionaries we use an adaption of stochastic gradient descent to the geometric setting. "
State-Based Hierarchical Clustering,"Once data points are grouped together into a cluster, standard complete linkage hierarchical agglomerative clustering does not allow them to migrate into separate clusters at higher strata in the hierarchy.  On the other hand, once data points are assigned to separate clusters, standard complete linkage hierarchical divisive clustering does not allow them to recombine within a single cluster at lower strata in the hierarchy.  Further, alternative dendrograms are used to resolve ties between the inter-cluster distances that determine which two clusters (cluster) will be combined (subdivided) next.  These problems make these methods difficult to use where correctness and relatively precise mathematical models are required.  The notion of finding sets of clusters based solely on the distances between the data points, as opposed to inter-cluster distances, is used to design a basic algorithm that overcomes these problems.  The state of a data set and the degrees of the data points as of a variable threshold distance are used to find the sets of clusters.  The algorithm was successfully tested on numerous test patterns and several real world data sets."
Efficient Incremental Feature Learning in Manufacturing Environments,"Most heavy duty manufacturing operations consist of hundreds of steps, where multiple measurements are taken at each step to monitor the qualityof the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate stepscan prove to be extremely useful in enhancing productivity. In this paper, we provide an approach for learning regression models in an environmentwhere features (i.e. measurements) and datapoints (i.e. individual products) are added incrementally. At each step, any finite number of features maybeadded and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares,weighted least squares, generalized least squares and ridge regression, but also for generalized linear models and lasso regression that useiterated re-weighted least squares for maximum likelihood estimation. For arbitrary regression methods, even a relaxation of the approach is noworse than using the model from a previous step or using a model that learns on the additional features and optimizes the residual of the modelat the previous step. We further validate these claims through experiments on a real industrial dataset."
Efficient Incremental Feature Learning in Manufacturing Environments,"Most heavy duty manufacturing operations consist of hundreds of steps, where multiple measurements are taken at each step to monitor the qualityof the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate stepscan prove to be extremely useful in enhancing productivity. In this paper, we provide an approach for learning regression models in an environmentwhere features (i.e. measurements) and datapoints (i.e. individual products) are added incrementally. At each step, any finite number of features maybeadded and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares,weighted least squares, generalized least squares and ridge regression, but also for generalized linear models and lasso regression that useiterated re-weighted least squares for maximum likelihood estimation. For arbitrary regression methods, even a relaxation of the approach is noworse than using the model from a previous step or using a model that learns on the additional features and optimizes the residual of the modelat the previous step. We further validate these claims through experiments on a real industrial dataset."
High Dimensional Semiparametric Scale-invariant Principal Component Analysis,"We propose a high dimensional semiparametric scale-invariant principal component analysis, named Copula Component Analysis (COCA). The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. The COCA accordingly estimates the leading eigenvector of the correlation matrix of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefficient estimator, Spearman?s rho, is exploited in estimation. We prove that, although the marginal distributions can be arbitrarily continuous, the COCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the simulated data are conducted under both ideal and noisy settings, which suggest that the COCA loses little even when the data are truely Gaussian. The COCA is also implemented on a large-scale genomic data to illustrate itsempirical usefulness."
High Dimensional Semiparametric Scale-invariant Principal Component Analysis,"We propose a high dimensional semiparametric scale-invariant principal component analysis, named Copula Component Analysis (COCA). The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. The COCA accordingly estimates the leading eigenvector of the correlation matrix of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefficient estimator, Spearman?s rho, is exploited in estimation. We prove that, although the marginal distributions can be arbitrarily continuous, the COCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the simulated data are conducted under both ideal and noisy settings, which suggest that the COCA loses little even when the data are truely Gaussian. The COCA is also implemented on a large-scale genomic data to illustrate itsempirical usefulness."
CODA: High Dimensional Copula Discriminant Analysis,"We propose a high dimensional classification method, named Copula Discriminant Analysis(CODA), which generalizes the normal-based linear discriminant analysis to the larger nonparanormal as proposed by Han Liu (2009). To simultaneously achieve estimation efficiency and robustness, the nonparametric rank-based methods including the Spearman?s rho and Kendall?s tau are exploited in estimating the covariance matrix. In high dimensional settings, we prove that the sparsity pattern of the discriminant features can be consistently recovered with the parametric rate, and the expected misclassification error is consistent to the Bayes risk. Our theory is backed up by careful numerical experiments, which show that the extra flexibility gained by the CODA method incurs little efficiency loss even when the data are truly Gaussian. These results suggest that the CODA method can be a safe replacement of the normal-based high dimensional linear discriminant analysis."
CODA: High Dimensional Copula Discriminant Analysis,"We propose a high dimensional classification method, named Copula Discriminant Analysis(CODA), which generalizes the normal-based linear discriminant analysis to the larger nonparanormal as proposed by Han Liu (2009). To simultaneously achieve estimation efficiency and robustness, the nonparametric rank-based methods including the Spearman?s rho and Kendall?s tau are exploited in estimating the covariance matrix. In high dimensional settings, we prove that the sparsity pattern of the discriminant features can be consistently recovered with the parametric rate, and the expected misclassification error is consistent to the Bayes risk. Our theory is backed up by careful numerical experiments, which show that the extra flexibility gained by the CODA method incurs little efficiency loss even when the data are truly Gaussian. These results suggest that the CODA method can be a safe replacement of the normal-based high dimensional linear discriminant analysis."
Optimal Calculation of Tensor Learning Approaches,"Tensors provide a general framework for exploring multiple factors in learning. A tensor representation also helps to reduce the small sample size problem in discriminative subspace selection and the overfitting problem in vector-based learning. Most algorithms have been extended to the tensor space to create algorithm versions with direct tensor inputs. However, very unfortunately basically all objective functions of algorithms in the tensor space are non-convex. However, sub-problems constructed by fixing all the modes but one are often convex and very easy to solve. So most of the algorithms use alternating projection optimization procedure to solve the problem. However, this method may lead to difficulty converging; iterative algorithms sometimes get stuck in a local minimum and have difficulty converging to the global solution. Here, we propose a computational framework for constrained and unconstrained tensor methods. Using our methods, the algorithm convergence situation can be improved to some extent and better solutions obtained. We applied our technique to Uncorrelated Multilinear Principal Component Analysis (UMPCA), Tensor Rank one Discriminant Analysis (TR1DA) and Support Tensor Machines (STM); results showthe effectiveness of our method."
Spectral Graph Cut from a Filtering Point of View,"We analyze spectral graph theory based image segmentation algorithms and show there is a natural connection with edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated application of bilateral filtering. Then, using this interpretation we present and implement a fast normalized cut algorithm. Experiments show that our implementation can solve the original optimization problem with a  10x-100x speedup. In addition to these practical advantages, our work shows a deep connection between two currently separate approaches to segmentation, which suggests further directions for improvements."
Spectral Graph Cut from a Filtering Point of View,"We analyze spectral graph theory based image segmentation algorithms and show there is a natural connection with edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated application of bilateral filtering. Then, using this interpretation we present and implement a fast normalized cut algorithm. Experiments show that our implementation can solve the original optimization problem with a  10x-100x speedup. In addition to these practical advantages, our work shows a deep connection between two currently separate approaches to segmentation, which suggests further directions for improvements."
Spectral Graph Cut from a Filtering Point of View,"We analyze spectral graph theory based image segmentation algorithms and show there is a natural connection with edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated application of bilateral filtering. Then, using this interpretation we present and implement a fast normalized cut algorithm. Experiments show that our implementation can solve the original optimization problem with a  10x-100x speedup. In addition to these practical advantages, our work shows a deep connection between two currently separate approaches to segmentation, which suggests further directions for improvements."
Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing,"Statistical features of neuronal spike trains are known to be non-Poisson. Here, we investigate the extent to which the non-Poissonian feature affects the efficiency of transmitting information on fluctuating firing rates. For this purpose, we introduce the Kullbuck-Leibler (KL) divergence as a measure of the efficiency of information encoding, and assume that spike trains are generated by time-rescaled renewal processes. We show that the KL divergence determines the lower bound of the degree of rate fluctuations below which the temporal variation of the firing rates is undetectable from sparse data. We also show that the KL divergence, as well as the lower bound, depends not only on the variability of spikes in terms of the coefficient of variation, but also significantly on the higher-order moments of interspike interval (ISI) distributions. We examine three specific models that are commonly used for describing the stochastic nature of spikes (the gamma, inverse Gaussian (IG) and lognormal ISI distributions), and find that the time-rescaled renewal process with the IG distribution achieves the largest KL divergence, followed by the lognormal and gamma distributions. "
Balanced Relative Margin Machines --- Closing the Gap Between Fisher's Discriminant and SVM Classification,"We approach the class of relative margin classification algorithms from the mathematical programming perspective. In particular, we propose a Balanced Relative Margin Machine and then extend it by a 1-norm regularization. Subsequently, we show the strong relations of the methods to SVMs as well as toregularized discriminant analysis techniques."
Balanced Relative Margin Machines --- Closing the Gap Between Fisher's Discriminant and SVM Classification,"We approach the class of relative margin classification algorithms from the mathematical programming perspective. In particular, we propose a Balanced Relative Margin Machine and then extend it by a 1-norm regularization. Subsequently, we show the strong relations of the methods to SVMs as well as toregularized discriminant analysis techniques."
Application of moving variance calculation to EMG based movement onset detection,"  Adaptation of human-machine interaction devices by means of physiological data requires online analysis.  To save memory and resources for realtime time series data processing  as, e.g. movement detection based on electromyographic (EMG) data,  new update formulas are needed,  when calculating mean and variance of the signal.  Applications were presented, where the length of the relevant time frame is fixed  and moving average and variance have to be calculated in realtime.  This differs from incremental calculations, which have been largely analyzed.  Formulas for an efficient calculation are introduced and applied on  synthetic and EMG data to show the benefits."
Application of moving variance calculation to EMG based movement onset detection,"  Adaptation of human-machine interaction devices by means of physiological data requires online analysis.  To save memory and resources for realtime time series data processing  as, e.g. movement detection based on electromyographic (EMG) data,  new update formulas are needed,  when calculating mean and variance of the signal.  Applications were presented, where the length of the relevant time frame is fixed  and moving average and variance have to be calculated in realtime.  This differs from incremental calculations, which have been largely analyzed.  Formulas for an efficient calculation are introduced and applied on  synthetic and EMG data to show the benefits."
Application of moving variance calculation to EMG based movement onset detection,"  Adaptation of human-machine interaction devices by means of physiological data requires online analysis.  To save memory and resources for realtime time series data processing  as, e.g. movement detection based on electromyographic (EMG) data,  new update formulas are needed,  when calculating mean and variance of the signal.  Applications were presented, where the length of the relevant time frame is fixed  and moving average and variance have to be calculated in realtime.  This differs from incremental calculations, which have been largely analyzed.  Formulas for an efficient calculation are introduced and applied on  synthetic and EMG data to show the benefits."
Application of moving variance calculation to EMG based movement onset detection,"  Adaptation of human-machine interaction devices by means of physiological data requires online analysis.  To save memory and resources for realtime time series data processing  as, e.g. movement detection based on electromyographic (EMG) data,  new update formulas are needed,  when calculating mean and variance of the signal.  Applications were presented, where the length of the relevant time frame is fixed  and moving average and variance have to be calculated in realtime.  This differs from incremental calculations, which have been largely analyzed.  Formulas for an efficient calculation are introduced and applied on  synthetic and EMG data to show the benefits."
The representer theorem for Hilbert spaces: a necessary and sufficient condition,"A family of regularization functionals is said to admit a linear representer theorem if every member of the family admits minimizers that lie in a fixed finite dimensional subspace. A recent characterization states that a general class of regularization functionals with differentiable regularizer admits a linear representer theorem if and only if the regularization term is a non-decreasing function of the norm. In this paper, we improve over such result by replacing the differentiability assumption with lower semi-continuity and deriving a proof that is independent of the dimensionality of the space."
The representer theorem for Hilbert spaces: a necessary and sufficient condition,"A family of regularization functionals is said to admit a linear representer theorem if every member of the family admits minimizers that lie in a fixed finite dimensional subspace. A recent characterization states that a general class of regularization functionals with differentiable regularizer admits a linear representer theorem if and only if the regularization term is a non-decreasing function of the norm. In this paper, we improve over such result by replacing the differentiability assumption with lower semi-continuity and deriving a proof that is independent of the dimensionality of the space."
"On the (Non-)existence of Convex, Calibrated Surrogate  Losses for Ranking","We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pairwise preferences. Our results cast lights on the intrinsic difficulty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk."
"On the (Non-)existence of Convex, Calibrated Surrogate  Losses for Ranking","We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pairwise preferences. Our results cast lights on the intrinsic difficulty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk."
"On the (Non-)existence of Convex, Calibrated Surrogate  Losses for Ranking","We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pairwise preferences. Our results cast lights on the intrinsic difficulty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk."
Hierarchical Clustered Importance Sampling,"We present a Bayesian approach that integrates sequential importance sampling within an unsupervised Dirichlet clustering model.  The sequential importance sampling provides refined estimates to hidden variables through iterative comparison with observed data, while the Dirichlet clustering partitions the space over which importance sampling is implemented.  This allows each importance sampling process to estimate a regional peak.  Without the partitioning of feature space through Dirichlet clustering, the sequential importance sampling would converge to the global peak.  We show the effectiveness of the model through a toy example as well as with high resolution radar data used for target analysis.  Additionally, we provide the inference equations for our proposed model using a variational Bayesian solution. "
Acquiring Dynamical Primitives from Unlabeled Demonstrations through Parameter Space Clustering,"In this paper we introduce a method to learn multiple dynamical systems from unlabeled data. This problem has applications in a wide range of domains where labeled data is expensive or even impossible to obtain. Applications include multi-task learning from demonstration in robotics, learning from multiple teachers that may have different strategies,  behavior modeling for surveillance or human motion interpretation. One of the difficulties of this problem is that, due to multiple objectives, trajectories can be very mixed in measurement space. Based on mixture models, we propose to cluster trajectories in a latent representation of potential functions. These potential functions are parameterized as linear combinations of a large number of features. We derive two algorithms based on Expectation Maximization for a known number of clusters and Dirichlet Processes to estimate this number from data. In both cases, we enforce sparsity to perform feature selection while clustering the data. We evaluate the proposed method using 2D synthetic trajectories and real 3D human motion data."
Acquiring Dynamical Primitives from Unlabeled Demonstrations through Parameter Space Clustering,"In this paper we introduce a method to learn multiple dynamical systems from unlabeled data. This problem has applications in a wide range of domains where labeled data is expensive or even impossible to obtain. Applications include multi-task learning from demonstration in robotics, learning from multiple teachers that may have different strategies,  behavior modeling for surveillance or human motion interpretation. One of the difficulties of this problem is that, due to multiple objectives, trajectories can be very mixed in measurement space. Based on mixture models, we propose to cluster trajectories in a latent representation of potential functions. These potential functions are parameterized as linear combinations of a large number of features. We derive two algorithms based on Expectation Maximization for a known number of clusters and Dirichlet Processes to estimate this number from data. In both cases, we enforce sparsity to perform feature selection while clustering the data. We evaluate the proposed method using 2D synthetic trajectories and real 3D human motion data."
Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress,"Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such asRmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which driveexploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions."
Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress,"Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such asRmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which driveexploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions."
Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress,"Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such asRmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which driveexploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions."
Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress,"Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such asRmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which driveexploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions."
Supervised Learning with Similarity Functions,"In this paper we address the problem of general supervised learning where data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multi-class classification problems. Inspired by an existing work on classification with similarity functions [Balcan-Blum '06], we propose a general ``goodness'' criterion for similarity functions w.r.t. a given supervised learning task. Our definition is generic enough to handle any supervised learning task and also subsumes the goodness condition of [Balcan-Blum '06]. We then adapt a landmarking technique by [Balcan-Blum '06, Balcan et al '08] to provide efficient algorithms for supervised learning using ``good'' similarity functions. In particular, we consider three important supervised learning problems : a) real-valued regression, b) ordinal regression and c) ranking. For each of these problems we show that our goodness definition satisfies the following two key properties : 1) Utility : given good similarity functions, our algorithms guarantee bounded generalization error with polynomial sample complexities, 2) Admissibility : our goodness definitions are flexible enough to at least admit all good PSD kernels; the goodness of a PSD kernel being defined according to standard definitions in literature. Furthermore, for the case of real-valued regression, we provide a natural goodness definition that when used in conjunction with a recent work on sparse vector recovery [Shalev-Schwartz '10], guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs. "
Supervised Learning with Similarity Functions,"In this paper we address the problem of general supervised learning where data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multi-class classification problems. Inspired by an existing work on classification with similarity functions [Balcan-Blum '06], we propose a general ``goodness'' criterion for similarity functions w.r.t. a given supervised learning task. Our definition is generic enough to handle any supervised learning task and also subsumes the goodness condition of [Balcan-Blum '06]. We then adapt a landmarking technique by [Balcan-Blum '06, Balcan et al '08] to provide efficient algorithms for supervised learning using ``good'' similarity functions. In particular, we consider three important supervised learning problems : a) real-valued regression, b) ordinal regression and c) ranking. For each of these problems we show that our goodness definition satisfies the following two key properties : 1) Utility : given good similarity functions, our algorithms guarantee bounded generalization error with polynomial sample complexities, 2) Admissibility : our goodness definitions are flexible enough to at least admit all good PSD kernels; the goodness of a PSD kernel being defined according to standard definitions in literature. Furthermore, for the case of real-valued regression, we provide a natural goodness definition that when used in conjunction with a recent work on sparse vector recovery [Shalev-Schwartz '10], guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs. "
Bilinear Low-Rank Matrix Hashing with Rank-Sensitive Block Permutation,"Conventional locality-sensitive hashing only handles the inputs in the forms of vectors or sets. This paper explores a new topic of matrix hashing. Search of nearest neighbor data in the matrix form can be found in many applications such as detection of image duplicates and geographical distributions. Reducing the task to 1D vector search may incur significant information loss. Our contributions are two-folds: first, under mild assumptions on the matrix, we investigate the relationship between the matrix rank and the difficulty of nearest matrix search. Based on the notation of \emph{random matrix similarity}, we show that low-rank matrices are often more favorable in such a task. Second, we compare different schemes on matrix hashing. Among them, the linear hashing scheme is a natural extension from the conventional vector field to matrices. However, for matrices of size $n \times m$, its complexity is $O(n m)$ for both computation and storage, which is unaffordable for large matrices.To solve this problem, this paper proposes a bilinear matrix hashing scheme which greatly reduces the complexity by exploiting the matrix singular structures. We present very interesting observations on bilinear matrix hashing: although the efficacy of the scheme is mainly determined by the matrix low-rankness, its performance will be enhanced when matrix blocks are permutated to increase the rank. Therefore the final bilinear scheme involves a very interesting and practical tradeoff between block-level high-rankness and inner-block low-rankness. We conduct an in-depth study of this issue based on kurtosis analysis and provide a rank-sensitive algorithm for learning a universal block permutation. Extensive experiments are presented to corroborate the effectiveness of the proposed matrix hashing scheme."
Cocktail Party Processing via Structured Prediction,"While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random fields (CRFs) to classify speech dominance withineach time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises."
Cocktail Party Processing via Structured Prediction,"While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random fields (CRFs) to classify speech dominance withineach time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises."
Robustness and risk-sensitivity in Markov decision processes,"We uncover relations between robust MDPs and risk-sensitive MDPs.  The objective of a robust MDP is to minimize a function, such as the expectation of cumulative cost, for the worst case when the parameters have uncertainties.  The objective of a risk-sensitive MDP is to minimize a risk measure of the cumulative cost when the parameters are known.  We show that a risk-sensitive MDP of minimizing the expected exponential utility is equivalent to a robust MDP of minimizing the worst-case expectation with a penalty for the deviation of the uncertain parameters from their nominal values, which is measured with the Kullback-Leibler divergence.  We also show that a risk-sensitive MDP of minimizing an iterated risk measure that is composed of certain coherent risk measures is equivalent to a robust MDP of minimizing the worst-case expectation when the possible deviations of uncertain parameters from their nominal values are characterized with a concave function."
Group Bridge Regression is Beta Uniformly Stable,"Sparsity and stability are the desired properties of a machine learning algorithm, especially in high-dimensional data problems. The group Lasso is a sparsity promoting method that exploits grouped variables and has been studied intensively in the literature. However, it has been recently discovered that like the Lasso, the group Lasso does not possess the desirable stability property which is used to establish generalization. As sparsity and uniform stability are conflicting goals, an immediate question of the optimal trade-off is raised: What would be a stable algorithm that trades off sparsity well? In the context of regression with grouped variables, we show that the existing bridge regression method already provides a natural trade-off between stability and sparsity. We show that group bridge regression, where in the group sparsity is controlled via its bridge order, is uniformly beta-stable and thus generalizes. Numerical studies on high-dimensional synthetic and splice detection problems demonstrate that group bridge regression is competitive to the group Lasso in  machine learning contexts."
Dynamical And-Or Graph Learning for Object Shape Modeling and Detection,"This paper studies a novel discriminative part-based model to represent and recognize object shapes with an ``And-Or graph''. We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP, is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to well handle large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization.  We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches."
Dynamical And-Or Graph Learning for Object Shape Modeling and Detection,"This paper studies a novel discriminative part-based model to represent and recognize object shapes with an ``And-Or graph''. We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP, is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to well handle large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization.  We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches."
Bayesian Score for Orders of Variables,"Analysing high-dimensional data is hard. Search spaces of many optimizationproblems grow exponentially with respect to the dimension of data, while at thesame time, a large number of data points is needed in order to havestatistically solid results.  One way to approach the curse of dimensionalityis to impose some structure on the variables. In this paper we study linearorders between variables, a natural structure for many datasets.  The goal ofthis paper is measure the quality of the order for the variables inmulti-dimensional data. Such a score will help us to decide if the order athand is genuinely significant. Our score will be based on the intuition thatthe order should have a good score when variables that depend on each other areclose to each in the given order.More specifically, given a dataset and an order we consider a set of Bayesiannetworks that depends on this order. If the order is good, that is, if thedependent variables are close to each other, then these models will have a goodposterior probability, which we will estimate with Bayesian InformationCriterion (BIC). Since we are not interested in the actual models, wemarginalize them out.  Since there are exponential number of such models forone given order, we will give a non-trivial technique for computing the orderin polynomial time. We also show how to significantly improve the computationaltime if we allow some minimal error in the score."
Some Results About the Vapnik-Chervonenkis Entropy and the Rademacher Complexity,"This paper deals with the problem of identifying a connection between the Vapnik-Chervonenkis (VC) Entropy, a notion of complexity introduced by Vapnik in his seminal work, and the Rademacher Complexity, a more powerful notion of complexity, which has been in the limelight of several works in the recent literature. In order to establish this connection, we refine some previously known relationships and derive a new result. Our proposal allows computing an admissible range for the Rademacher Complexity, given a value of the VC-Entropy, and vice versa, therefore opening new appealing research perspectives in the field of assessing the complexity of an hypothesis space. "
Some Results About the Vapnik-Chervonenkis Entropy and the Rademacher Complexity,"This paper deals with the problem of identifying a connection between the Vapnik-Chervonenkis (VC) Entropy, a notion of complexity introduced by Vapnik in his seminal work, and the Rademacher Complexity, a more powerful notion of complexity, which has been in the limelight of several works in the recent literature. In order to establish this connection, we refine some previously known relationships and derive a new result. Our proposal allows computing an admissible range for the Rademacher Complexity, given a value of the VC-Entropy, and vice versa, therefore opening new appealing research perspectives in the field of assessing the complexity of an hypothesis space. "
Some Results About the Vapnik-Chervonenkis Entropy and the Rademacher Complexity,"This paper deals with the problem of identifying a connection between the Vapnik-Chervonenkis (VC) Entropy, a notion of complexity introduced by Vapnik in his seminal work, and the Rademacher Complexity, a more powerful notion of complexity, which has been in the limelight of several works in the recent literature. In order to establish this connection, we refine some previously known relationships and derive a new result. Our proposal allows computing an admissible range for the Rademacher Complexity, given a value of the VC-Entropy, and vice versa, therefore opening new appealing research perspectives in the field of assessing the complexity of an hypothesis space. "
Some Results About the Vapnik-Chervonenkis Entropy and the Rademacher Complexity,"This paper deals with the problem of identifying a connection between the Vapnik-Chervonenkis (VC) Entropy, a notion of complexity introduced by Vapnik in his seminal work, and the Rademacher Complexity, a more powerful notion of complexity, which has been in the limelight of several works in the recent literature. In order to establish this connection, we refine some previously known relationships and derive a new result. Our proposal allows computing an admissible range for the Rademacher Complexity, given a value of the VC-Entropy, and vice versa, therefore opening new appealing research perspectives in the field of assessing the complexity of an hypothesis space. "
Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning,"Many problems in machine learning can be (re)formulated as linearly constrained convex programs. When there are only two variables, such problems can be efficiently solved by the alternating direction method (ADM) or its linearized version (LADM). However, more often there are more than two variables, but the corresponding theories on ADM and LADM are rather scarce. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-variable separable convex programs efficiently. LADMPSAP is particularly suitable for sparse representation and low rank recovery problems because its subproblems have closed form solutions and the sparsity and low rankness of the iterates can be preserved during the iteration. It is also highly parallelizable and hence fits for parallel or distributed computing. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the sufficient and necessary conditions} for global convergence. We further propose a simple optimality measure and reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, we devise a practical version of LADMPSAP for faster convergence. Numerical experiments on the latent low rank representation problem testify to the speed and accuracy advantages of LADMPSAP."
Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning,"Many problems in machine learning can be (re)formulated as linearly constrained convex programs. When there are only two variables, such problems can be efficiently solved by the alternating direction method (ADM) or its linearized version (LADM). However, more often there are more than two variables, but the corresponding theories on ADM and LADM are rather scarce. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-variable separable convex programs efficiently. LADMPSAP is particularly suitable for sparse representation and low rank recovery problems because its subproblems have closed form solutions and the sparsity and low rankness of the iterates can be preserved during the iteration. It is also highly parallelizable and hence fits for parallel or distributed computing. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the sufficient and necessary conditions} for global convergence. We further propose a simple optimality measure and reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, we devise a practical version of LADMPSAP for faster convergence. Numerical experiments on the latent low rank representation problem testify to the speed and accuracy advantages of LADMPSAP."
Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a differentiable function given a finite number of evaluations to the function. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost as accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and we provide a finite-sample analysis."
Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a differentiable function given a finite number of evaluations to the function. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost as accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and we provide a finite-sample analysis."
"Managing sparsity, time, and quality of inference in topic models","Inference is an integral part of probabilistic topic models, but is often non-trivial to derive an efficient algorithm for a specific model. It is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents. In this paper we propose a simple framework for inference in probabilistic topic models, denoted by FW. This framework is general and flexible enough to be easily adapted to mixture models. It has a linear convergence rate, offers an easy way to incorporate prior knowledge, and allows us to swiftly recover sparse latent representations of documents and to trade off sparsity against quality and time. We demonstrate the goodness and flexibility of FW over existing inference methods by a number of tasks and experiments."
"Managing sparsity, time, and quality of inference in topic models","Inference is an integral part of probabilistic topic models, but is often non-trivial to derive an efficient algorithm for a specific model. It is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents. In this paper we propose a simple framework for inference in probabilistic topic models, denoted by FW. This framework is general and flexible enough to be easily adapted to mixture models. It has a linear convergence rate, offers an easy way to incorporate prior knowledge, and allows us to swiftly recover sparse latent representations of documents and to trade off sparsity against quality and time. We demonstrate the goodness and flexibility of FW over existing inference methods by a number of tasks and experiments."
HMM-based Temporal Pattern Modeling of Brain States in Smoke Rehabilitation using fMRI,"Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging method widely used in research on human physio-cognitive architecture. Substantial work has been performed using fMRI to map various cognitive functions to regions of the brain or to functional networks. Also, considerable effort has been made to interpret spatial activation patterns of these functional networks to better explain and understand the underlying cognitive states. Recent approaches to this problem involve data-driven analysis such as Independent Component Analysis (ICA) and Machine Learning (ML). However, these approaches do not fully account for the intrinsic temporal properties of fMRI data and do not always provide the explanatory power necessary to reveal underlying neural processes. To achieve a more thorough representation, we propose a novel technique based upon Hidden Markov Models (HMM). We apply it to classify cognitive states such as craving and resisting using fMRI data collected in a prior study. We address the challenges in modifying Independent Component (IC) based features to fit the proposed model, evaluate its classification accuracy along with its explanatory power, and compare it to other popular ML-based techniques such as Support Vector Machine (SVM) and Neural Network (NN). The results show that HMM-based models achieve an average classification accuracy of 83% which compares favorably to SVM. We also show that although NN achieves similar average accuracy across various numbers of classes, the HMM-based models show superior performance when distinguishing among a larger number of classes. Further, and most interestingly, we find that the optimal number of hidden states in HMM-based models agrees with the optimal number of functional networks for cognitive state classification previously found in the literature, which suggests that in this context HMM-based models possess the explanatory power often lacking in ML-based approaches."
HMM-based Temporal Pattern Modeling of Brain States in Smoke Rehabilitation using fMRI,"Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging method widely used in research on human physio-cognitive architecture. Substantial work has been performed using fMRI to map various cognitive functions to regions of the brain or to functional networks. Also, considerable effort has been made to interpret spatial activation patterns of these functional networks to better explain and understand the underlying cognitive states. Recent approaches to this problem involve data-driven analysis such as Independent Component Analysis (ICA) and Machine Learning (ML). However, these approaches do not fully account for the intrinsic temporal properties of fMRI data and do not always provide the explanatory power necessary to reveal underlying neural processes. To achieve a more thorough representation, we propose a novel technique based upon Hidden Markov Models (HMM). We apply it to classify cognitive states such as craving and resisting using fMRI data collected in a prior study. We address the challenges in modifying Independent Component (IC) based features to fit the proposed model, evaluate its classification accuracy along with its explanatory power, and compare it to other popular ML-based techniques such as Support Vector Machine (SVM) and Neural Network (NN). The results show that HMM-based models achieve an average classification accuracy of 83% which compares favorably to SVM. We also show that although NN achieves similar average accuracy across various numbers of classes, the HMM-based models show superior performance when distinguishing among a larger number of classes. Further, and most interestingly, we find that the optimal number of hidden states in HMM-based models agrees with the optimal number of functional networks for cognitive state classification previously found in the literature, which suggests that in this context HMM-based models possess the explanatory power often lacking in ML-based approaches."
HMM-based Temporal Pattern Modeling of Brain States in Smoke Rehabilitation using fMRI,"Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging method widely used in research on human physio-cognitive architecture. Substantial work has been performed using fMRI to map various cognitive functions to regions of the brain or to functional networks. Also, considerable effort has been made to interpret spatial activation patterns of these functional networks to better explain and understand the underlying cognitive states. Recent approaches to this problem involve data-driven analysis such as Independent Component Analysis (ICA) and Machine Learning (ML). However, these approaches do not fully account for the intrinsic temporal properties of fMRI data and do not always provide the explanatory power necessary to reveal underlying neural processes. To achieve a more thorough representation, we propose a novel technique based upon Hidden Markov Models (HMM). We apply it to classify cognitive states such as craving and resisting using fMRI data collected in a prior study. We address the challenges in modifying Independent Component (IC) based features to fit the proposed model, evaluate its classification accuracy along with its explanatory power, and compare it to other popular ML-based techniques such as Support Vector Machine (SVM) and Neural Network (NN). The results show that HMM-based models achieve an average classification accuracy of 83% which compares favorably to SVM. We also show that although NN achieves similar average accuracy across various numbers of classes, the HMM-based models show superior performance when distinguishing among a larger number of classes. Further, and most interestingly, we find that the optimal number of hidden states in HMM-based models agrees with the optimal number of functional networks for cognitive state classification previously found in the literature, which suggests that in this context HMM-based models possess the explanatory power often lacking in ML-based approaches."
HMM-based Temporal Pattern Modeling of Brain States in Smoke Rehabilitation using fMRI,"Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging method widely used in research on human physio-cognitive architecture. Substantial work has been performed using fMRI to map various cognitive functions to regions of the brain or to functional networks. Also, considerable effort has been made to interpret spatial activation patterns of these functional networks to better explain and understand the underlying cognitive states. Recent approaches to this problem involve data-driven analysis such as Independent Component Analysis (ICA) and Machine Learning (ML). However, these approaches do not fully account for the intrinsic temporal properties of fMRI data and do not always provide the explanatory power necessary to reveal underlying neural processes. To achieve a more thorough representation, we propose a novel technique based upon Hidden Markov Models (HMM). We apply it to classify cognitive states such as craving and resisting using fMRI data collected in a prior study. We address the challenges in modifying Independent Component (IC) based features to fit the proposed model, evaluate its classification accuracy along with its explanatory power, and compare it to other popular ML-based techniques such as Support Vector Machine (SVM) and Neural Network (NN). The results show that HMM-based models achieve an average classification accuracy of 83% which compares favorably to SVM. We also show that although NN achieves similar average accuracy across various numbers of classes, the HMM-based models show superior performance when distinguishing among a larger number of classes. Further, and most interestingly, we find that the optimal number of hidden states in HMM-based models agrees with the optimal number of functional networks for cognitive state classification previously found in the literature, which suggests that in this context HMM-based models possess the explanatory power often lacking in ML-based approaches."
Distributed Non-Stochastic Experts,"We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator. At each time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon T, while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal O(\sqrt{log(n)T}) regret bound at the cost of T communication. (ii) No communication: Each site runs an independent copy ? the regret is O(\sqrt{log(n)kT}) and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than \sqrt{kT} and communication better than T. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret O(\sqrt{k^{5(1+\epsilon)/6} T}) and communication O(T/k^\epsilon), for any value of \epsilon in (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off."
A Fast Greedy Algorithm for Structure Learning of Markov Random Fields,The problem of automatic structure learning of Markov Random Fields (MRFs) from data is considered. Structure learning is an NP-hard problem whose exact solution is intractable even for a fairly small number of variables. There are several approximate algorithms for solving this problem. Only few of them work with arbitrary size factors (not limited to pairwise factors) and they are still slow. We present a new faster algorithm for structure learning of discrete MRFs with arbitrary size factors. It is based on a greedy approach and uses a heuristics that reduces the search space to two subspaces guaranteed to have features with the highest score at each iteration of the algorithm. We show through experiments on real-world and simulated data sets that the proposed algorithm gives same accuracy and significantly improves learning time compared to the existing commonly used algorithms for structure learning of MRFs with arbitrary size factors.
A Fast Greedy Algorithm for Structure Learning of Markov Random Fields,The problem of automatic structure learning of Markov Random Fields (MRFs) from data is considered. Structure learning is an NP-hard problem whose exact solution is intractable even for a fairly small number of variables. There are several approximate algorithms for solving this problem. Only few of them work with arbitrary size factors (not limited to pairwise factors) and they are still slow. We present a new faster algorithm for structure learning of discrete MRFs with arbitrary size factors. It is based on a greedy approach and uses a heuristics that reduces the search space to two subspaces guaranteed to have features with the highest score at each iteration of the algorithm. We show through experiments on real-world and simulated data sets that the proposed algorithm gives same accuracy and significantly improves learning time compared to the existing commonly used algorithms for structure learning of MRFs with arbitrary size factors.
Learning Image Descriptors with the Boosting-Trick,"In this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection. The main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes. This representation can  be improved using machine learning, however, past approaches  have been limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.  While kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive. We propose  to use the {\em  boosting-trick}  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space. The non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance."
Learning Image Descriptors with the Boosting-Trick,"In this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection. The main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes. This representation can  be improved using machine learning, however, past approaches  have been limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.  While kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive. We propose  to use the {\em  boosting-trick}  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space. The non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance."
Learning Image Descriptors with the Boosting-Trick,"In this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection. The main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes. This representation can  be improved using machine learning, however, past approaches  have been limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.  While kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive. We propose  to use the {\em  boosting-trick}  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space. The non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance."
Learning Image Descriptors with the Boosting-Trick,"In this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection. The main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes. This representation can  be improved using machine learning, however, past approaches  have been limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.  While kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive. We propose  to use the {\em  boosting-trick}  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space. The non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance."
Fast Resampling Weighted v-Statistics,"In this paper, a novel, computationally fast, and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed. To avoid any real resampling, we have linked this problem with finite group action and converted it into a problem of orbit enumeration. For further computational cost reduction, an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively. The computational complexity analysis shows reduction in the computational cost from n! or nn level to low-order polynomial level."
Fast Resampling Weighted v-Statistics,"In this paper, a novel, computationally fast, and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed. To avoid any real resampling, we have linked this problem with finite group action and converted it into a problem of orbit enumeration. For further computational cost reduction, an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively. The computational complexity analysis shows reduction in the computational cost from n! or nn level to low-order polynomial level."
Fast Resampling Weighted v-Statistics,"In this paper, a novel, computationally fast, and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed. To avoid any real resampling, we have linked this problem with finite group action and converted it into a problem of orbit enumeration. For further computational cost reduction, an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively. The computational complexity analysis shows reduction in the computational cost from n! or nn level to low-order polynomial level."
Multi-task Vector Field Learning,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach."
Multi-task Vector Field Learning,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach."
Squared-loss Mutual Information Regularization,"The information maximization principle, which prefers classifiers that maximize an information measure between data and labels, is a useful probabilistic alternative to the low-density separation principle. In this paper, we specify the squared-loss mutual information (SMI) as the information measure to be maximized and propose SMI regularization (SMIR) for semi-supervised classification. SMIR offers all of the following four abilities to semi-supervised algorithms: analytical solution, out-of-sample and multi-class classification, and probabilistic output. Furthermore, SMIR results in learning algorithms with data-dependent risk bounds that even incorporate the information of unlabeled data. Experiments demonstrate that SMIR compares favorably with state-of-the-art information-theoretic regularization approaches in terms of both accuracy and computational efficiency. "
Squared-loss Mutual Information Regularization,"The information maximization principle, which prefers classifiers that maximize an information measure between data and labels, is a useful probabilistic alternative to the low-density separation principle. In this paper, we specify the squared-loss mutual information (SMI) as the information measure to be maximized and propose SMI regularization (SMIR) for semi-supervised classification. SMIR offers all of the following four abilities to semi-supervised algorithms: analytical solution, out-of-sample and multi-class classification, and probabilistic output. Furthermore, SMIR results in learning algorithms with data-dependent risk bounds that even incorporate the information of unlabeled data. Experiments demonstrate that SMIR compares favorably with state-of-the-art information-theoretic regularization approaches in terms of both accuracy and computational efficiency. "
Squared-loss Mutual Information Regularization,"The information maximization principle, which prefers classifiers that maximize an information measure between data and labels, is a useful probabilistic alternative to the low-density separation principle. In this paper, we specify the squared-loss mutual information (SMI) as the information measure to be maximized and propose SMI regularization (SMIR) for semi-supervised classification. SMIR offers all of the following four abilities to semi-supervised algorithms: analytical solution, out-of-sample and multi-class classification, and probabilistic output. Furthermore, SMIR results in learning algorithms with data-dependent risk bounds that even incorporate the information of unlabeled data. Experiments demonstrate that SMIR compares favorably with state-of-the-art information-theoretic regularization approaches in terms of both accuracy and computational efficiency. "
Squared-loss Mutual Information Regularization,"The information maximization principle, which prefers classifiers that maximize an information measure between data and labels, is a useful probabilistic alternative to the low-density separation principle. In this paper, we specify the squared-loss mutual information (SMI) as the information measure to be maximized and propose SMI regularization (SMIR) for semi-supervised classification. SMIR offers all of the following four abilities to semi-supervised algorithms: analytical solution, out-of-sample and multi-class classification, and probabilistic output. Furthermore, SMIR results in learning algorithms with data-dependent risk bounds that even incorporate the information of unlabeled data. Experiments demonstrate that SMIR compares favorably with state-of-the-art information-theoretic regularization approaches in terms of both accuracy and computational efficiency. "
Stochastic Frank-Wolfe Optimization for Large Margin Structured Prediction,"We consider the use of Frank-Wolfe optimization algorithms on the dualformulation of structural SVMs. These yield simple algorithms which only needaccess to an approximate maximization oracle for the structured predictionproblem and thus have wide applicability. This perspective provides insightson previous popular algorithms as we show that batch subgradient as well asthe cutting plane algorithms are equivalent to versions of Frank-Wolfealgorithms, enabling us to improve on their convergence analysis by harvestingthe Frank-Wolfe literature. Moreover, we propose a new stochastic coordinatedescent version of Frank-Wolfe which yields a provably convergent optimizationalgorithm for structural SVMs with total run-time \note{independent} of the number of training examples, like Pegasos, but with duality gap certificate guarantees and step-size robustness thanks to the use of line-search. Our experiments on sequence prediction indicate that this simple algorithm outperforms all other optimization algorithms which only have access to the maximization oracle."
Stochastic Frank-Wolfe Optimization for Large Margin Structured Prediction,"We consider the use of Frank-Wolfe optimization algorithms on the dualformulation of structural SVMs. These yield simple algorithms which only needaccess to an approximate maximization oracle for the structured predictionproblem and thus have wide applicability. This perspective provides insightson previous popular algorithms as we show that batch subgradient as well asthe cutting plane algorithms are equivalent to versions of Frank-Wolfealgorithms, enabling us to improve on their convergence analysis by harvestingthe Frank-Wolfe literature. Moreover, we propose a new stochastic coordinatedescent version of Frank-Wolfe which yields a provably convergent optimizationalgorithm for structural SVMs with total run-time \note{independent} of the number of training examples, like Pegasos, but with duality gap certificate guarantees and step-size robustness thanks to the use of line-search. Our experiments on sequence prediction indicate that this simple algorithm outperforms all other optimization algorithms which only have access to the maximization oracle."
Stochastic Frank-Wolfe Optimization for Large Margin Structured Prediction,"We consider the use of Frank-Wolfe optimization algorithms on the dualformulation of structural SVMs. These yield simple algorithms which only needaccess to an approximate maximization oracle for the structured predictionproblem and thus have wide applicability. This perspective provides insightson previous popular algorithms as we show that batch subgradient as well asthe cutting plane algorithms are equivalent to versions of Frank-Wolfealgorithms, enabling us to improve on their convergence analysis by harvestingthe Frank-Wolfe literature. Moreover, we propose a new stochastic coordinatedescent version of Frank-Wolfe which yields a provably convergent optimizationalgorithm for structural SVMs with total run-time \note{independent} of the number of training examples, like Pegasos, but with duality gap certificate guarantees and step-size robustness thanks to the use of line-search. Our experiments on sequence prediction indicate that this simple algorithm outperforms all other optimization algorithms which only have access to the maximization oracle."
Stochastic Frank-Wolfe Optimization for Large Margin Structured Prediction,"We consider the use of Frank-Wolfe optimization algorithms on the dualformulation of structural SVMs. These yield simple algorithms which only needaccess to an approximate maximization oracle for the structured predictionproblem and thus have wide applicability. This perspective provides insightson previous popular algorithms as we show that batch subgradient as well asthe cutting plane algorithms are equivalent to versions of Frank-Wolfealgorithms, enabling us to improve on their convergence analysis by harvestingthe Frank-Wolfe literature. Moreover, we propose a new stochastic coordinatedescent version of Frank-Wolfe which yields a provably convergent optimizationalgorithm for structural SVMs with total run-time \note{independent} of the number of training examples, like Pegasos, but with duality gap certificate guarantees and step-size robustness thanks to the use of line-search. Our experiments on sequence prediction indicate that this simple algorithm outperforms all other optimization algorithms which only have access to the maximization oracle."
Regularization Cascade for Joint Learning,"We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. We propose a top-down iterative method which starts with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased, until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes where different groupings of tasks seem particularly useful for effective sharing, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods."
Regularization Cascade for Joint Learning,"We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. We propose a top-down iterative method which starts with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased, until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes where different groupings of tasks seem particularly useful for effective sharing, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods."
User Distances in Composite Social Networks,"An important challenge in social network analysis is how to measure users' distances or latent similarity as a single measure. It is the basis for link prediction, community detection, social marketing, etc. Due to the sparsity of data, where each user may just have a few connected friends, it is hard to effectively learn distance measures for any given pair of users in a single network. Nowadays however, people engage in multiple social networks, such as Facebook, Twitter, LinkedIn, etc., where these networks form a composite social network. To alleviate the data sparsity problem for a given single network, we propose a transfer metric learning approach to collectively exploit the knowledge from multiple networks, to extract related and richer knowledge through an optimization and boosting-based framework. Then, we use this knowledge for user distance modeling in a target network. It projects and combine social information, behaviors and profile attributes, measured in different magnitudes, in an embedding space that preserves important community information and network structure. We empirically evaluate the effectiveness of the constructed user distance measure on link prediction - an important social modeling task, and state-of-the-art methods are typically based on user distances. Empirical studies demonstrate that the proposed approach significantly improves the link-prediction precision over several state-of-the-art baselines."
User Distances in Composite Social Networks,"An important challenge in social network analysis is how to measure users' distances or latent similarity as a single measure. It is the basis for link prediction, community detection, social marketing, etc. Due to the sparsity of data, where each user may just have a few connected friends, it is hard to effectively learn distance measures for any given pair of users in a single network. Nowadays however, people engage in multiple social networks, such as Facebook, Twitter, LinkedIn, etc., where these networks form a composite social network. To alleviate the data sparsity problem for a given single network, we propose a transfer metric learning approach to collectively exploit the knowledge from multiple networks, to extract related and richer knowledge through an optimization and boosting-based framework. Then, we use this knowledge for user distance modeling in a target network. It projects and combine social information, behaviors and profile attributes, measured in different magnitudes, in an embedding space that preserves important community information and network structure. We empirically evaluate the effectiveness of the constructed user distance measure on link prediction - an important social modeling task, and state-of-the-art methods are typically based on user distances. Empirical studies demonstrate that the proposed approach significantly improves the link-prediction precision over several state-of-the-art baselines."
User Distances in Composite Social Networks,"An important challenge in social network analysis is how to measure users' distances or latent similarity as a single measure. It is the basis for link prediction, community detection, social marketing, etc. Due to the sparsity of data, where each user may just have a few connected friends, it is hard to effectively learn distance measures for any given pair of users in a single network. Nowadays however, people engage in multiple social networks, such as Facebook, Twitter, LinkedIn, etc., where these networks form a composite social network. To alleviate the data sparsity problem for a given single network, we propose a transfer metric learning approach to collectively exploit the knowledge from multiple networks, to extract related and richer knowledge through an optimization and boosting-based framework. Then, we use this knowledge for user distance modeling in a target network. It projects and combine social information, behaviors and profile attributes, measured in different magnitudes, in an embedding space that preserves important community information and network structure. We empirically evaluate the effectiveness of the constructed user distance measure on link prediction - an important social modeling task, and state-of-the-art methods are typically based on user distances. Empirical studies demonstrate that the proposed approach significantly improves the link-prediction precision over several state-of-the-art baselines."
Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions,"We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to be met in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efficient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains."
Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions,"We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to be met in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efficient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains."
Kernel Information Bottleneck,"The Information Bottleneck (IB) method was introduced as a principled approach to extracting efficient representations of one set of variables with respect to another from empirical data, thus extending the classical notion of minimal sufficient statistics. The method was proposed as a general computational principle for information processing in the brain and has been used in many machine learning and neuroscience applications. The original algorithm for solving the problem was based on the Arimoto-Blahut alternating projection algorithm, but was not guaranteed to converge to a global optimum, which jeopardized the practicality of the approach. One exception was the multivariate Gaussian case, for which the IB was shown to have an efficient globally converging algorithm (GIB) that extended Canonical Correlation Analysis (CCA). The main advantage over CCA was that it provided a continuous optimal tradeoff between the minimality and sufficiency of the representation (described by the information curve), hence allowing for optimal multi-scale analysis of the data using simple spectral methods. Here we extend the Gaussian solution of the Information Bottleneck (GIB) to a much wider family of distributions using the kernel trick, and make it practical for essentially any empirical data. Our main theoretical result is in proving that for any kernel we can obtain a lower bound on the true information-curve, using information geometry. We illustrate the algorithm on real data and discuss some of its potential new applications. "
Kernel Information Bottleneck,"The Information Bottleneck (IB) method was introduced as a principled approach to extracting efficient representations of one set of variables with respect to another from empirical data, thus extending the classical notion of minimal sufficient statistics. The method was proposed as a general computational principle for information processing in the brain and has been used in many machine learning and neuroscience applications. The original algorithm for solving the problem was based on the Arimoto-Blahut alternating projection algorithm, but was not guaranteed to converge to a global optimum, which jeopardized the practicality of the approach. One exception was the multivariate Gaussian case, for which the IB was shown to have an efficient globally converging algorithm (GIB) that extended Canonical Correlation Analysis (CCA). The main advantage over CCA was that it provided a continuous optimal tradeoff between the minimality and sufficiency of the representation (described by the information curve), hence allowing for optimal multi-scale analysis of the data using simple spectral methods. Here we extend the Gaussian solution of the Information Bottleneck (GIB) to a much wider family of distributions using the kernel trick, and make it practical for essentially any empirical data. Our main theoretical result is in proving that for any kernel we can obtain a lower bound on the true information-curve, using information geometry. We illustrate the algorithm on real data and discuss some of its potential new applications. "
Compressed learning: learning in the smallest capacity machine,"Supervised  learning focuses on using finite input-output samples topredict the intrinsic relationship  between  input and output.Several methods such as neural networks, kernel method and redundantdictionary learning have been developed to tackle the problem andall of them have been proved to be universal approximants anduniversal consistent learners. However, the good approximation andgeneralization capabilities are established on the basis ofimplementing high computational complexity algorithms in largecapacity machines, which results extremely high computationalburdens in the learning processes. Motivated by the well knownKolmogorov width theory, we deduce optimal linear sparserepresentations for a large number of priors. Taking the set ofsparse representations as the desired machine, we construct smallestcapacity machines which achieves the optimal learning rate. Based onthis, we propose a new learning methodology called the compressedlearning that implements learning in the smallest capacity machinewith simple linear problem algorithms. Our analysis reveals that thecompressed learning is a high quality method that possesses palmaryapproximation capability, prominent generalization capability andlow computational burden."
Compressed learning: learning in the smallest capacity machine,"Supervised  learning focuses on using finite input-output samples topredict the intrinsic relationship  between  input and output.Several methods such as neural networks, kernel method and redundantdictionary learning have been developed to tackle the problem andall of them have been proved to be universal approximants anduniversal consistent learners. However, the good approximation andgeneralization capabilities are established on the basis ofimplementing high computational complexity algorithms in largecapacity machines, which results extremely high computationalburdens in the learning processes. Motivated by the well knownKolmogorov width theory, we deduce optimal linear sparserepresentations for a large number of priors. Taking the set ofsparse representations as the desired machine, we construct smallestcapacity machines which achieves the optimal learning rate. Based onthis, we propose a new learning methodology called the compressedlearning that implements learning in the smallest capacity machinewith simple linear problem algorithms. Our analysis reveals that thecompressed learning is a high quality method that possesses palmaryapproximation capability, prominent generalization capability andlow computational burden."
Compressed learning: learning in the smallest capacity machine,"Supervised  learning focuses on using finite input-output samples topredict the intrinsic relationship  between  input and output.Several methods such as neural networks, kernel method and redundantdictionary learning have been developed to tackle the problem andall of them have been proved to be universal approximants anduniversal consistent learners. However, the good approximation andgeneralization capabilities are established on the basis ofimplementing high computational complexity algorithms in largecapacity machines, which results extremely high computationalburdens in the learning processes. Motivated by the well knownKolmogorov width theory, we deduce optimal linear sparserepresentations for a large number of priors. Taking the set ofsparse representations as the desired machine, we construct smallestcapacity machines which achieves the optimal learning rate. Based onthis, we propose a new learning methodology called the compressedlearning that implements learning in the smallest capacity machinewith simple linear problem algorithms. Our analysis reveals that thecompressed learning is a high quality method that possesses palmaryapproximation capability, prominent generalization capability andlow computational burden."
Compressed learning: learning in the smallest capacity machine,"Supervised  learning focuses on using finite input-output samples topredict the intrinsic relationship  between  input and output.Several methods such as neural networks, kernel method and redundantdictionary learning have been developed to tackle the problem andall of them have been proved to be universal approximants anduniversal consistent learners. However, the good approximation andgeneralization capabilities are established on the basis ofimplementing high computational complexity algorithms in largecapacity machines, which results extremely high computationalburdens in the learning processes. Motivated by the well knownKolmogorov width theory, we deduce optimal linear sparserepresentations for a large number of priors. Taking the set ofsparse representations as the desired machine, we construct smallestcapacity machines which achieves the optimal learning rate. Based onthis, we propose a new learning methodology called the compressedlearning that implements learning in the smallest capacity machinewith simple linear problem algorithms. Our analysis reveals that thecompressed learning is a high quality method that possesses palmaryapproximation capability, prominent generalization capability andlow computational burden."
Layered Dirichlet Process for Hierarchical Modeling and Multi-Level Segmentation of Sequential Data,"Hierarchical Bayesian Non-parametric models, such as the Hierarchical Dirichlet Process (HDP) and the HDP-HMM, have been proposed as infinite dimensional mixture models for grouped data problems. Many applications, such as multi-layer segmentation of news transcripts into broad categories and individual stories, require incorporation of prior knowledge at different layers of the model hierarchy. Such prior knowledge may include layer-specific exchangeability assumptions for the data, and group-specific prior distribution on atoms. Elegant incorporation of such knowledge requires a hierarchy of non-parametric processes where atoms at each layer correspond to one layer in the Bayesian parameter hierarchy. We propose the Layered Dirichlet Process (LDP), which consists of layered sets of DPs, where atoms at each layer map to Dirichlet Processes  for the next layer. This can also be interpreted as Layered Chinese Restaurant Process (LCRP), where table assignments for customers at one layer provide restaurant assignments at the next layer. We show how prior information at different layers can be naturally incorporated in our framework. For learning and inference, we propose a block-wise Gibbs sampling algorithm, that samples the layered table assignment of each data item as a block. We demonstrate using experiments that the proposed model outperforms the HDP-HMM and the sticky HDP-HMM in modeling and multi-layer segmentation of news transcripts. "
Layered Dirichlet Process for Hierarchical Modeling and Multi-Level Segmentation of Sequential Data,"Hierarchical Bayesian Non-parametric models, such as the Hierarchical Dirichlet Process (HDP) and the HDP-HMM, have been proposed as infinite dimensional mixture models for grouped data problems. Many applications, such as multi-layer segmentation of news transcripts into broad categories and individual stories, require incorporation of prior knowledge at different layers of the model hierarchy. Such prior knowledge may include layer-specific exchangeability assumptions for the data, and group-specific prior distribution on atoms. Elegant incorporation of such knowledge requires a hierarchy of non-parametric processes where atoms at each layer correspond to one layer in the Bayesian parameter hierarchy. We propose the Layered Dirichlet Process (LDP), which consists of layered sets of DPs, where atoms at each layer map to Dirichlet Processes  for the next layer. This can also be interpreted as Layered Chinese Restaurant Process (LCRP), where table assignments for customers at one layer provide restaurant assignments at the next layer. We show how prior information at different layers can be naturally incorporated in our framework. For learning and inference, we propose a block-wise Gibbs sampling algorithm, that samples the layered table assignment of each data item as a block. We demonstrate using experiments that the proposed model outperforms the HDP-HMM and the sticky HDP-HMM in modeling and multi-layer segmentation of news transcripts. "
Layered Dirichlet Process for Hierarchical Modeling and Multi-Level Segmentation of Sequential Data,"Hierarchical Bayesian Non-parametric models, such as the Hierarchical Dirichlet Process (HDP) and the HDP-HMM, have been proposed as infinite dimensional mixture models for grouped data problems. Many applications, such as multi-layer segmentation of news transcripts into broad categories and individual stories, require incorporation of prior knowledge at different layers of the model hierarchy. Such prior knowledge may include layer-specific exchangeability assumptions for the data, and group-specific prior distribution on atoms. Elegant incorporation of such knowledge requires a hierarchy of non-parametric processes where atoms at each layer correspond to one layer in the Bayesian parameter hierarchy. We propose the Layered Dirichlet Process (LDP), which consists of layered sets of DPs, where atoms at each layer map to Dirichlet Processes  for the next layer. This can also be interpreted as Layered Chinese Restaurant Process (LCRP), where table assignments for customers at one layer provide restaurant assignments at the next layer. We show how prior information at different layers can be naturally incorporated in our framework. For learning and inference, we propose a block-wise Gibbs sampling algorithm, that samples the layered table assignment of each data item as a block. We demonstrate using experiments that the proposed model outperforms the HDP-HMM and the sticky HDP-HMM in modeling and multi-layer segmentation of news transcripts. "
Layered Dirichlet Process for Hierarchical Modeling and Multi-Level Segmentation of Sequential Data,"Hierarchical Bayesian Non-parametric models, such as the Hierarchical Dirichlet Process (HDP) and the HDP-HMM, have been proposed as infinite dimensional mixture models for grouped data problems. Many applications, such as multi-layer segmentation of news transcripts into broad categories and individual stories, require incorporation of prior knowledge at different layers of the model hierarchy. Such prior knowledge may include layer-specific exchangeability assumptions for the data, and group-specific prior distribution on atoms. Elegant incorporation of such knowledge requires a hierarchy of non-parametric processes where atoms at each layer correspond to one layer in the Bayesian parameter hierarchy. We propose the Layered Dirichlet Process (LDP), which consists of layered sets of DPs, where atoms at each layer map to Dirichlet Processes  for the next layer. This can also be interpreted as Layered Chinese Restaurant Process (LCRP), where table assignments for customers at one layer provide restaurant assignments at the next layer. We show how prior information at different layers can be naturally incorporated in our framework. For learning and inference, we propose a block-wise Gibbs sampling algorithm, that samples the layered table assignment of each data item as a block. We demonstrate using experiments that the proposed model outperforms the HDP-HMM and the sticky HDP-HMM in modeling and multi-layer segmentation of news transcripts. "
Part-segment Features for Body Pose Estimation,"We propose part-segment (PS) features for estimating an articulated pose in still images. The proposed PS features are developed to evaluate image likelihood of each body part (e.g. head, torso, and arms) robustly to background clutter and nuisance textures on the body and clothing. While general gradient-based features (e.g. HOG) might include many nuisance responses, the PS features represent only the boundaries of the body parts by iterative binary segmentation with updating shape prior on each part. The PS features are fused complementarily with gradient features using discriminative training and adaptive weighting. Comparative experiments with public datasets demonstrate improvement in pose estimation by the PS features compared with conventional features."
Mean Shift by Hebbian Learning for the L0-norm based Sparse Coding Problem,"It is well known that the emergence of Gabor-like receptive fields of simple cells in the visual cortex can be predicted by sparse coding, which has been validated to be a general coding strategy for sensory systems. Until now, the neural mechanism of the learning procedure is far to be understood. In this paper, a novel mean shift algorithm using Hebbian learning for the L0-norm based sparse coding problem is proposed. Different from other studies on sparse coding, our work do not consider the coefficients of basis functions but model the selection of basis functions. We perform an analysis on the spatial distribution of input samples and conclude that the basis functions are related to the local maxima of the distribution. Detailed theoretical investigation affirms this conclusion, showing that the sparse coding problem with the L0-norm is essentially one of mode detection and the basis functions are the modes of the kernel density estimate. The mean shift algorithm is presented for mode detection, and its updating rule is proved to be Hebbian. Experimental results demonstrate the robustness of the algorithm in producing basis functions well tuned for orientation as well as spatial frequency."
Mean Shift by Hebbian Learning for the L0-norm based Sparse Coding Problem,"It is well known that the emergence of Gabor-like receptive fields of simple cells in the visual cortex can be predicted by sparse coding, which has been validated to be a general coding strategy for sensory systems. Until now, the neural mechanism of the learning procedure is far to be understood. In this paper, a novel mean shift algorithm using Hebbian learning for the L0-norm based sparse coding problem is proposed. Different from other studies on sparse coding, our work do not consider the coefficients of basis functions but model the selection of basis functions. We perform an analysis on the spatial distribution of input samples and conclude that the basis functions are related to the local maxima of the distribution. Detailed theoretical investigation affirms this conclusion, showing that the sparse coding problem with the L0-norm is essentially one of mode detection and the basis functions are the modes of the kernel density estimate. The mean shift algorithm is presented for mode detection, and its updating rule is proved to be Hebbian. Experimental results demonstrate the robustness of the algorithm in producing basis functions well tuned for orientation as well as spatial frequency."
Incremental Subspace Learning for Unsupervised Domain Adaptation Using Manifold Optimization,"Developing recognition methods to handle cases in which the distribution of data changes between the training and testing phases of model building is a common problem in many fields. In computer vision, it is often the case that both well-defined geometric changes can be estimated, such as rotation in images, as well as more abstract transformations, such as changes in camera quality. It has recently been proposed that utilizing an incremental framework for adapting between two different domains is beneficial as it is able to encapsulate much of the change in the distribution between training and testing sets, regardless of the domain shift. In this paper, we adopt a general method for learning intermediate subspaces between training and testing domains on which we build models to perform both regression andclassification. We show that utilizing this method to obtain intermediate subspaces is more beneficial than similar methods both from a practical and a theoretical standpoint, as the proposed method admits an attractive interpretation in terms of posing it as an optimization problem. The method is used to improve performance in object recognition tasks when the shift in domain is not a geometric one and in age estimation tasks both when domain shifts are geometric and nongeometric."
Incremental Subspace Learning for Unsupervised Domain Adaptation Using Manifold Optimization,"Developing recognition methods to handle cases in which the distribution of data changes between the training and testing phases of model building is a common problem in many fields. In computer vision, it is often the case that both well-defined geometric changes can be estimated, such as rotation in images, as well as more abstract transformations, such as changes in camera quality. It has recently been proposed that utilizing an incremental framework for adapting between two different domains is beneficial as it is able to encapsulate much of the change in the distribution between training and testing sets, regardless of the domain shift. In this paper, we adopt a general method for learning intermediate subspaces between training and testing domains on which we build models to perform both regression andclassification. We show that utilizing this method to obtain intermediate subspaces is more beneficial than similar methods both from a practical and a theoretical standpoint, as the proposed method admits an attractive interpretation in terms of posing it as an optimization problem. The method is used to improve performance in object recognition tasks when the shift in domain is not a geometric one and in age estimation tasks both when domain shifts are geometric and nongeometric."
On the Distribution of Salient Objects in Photographs,"In recent years it has become apparent that a Gaussian center bias can serve as an important prior for visual saliency detection, which has been demonstrated for predicting human eye fixations and salient object detection. Tseng et al. have shown that the photographer?s tendency to place interesting objects in the center is a likely cause for the center bias of eye fixations. In this contribution, we investigate the potential influence of the photographer?s center bias on salient object detection. Most importantly, we show that the centroid locations of salient objects in photographs correlate strongly with a Gaussian model. This is an important insight, because it provides a theoretical motivation and justification for the integration of such a center bias in salient object detection algorithms and helps to understand why Gaussian models are so effective. To assess the influence of the center bias on salient object detection, we integrate an explicit Gaussian center bias model into two recently proposed salient object detection algorithms. This way, we not just quantify the influence of the Gaussian center bias on pixel- and segment-based salient object detection, but we are also able to improve the state-of-the-art in terms of F1 score, F_\beta score, area under the recall-precision curve, area under the receiver operating characteristic curve, and hit-rate on the well-known data set by Achanta and Liu."
On the Distribution of Salient Objects in Photographs,"In recent years it has become apparent that a Gaussian center bias can serve as an important prior for visual saliency detection, which has been demonstrated for predicting human eye fixations and salient object detection. Tseng et al. have shown that the photographer?s tendency to place interesting objects in the center is a likely cause for the center bias of eye fixations. In this contribution, we investigate the potential influence of the photographer?s center bias on salient object detection. Most importantly, we show that the centroid locations of salient objects in photographs correlate strongly with a Gaussian model. This is an important insight, because it provides a theoretical motivation and justification for the integration of such a center bias in salient object detection algorithms and helps to understand why Gaussian models are so effective. To assess the influence of the center bias on salient object detection, we integrate an explicit Gaussian center bias model into two recently proposed salient object detection algorithms. This way, we not just quantify the influence of the Gaussian center bias on pixel- and segment-based salient object detection, but we are also able to improve the state-of-the-art in terms of F1 score, F_\beta score, area under the recall-precision curve, area under the receiver operating characteristic curve, and hit-rate on the well-known data set by Achanta and Liu."
An Optimal Policy for Target Localization with Application to Electron Microscopy,"This paper considers the task of finding a target location by making a limited number of sequential observation. Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position. Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked. In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon. Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies."
An Optimal Policy for Target Localization with Application to Electron Microscopy,"This paper considers the task of finding a target location by making a limited number of sequential observation. Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position. Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked. In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon. Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies."
An Optimal Policy for Target Localization with Application to Electron Microscopy,"This paper considers the task of finding a target location by making a limited number of sequential observation. Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position. Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked. In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon. Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies."
Data split strategies for evolving predictive models,"A conventional textbook prescription for building good predictive models is to split the data into three parts: training set (for model fitting), validation set (for model selection), and test set (for final model assessment).  Predictive models can potentially evolve over time as developers improve their performance either by acquiring new data or improving the existing model. The main contribution of this paper is to discuss problems encountered and propose various workflows to manage the allocation of newly acquired data into different sets in such dynamic model building and updating scenarios. We propose three different workflows (parallel dump, serial waterfall, and hybrid) for allocating new data into the existing training, validation, and test splits. Particular emphasis is laid on avoiding the bias due to the repeated use of the existing validation or the test set. "
Multimodal similarity-preserving hashing,"We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable.The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches, our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks."
Multimodal similarity-preserving hashing,"We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable.The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches, our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks."
Multimodal similarity-preserving hashing,"We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable.The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches, our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks."
Multimodal similarity-preserving hashing,"We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable.The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches, our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks."
Multimodal diffusion geometry by joint diagonalization of Laplacians,"We construct an extension of diffusion geometry to multiple modalities through joint approximate diagonalization of Laplacian matrices. This naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of manifold learning, retrieval, and clustering demonstrating that the joint diffusion geometry frequently better captures the inherent structure of multi-modal data. We also show that many previous attempts to construct multimodal spectral clustering can be seen as particular cases of joint approximate diagonalization of the Laplacians."
Multimodal diffusion geometry by joint diagonalization of Laplacians,"We construct an extension of diffusion geometry to multiple modalities through joint approximate diagonalization of Laplacian matrices. This naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of manifold learning, retrieval, and clustering demonstrating that the joint diffusion geometry frequently better captures the inherent structure of multi-modal data. We also show that many previous attempts to construct multimodal spectral clustering can be seen as particular cases of joint approximate diagonalization of the Laplacians."
Multimodal diffusion geometry by joint diagonalization of Laplacians,"We construct an extension of diffusion geometry to multiple modalities through joint approximate diagonalization of Laplacian matrices. This naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of manifold learning, retrieval, and clustering demonstrating that the joint diffusion geometry frequently better captures the inherent structure of multi-modal data. We also show that many previous attempts to construct multimodal spectral clustering can be seen as particular cases of joint approximate diagonalization of the Laplacians."
Multimodal diffusion geometry by joint diagonalization of Laplacians,"We construct an extension of diffusion geometry to multiple modalities through joint approximate diagonalization of Laplacian matrices. This naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of manifold learning, retrieval, and clustering demonstrating that the joint diffusion geometry frequently better captures the inherent structure of multi-modal data. We also show that many previous attempts to construct multimodal spectral clustering can be seen as particular cases of joint approximate diagonalization of the Laplacians."
Sparse Locality Preserving,"In this paper, we introduce a new subspace learning framework called Sparse Locality Preserving (SLP). Compared with the conventional methods considering global data structure, e.g., PCA, LDA, SLP aims at preserving the local neighborhood structure on data manifold and provides a more accurate data representation via locality sparse coding. In addition, it removes the common concerns of many local structure based subspace learning methods e.g., Local Linear Embedding (LLE), Neighborhood Preserving Embedding (NPE), that how to choose appropriate neighbors. SLP adaptively select neighbors based on their distances and importance, which is less sensitive to outliers than NPE. Moreover, the dual-sparse processes, i.e., the locality sparse coding, and sparse eigen-decomposition in graph embedding yield a noise-tolerant framework. Finally, SLP is learned in an inductive fashion, and therefore easily extended to different tests. We exhibit experimental results on several databases and demonstrate the effectiveness of the proposed method."
Sparse Locality Preserving,"In this paper, we introduce a new subspace learning framework called Sparse Locality Preserving (SLP). Compared with the conventional methods considering global data structure, e.g., PCA, LDA, SLP aims at preserving the local neighborhood structure on data manifold and provides a more accurate data representation via locality sparse coding. In addition, it removes the common concerns of many local structure based subspace learning methods e.g., Local Linear Embedding (LLE), Neighborhood Preserving Embedding (NPE), that how to choose appropriate neighbors. SLP adaptively select neighbors based on their distances and importance, which is less sensitive to outliers than NPE. Moreover, the dual-sparse processes, i.e., the locality sparse coding, and sparse eigen-decomposition in graph embedding yield a noise-tolerant framework. Finally, SLP is learned in an inductive fashion, and therefore easily extended to different tests. We exhibit experimental results on several databases and demonstrate the effectiveness of the proposed method."
Sparse Locality Preserving,"In this paper, we introduce a new subspace learning framework called Sparse Locality Preserving (SLP). Compared with the conventional methods considering global data structure, e.g., PCA, LDA, SLP aims at preserving the local neighborhood structure on data manifold and provides a more accurate data representation via locality sparse coding. In addition, it removes the common concerns of many local structure based subspace learning methods e.g., Local Linear Embedding (LLE), Neighborhood Preserving Embedding (NPE), that how to choose appropriate neighbors. SLP adaptively select neighbors based on their distances and importance, which is less sensitive to outliers than NPE. Moreover, the dual-sparse processes, i.e., the locality sparse coding, and sparse eigen-decomposition in graph embedding yield a noise-tolerant framework. Finally, SLP is learned in an inductive fashion, and therefore easily extended to different tests. We exhibit experimental results on several databases and demonstrate the effectiveness of the proposed method."
Active sampling as a curriculum for model selection,Conventionally active learning has been used to query the best example to label in order to reduce the labeling cost. In this paper we show how active learning (uncertainty sampling in particular) can be used as a curriculum strategy for nested model selection problems. We exploit the phase transition like phenomenon observed in active learning for misspecified models to select the number of components in mixture models.
Annotation models for crowdsourced ordinal labels,"In supervised learning scenarios when acquiring good quality labels is hard, practitioners often resort to getting the data labeled by multiple noisy annotators. Various methods have been proposed to estimate the consensus labels for binary and categorical labels bycorrecting for the bias of annotators. A commonly used paradigm to annotate instances when the labels are inherently subjective is to use ordinal scales. Theannotator is asked to rate an instance on a certain discrete ordinal scale. In this paper we propose annotator models based on Receiver Operating Characteristic (ROC) curve analysis to consolidate the ordinal annotations from multiple annotators.  The models lead to simple Expectation-Maximization (EM) algorithms that estimate both the consensus labels and annotator performance jointly. Experiments on data from different domains indicate that the proposed algorithm is superior to the commonly used majority voting rule. The ROC based models have an added advantage that the annotators can be ranked using the area under the estimated ROC curve. "
Annotation models for crowdsourced ordinal labels,"In supervised learning scenarios when acquiring good quality labels is hard, practitioners often resort to getting the data labeled by multiple noisy annotators. Various methods have been proposed to estimate the consensus labels for binary and categorical labels bycorrecting for the bias of annotators. A commonly used paradigm to annotate instances when the labels are inherently subjective is to use ordinal scales. Theannotator is asked to rate an instance on a certain discrete ordinal scale. In this paper we propose annotator models based on Receiver Operating Characteristic (ROC) curve analysis to consolidate the ordinal annotations from multiple annotators.  The models lead to simple Expectation-Maximization (EM) algorithms that estimate both the consensus labels and annotator performance jointly. Experiments on data from different domains indicate that the proposed algorithm is superior to the commonly used majority voting rule. The ROC based models have an added advantage that the annotators can be ranked using the area under the estimated ROC curve. "
MILEAGE: Multiple Instance LEArning with Global Embedding," Multiple Instance Learning (MIL) methods generally represent each example as a collection of  individual instances, whereastraditional learning methods typically extract a global featurevector for the whole content of each example. Substantial priorresearch work has been proposed to solve  MIL problems. However, MILmethods do not always perform better than traditional learningmethods in all the cases. Limited research work has studied thisissue. This paper proposes a novel framework -- \emph{MultipleInstance LEArning with Global Embedding (MILEAGE)}, in which theglobal feature vectors for traditional learning methods areintegrated into the MIL setting. MILEAGE can leverage the benefitsderived from both learning settings. Within the proposed framework,a large margin method is formulated. In particular,  the proposedmethod adaptively tunes the weights on the two different kinds offeature representations (i.e., global and multiple instance) foreach example and trains the classifier simultaneously. Analternative algorithm is proposed to solve the resultingoptimization problem, which extends the bundle method to thenon-convex case. Some important properties of the proposed method,such as the convergence rate and the generalization error rate, areanalyzed. A series of experiments on both the image and textclassification tasks have been conducted to demonstrate theadvantages of the proposed method over several state-of-the-artmultiple instance and traditional learning methods."
MILEAGE: Multiple Instance LEArning with Global Embedding," Multiple Instance Learning (MIL) methods generally represent each example as a collection of  individual instances, whereastraditional learning methods typically extract a global featurevector for the whole content of each example. Substantial priorresearch work has been proposed to solve  MIL problems. However, MILmethods do not always perform better than traditional learningmethods in all the cases. Limited research work has studied thisissue. This paper proposes a novel framework -- \emph{MultipleInstance LEArning with Global Embedding (MILEAGE)}, in which theglobal feature vectors for traditional learning methods areintegrated into the MIL setting. MILEAGE can leverage the benefitsderived from both learning settings. Within the proposed framework,a large margin method is formulated. In particular,  the proposedmethod adaptively tunes the weights on the two different kinds offeature representations (i.e., global and multiple instance) foreach example and trains the classifier simultaneously. Analternative algorithm is proposed to solve the resultingoptimization problem, which extends the bundle method to thenon-convex case. Some important properties of the proposed method,such as the convergence rate and the generalization error rate, areanalyzed. A series of experiments on both the image and textclassification tasks have been conducted to demonstrate theadvantages of the proposed method over several state-of-the-artmultiple instance and traditional learning methods."
MILEAGE: Multiple Instance LEArning with Global Embedding," Multiple Instance Learning (MIL) methods generally represent each example as a collection of  individual instances, whereastraditional learning methods typically extract a global featurevector for the whole content of each example. Substantial priorresearch work has been proposed to solve  MIL problems. However, MILmethods do not always perform better than traditional learningmethods in all the cases. Limited research work has studied thisissue. This paper proposes a novel framework -- \emph{MultipleInstance LEArning with Global Embedding (MILEAGE)}, in which theglobal feature vectors for traditional learning methods areintegrated into the MIL setting. MILEAGE can leverage the benefitsderived from both learning settings. Within the proposed framework,a large margin method is formulated. In particular,  the proposedmethod adaptively tunes the weights on the two different kinds offeature representations (i.e., global and multiple instance) foreach example and trains the classifier simultaneously. Analternative algorithm is proposed to solve the resultingoptimization problem, which extends the bundle method to thenon-convex case. Some important properties of the proposed method,such as the convergence rate and the generalization error rate, areanalyzed. A series of experiments on both the image and textclassification tasks have been conducted to demonstrate theadvantages of the proposed method over several state-of-the-artmultiple instance and traditional learning methods."
Automatic Feature Induction for Stagewise Collaborative Filtering,"Recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms."
Learning Heteroscedastic Models via SOCP under Group Sparsity,"Sparse estimation methods based on  $\ell_1$  relaxation, such as the Lasso and the Dantzig selector,are powerful tools for estimating high dimensional linear models. However, in order to properly tune these methods, the variance ofthe noise is often required. This constitutes a major obstacle in applying these methods in several frameworks---such astime series, random fields, inverse problems---for which noise is rarely homoscedastic and the noise level is hard to know in advance.In this paper, we propose a new approach to the joint estimation of the conditional mean andthe conditional variance in a high-dimensional (auto-)regression setting. An attractive feature of the proposed estimator isthat it is efficiently computable even for very large scale problems by solving a second-order cone program (SOCP). We present numericalresults assessing the performance of the proposed procedure. We also establish non-asymptotic risk bounds which are nearly asstrong as those for original $\ell_1$-penalized estimators: the Lasso, the Dantzig selector and their grouped counterparts."
Learning Heteroscedastic Models via SOCP under Group Sparsity,"Sparse estimation methods based on  $\ell_1$  relaxation, such as the Lasso and the Dantzig selector,are powerful tools for estimating high dimensional linear models. However, in order to properly tune these methods, the variance ofthe noise is often required. This constitutes a major obstacle in applying these methods in several frameworks---such astime series, random fields, inverse problems---for which noise is rarely homoscedastic and the noise level is hard to know in advance.In this paper, we propose a new approach to the joint estimation of the conditional mean andthe conditional variance in a high-dimensional (auto-)regression setting. An attractive feature of the proposed estimator isthat it is efficiently computable even for very large scale problems by solving a second-order cone program (SOCP). We present numericalresults assessing the performance of the proposed procedure. We also establish non-asymptotic risk bounds which are nearly asstrong as those for original $\ell_1$-penalized estimators: the Lasso, the Dantzig selector and their grouped counterparts."
Learning Heteroscedastic Models via SOCP under Group Sparsity,"Sparse estimation methods based on  $\ell_1$  relaxation, such as the Lasso and the Dantzig selector,are powerful tools for estimating high dimensional linear models. However, in order to properly tune these methods, the variance ofthe noise is often required. This constitutes a major obstacle in applying these methods in several frameworks---such astime series, random fields, inverse problems---for which noise is rarely homoscedastic and the noise level is hard to know in advance.In this paper, we propose a new approach to the joint estimation of the conditional mean andthe conditional variance in a high-dimensional (auto-)regression setting. An attractive feature of the proposed estimator isthat it is efficiently computable even for very large scale problems by solving a second-order cone program (SOCP). We present numericalresults assessing the performance of the proposed procedure. We also establish non-asymptotic risk bounds which are nearly asstrong as those for original $\ell_1$-penalized estimators: the Lasso, the Dantzig selector and their grouped counterparts."
Selective Labeling via Error Bound Minimization,"In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods."
Selective Labeling via Error Bound Minimization,"In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods."
Textual features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks."
Textual features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks."
Textual features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks."
Textual features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks."
Volume Regularization for Binary Classification,"We introduce a large-volume box classification for binary  prediction, which maintains a subset of weight vectors, and  specifically axis-aligned boxes. Our learning algorithm seeks for a  box of large volume that contains ``simple'' weight vectors which  most of are accurate on the training set. Two versions of the  learning process are cast as convex optimization problems, and it  is shown how to solve them efficiently.  The formulation yields a  natural PAC-Bayesian performance bound and it is shown to minimize a  quantity directly aligned with it. The algorithm outperforms SVM and  the recently proposed AROW algorithm on a majority of $30$ NLP  datasets and binarized USPS optical character recognition datasets."
Volume Regularization for Binary Classification,"We introduce a large-volume box classification for binary  prediction, which maintains a subset of weight vectors, and  specifically axis-aligned boxes. Our learning algorithm seeks for a  box of large volume that contains ``simple'' weight vectors which  most of are accurate on the training set. Two versions of the  learning process are cast as convex optimization problems, and it  is shown how to solve them efficiently.  The formulation yields a  natural PAC-Bayesian performance bound and it is shown to minimize a  quantity directly aligned with it. The algorithm outperforms SVM and  the recently proposed AROW algorithm on a majority of $30$ NLP  datasets and binarized USPS optical character recognition datasets."
Towards Massive Multi-Way Classification: Structured Sparse Output Coding,"Multi-way classification with massive classes is a practical and challenging problem. In this paper, we propose structured sparse output coding, a principled way for massive multi-way classification, where a sparse output coding matrix is learned to maximize codeword separation and accuracy of each bit predictor. Moreover, we provide a concave-convex procedure based algorithm for the resultant optimization problem, which solves a series of l1 regularized convex optimization problems under linear constraints, using dual proximal gradient method. Experimental results demonstrate the effectiveness of our proposed approach."
Image Denoising and Inpainting with Deep Neural Networks,"We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method achieves state-of-the-art performance in the image denoising task. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning."
Image Denoising and Inpainting with Deep Neural Networks,"We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method achieves state-of-the-art performance in the image denoising task. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning."
Low Rank Tensor Completion with Spatio-Temporal Consistency,"Video completion is a computer vision technique to recover the missing values in video sequences by filling the unknown regions with the known information.  In recent research, tensor completion, a generalization of matrix completion for higher order data, emerges as a new solution to estimate the missing information in video with the assumption that the video frames are homogenous and correlated.  However, each video clip often stores the heterogeneous episodes and the correlations among all video frames are not high. Thus, the regular tenor completion methods are not suitable to recover the video missing values in practical applications.  To solve this problem, we propose a novel spatially-temporally consistent tensor completion method for recovering the video missing data. Instead of minimizing the average of the trace norms of all matrices unfolded along each mode in a tensor data, we introduce a new smoothness regularization along video time direction to utilize the temporal information between consecutive video frames. Meanwhile, we also minimize the trace norm of each individual video frame to employ the spatial correlations among pixels. Different to previous tensor completion approaches, our new method can keep the spatio-temporal consistency in video and do not assume the global correlation in video frames. Thus, the proposed method can be applied to the general and practical video completion applications. Our method shows promising results in all evaluations on 3D biomedical image sequence and video benchmark data sets.  "
Low Rank Tensor Completion with Spatio-Temporal Consistency,"Video completion is a computer vision technique to recover the missing values in video sequences by filling the unknown regions with the known information.  In recent research, tensor completion, a generalization of matrix completion for higher order data, emerges as a new solution to estimate the missing information in video with the assumption that the video frames are homogenous and correlated.  However, each video clip often stores the heterogeneous episodes and the correlations among all video frames are not high. Thus, the regular tenor completion methods are not suitable to recover the video missing values in practical applications.  To solve this problem, we propose a novel spatially-temporally consistent tensor completion method for recovering the video missing data. Instead of minimizing the average of the trace norms of all matrices unfolded along each mode in a tensor data, we introduce a new smoothness regularization along video time direction to utilize the temporal information between consecutive video frames. Meanwhile, we also minimize the trace norm of each individual video frame to employ the spatial correlations among pixels. Different to previous tensor completion approaches, our new method can keep the spatio-temporal consistency in video and do not assume the global correlation in video frames. Thus, the proposed method can be applied to the general and practical video completion applications. Our method shows promising results in all evaluations on 3D biomedical image sequence and video benchmark data sets.  "
TCA: High Dimensional Principal Component Analysis for non-Gaussian Data,"We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate t and logistic and it is extended to the meta-elliptical by Fang (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s(log d/n)^{1/2} estimation consistency rate in the transelliptical distribution family, even if the distributions are very heavy-tailed, have infinite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is also implemented in both numerical simulations and large-scale stock data to illustrate its empirical performance. Both theories and experiments confirm that TCA can achieve model flexibility, estimation accuracy and robustness at almost no cost."
TCA: High Dimensional Principal Component Analysis for non-Gaussian Data,"We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate t and logistic and it is extended to the meta-elliptical by Fang (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s(log d/n)^{1/2} estimation consistency rate in the transelliptical distribution family, even if the distributions are very heavy-tailed, have infinite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is also implemented in both numerical simulations and large-scale stock data to illustrate its empirical performance. Both theories and experiments confirm that TCA can achieve model flexibility, estimation accuracy and robustness at almost no cost."
Matching Objects across the Textured-Smooth Continuum,"The problem of 3D object recognition is of immense practical importance and potential, with the last decade witnessing a number of breakthroughs in the state of the art. Most of the past work focused on the matching of textured objects using local appearance descriptors extracted around salient points in an image. The recently proposed bag of boundaries method was the first to address directly the problem of matching smooth objects using image based boundary features. However, no previous work has attempted to achieve a holistic treatment of the problem by jointly using textural and shape features which is what we describe in the present paper. Due to the complementarity of the two modalities (texture and shape), we combine the matching scores of textural and shape based representations by weighted summation. The optimal weighting is learnt in a data specific manner by optimizing discriminative performance on synthetically distorted data. For the textural description of an object we adopt a representation in the form of a histogram of SIFT based visual words. Similarly the apparent shape of an object is represented by a histogram of discretized features capturing local shape. Unlike previous work which uses an implicit, image based shape descriptor, we propose a more compact descriptor based on the local profile of boundary normals' directions. This descriptor is extracted at salient object boundary loci and at the corresponding characteristic scale, both detected automatically. On a large database of a diverse set of objects, the proposed method is shown to significantly outperform both purely textural and purely shape based approaches for matching across viewpoint variation. The advantage was particularly significant in the cases of large viewpoint changes between training and query data, when the correct rank-1 recognition rate was nearly twice that achieved using either of the modalities in isolation."
Efficiently Sampling Probabilistic Programs via Symbolic Execution,"Probabilistic programs are intuitive and succinct representations of complex  probability distributions. A natural approach to performing inference over these programs is to execute them, collect samples and compute statistics over the events of  interest. Indeed, this approach has been taken before in a number of probabilistic  programming tools. In this paper,  we address two key challenges of this paradigm: (i) ensuring samples are well distributed in the combinatorial space of the program, and (ii) efficiently  generating samples with minimal rejection. Our technique tackles the  first via systematic exploration across combinatorial choices, even in unbounded recursive programs, using symbolic computation. To solve the latter,  we compute weakest preconditions to hoist conditions to elementary distributions, and sample from the resulting conditional distributions thus avoiding rejection. We have implemented our algorithm in a tool called ESP and have used it on a number of benchmarks that include Probabilistic Context Free Grammars (PCFG) and Latent Dirichlet Allocation (LDA). Our results are encouraging -- we show comparable results with the state-of-the-art on these benchmarks, and significantly outperform other sampling based probabilistic inference approaches."
Efficiently Sampling Probabilistic Programs via Symbolic Execution,"Probabilistic programs are intuitive and succinct representations of complex  probability distributions. A natural approach to performing inference over these programs is to execute them, collect samples and compute statistics over the events of  interest. Indeed, this approach has been taken before in a number of probabilistic  programming tools. In this paper,  we address two key challenges of this paradigm: (i) ensuring samples are well distributed in the combinatorial space of the program, and (ii) efficiently  generating samples with minimal rejection. Our technique tackles the  first via systematic exploration across combinatorial choices, even in unbounded recursive programs, using symbolic computation. To solve the latter,  we compute weakest preconditions to hoist conditions to elementary distributions, and sample from the resulting conditional distributions thus avoiding rejection. We have implemented our algorithm in a tool called ESP and have used it on a number of benchmarks that include Probabilistic Context Free Grammars (PCFG) and Latent Dirichlet Allocation (LDA). Our results are encouraging -- we show comparable results with the state-of-the-art on these benchmarks, and significantly outperform other sampling based probabilistic inference approaches."
Efficiently Sampling Probabilistic Programs via Symbolic Execution,"Probabilistic programs are intuitive and succinct representations of complex  probability distributions. A natural approach to performing inference over these programs is to execute them, collect samples and compute statistics over the events of  interest. Indeed, this approach has been taken before in a number of probabilistic  programming tools. In this paper,  we address two key challenges of this paradigm: (i) ensuring samples are well distributed in the combinatorial space of the program, and (ii) efficiently  generating samples with minimal rejection. Our technique tackles the  first via systematic exploration across combinatorial choices, even in unbounded recursive programs, using symbolic computation. To solve the latter,  we compute weakest preconditions to hoist conditions to elementary distributions, and sample from the resulting conditional distributions thus avoiding rejection. We have implemented our algorithm in a tool called ESP and have used it on a number of benchmarks that include Probabilistic Context Free Grammars (PCFG) and Latent Dirichlet Allocation (LDA). Our results are encouraging -- we show comparable results with the state-of-the-art on these benchmarks, and significantly outperform other sampling based probabilistic inference approaches."
A Data-Dependent Risk Bound for Incremental Learning of Radial Basis Function Networks,"A data-dependent upper bound of an expected risk for radial basisfunction networks (RBFNs) is investigated. Because the risk bound isprovided in a quite practical form, it can be used for modelselection in nonlinear regression problems, especially when the RBFNincrementally recruits its basis functions. Asymptotic properties ofRBFNs are described for consistency of the incremental learningmethods. The risk bound is closely investigated for the leastsquares method and its properties are discussed."
A Data-Dependent Risk Bound for Incremental Learning of Radial Basis Function Networks,"A data-dependent upper bound of an expected risk for radial basisfunction networks (RBFNs) is investigated. Because the risk bound isprovided in a quite practical form, it can be used for modelselection in nonlinear regression problems, especially when the RBFNincrementally recruits its basis functions. Asymptotic properties ofRBFNs are described for consistency of the incremental learningmethods. The risk bound is closely investigated for the leastsquares method and its properties are discussed."
A Data-Dependent Risk Bound for Incremental Learning of Radial Basis Function Networks,"A data-dependent upper bound of an expected risk for radial basisfunction networks (RBFNs) is investigated. Because the risk bound isprovided in a quite practical form, it can be used for modelselection in nonlinear regression problems, especially when the RBFNincrementally recruits its basis functions. Asymptotic properties ofRBFNs are described for consistency of the incremental learningmethods. The risk bound is closely investigated for the leastsquares method and its properties are discussed."
Estimating the Maximum Expected Value: An Analysis of (Nested) Cross Validation and the Maximum Sample Average,"We investigate the accuracy of the two most commonestimators for the maximum expected value ofa general set of random variables: a generalizationof the maximum sample average, and cross validation.No unbiased estimator exists and we show that it isnon-trivial to select a good estimator without knowledgeabout the distributions of the random variables.We investigate and bound the bias and variance of theaforementioned estimators and prove consistency.The variance of cross validation can besignificantly reduced, but not without riskinga large bias. The bias and variance ofdifferent variants of cross validation are veryproblem-dependent, and a wrong choice canlead to very inaccurate estimates."
Action-Model Based Multi-agent Plan Recognition,"Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. Previous approaches require a library of team activity sequences (team plans) be given as input. However, collecting or maintaining a library of team plans is often difficult or costly. In this paper we relax this constraint, i.e., team plans are not required to be provided beforehand. We assume that a set of action models are available, which are already created to describe domain physics, i.e., preconditions that an activity is applied and effects after the activity is applied. These action models can be used to help identify team plans. We propose a novel approach to recognizing multi-agent plans based on action models rather than libraries of team plans. We encode the MAPR problem as a satisfaction problem and solve the problem using a state-of-the-art weighted MAX-SAT solver. In the experiment, we show that our algorithm is both effective and efficient."
On the L1 Distance Between Subspaces,"Subspaces are a popular representation in many computer vision applications.Among others, subspaces are used to represent faces under varying illumination, articulation of 2D shapes, and motion trajectories of rigid as well as non-rigid objects.As such, metrics for measuring the distance between subspaces are required for utilizing the subspace representation, e.g., for classification, matching, and clustering of subspaces.Indeed, in recent years various distance measures have been explored, often with a particular application in mind.Mutual to these measures is their reliance on the L2 metric, that ties the distance between the subspaces with the principal angles.While the L2-based distance measures are intuitive, it is long known that L1 norms are more adequate for optimization due to lower sensitivity to outliers.Therefore, in this paper we explore the L1 distance between subspaces.We analyze it's properties and provide intuition on its nature.We further suggest methods for speeding-up its computation.Finally, we show empirically that it is superior to the L2 distance in several applications."
Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity,"Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superfluous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach."
Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity,"Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superfluous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach."
Projection Kurtosis Concentration and Noise Estimation,"Kurtosis of 1D projections provides important characteristics of high dimensional data. In this work, based on the Gaussian scale mixture models of natural sensory signals, we first provide a theoretical underpinning to an empirically observed phenomenon that the kurtosis of band-pass filtered natural sensory signals tend to concentrate around a ``typical'' value.  Based on this result, we further describe a new effective methodology to estimate the variance and covariance matrix of Gaussian noise from a noise corrupted signal using {\em randomly} selected band-pass filters. The noise variance estimation method uses an objective function that has a closed-form solution and is robust to infrequent outlying kurtosis values. The estimation method of covariance matrix also affords efficient coordinate descent optimization. We demonstrate significant performance improvement of our methods on natural image and audio data sets over the current state-of-the-art methods."
Projection Kurtosis Concentration and Noise Estimation,"Kurtosis of 1D projections provides important characteristics of high dimensional data. In this work, based on the Gaussian scale mixture models of natural sensory signals, we first provide a theoretical underpinning to an empirically observed phenomenon that the kurtosis of band-pass filtered natural sensory signals tend to concentrate around a ``typical'' value.  Based on this result, we further describe a new effective methodology to estimate the variance and covariance matrix of Gaussian noise from a noise corrupted signal using {\em randomly} selected band-pass filters. The noise variance estimation method uses an objective function that has a closed-form solution and is robust to infrequent outlying kurtosis values. The estimation method of covariance matrix also affords efficient coordinate descent optimization. We demonstrate significant performance improvement of our methods on natural image and audio data sets over the current state-of-the-art methods."
Projection Kurtosis Concentration and Noise Estimation,"Kurtosis of 1D projections provides important characteristics of high dimensional data. In this work, based on the Gaussian scale mixture models of natural sensory signals, we first provide a theoretical underpinning to an empirically observed phenomenon that the kurtosis of band-pass filtered natural sensory signals tend to concentrate around a ``typical'' value.  Based on this result, we further describe a new effective methodology to estimate the variance and covariance matrix of Gaussian noise from a noise corrupted signal using {\em randomly} selected band-pass filters. The noise variance estimation method uses an objective function that has a closed-form solution and is robust to infrequent outlying kurtosis values. The estimation method of covariance matrix also affords efficient coordinate descent optimization. We demonstrate significant performance improvement of our methods on natural image and audio data sets over the current state-of-the-art methods."
Non-parametric Approximate Dynamic Programming via the Kernel	Method,"This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful, dimension-independent approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our non-parametric procedure is competitive with parametric ADP approaches."
Multiclass Active Learning with Hierarchical-Structured Embedded Variance,"  We consider the problem of multiclass active learning where the relationship of the labels are represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to exploit the hierarchical structure of the label tree as well as the characteristics of the training data to select the most informative data for human labeling.  This goal can be achieved by a novel embedding-based approach called hierarchical-structured embedded variance, which learns an embedding of the labels that both preserves the structure of the label tree and reflects the characteristics of the training data. We show that the proposed approach is a generalization of entropy-based and cost-based uncertainty measure. We also demonstrate that notable improvement on the performance can be achieved with the proposed approach on synthetic and benchmark datasets."
Multiclass Active Learning with Hierarchical-Structured Embedded Variance,"  We consider the problem of multiclass active learning where the relationship of the labels are represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to exploit the hierarchical structure of the label tree as well as the characteristics of the training data to select the most informative data for human labeling.  This goal can be achieved by a novel embedding-based approach called hierarchical-structured embedded variance, which learns an embedding of the labels that both preserves the structure of the label tree and reflects the characteristics of the training data. We show that the proposed approach is a generalization of entropy-based and cost-based uncertainty measure. We also demonstrate that notable improvement on the performance can be achieved with the proposed approach on synthetic and benchmark datasets."
Multiclass Active Learning with Hierarchical-Structured Embedded Variance,"  We consider the problem of multiclass active learning where the relationship of the labels are represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to exploit the hierarchical structure of the label tree as well as the characteristics of the training data to select the most informative data for human labeling.  This goal can be achieved by a novel embedding-based approach called hierarchical-structured embedded variance, which learns an embedding of the labels that both preserves the structure of the label tree and reflects the characteristics of the training data. We show that the proposed approach is a generalization of entropy-based and cost-based uncertainty measure. We also demonstrate that notable improvement on the performance can be achieved with the proposed approach on synthetic and benchmark datasets."
Nearest Neighbor with Part-To-Whole Voting,"Near and nearest neighbor finding in high dimensions is an important building block in several machine learning paradigms, including KNN and RBF based classification and regression. In several applications, like visual object detection, near neighbor finding is the bottleneck for real time performance of such methods. We propose a new method for the problem, based on decomposition of the database vectors into parts, finding near neighbor for the parts, and voting from part to wholes to find the final neighbors. The algorithm is based on pre-computation of part to whole relations, enabling high run-time efficiency and effective trading of computation for memory resources. Analysis reveals that the method complexity is a sum of two terms, both sub-linear in the database size. Comparison with the state of the art methods shows the advantage of the proposed method."
Nearest Neighbor with Part-To-Whole Voting,"Near and nearest neighbor finding in high dimensions is an important building block in several machine learning paradigms, including KNN and RBF based classification and regression. In several applications, like visual object detection, near neighbor finding is the bottleneck for real time performance of such methods. We propose a new method for the problem, based on decomposition of the database vectors into parts, finding near neighbor for the parts, and voting from part to wholes to find the final neighbors. The algorithm is based on pre-computation of part to whole relations, enabling high run-time efficiency and effective trading of computation for memory resources. Analysis reveals that the method complexity is a sum of two terms, both sub-linear in the database size. Comparison with the state of the art methods shows the advantage of the proposed method."
Nearest Neighbor with Part-To-Whole Voting,"Near and nearest neighbor finding in high dimensions is an important building block in several machine learning paradigms, including KNN and RBF based classification and regression. In several applications, like visual object detection, near neighbor finding is the bottleneck for real time performance of such methods. We propose a new method for the problem, based on decomposition of the database vectors into parts, finding near neighbor for the parts, and voting from part to wholes to find the final neighbors. The algorithm is based on pre-computation of part to whole relations, enabling high run-time efficiency and effective trading of computation for memory resources. Analysis reveals that the method complexity is a sum of two terms, both sub-linear in the database size. Comparison with the state of the art methods shows the advantage of the proposed method."
The variational hierarchical EM algorithm for clustering hidden Markov models.,"In this paper, we derive a novel algorithm to cluster  hidden Markov models (HMMs) according to their probability distributions.We propose a variational hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ``cluster center'', i.e., a novel HMM that is representative for the group.We illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging."
Measuring Crowd Collectiveness,"Collective motions widely exist in crowd systems and receive many attentions from multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union, is a fundamental measurement for the studies of various crowd systems in nature. In this paper, by quantifying the topological properties of collective manifold of crowd, we propose a measurement of collectiveness for crowd systems as well as their constituent individuals, along with its efficient computation. \emph{Collective Thresholding} is then proposed to detect collective motions from random motions. We validate the proposed collectiveness on the system of self-driven particles, and analyze its effectiveness and robustness to identify collectively moving particles from randomly moving particles.  It is further evaluated through experiments on real bacterial colonies and pedestrian crowds."
Measuring Crowd Collectiveness,"Collective motions widely exist in crowd systems and receive many attentions from multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union, is a fundamental measurement for the studies of various crowd systems in nature. In this paper, by quantifying the topological properties of collective manifold of crowd, we propose a measurement of collectiveness for crowd systems as well as their constituent individuals, along with its efficient computation. \emph{Collective Thresholding} is then proposed to detect collective motions from random motions. We validate the proposed collectiveness on the system of self-driven particles, and analyze its effectiveness and robustness to identify collectively moving particles from randomly moving particles.  It is further evaluated through experiments on real bacterial colonies and pedestrian crowds."
Measuring Crowd Collectiveness,"Collective motions widely exist in crowd systems and receive many attentions from multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union, is a fundamental measurement for the studies of various crowd systems in nature. In this paper, by quantifying the topological properties of collective manifold of crowd, we propose a measurement of collectiveness for crowd systems as well as their constituent individuals, along with its efficient computation. \emph{Collective Thresholding} is then proposed to detect collective motions from random motions. We validate the proposed collectiveness on the system of self-driven particles, and analyze its effectiveness and robustness to identify collectively moving particles from randomly moving particles.  It is further evaluated through experiments on real bacterial colonies and pedestrian crowds."
Consistency Analysis of  Empirical MEE Algorithm,"In this paper we study the consistency of the empirical minimum error entropy (MEE) algorithm for regression learning.Two types of consistency are studied. The error entropy consistency,which requires the error entropy of the learned functionapproximates the minimum error entropy, is shown to be always trueif the bandwidth parameter tends to 0 at an appropriate rate. Theregression consistency, which requires the learned functionapproximates the regression function, however, is a complicatedissue. We prove that the error entropy consistency implies theregression consistency for homoskedastic models where the noise isindependent of the input variable. But for heteroskedastic models, acounter-example is used to show the two types of consistency do notcoincide. A surprising result is that the regression consistency isalways true, provided that the bandwidth parameter tends to infinityat certain rates. This result, however, contradicts the motivationof MEE principle because the minimum error entropy is believed to benot approximated well with this choice of bandwidth parameter."
Consistency Analysis of  Empirical MEE Algorithm,"In this paper we study the consistency of the empirical minimum error entropy (MEE) algorithm for regression learning.Two types of consistency are studied. The error entropy consistency,which requires the error entropy of the learned functionapproximates the minimum error entropy, is shown to be always trueif the bandwidth parameter tends to 0 at an appropriate rate. Theregression consistency, which requires the learned functionapproximates the regression function, however, is a complicatedissue. We prove that the error entropy consistency implies theregression consistency for homoskedastic models where the noise isindependent of the input variable. But for heteroskedastic models, acounter-example is used to show the two types of consistency do notcoincide. A surprising result is that the regression consistency isalways true, provided that the bandwidth parameter tends to infinityat certain rates. This result, however, contradicts the motivationof MEE principle because the minimum error entropy is believed to benot approximated well with this choice of bandwidth parameter."
Multi-Granularity Image Categorization via Probabilistic Decoding,"Modern image data sets organize classes in a hierarchical taxonomy structure, such as a tree or DAG. Usually formulated as a multi-way classification, classical image categorization approaches can only predict leaf labels in such hierarchy. In this paper, based on the error correcting output coding formulation for multi-way classification, we propose a probabilistic decoding approach for both single-granularity, where only leaf level labels are allowed, and multi-granularity image categorization, which could generate internal labels from the taxonomy hierarchy if it is uncertain on leaf level labels. Experimental results demonstrate the effectiveness of our proposed image categorization approach on both single-granularity scenario and multi-granularity case."
Maximize the Ratio of Split to Sum of Diameters with Applications to Image Segmentation,"The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster, and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster. In this paper, we study the following problem: maximize the ratio of the minimum split to the sum of cluster diameters. In general, the problem is NP-hard for k >= 3 (k is the number of clusters). Here, we present an exact bipartition algorithm with the worst-case runtime O(n^4logn), where n is the number of objects. We apply the proposed algorithm to image segmentation to verify the validity of the proposed clustering criterion. Since the proposed algorithm is with high computational complexity, it is impractical to directly apply it to an image. So, we first use the farthest-point clustering algorithm to obtain a given number of superpixels of the original images, and then apply the proposed algorithm to those superpixels. The experimental results on Weizmann image segmentation challenge database demonstrate that the proposed algorithm is promising."
Maximize the Ratio of Split to Sum of Diameters with Applications to Image Segmentation,"The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster, and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster. In this paper, we study the following problem: maximize the ratio of the minimum split to the sum of cluster diameters. In general, the problem is NP-hard for k >= 3 (k is the number of clusters). Here, we present an exact bipartition algorithm with the worst-case runtime O(n^4logn), where n is the number of objects. We apply the proposed algorithm to image segmentation to verify the validity of the proposed clustering criterion. Since the proposed algorithm is with high computational complexity, it is impractical to directly apply it to an image. So, we first use the farthest-point clustering algorithm to obtain a given number of superpixels of the original images, and then apply the proposed algorithm to those superpixels. The experimental results on Weizmann image segmentation challenge database demonstrate that the proposed algorithm is promising."
Maximize the Ratio of Split to Sum of Diameters with Applications to Image Segmentation,"The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster, and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster. In this paper, we study the following problem: maximize the ratio of the minimum split to the sum of cluster diameters. In general, the problem is NP-hard for k >= 3 (k is the number of clusters). Here, we present an exact bipartition algorithm with the worst-case runtime O(n^4logn), where n is the number of objects. We apply the proposed algorithm to image segmentation to verify the validity of the proposed clustering criterion. Since the proposed algorithm is with high computational complexity, it is impractical to directly apply it to an image. So, we first use the farthest-point clustering algorithm to obtain a given number of superpixels of the original images, and then apply the proposed algorithm to those superpixels. The experimental results on Weizmann image segmentation challenge database demonstrate that the proposed algorithm is promising."
Maximize the Ratio of Split to Sum of Diameters with Applications to Image Segmentation,"The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster, and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster. In this paper, we study the following problem: maximize the ratio of the minimum split to the sum of cluster diameters. In general, the problem is NP-hard for k >= 3 (k is the number of clusters). Here, we present an exact bipartition algorithm with the worst-case runtime O(n^4logn), where n is the number of objects. We apply the proposed algorithm to image segmentation to verify the validity of the proposed clustering criterion. Since the proposed algorithm is with high computational complexity, it is impractical to directly apply it to an image. So, we first use the farthest-point clustering algorithm to obtain a given number of superpixels of the original images, and then apply the proposed algorithm to those superpixels. The experimental results on Weizmann image segmentation challenge database demonstrate that the proposed algorithm is promising."
Efficient Sample Reuse in Policy Gradients with Parameter-based Exploration,"The policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control.A common challenge in this scenario is how to stabilizepolicy gradient estimates for reliable policy updates.In this paper, we combine the following three ideas and givea highly stable and practical policy gradient method:(a) the policy gradients with parameter based exploration,which is a recently proposed policy search method with high stability,(b) an importance sampling technique, which allows us to reuse previously gathered data in a consistent way,and (c) an optimal baseline, which minimizes the variance of gradient estimates with their unbiasedness being maintained.For the proposed method, we give theoretical analysis of the variance of gradient estimates and show its usefulness through experiments."
Efficient Sample Reuse in Policy Gradients with Parameter-based Exploration,"The policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control.A common challenge in this scenario is how to stabilizepolicy gradient estimates for reliable policy updates.In this paper, we combine the following three ideas and givea highly stable and practical policy gradient method:(a) the policy gradients with parameter based exploration,which is a recently proposed policy search method with high stability,(b) an importance sampling technique, which allows us to reuse previously gathered data in a consistent way,and (c) an optimal baseline, which minimizes the variance of gradient estimates with their unbiasedness being maintained.For the proposed method, we give theoretical analysis of the variance of gradient estimates and show its usefulness through experiments."
Multi-Label Learning With Millions of Categories,"Our objective is to build an algorithm for classifying a data point into a set of labels when the output space contains millions of categories. This is a relatively novel setting in supervised learning and brings forth interesting challenges such as efficient training and prediction, learning from only positively labeled data with missing and incorrect labels and handling label correlations. We propose a random forest based solution for jointly tackling these issues. We develop a novel extension of random forests for multi-label classification which can learn from positive data alone and can scale to large data sets. We generate real valued beliefs indicating the state of labels and adapt our classifier to train on these belief vectors so as to compensate for missing and noisy labels. In addition, we modify the random forest cost function to avoid overfitting in high dimensional feature spaces and learn short, balanced trees. Finally, we write highly efficient  training routines which let us train on problems with forty million training points, over a million dimensional sparse feature vector and over a million categories. Extensive experiments reveal that our proposed solution is not only significantly better than other multi-label classification algorithms but also more than 10% better than the state-of-the-art in our application domain."
Multi-Label Learning With Millions of Categories,"Our objective is to build an algorithm for classifying a data point into a set of labels when the output space contains millions of categories. This is a relatively novel setting in supervised learning and brings forth interesting challenges such as efficient training and prediction, learning from only positively labeled data with missing and incorrect labels and handling label correlations. We propose a random forest based solution for jointly tackling these issues. We develop a novel extension of random forests for multi-label classification which can learn from positive data alone and can scale to large data sets. We generate real valued beliefs indicating the state of labels and adapt our classifier to train on these belief vectors so as to compensate for missing and noisy labels. In addition, we modify the random forest cost function to avoid overfitting in high dimensional feature spaces and learn short, balanced trees. Finally, we write highly efficient  training routines which let us train on problems with forty million training points, over a million dimensional sparse feature vector and over a million categories. Extensive experiments reveal that our proposed solution is not only significantly better than other multi-label classification algorithms but also more than 10% better than the state-of-the-art in our application domain."
Multi-Label Learning With Millions of Categories,"Our objective is to build an algorithm for classifying a data point into a set of labels when the output space contains millions of categories. This is a relatively novel setting in supervised learning and brings forth interesting challenges such as efficient training and prediction, learning from only positively labeled data with missing and incorrect labels and handling label correlations. We propose a random forest based solution for jointly tackling these issues. We develop a novel extension of random forests for multi-label classification which can learn from positive data alone and can scale to large data sets. We generate real valued beliefs indicating the state of labels and adapt our classifier to train on these belief vectors so as to compensate for missing and noisy labels. In addition, we modify the random forest cost function to avoid overfitting in high dimensional feature spaces and learn short, balanced trees. Finally, we write highly efficient  training routines which let us train on problems with forty million training points, over a million dimensional sparse feature vector and over a million categories. Extensive experiments reveal that our proposed solution is not only significantly better than other multi-label classification algorithms but also more than 10% better than the state-of-the-art in our application domain."
Multi-Label Learning With Millions of Categories,"Our objective is to build an algorithm for classifying a data point into a set of labels when the output space contains millions of categories. This is a relatively novel setting in supervised learning and brings forth interesting challenges such as efficient training and prediction, learning from only positively labeled data with missing and incorrect labels and handling label correlations. We propose a random forest based solution for jointly tackling these issues. We develop a novel extension of random forests for multi-label classification which can learn from positive data alone and can scale to large data sets. We generate real valued beliefs indicating the state of labels and adapt our classifier to train on these belief vectors so as to compensate for missing and noisy labels. In addition, we modify the random forest cost function to avoid overfitting in high dimensional feature spaces and learn short, balanced trees. Finally, we write highly efficient  training routines which let us train on problems with forty million training points, over a million dimensional sparse feature vector and over a million categories. Extensive experiments reveal that our proposed solution is not only significantly better than other multi-label classification algorithms but also more than 10% better than the state-of-the-art in our application domain."
Non-rigid Segmentation of Deformable Contour Objects in Sparse Low Dimensional Manifolds,"The segmentation of non-rigid visual objects using machine learning techniques usually involves high complexity search and training methodologies. This complexity is a consequence of the large dimensionality of the presentation used for the object contour, where the approach usually taken to circumvent this problem is to subdivide the original problem into a rigid detection followed by a non-rigid segmentation. The rationale behind this sub-division is based on the fact that the rigid detection is run in a lower dimensionality space than the original contour space and its result is then used to initialize and constrain the non-rigid segmentation in a higher dimensionality space. In this paper, we propose a new methodology for segmenting non-rigid visual objects, where the contours are directly represented in sparse low dimensional manifolds without requiring the sub-division of the problem described above. Our proposal shows significant smaller search and training complexities given that the dimensionality of the manifoldis much smaller than the dimensionality of the rigid and non-rigid search spaces aforementioned, and that we no longer require a two-stage segmentation process. We focus on the problem of left ventricle (LV) endocardial segmentation from ultrasound images, and our experiments testify that the use of sparse low dimensional manifolds reduces the search and training complexities of currentsegmentation approaches without a significant impact on thesegmentation precision shown by state-of-the-art approaches."
Non-rigid Segmentation of Deformable Contour Objects in Sparse Low Dimensional Manifolds,"The segmentation of non-rigid visual objects using machine learning techniques usually involves high complexity search and training methodologies. This complexity is a consequence of the large dimensionality of the presentation used for the object contour, where the approach usually taken to circumvent this problem is to subdivide the original problem into a rigid detection followed by a non-rigid segmentation. The rationale behind this sub-division is based on the fact that the rigid detection is run in a lower dimensionality space than the original contour space and its result is then used to initialize and constrain the non-rigid segmentation in a higher dimensionality space. In this paper, we propose a new methodology for segmenting non-rigid visual objects, where the contours are directly represented in sparse low dimensional manifolds without requiring the sub-division of the problem described above. Our proposal shows significant smaller search and training complexities given that the dimensionality of the manifoldis much smaller than the dimensionality of the rigid and non-rigid search spaces aforementioned, and that we no longer require a two-stage segmentation process. We focus on the problem of left ventricle (LV) endocardial segmentation from ultrasound images, and our experiments testify that the use of sparse low dimensional manifolds reduces the search and training complexities of currentsegmentation approaches without a significant impact on thesegmentation precision shown by state-of-the-art approaches."
Which Ranking Measure shall We Use --- Some Suggestions from a Theoretical Perspective,"Ranking has been extensively studied in information retrieval, machine learning and statistics. A central problem in ranking is how to design or choose a ranking measure for the evaluation of ranking functions. In this paper we study, from a theoretical perspective, a class of ranking measures including NDCG, NDCG@k and Precision@k. Weanalyze, under some theoretical assumptions, the behavior of these ranking measures as the number of objects to rank getting large. Our theoretical results provide several suggestions for choosing ranking measures when there is a large set of objects to rank: 1) When employing NDCG as the ranking measure, it would be better to choose its cut-offversion NDCG@k and let $k$ grows with the number of objects; 2) If the users prefer a not-too-small $k$, it would be better to use a $r^{-\alpha}$ ($\alpha \in (0,1)$) discount instead of the $\frac{1}{\log(1+r)}$ in NDCG@k. We also conduct experiments on real data and find that our theory works well although the assumptions for the theorems may not hold in the real dataset."
Which Ranking Measure shall We Use --- Some Suggestions from a Theoretical Perspective,"Ranking has been extensively studied in information retrieval, machine learning and statistics. A central problem in ranking is how to design or choose a ranking measure for the evaluation of ranking functions. In this paper we study, from a theoretical perspective, a class of ranking measures including NDCG, NDCG@k and Precision@k. Weanalyze, under some theoretical assumptions, the behavior of these ranking measures as the number of objects to rank getting large. Our theoretical results provide several suggestions for choosing ranking measures when there is a large set of objects to rank: 1) When employing NDCG as the ranking measure, it would be better to choose its cut-offversion NDCG@k and let $k$ grows with the number of objects; 2) If the users prefer a not-too-small $k$, it would be better to use a $r^{-\alpha}$ ($\alpha \in (0,1)$) discount instead of the $\frac{1}{\log(1+r)}$ in NDCG@k. We also conduct experiments on real data and find that our theory works well although the assumptions for the theorems may not hold in the real dataset."
Which Ranking Measure shall We Use --- Some Suggestions from a Theoretical Perspective,"Ranking has been extensively studied in information retrieval, machine learning and statistics. A central problem in ranking is how to design or choose a ranking measure for the evaluation of ranking functions. In this paper we study, from a theoretical perspective, a class of ranking measures including NDCG, NDCG@k and Precision@k. Weanalyze, under some theoretical assumptions, the behavior of these ranking measures as the number of objects to rank getting large. Our theoretical results provide several suggestions for choosing ranking measures when there is a large set of objects to rank: 1) When employing NDCG as the ranking measure, it would be better to choose its cut-offversion NDCG@k and let $k$ grows with the number of objects; 2) If the users prefer a not-too-small $k$, it would be better to use a $r^{-\alpha}$ ($\alpha \in (0,1)$) discount instead of the $\frac{1}{\log(1+r)}$ in NDCG@k. We also conduct experiments on real data and find that our theory works well although the assumptions for the theorems may not hold in the real dataset."
Which Ranking Measure shall We Use --- Some Suggestions from a Theoretical Perspective,"Ranking has been extensively studied in information retrieval, machine learning and statistics. A central problem in ranking is how to design or choose a ranking measure for the evaluation of ranking functions. In this paper we study, from a theoretical perspective, a class of ranking measures including NDCG, NDCG@k and Precision@k. Weanalyze, under some theoretical assumptions, the behavior of these ranking measures as the number of objects to rank getting large. Our theoretical results provide several suggestions for choosing ranking measures when there is a large set of objects to rank: 1) When employing NDCG as the ranking measure, it would be better to choose its cut-offversion NDCG@k and let $k$ grows with the number of objects; 2) If the users prefer a not-too-small $k$, it would be better to use a $r^{-\alpha}$ ($\alpha \in (0,1)$) discount instead of the $\frac{1}{\log(1+r)}$ in NDCG@k. We also conduct experiments on real data and find that our theory works well although the assumptions for the theorems may not hold in the real dataset."
Efficient Pool-Based Active Learning of Halfspaces,"We study pool-based active learning of halfspaces, in which a learner receives a pool of unlabeled examples, and iteratively queries a teacher for the labels of examples from the pool, in order to identify all the labels of pool examples. We revisit the idea of greedily selecting examples to label, and use it to derive an efficient algorithm, called ALuMA, that approximates the optimal label complexity for a given pool in $\reals^d$. We show that ALuMA obtains an $O(d^2 \log(d))$ approximation factor if the examples in the pool are numbers with a finite accuracy. We further prove a result for general hypothesis classes, showing that a slight change to the greedy approach leads to an improved target-dependent guarantee on the label complexity. In particular, we conclude a better guarantee for ALuMA if the target hypothesis has a large margin.  We further compare our approach to other common active learning strategies, and provide a theoretical and empirical evaluation of the advantages and disadvantages of the approach. "
Efficient Pool-Based Active Learning of Halfspaces,"We study pool-based active learning of halfspaces, in which a learner receives a pool of unlabeled examples, and iteratively queries a teacher for the labels of examples from the pool, in order to identify all the labels of pool examples. We revisit the idea of greedily selecting examples to label, and use it to derive an efficient algorithm, called ALuMA, that approximates the optimal label complexity for a given pool in $\reals^d$. We show that ALuMA obtains an $O(d^2 \log(d))$ approximation factor if the examples in the pool are numbers with a finite accuracy. We further prove a result for general hypothesis classes, showing that a slight change to the greedy approach leads to an improved target-dependent guarantee on the label complexity. In particular, we conclude a better guarantee for ALuMA if the target hypothesis has a large margin.  We further compare our approach to other common active learning strategies, and provide a theoretical and empirical evaluation of the advantages and disadvantages of the approach. "
Efficient Pool-Based Active Learning of Halfspaces,"We study pool-based active learning of halfspaces, in which a learner receives a pool of unlabeled examples, and iteratively queries a teacher for the labels of examples from the pool, in order to identify all the labels of pool examples. We revisit the idea of greedily selecting examples to label, and use it to derive an efficient algorithm, called ALuMA, that approximates the optimal label complexity for a given pool in $\reals^d$. We show that ALuMA obtains an $O(d^2 \log(d))$ approximation factor if the examples in the pool are numbers with a finite accuracy. We further prove a result for general hypothesis classes, showing that a slight change to the greedy approach leads to an improved target-dependent guarantee on the label complexity. In particular, we conclude a better guarantee for ALuMA if the target hypothesis has a large margin.  We further compare our approach to other common active learning strategies, and provide a theoretical and empirical evaluation of the advantages and disadvantages of the approach. "
Context-Sensitive Decision Forests for Object Detection,"In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods."
Context-Sensitive Decision Forests for Object Detection,"In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods."
Context-Sensitive Decision Forests for Object Detection,"In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods."
Context-Sensitive Decision Forests for Object Detection,"In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods."
Context-Sensitive Decision Forests for Object Detection,"In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods."
Context-Sensitive Decision Forests for Object Detection,"In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods."
A Topic Model for Continuous Word Embeddedings,"While words in documents are generally treated as discrete entities,they can be embedded in a Euclidean space which reflects an \textit{a priori} notion of similarity between them.In such a case, a text document can be viewed as a bag-of-embedded-words (BoEW):a set of real-valued vectors.We propose a topic model for the BoEW:the generation process of words is modeled with a continuous mixture modelwhere each mixture component can be identified with a different topic.Retrieval and clustering experiments with the proposed BoEW representationshow significant improvements with respect to topic models computed from the traditional bag-of-words."
A Topic Model for Continuous Word Embeddedings,"While words in documents are generally treated as discrete entities,they can be embedded in a Euclidean space which reflects an \textit{a priori} notion of similarity between them.In such a case, a text document can be viewed as a bag-of-embedded-words (BoEW):a set of real-valued vectors.We propose a topic model for the BoEW:the generation process of words is modeled with a continuous mixture modelwhere each mixture component can be identified with a different topic.Retrieval and clustering experiments with the proposed BoEW representationshow significant improvements with respect to topic models computed from the traditional bag-of-words."
Estimating Nonstationary Inputs from a Single Spike Train Based on a Neuron Model with Adaptation,"Because every spike of a neuron is determined by input signals, a train of spikes may contain information about the dynamics of unobserved neurons. A state-space method based on the leaky integrate-and-fire (LIF) model, describing neuronal transformation from input signals to a spike train has been proposed for tracking input parameters represented by their mean and fluctuation. In the present paper, we propose to make the estimation more realistic by adopting an LIF model augmented with an adaptive moving threshold. Moreover, because the direct state-space method is computationally infeasible for a data set comprising thousands of spikes, we further develop a practical method for transforming instantaneous firing characteristics back to input parameters. The instantaneous firing characteristics, represented by the firing rate and non-Poisson irregularity, can be estimated using a computationally feasible algorithm. We applied our proposed methods to synthetic data and experimental data to clarify that they perform well."
Estimating Nonstationary Inputs from a Single Spike Train Based on a Neuron Model with Adaptation,"Because every spike of a neuron is determined by input signals, a train of spikes may contain information about the dynamics of unobserved neurons. A state-space method based on the leaky integrate-and-fire (LIF) model, describing neuronal transformation from input signals to a spike train has been proposed for tracking input parameters represented by their mean and fluctuation. In the present paper, we propose to make the estimation more realistic by adopting an LIF model augmented with an adaptive moving threshold. Moreover, because the direct state-space method is computationally infeasible for a data set comprising thousands of spikes, we further develop a practical method for transforming instantaneous firing characteristics back to input parameters. The instantaneous firing characteristics, represented by the firing rate and non-Poisson irregularity, can be estimated using a computationally feasible algorithm. We applied our proposed methods to synthetic data and experimental data to clarify that they perform well."
Generalized sequential tree-reweighted message passing,"This paper addresses the problem of approximate MAP-MRF inference in general graphical models. Following [23], we consider a family of linear programming relaxations of the problem where each relaxation is specified by a set of nested pairs of factors for which the  marginalization constraint needs to be enforced. We develop a generalization of the TRW-S algorithm [6] for this problem, where we use a decomposition into junction chains, monotonic w.r.t. some ordering on the nodes. This generalizes the monotonic chains in [6] in a natural way. We also show how to deal with nested factors in an efficient way. Experiments show an improvement over min-sum diffusion, MPLP and subgradient ascent algorithms on a number of computer vision and natural language processing problems."
Generalized sequential tree-reweighted message passing,"This paper addresses the problem of approximate MAP-MRF inference in general graphical models. Following [23], we consider a family of linear programming relaxations of the problem where each relaxation is specified by a set of nested pairs of factors for which the  marginalization constraint needs to be enforced. We develop a generalization of the TRW-S algorithm [6] for this problem, where we use a decomposition into junction chains, monotonic w.r.t. some ordering on the nodes. This generalizes the monotonic chains in [6] in a natural way. We also show how to deal with nested factors in an efficient way. Experiments show an improvement over min-sum diffusion, MPLP and subgradient ascent algorithms on a number of computer vision and natural language processing problems."
Dense Scattering Layer Removal ,"We propose a new model, together with advanced optimization, to separate a thick scattering media layer from a single natural image. It is able to handle challenging underwater scenes and images taken in fog and sandstorm, both of which are with significantly reduced visibility. Our method addresses the critical issue -- this is, originally unnoticeable impurities will be greatly magnified after removing the scattering media layer -- with transmission-aware optimization. We introduce non-local structure-aware regularization to properly constrain transmission estimation without introducing the halo artifacts. A selective-neighbor criterion is presented to convert the unconventional constrained optimization problem to an unconstrained one where the latter can be efficiently solved."
Dense Scattering Layer Removal ,"We propose a new model, together with advanced optimization, to separate a thick scattering media layer from a single natural image. It is able to handle challenging underwater scenes and images taken in fog and sandstorm, both of which are with significantly reduced visibility. Our method addresses the critical issue -- this is, originally unnoticeable impurities will be greatly magnified after removing the scattering media layer -- with transmission-aware optimization. We introduce non-local structure-aware regularization to properly constrain transmission estimation without introducing the halo artifacts. A selective-neighbor criterion is presented to convert the unconventional constrained optimization problem to an unconstrained one where the latter can be efficiently solved."
Dense Scattering Layer Removal ,"We propose a new model, together with advanced optimization, to separate a thick scattering media layer from a single natural image. It is able to handle challenging underwater scenes and images taken in fog and sandstorm, both of which are with significantly reduced visibility. Our method addresses the critical issue -- this is, originally unnoticeable impurities will be greatly magnified after removing the scattering media layer -- with transmission-aware optimization. We introduce non-local structure-aware regularization to properly constrain transmission estimation without introducing the halo artifacts. A selective-neighbor criterion is presented to convert the unconventional constrained optimization problem to an unconstrained one where the latter can be efficiently solved."
Unsupervised Object Matching via Probabilistic Latent Variable Models,"We propose a probabilistic latent variable model for unsupervised object matching, which is the task of finding correspondences between objects in different domains. With existing object matching methods, the numbers of objects in different domains must be the same, and the methods find one-to-one matching in two domains. The proposed model can handle multiple domains with different numbers of objects, and can find many-to-many matching. The proposed model assumes that there is a set of latent vectors that is shared by all domains, and each object is generated using one of the latent vectors and a domain-specific linear projection. By inferring a latent vector to be used for generating each object, we can match objects in an unsupervised manner. We demonstrate the effectiveness of the proposed model with experiments using synthetic, handwritten digit, music, and text data sets."
Unsupervised Object Matching via Probabilistic Latent Variable Models,"We propose a probabilistic latent variable model for unsupervised object matching, which is the task of finding correspondences between objects in different domains. With existing object matching methods, the numbers of objects in different domains must be the same, and the methods find one-to-one matching in two domains. The proposed model can handle multiple domains with different numbers of objects, and can find many-to-many matching. The proposed model assumes that there is a set of latent vectors that is shared by all domains, and each object is generated using one of the latent vectors and a domain-specific linear projection. By inferring a latent vector to be used for generating each object, we can match objects in an unsupervised manner. We demonstrate the effectiveness of the proposed model with experiments using synthetic, handwritten digit, music, and text data sets."
Unsupervised Object Matching via Probabilistic Latent Variable Models,"We propose a probabilistic latent variable model for unsupervised object matching, which is the task of finding correspondences between objects in different domains. With existing object matching methods, the numbers of objects in different domains must be the same, and the methods find one-to-one matching in two domains. The proposed model can handle multiple domains with different numbers of objects, and can find many-to-many matching. The proposed model assumes that there is a set of latent vectors that is shared by all domains, and each object is generated using one of the latent vectors and a domain-specific linear projection. By inferring a latent vector to be used for generating each object, we can match objects in an unsupervised manner. We demonstrate the effectiveness of the proposed model with experiments using synthetic, handwritten digit, music, and text data sets."
Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button,"A brain-computer interface (BCI) allows users to ?communicate? with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue.This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm UCB-classif based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session.Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%."
Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button,"A brain-computer interface (BCI) allows users to ?communicate? with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue.This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm UCB-classif based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session.Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%."
Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button,"A brain-computer interface (BCI) allows users to ?communicate? with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue.This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm UCB-classif based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session.Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%."
Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button,"A brain-computer interface (BCI) allows users to ?communicate? with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue.This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm UCB-classif based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session.Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%."
Online Discrimination of Nonlinear Dynamics with Switching Differential Equations,"How to recognise whether an observed person walks or runs? We consider a dynamic environment where observations (e.g. the posture of a person) are caused by different dynamic processes (walking or running) which are active one at a time and which may transition from one to another at any time. For this setup, switching dynamic models have been suggested previously, mostly, for linear and nonlinear dynamics in discrete time. Motivated by basic principles of computations in the brain (dynamic, internal models) we suggest a model for switching nonlinear differential equations. The switching process in the model is implemented by a Hopfield network and we use parametric dynamic movement primitives to represent arbitrary rhythmic motions. The model generates observed dynamics by linearly interpolating the primitives weighted by the switching variables and it is constructed such that standard filtering algorithms can be applied. In two experiments with synthetic planar motion and a human motion capture data set we show that inference with the unscented Kalman filter can successfully discriminate several dynamic processes online. "
Online Discrimination of Nonlinear Dynamics with Switching Differential Equations,"How to recognise whether an observed person walks or runs? We consider a dynamic environment where observations (e.g. the posture of a person) are caused by different dynamic processes (walking or running) which are active one at a time and which may transition from one to another at any time. For this setup, switching dynamic models have been suggested previously, mostly, for linear and nonlinear dynamics in discrete time. Motivated by basic principles of computations in the brain (dynamic, internal models) we suggest a model for switching nonlinear differential equations. The switching process in the model is implemented by a Hopfield network and we use parametric dynamic movement primitives to represent arbitrary rhythmic motions. The model generates observed dynamics by linearly interpolating the primitives weighted by the switching variables and it is constructed such that standard filtering algorithms can be applied. In two experiments with synthetic planar motion and a human motion capture data set we show that inference with the unscented Kalman filter can successfully discriminate several dynamic processes online. "
Online Discrimination of Nonlinear Dynamics with Switching Differential Equations,"How to recognise whether an observed person walks or runs? We consider a dynamic environment where observations (e.g. the posture of a person) are caused by different dynamic processes (walking or running) which are active one at a time and which may transition from one to another at any time. For this setup, switching dynamic models have been suggested previously, mostly, for linear and nonlinear dynamics in discrete time. Motivated by basic principles of computations in the brain (dynamic, internal models) we suggest a model for switching nonlinear differential equations. The switching process in the model is implemented by a Hopfield network and we use parametric dynamic movement primitives to represent arbitrary rhythmic motions. The model generates observed dynamics by linearly interpolating the primitives weighted by the switching variables and it is constructed such that standard filtering algorithms can be applied. In two experiments with synthetic planar motion and a human motion capture data set we show that inference with the unscented Kalman filter can successfully discriminate several dynamic processes online. "
Discriminative Low-Rank Representation Graph for Semi-supervised Learning,"The recently proposed low-rank representation (LRR) method is effective in ex-ploring subspace structures. However, LRR graph has no explicit connection tothe classification task. In this paper, we propose a discriminative low-rank rep-resentation (DisLRR) graph for semi-supervised learning. Our DisLRR graphcould not only seek the lowest rank representations among all the elements in thedictionary, but also incorporate discriminative information from labeled and un-labeled samples. The convergence of our algorithm is theoretically proved. Thenwe present a semi-supervised learning method by incorporating DisLRR graphand Gaussian harmonic function (GHF). Experimental results on a toy data set,the PIE, Extended YaleB and ORL databases demonstrate that our DisLRR graphoutperforms other related graphs, especially when the data are heavily corrupted."
Discriminative Low-Rank Representation Graph for Semi-supervised Learning,"The recently proposed low-rank representation (LRR) method is effective in ex-ploring subspace structures. However, LRR graph has no explicit connection tothe classification task. In this paper, we propose a discriminative low-rank rep-resentation (DisLRR) graph for semi-supervised learning. Our DisLRR graphcould not only seek the lowest rank representations among all the elements in thedictionary, but also incorporate discriminative information from labeled and un-labeled samples. The convergence of our algorithm is theoretically proved. Thenwe present a semi-supervised learning method by incorporating DisLRR graphand Gaussian harmonic function (GHF). Experimental results on a toy data set,the PIE, Extended YaleB and ORL databases demonstrate that our DisLRR graphoutperforms other related graphs, especially when the data are heavily corrupted."
PAC-Bayesian Structured Output Regression,We provide a theoretical basis for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the structured prediction loss when it is defined in terms of a positive definite kernel in the output space. We provide two PAC-Bayes upper bounds of the structured prediction risk that depend on the empirical quadratic risk of the deterministic predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that has never been proposed so far. Both predictors are compared on practical tasks.
PAC-Bayesian Structured Output Regression,We provide a theoretical basis for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the structured prediction loss when it is defined in terms of a positive definite kernel in the output space. We provide two PAC-Bayes upper bounds of the structured prediction risk that depend on the empirical quadratic risk of the deterministic predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that has never been proposed so far. Both predictors are compared on practical tasks.
PAC-Bayesian Structured Output Regression,We provide a theoretical basis for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the structured prediction loss when it is defined in terms of a positive definite kernel in the output space. We provide two PAC-Bayes upper bounds of the structured prediction risk that depend on the empirical quadratic risk of the deterministic predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that has never been proposed so far. Both predictors are compared on practical tasks.
PAC-Bayesian Structured Output Regression,We provide a theoretical basis for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the structured prediction loss when it is defined in terms of a positive definite kernel in the output space. We provide two PAC-Bayes upper bounds of the structured prediction risk that depend on the empirical quadratic risk of the deterministic predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that has never been proposed so far. Both predictors are compared on practical tasks.
Information-Theoretic Limits on Model Selection for Gaussian Markov Random Fields in the High-Dimensional Setting,"This paper focuses on the information-theoretic limitations of model selection for Gaussian Markov random fields in the high-dimensional setting, where the graph size $p$ and the number of edges $k$ are allowed to scale with the sample size $n$. We provide an a rigorous analysis of this problem for generic graphs in an ensemble. Our result establishes a necessary condition on the sample size $n(p,k)$ for any procedure, regardless of its computational complexity, to consistently recover the underlying graph. Moreover, our analysis implies a connection between that graphical model selection limits and eigenvalues of concentration matrices. The key way out of the difficulty is found via investigating the orthogonal systems from concentration matrices, making it possible to calculate the symmetric Kullback-Leibler divergence between generic graphs and obtain the final simple result. Our method of analyzing generic graphs using orthogonal systems would be of use to other model selection problems."
Dynamic Classification of Ballistic Missiles Using Neural Network and HMM,"This paper addresses dynamic classification of different Ballistic Missiles based on kinematic attributes acquired by radars for taking appropriate measures to tackle them. Real Time Neural Network and Hidden Markov Model is applied for dynamic classification of the target trajectory and results compared for performance and time taken in each case. There are many applications which require capturing scenario dynamically and predicting its class. For air defense application, it is a major challenge to recognize the kind of incoming threats and counteract with corresponding counter-measures. A model is developed and experiment conducted with 6DOF simulated data for evaluating the model. "
Learning Useful Abstractions from the Web: Case Study on Patient Medications and Outcomes,"The successful application of machine learning to electronic medical records typically turns on the construction of an appropriate feature vector.  That often depends upon the ability to find an appropriate way to abstract the large number of variables found in such records.  In this paper, we explore the use of topic modeling to design feature vectors in an automated manner by harnessing expertise available on the Web. We test the proposed methods on the task of inferring useful abstractions from a list of thousands of medications. Using Latent Dirichlet Allocation we learn a topic model based on Web entries corresponding to each drug in the list. Using only knowledge from Wikipedia pages, we were able to learn a model that is similar to the curated drug classification scheme that serves as an industry standard. We further demonstrate the utility of these learned abstractions through the construction of a kernel based on the earth mover's distance and derived from the learned topic model. Applied to a corpus of 25,000 patient admissions, we use this kernel to predict three different adverse outcomes (death, an abnormally long stay, or admission through the emergency room) for the next hospital admission.  Somewhat surprisingly,  the classifiers built using the learned abstractions outperform classifiers learned from the curated drug classification scheme."
Learning Model-Based Sparsity via Projected Gradient Descent,"Several convex formulation methods have been proposed previously for statistical estimation with structured sparsity as the prior. These methods often require a carefully tuned regularization parameter, often a cumbersome or heuristic exercise. Furthermore, the estimate that these methods produce might not belong to the desired sparsity model, albeit accurately approximating the true parameter. Therefore, greedy-type algorithms could often be more desirable in estimating structured-sparse parameters. So far, these greedy methods have mostly focused on linear statistical models. In this paper we study the projected gradient descent with non-convex structured-sparse parameter model as the constraint set. Should the cost function have a Stable Model-Restricted Hessian the algorithm converges to the desired minimizer up to an approximation error. As an example we elaborate on application of the main results to estimation in Generalized Linear Model."
Fast Probabilistic Optimization from Noisy Gradients,"Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, we construct a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from *noisy* evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions."
Hierarchical Visual Feature Learning for Computer Aided Diagnosis Using 3D Medical Images on Large-Scale Evaluation,"Computer aided diagnosis (CAD) of cancerous anatomical structures via 3D medical images has emerged as an intensively studied research area. In this paper, we present a principled three-tiered image feature learning approach, to capture task specific and data-driven class discriminative statistics from annotated image database, apart from often hand-crafted, heuristic approaches. It integrates voxel-level, instance-level and database-level feature learning, aggregation and parsing. We demonstrate its effectiveness in unified lung nodule, lung vascular structure and colon polyp feature computation and detection by classification in CAD applications. The other advantage includes that it only requires fast segmentation-less (e.g., simple thresholding on voxel-level labeling probabilities) image processing in training and runtime, which also alleviates the classification bias on certain (over or under) segmentation algorithms. After instance-level aggregation, features can also be flexibly tuned for classifying positive and negative cancers/tumors, or discriminating different subcategories of nodule/polyp, e.g., according to the clinically-relevant size or shape morphologies, within our designed gating classifier. Our hierarchical feature learning framework enables to achieve significantly superior performances than previous state-of-the-art CAD systems, extensively validated on highly representative multi-site clinical datasets using $879$ and $770$ CT volumes, for lung and colon CAD tasks respectively. "
Hierarchical Visual Feature Learning for Computer Aided Diagnosis Using 3D Medical Images on Large-Scale Evaluation,"Computer aided diagnosis (CAD) of cancerous anatomical structures via 3D medical images has emerged as an intensively studied research area. In this paper, we present a principled three-tiered image feature learning approach, to capture task specific and data-driven class discriminative statistics from annotated image database, apart from often hand-crafted, heuristic approaches. It integrates voxel-level, instance-level and database-level feature learning, aggregation and parsing. We demonstrate its effectiveness in unified lung nodule, lung vascular structure and colon polyp feature computation and detection by classification in CAD applications. The other advantage includes that it only requires fast segmentation-less (e.g., simple thresholding on voxel-level labeling probabilities) image processing in training and runtime, which also alleviates the classification bias on certain (over or under) segmentation algorithms. After instance-level aggregation, features can also be flexibly tuned for classifying positive and negative cancers/tumors, or discriminating different subcategories of nodule/polyp, e.g., according to the clinically-relevant size or shape morphologies, within our designed gating classifier. Our hierarchical feature learning framework enables to achieve significantly superior performances than previous state-of-the-art CAD systems, extensively validated on highly representative multi-site clinical datasets using $879$ and $770$ CT volumes, for lung and colon CAD tasks respectively. "
Hierarchical Visual Feature Learning for Computer Aided Diagnosis Using 3D Medical Images on Large-Scale Evaluation,"Computer aided diagnosis (CAD) of cancerous anatomical structures via 3D medical images has emerged as an intensively studied research area. In this paper, we present a principled three-tiered image feature learning approach, to capture task specific and data-driven class discriminative statistics from annotated image database, apart from often hand-crafted, heuristic approaches. It integrates voxel-level, instance-level and database-level feature learning, aggregation and parsing. We demonstrate its effectiveness in unified lung nodule, lung vascular structure and colon polyp feature computation and detection by classification in CAD applications. The other advantage includes that it only requires fast segmentation-less (e.g., simple thresholding on voxel-level labeling probabilities) image processing in training and runtime, which also alleviates the classification bias on certain (over or under) segmentation algorithms. After instance-level aggregation, features can also be flexibly tuned for classifying positive and negative cancers/tumors, or discriminating different subcategories of nodule/polyp, e.g., according to the clinically-relevant size or shape morphologies, within our designed gating classifier. Our hierarchical feature learning framework enables to achieve significantly superior performances than previous state-of-the-art CAD systems, extensively validated on highly representative multi-site clinical datasets using $879$ and $770$ CT volumes, for lung and colon CAD tasks respectively. "
On Consistent Classification with Imbalanced Classes,"We consider the problem of imbalanced classes in binary classification, where one class is rare compared to the other. This problem arises frequently in practice and has been widely studied. However very little is understood in terms of the theoretical properties of the problem or of the algorithms proposed: what performance measures are appropriate, how these affect the learning process, and whether the algorithms are statistically consistent with respect to the desired performance measures. In this paper, we initiate a formal study of these issues, focusing on the balanced 0-1 error that evaluates errors on the majority and minority classes separately and effectively balances the two. The underlying balanced 0-1 loss bears similarity to cost-sensitive losses; however a critical difference between the two is that the balanced loss depends on the underlying distribution, while cost-sensitive losses are defined independent of the distribution. We establish statistical consistency of two types of algorithms with respect to the balanced 0-1 error: plug-in rules that use an empirically determined threshold, and certain types of empirically balanced risk minimization algorithms. Our experiments support our theoretical results, showing that both these approaches perform as well as (or better than) under-/over-sampling methods that are currently viewed as the state of the art."
Passivity-based Monitoring of POMDPs,"Maintaining exact belief states in POMDPs can be a difficult task since the size of the belief state grows exponentially with the number of state variables. Boyen and Koller described an approximation method which exploits locality in the process by maintaining smaller belief states over clusters of correlated variables. While this is a useful method, it does not fully account for the causal relations between the variables. We study a particular type of causal relation, called passivity, which captures the notion that a variable changes only if any of the variables that directly influence it change or if it is the target of an action. We show that passivity can be exploited in conjunction with locality to accelerate the monitoring task. The idea is to maintain separate beliefs over subsets of correlated variables, and to update only those beliefs whose variables we suspect to have changed. We present an algorithm, called Passivity-based Parallel Monitoring (PPM), that implements this idea. We show empirically that PPM outperforms two state-of-the-art solutions, while maintaining competitive accuracy. Our experiments indicate that the relative computational gains grow significantly with the size of the process."
Multiplicative Forests for Continuous-Time Processes,"Learning temporal dependencies between variables over continuous time is an important and challenging task. Continuous-time Bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices, which grows exponentially in the number of parents per variable. We develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits. Using a multiplicative assumption we show how to update the forest likelihood in closed form, producing efficient model updates. Our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability."
Group-wise FMRI Activation Detection Based on DICCCOL ,"Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and its less sensitivity to variabilities in individual subject?s brains. However, current group-wise fMRI activation detection methods rely on the co-registration of individual brain?s fMRI images into the same atlas space, which has difficulty in dealing with the remarkable anatomical variation of different brains. As a consequence, the resulted misalignments between different brains could significantly degrade the required inter-subject correspondences, thus reducing the sensitivity and specificity of group-wise fMRI activation detection. This paper presents a novel approach to detect group-wise fMRI activation on the recently developed and validated Dense Individualized and Common Connectivity-based Cortical Landmarks (DICCCOL), which is a dense map of cortical landmarks that possess intrinsic structural and anatomical correspondences across individuals and populations. The basic idea in this paper is that fMRI activation detection is first performed on each corresponding DICCCOL landmark in the individual brain?s own space using the general linear model (GLM), and then the activation significances of the same landmark from a group of subjects are statistically integrated and assessed at the group level. Finally, the consistently activated landmarks are determined and declared as the detected brain locations in response to external stimuli. Our experimental results demonstrated that the proposed approach can locate activation substantially more precisely than the traditional group-wise activation detection methods. "
Group-wise FMRI Activation Detection Based on DICCCOL ,"Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and its less sensitivity to variabilities in individual subject?s brains. However, current group-wise fMRI activation detection methods rely on the co-registration of individual brain?s fMRI images into the same atlas space, which has difficulty in dealing with the remarkable anatomical variation of different brains. As a consequence, the resulted misalignments between different brains could significantly degrade the required inter-subject correspondences, thus reducing the sensitivity and specificity of group-wise fMRI activation detection. This paper presents a novel approach to detect group-wise fMRI activation on the recently developed and validated Dense Individualized and Common Connectivity-based Cortical Landmarks (DICCCOL), which is a dense map of cortical landmarks that possess intrinsic structural and anatomical correspondences across individuals and populations. The basic idea in this paper is that fMRI activation detection is first performed on each corresponding DICCCOL landmark in the individual brain?s own space using the general linear model (GLM), and then the activation significances of the same landmark from a group of subjects are statistically integrated and assessed at the group level. Finally, the consistently activated landmarks are determined and declared as the detected brain locations in response to external stimuli. Our experimental results demonstrated that the proposed approach can locate activation substantially more precisely than the traditional group-wise activation detection methods. "
Group-wise FMRI Activation Detection Based on DICCCOL ,"Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and its less sensitivity to variabilities in individual subject?s brains. However, current group-wise fMRI activation detection methods rely on the co-registration of individual brain?s fMRI images into the same atlas space, which has difficulty in dealing with the remarkable anatomical variation of different brains. As a consequence, the resulted misalignments between different brains could significantly degrade the required inter-subject correspondences, thus reducing the sensitivity and specificity of group-wise fMRI activation detection. This paper presents a novel approach to detect group-wise fMRI activation on the recently developed and validated Dense Individualized and Common Connectivity-based Cortical Landmarks (DICCCOL), which is a dense map of cortical landmarks that possess intrinsic structural and anatomical correspondences across individuals and populations. The basic idea in this paper is that fMRI activation detection is first performed on each corresponding DICCCOL landmark in the individual brain?s own space using the general linear model (GLM), and then the activation significances of the same landmark from a group of subjects are statistically integrated and assessed at the group level. Finally, the consistently activated landmarks are determined and declared as the detected brain locations in response to external stimuli. Our experimental results demonstrated that the proposed approach can locate activation substantially more precisely than the traditional group-wise activation detection methods. "
Group-wise FMRI Activation Detection Based on DICCCOL ,"Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and its less sensitivity to variabilities in individual subject?s brains. However, current group-wise fMRI activation detection methods rely on the co-registration of individual brain?s fMRI images into the same atlas space, which has difficulty in dealing with the remarkable anatomical variation of different brains. As a consequence, the resulted misalignments between different brains could significantly degrade the required inter-subject correspondences, thus reducing the sensitivity and specificity of group-wise fMRI activation detection. This paper presents a novel approach to detect group-wise fMRI activation on the recently developed and validated Dense Individualized and Common Connectivity-based Cortical Landmarks (DICCCOL), which is a dense map of cortical landmarks that possess intrinsic structural and anatomical correspondences across individuals and populations. The basic idea in this paper is that fMRI activation detection is first performed on each corresponding DICCCOL landmark in the individual brain?s own space using the general linear model (GLM), and then the activation significances of the same landmark from a group of subjects are statistically integrated and assessed at the group level. Finally, the consistently activated landmarks are determined and declared as the detected brain locations in response to external stimuli. Our experimental results demonstrated that the proposed approach can locate activation substantially more precisely than the traditional group-wise activation detection methods. "
Group-wise FMRI Activation Detection Based on DICCCOL ,"Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and its less sensitivity to variabilities in individual subject?s brains. However, current group-wise fMRI activation detection methods rely on the co-registration of individual brain?s fMRI images into the same atlas space, which has difficulty in dealing with the remarkable anatomical variation of different brains. As a consequence, the resulted misalignments between different brains could significantly degrade the required inter-subject correspondences, thus reducing the sensitivity and specificity of group-wise fMRI activation detection. This paper presents a novel approach to detect group-wise fMRI activation on the recently developed and validated Dense Individualized and Common Connectivity-based Cortical Landmarks (DICCCOL), which is a dense map of cortical landmarks that possess intrinsic structural and anatomical correspondences across individuals and populations. The basic idea in this paper is that fMRI activation detection is first performed on each corresponding DICCCOL landmark in the individual brain?s own space using the general linear model (GLM), and then the activation significances of the same landmark from a group of subjects are statistically integrated and assessed at the group level. Finally, the consistently activated landmarks are determined and declared as the detected brain locations in response to external stimuli. Our experimental results demonstrated that the proposed approach can locate activation substantially more precisely than the traditional group-wise activation detection methods. "
Group-wise FMRI Activation Detection Based on DICCCOL ,"Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and its less sensitivity to variabilities in individual subject?s brains. However, current group-wise fMRI activation detection methods rely on the co-registration of individual brain?s fMRI images into the same atlas space, which has difficulty in dealing with the remarkable anatomical variation of different brains. As a consequence, the resulted misalignments between different brains could significantly degrade the required inter-subject correspondences, thus reducing the sensitivity and specificity of group-wise fMRI activation detection. This paper presents a novel approach to detect group-wise fMRI activation on the recently developed and validated Dense Individualized and Common Connectivity-based Cortical Landmarks (DICCCOL), which is a dense map of cortical landmarks that possess intrinsic structural and anatomical correspondences across individuals and populations. The basic idea in this paper is that fMRI activation detection is first performed on each corresponding DICCCOL landmark in the individual brain?s own space using the general linear model (GLM), and then the activation significances of the same landmark from a group of subjects are statistically integrated and assessed at the group level. Finally, the consistently activated landmarks are determined and declared as the detected brain locations in response to external stimuli. Our experimental results demonstrated that the proposed approach can locate activation substantially more precisely than the traditional group-wise activation detection methods. "
An Analytic and Empirical Evaluation of Return-on-Investment-Based Active Learning,"Return-on-Investment (ROI) is a cost-conscious approach to active learning (AL)that considers both estimates of cost and of benefit in active sample selection.In this paper, we investigate the conditions for successful cost-conscious ALusing ROI by proving the conditions under which ROI would be optimal. We thenempirically measure the degree to which optimality is jeopardized in practicewhen the conditions are violated. We find that the more linearly related abenefit estimator is to true benefit, the better it performs when paired with animperfect cost estimate in ROI.  Lastly, we use our analysis to explain themixed results of previous work. Our results show that ROI can indeedsuccessfully reduce total annotation costs."
An Analytic and Empirical Evaluation of Return-on-Investment-Based Active Learning,"Return-on-Investment (ROI) is a cost-conscious approach to active learning (AL)that considers both estimates of cost and of benefit in active sample selection.In this paper, we investigate the conditions for successful cost-conscious ALusing ROI by proving the conditions under which ROI would be optimal. We thenempirically measure the degree to which optimality is jeopardized in practicewhen the conditions are violated. We find that the more linearly related abenefit estimator is to true benefit, the better it performs when paired with animperfect cost estimate in ROI.  Lastly, we use our analysis to explain themixed results of previous work. Our results show that ROI can indeedsuccessfully reduce total annotation costs."
Patient Risk Stratification for Hospital-Associated C. Diff as a Time-Series Classification Task,"A patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state. We explore representing patient risk as a time series. In doing so, patient risk stratification becomes a time-series classification task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted. Thus, we begin by defining and extracting approximate \textit{risk processes}, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns. We apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \textit{Clostridium Difficile}. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05). "
A Stochastic Spiking Network Model of Sensorimotor Control,"Despite decades of research on sensorimotor control, there are no models thatprovide a link between the stochastic activity of neuron populations and humanbehavior when faced with uncertain, redundant, or novel environments. Here wepropose a new computational model that provides a direct link between neuronalactivity and behavior. Our model is based on a recent mathematical frameworkthat works with state probability densities rather than explicit state variables. Itcomprises a spiking neural network including sensory receptors, sensory cortex,control operators, and motoneurons. Sensory cortex computes internal state probabilityestimates by Bayesian filtering of measurements from sensory receptorsand efference information from the motor neurons. Motoneuron commands arecalculated by optimizing a cost/value function, using internal estimates and controloperators stored as synaptic weights. We simulated the model in Matlab andimplemented it on a Phantom robot arm. Simulations and robotic demonstrationspredicted a wide variety of behavior, such as reaching and tracking, reflexes intask-relevant directions, and reward/penalty trade-off responses. Our new computationalmodel can provide a neurophysiological explanation of specific humansensorimotor functions under uncertainty."
A Stochastic Spiking Network Model of Sensorimotor Control,"Despite decades of research on sensorimotor control, there are no models thatprovide a link between the stochastic activity of neuron populations and humanbehavior when faced with uncertain, redundant, or novel environments. Here wepropose a new computational model that provides a direct link between neuronalactivity and behavior. Our model is based on a recent mathematical frameworkthat works with state probability densities rather than explicit state variables. Itcomprises a spiking neural network including sensory receptors, sensory cortex,control operators, and motoneurons. Sensory cortex computes internal state probabilityestimates by Bayesian filtering of measurements from sensory receptorsand efference information from the motor neurons. Motoneuron commands arecalculated by optimizing a cost/value function, using internal estimates and controloperators stored as synaptic weights. We simulated the model in Matlab andimplemented it on a Phantom robot arm. Simulations and robotic demonstrationspredicted a wide variety of behavior, such as reaching and tracking, reflexes intask-relevant directions, and reward/penalty trade-off responses. Our new computationalmodel can provide a neurophysiological explanation of specific humansensorimotor functions under uncertainty."
Multiclass Learning Approaches: A Theoretical Comparison with Implications,"We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error."
Multiclass Learning Approaches: A Theoretical Comparison with Implications,"We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error."
Multiclass Learning Approaches: A Theoretical Comparison with Implications,"We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error."
"Top-k Feature Selection via ?2,0-Norm Constraint","Real-world applications, such as bioinformatics, often need select top-$k$ features in the classification tasks. Previous sparse learning based feature selection methods impose the sparsity regularization to learn the features weights and rank them to select the top-$k$ features. Because the ranking weights were learned not for the exact top-$k$ features, such feature selection methods may not get the optimal results. We propose a novel and robust exact top-$k$ feature selection approach using an explicit $\ell_{2,0}$-norm constraint without any extra parameter. An efficient algorithm based on augmented Lagrangian method is derived to solve the $\ell_{2,0}$-norm constrained objective to find out the stable local solution. Extensive experiments on four biological datasets show that although our proposed model is not a convex problem, it outperforms the approximate convex counterparts and state-of-the-art feature selection methods in terms of classification accuracy on two popular classifiers. Because the regularization parameter of our method has explicit meaning (in bioinformatics applications $k$ is often fixed), \emph{i.e.} the number of selected feature, it avoids the burden of tuning the parameter and is a pragmatic feature selection method."
Stochastic Gradient Descent with Only One Projection,"Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $O(1/\sqrt{T})$ convergence rate for general convex optimization, and an $O(\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function."
Learning for Structured Prediction Using Stochastic Descent with Working Sets,"We propose an approximate subgradient descent algorithm using working sets to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM.  We focus on the settings of general graphical models, such as MRF and CRF with loops, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM as well as existing subgradient based methods. We show that under mild conditions the proposed method approximates subgradient descent arbitrarily closely by finding a true subgradient with probability tending to one as the working set grows.  This is desirable in our settings, since the intractability of exact inference implies that true subgradients cannot be reliably obtained.  Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, dramatically reducing learning time while maintaining the same level of performance.  We demonstrate the strength of our method empirically in the context of image segmentation."
Learning for Structured Prediction Using Stochastic Descent with Working Sets,"We propose an approximate subgradient descent algorithm using working sets to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM.  We focus on the settings of general graphical models, such as MRF and CRF with loops, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM as well as existing subgradient based methods. We show that under mild conditions the proposed method approximates subgradient descent arbitrarily closely by finding a true subgradient with probability tending to one as the working set grows.  This is desirable in our settings, since the intractability of exact inference implies that true subgradients cannot be reliably obtained.  Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, dramatically reducing learning time while maintaining the same level of performance.  We demonstrate the strength of our method empirically in the context of image segmentation."
Learning for Structured Prediction Using Stochastic Descent with Working Sets,"We propose an approximate subgradient descent algorithm using working sets to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM.  We focus on the settings of general graphical models, such as MRF and CRF with loops, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM as well as existing subgradient based methods. We show that under mild conditions the proposed method approximates subgradient descent arbitrarily closely by finding a true subgradient with probability tending to one as the working set grows.  This is desirable in our settings, since the intractability of exact inference implies that true subgradients cannot be reliably obtained.  Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, dramatically reducing learning time while maintaining the same level of performance.  We demonstrate the strength of our method empirically in the context of image segmentation."
Affect Sensitive Music Recommendation using Brain Computer Interfaces,"This paper aims to explore the problem of building an affect sensitive music recommendation system. Specifically, the proposed system harnesses electroencephalograph (EEG) signals for the purpose of assessing the listener's affective state. The core idea lies on the hypothesis that cortical signals captured by off-the-shelf electrodes carry enough information about mental state of a listener and can be used to build preference models over musical taste for each individual user. We present a reinforcement learning algorithm that aims to build such models over a period of time and then use it effectively to provide recommendations such that enable the listener to achieve the target mental state. Our experiments on real users indicate that the recommendation policy learnt via the brain-computer interface provides better recommendations than commercial services such as Pandora, that do not incorporate affect."
Affect Sensitive Music Recommendation using Brain Computer Interfaces,"This paper aims to explore the problem of building an affect sensitive music recommendation system. Specifically, the proposed system harnesses electroencephalograph (EEG) signals for the purpose of assessing the listener's affective state. The core idea lies on the hypothesis that cortical signals captured by off-the-shelf electrodes carry enough information about mental state of a listener and can be used to build preference models over musical taste for each individual user. We present a reinforcement learning algorithm that aims to build such models over a period of time and then use it effectively to provide recommendations such that enable the listener to achieve the target mental state. Our experiments on real users indicate that the recommendation policy learnt via the brain-computer interface provides better recommendations than commercial services such as Pandora, that do not incorporate affect."
On the Reconstruction of Piecewise Constant Dependences,"An approach to the restoration of dependences (regressions) is proposed that is based on solving problems of supervised classification. The main task is finding the optimal partitioning of the range of values of dependent variable on a finite number of intervals. It is necessary to find optimal number of change-points and their positions. This task is formulated as search and application of  piece-wise constant function. When restoring piecewise constant functions, the problem of local discrete optimization using a model of logic supervised classification in leave ?one-out mode is solved. The value of the dependent value is calculated in two steps. At first, the problem of classification of feature vector is solved. Further, the dependent variables is calculated as half of the sum of change-points values of the corresponding class"
"Neuronal spike generation mechanism as an oversampling, noise-shaping A-to-D converter","We explore the hypothesis that the neuronal spike generation mechanism is an analog-to-digital converter, which rectifies low-pass filtered summed synaptic currents and encodes them into spike trains linearly decodable in post-synaptic neurons. To digitally encode an analog current waveform, the sampling rate of the spike generation mechanism must exceed its Nyquist rate. Such oversampling is consistent with the experimental observation that the precision of the spike-generation mechanism is an order of magnitude greater than the cut-off frequency of dendritic low-pass filtering. To achieve additional reduction in the error of analog-to-digital conversion, electrical engineers rely on noise-shaping. If noise-shaping were used in neurons, it would introduce correlations in spike timing to reduce low-frequency (up to Nyquist) transmission error at the cost of high-frequency one (from Nyquist to sampling rate). Using experimental data from three different classes of neurons, we demonstrate that biological neurons utilize noise-shaping. We also argue that rectification by the spike-generation mechanism may improve energy efficiency and carry out de-noising. Finally, the zoo of ion channels in neurons may be viewed as a set of predictors, various subsets of which are activated depending on the statistics of the input current. "
Assessing Blinding in Clinical Trials,"The interaction between the patient's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants' feedback."
Supervised $N$-gram Topic Model,"We propose a Bayesian nonparametric topic model that detects both label and the corresponding word/phrase, simultaneously, from labeled data. Unlike existing supervised topic models, which are based on the bag-of-words assumption, our proposal, labeled $N$-gram topic model (LNT) focuses on word order, and the interdependency between a given label and word/phrase in a text to detect objective information. Toward this direction, this model takes a hierarchical Bayesian nonparametric approach where each hidden variable is distributed  according to a Pitman-Yor process.Theoretically, this model overcomes several inherent limitations and weaknesses of conventional topic models: the decision of selecting appropriate number of topics and the lack of power-law distribution in the word frequencies.  Experiments on review articles show that LNT is useful as a generative model to discover objective phrase, and provides a complement of a human expert and domain specific knowledge. "
Loss-Regularized CRF for Preference Aggregation,"We develop a flexible Conditional Random Field framework for supervised preference aggregation, which combines preferences from multiple experts over items to form a distribution over rankings. We explore methods of optimizing the parameters of this distribution given the expert preferences, some of which incorporate the loss metric used at test time, and propose a new loss-based training objective. Unlike existing aggregation methods the resulting model can incorporate most existing test metrics, and permits efficient optimization. Experiments on benchmark tasks demonstrates significant gains over existing rank aggregation methods."
Loss-Regularized CRF for Preference Aggregation,"We develop a flexible Conditional Random Field framework for supervised preference aggregation, which combines preferences from multiple experts over items to form a distribution over rankings. We explore methods of optimizing the parameters of this distribution given the expert preferences, some of which incorporate the loss metric used at test time, and propose a new loss-based training objective. Unlike existing aggregation methods the resulting model can incorporate most existing test metrics, and permits efficient optimization. Experiments on benchmark tasks demonstrates significant gains over existing rank aggregation methods."
Loss-Regularized CRF for Preference Aggregation,"We develop a flexible Conditional Random Field framework for supervised preference aggregation, which combines preferences from multiple experts over items to form a distribution over rankings. We explore methods of optimizing the parameters of this distribution given the expert preferences, some of which incorporate the loss metric used at test time, and propose a new loss-based training objective. Unlike existing aggregation methods the resulting model can incorporate most existing test metrics, and permits efficient optimization. Experiments on benchmark tasks demonstrates significant gains over existing rank aggregation methods."
Tree-structured Kernel Dimension Reduction,"Tree-structured approaches are to iteratively split data until some termination criterion is satisfied.  Most of the approaches can deal with either supervised learning, or unsupervised learning, but not both.  In this paper, we propose (residual) tree-structured Kernel Dimension Reduction (rtKDR/tKDR) approaches to address the issue. Specifically, we alternatively use kernel dimension reduction (KDR) to estimate a linear projection direction of (residual) response variables in each non-terminal node for maximizing conditional independence between explanatory variables and (residual) response variables under Reproducing Kernel Hilbert Spaces, and split the (residual) data based on the median of projection indices that the (residual) data project into the direction. When the explanatory variables are continuous, discrete and response ones, rtKDR/tKDR can deal with not only supervised learning but also unsupervised learning.  Furthermore, rtKDR has the potential of discovering the intrinsic dimensions from high-dimensional nonlinear data sets. Experiments in several benchmark datasets indicate that rtKDR/tKDR attain better space partition and prediction performances compared with several recently published space partition algorithms. "
Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification,"In graph-based semi-supervised learning approaches, the classification rateis highly dependent on the size of the available labeled data, as well as the accuracy of the similaritymeasures (or equivalently the distance metric).Here, we propose a semi-supervised multi-class/multi-label classification scheme,dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process.Existing classification methods often have difficulty in dealing withmulti-class/multi-label problems due to the use of fixed similarity measures; our algorithm insteademphasizes metric fusion in a dynamic way. Significant improvement over thestate-of-the-art methods is observed onbenchmark datasets for hand-written digits and object recognition."
Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification,"In graph-based semi-supervised learning approaches, the classification rateis highly dependent on the size of the available labeled data, as well as the accuracy of the similaritymeasures (or equivalently the distance metric).Here, we propose a semi-supervised multi-class/multi-label classification scheme,dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process.Existing classification methods often have difficulty in dealing withmulti-class/multi-label problems due to the use of fixed similarity measures; our algorithm insteademphasizes metric fusion in a dynamic way. Significant improvement over thestate-of-the-art methods is observed onbenchmark datasets for hand-written digits and object recognition."
A Recalibration Procedure which maximizes the AUC: A Use-Case for Bi-Normal Assumptions,"Area under the receiver operating characteristic curve (AUC) is a popular measure for evaluating the quality of binary classification rules. Commonly used score-based classifiers label an outcome as a positive if the score is greater than a certain threshold. We show that this may not be optimal in terms of maximizing the AUC. Under certain assumptions the optimal thresholding rule is derived using the Neyman-Pearson lemma. Specifically, we show that a thresholding rule that isquadratic in the score dominates the commonly used linear thresholding rule. Our work provides a real data use-case where the recalibration significantly improvesthe AUC."
Scalable nonconvex inexact proximal splitting,"We study large-scale, nonsmooth, nonconconvex optimization problems. In particular, we focus on nonconvex problems with \emph{composite} objectives. This class of problems includes the extensively studied convex, composite objective problems as a special case. To tackle composite nonconvex problems, we introduce a powerful new framework based on asymptotically \emph{nonvanishing} errors, avoiding the common convenient assumption of eventually vanishing errors. Within our framework we derive both batch and incremental nonconvex proximal splitting algorithms. To our knowledge, our framework is first to develop and analyze incremental \emph{nonconvex} proximal-splitting algorithms, even if we disregard the ability to handle nonvanishing errors. We illustrate our theoretical framework by showing how it applies to difficult large-scale, nonsmooth, and nonconvex problems."
Learning to Discover Social Circles in Ego Networks,"Our personal social networks are big and cluttered, and currently there is no good way to organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g. `circles' on Google+, and `lists' on Facebook and Twitter), however they are laborious to construct and must be updated whenever a user's network grows. We define a novel machine learning task of identifying users' social circles. We pose the problem as a node clustering problem on a user's ego-network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user profile information. For each circle we learn its members and the circle-specific user profile similarity metric. Modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter for all of which we obtain hand-labeled ground-truth data."
Probabilistic Topic Coding for Superset Label Learning,"In the superset label learning problem, each training instance provides a set of candidate labels of which one is the true label of the instance.  Most approaches learn a discriminative classifier that tries to minimize an upper bound of the unobserved 0/1 loss. In this work, we propose a probabilistic model, Probabilistic Topic Coding (PTC), for the superset label learning problem. The PTC model is derived from logistic stick breaking process. It first maps the data to ``topics'', and then assigns to each topic a label drawn from a multinomial distribution.  The layer of topics can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art.  The discovered underlying structures also provide improved explanations of the classification predictions."
On Learning Camera Motion for Cinematographic Shot Classification,"In this paper, we propose a discriminative representation of a video shot based on its camera motion and demonstrate how the representation can be used for high level tasks like event recognition. In our technique, we assume that a homography exists between subsequent pairs of frames in a given video shot. Using purely image-based methods, we compute homography parameters that serve as coarseindicators of the camera motion. Next, using Lie algebra, we map the homography matrices to an intermediate vector space that preserves the intrinsic geometric structure of the transformation. Multiple time series are then constructed from these mappings. Features computed on these time series are used for discriminative classification of video shots. Our empirical evaluations on eight cinematographicshot classes show that our technique performs better than approaches that are based on image-based estimation of camera trajectories. Finally we show an application of our shot representation for detection of complex events in consumer videos."
Weighted Online Learning,"We consider an unconventional online learning problem which we  call Weighted Online Learning (WOL), where each training example has associated with it a (non-uniform) non-negative weight. WOL problems occur in many real life applications including banking business, medical diagnosis and visual tracking, where different samples are of differing value to the learning process. We propose several algorithms for WOL and show these algorithms have similar regret bounds and convergence rates to Pegasos. Applications in bank credit estimation, medical diagnosis and visual tracking show a significant improvement over state-of-the-art methods using traditional online learning."
Weighted Online Learning,"We consider an unconventional online learning problem which we  call Weighted Online Learning (WOL), where each training example has associated with it a (non-uniform) non-negative weight. WOL problems occur in many real life applications including banking business, medical diagnosis and visual tracking, where different samples are of differing value to the learning process. We propose several algorithms for WOL and show these algorithms have similar regret bounds and convergence rates to Pegasos. Applications in bank credit estimation, medical diagnosis and visual tracking show a significant improvement over state-of-the-art methods using traditional online learning."
Solving L1 Norm Matrix Factorization from the Elementary Unit by Coordinate Descent,"The L1 norm low-rank matrix factorization (LRMF) has been attracting much attention due to its intrinsic robustness to outliers and missing data. However, L1 norm LRMF is difficult to be resolved due to its non-convexity and non-smoothness. In this paper, we propose a novel solution, which is essentially a coordinate descent approach, to L1 norm LRMF. The main idea is to break the original difficult problem into its elementary sub-problems, each involving only one single scalar parameter, and recursively solve them. Each of these one-scalar sub-problems is convex and has a closed-form solution. The proposed method thus involves only simple computations and avoids any time-consuming inner loop numerical optimization. One important advantage of our method is that it provides a unified framework of solving L1 norm LRMF problems with or without missing data, and can be naturally extended to other matrix factorization tasks, such as nonnegative matrix factorization and sparse PCA. The extensive experimental results validate that our method outperforms state-of-the-arts in term of both computational time and accuracy, especially on large-scale applications such as face recognition and structure from motion."
Solving L1 Norm Matrix Factorization from the Elementary Unit by Coordinate Descent,"The L1 norm low-rank matrix factorization (LRMF) has been attracting much attention due to its intrinsic robustness to outliers and missing data. However, L1 norm LRMF is difficult to be resolved due to its non-convexity and non-smoothness. In this paper, we propose a novel solution, which is essentially a coordinate descent approach, to L1 norm LRMF. The main idea is to break the original difficult problem into its elementary sub-problems, each involving only one single scalar parameter, and recursively solve them. Each of these one-scalar sub-problems is convex and has a closed-form solution. The proposed method thus involves only simple computations and avoids any time-consuming inner loop numerical optimization. One important advantage of our method is that it provides a unified framework of solving L1 norm LRMF problems with or without missing data, and can be naturally extended to other matrix factorization tasks, such as nonnegative matrix factorization and sparse PCA. The extensive experimental results validate that our method outperforms state-of-the-arts in term of both computational time and accuracy, especially on large-scale applications such as face recognition and structure from motion."
Solving L1 Norm Matrix Factorization from the Elementary Unit by Coordinate Descent,"The L1 norm low-rank matrix factorization (LRMF) has been attracting much attention due to its intrinsic robustness to outliers and missing data. However, L1 norm LRMF is difficult to be resolved due to its non-convexity and non-smoothness. In this paper, we propose a novel solution, which is essentially a coordinate descent approach, to L1 norm LRMF. The main idea is to break the original difficult problem into its elementary sub-problems, each involving only one single scalar parameter, and recursively solve them. Each of these one-scalar sub-problems is convex and has a closed-form solution. The proposed method thus involves only simple computations and avoids any time-consuming inner loop numerical optimization. One important advantage of our method is that it provides a unified framework of solving L1 norm LRMF problems with or without missing data, and can be naturally extended to other matrix factorization tasks, such as nonnegative matrix factorization and sparse PCA. The extensive experimental results validate that our method outperforms state-of-the-arts in term of both computational time and accuracy, especially on large-scale applications such as face recognition and structure from motion."
Inverse of Lorentzian Mixture for Supervised Learning of Prototypes and Weights,"This paper presents a novel distance-based classifier based on the multiplicative inverse of Lorentzian mixture, which can be regarded as a natural extension of the conventional nearest neighbor rule. We show that prototypes and weights can be trained simultaneously by General Loss Minimization, which is a generalized version of supervised learning framework used in Generalized Learning Vector Quantization. Experimental results for UCI machine learning repository reveal that the proposed method achieves higher classification accuracy than Support Vector Machine with a much fewer prototypes than support vectors."
Inverse of Lorentzian Mixture for Supervised Learning of Prototypes and Weights,"This paper presents a novel distance-based classifier based on the multiplicative inverse of Lorentzian mixture, which can be regarded as a natural extension of the conventional nearest neighbor rule. We show that prototypes and weights can be trained simultaneously by General Loss Minimization, which is a generalized version of supervised learning framework used in Generalized Learning Vector Quantization. Experimental results for UCI machine learning repository reveal that the proposed method achieves higher classification accuracy than Support Vector Machine with a much fewer prototypes than support vectors."
Contour Detection using Sparse Code Gradients,"Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most existing approaches including the state-of-the-art Global Pb (gPb) operator.  In this work, we show that contour detection accuracy can be vastly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding.  We use K-SVD and Orthogonal Matching Pursuit for efficient dictionary learning and encoding, and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear SVM. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and find contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours).  Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth images and surface normals lead to promising contour detection using depth and depth+color, as verified on the NYU Depth Dataset.  Our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation."
Contour Detection using Sparse Code Gradients,"Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most existing approaches including the state-of-the-art Global Pb (gPb) operator.  In this work, we show that contour detection accuracy can be vastly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding.  We use K-SVD and Orthogonal Matching Pursuit for efficient dictionary learning and encoding, and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear SVM. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and find contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours).  Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth images and surface normals lead to promising contour detection using depth and depth+color, as verified on the NYU Depth Dataset.  Our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation."
Analyzing 3D Objects in Cluttered Images,"We present an approach to detecting and analyzing the 3D configuration of objects in real-world images with heavy occlusion and clutter. We focus on the application of finding and analyzing cars. We do so with a two-stage model; the first stage reasons about 2D shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then refined by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset."
Analyzing 3D Objects in Cluttered Images,"We present an approach to detecting and analyzing the 3D configuration of objects in real-world images with heavy occlusion and clutter. We focus on the application of finding and analyzing cars. We do so with a two-stage model; the first stage reasons about 2D shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then refined by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset."
"Nonconvex Penalization, Levy Processes and Concave Conjugates","In this paper we study sparsity-inducing nonconvex penalty functions using Levy processes. We define such a  penalty as the Laplace exponent of a subordinator. Accordingly, we propose a novel approach for the construction of sparsity-inducing nonconvex penalties. Particularly,  we show that the nonconvex logarithmic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionaly, we explore the concave conjugate of nonconvex penalties. We find that the LOG and EXP penalties are the concave conjugates of the negatives of Kullback-Leiber (KL) distance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance."
"Nonconvex Penalization, Levy Processes and Concave Conjugates","In this paper we study sparsity-inducing nonconvex penalty functions using Levy processes. We define such a  penalty as the Laplace exponent of a subordinator. Accordingly, we propose a novel approach for the construction of sparsity-inducing nonconvex penalties. Particularly,  we show that the nonconvex logarithmic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionaly, we explore the concave conjugate of nonconvex penalties. We find that the LOG and EXP penalties are the concave conjugates of the negatives of Kullback-Leiber (KL) distance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance."
3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model,"This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects  in 3D by enclosing them with tight  oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model[Felz.] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are  continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2D[Felz09] and 3D object detection[Hedau12]. "
3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model,"This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects  in 3D by enclosing them with tight  oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model[Felz.] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are  continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2D[Felz09] and 3D object detection[Hedau12]. "
3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model,"This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects  in 3D by enclosing them with tight  oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model[Felz.] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are  continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2D[Felz09] and 3D object detection[Hedau12]. "
Computing the M Most Probable Modes of a Graphical Model,"We introduce the M-modes problem for graphical models: predicting the M label configurations of highest probability that are at the same time local maxima of the probability landscape. M-modes have multiple possible applications: because they are intrinsically diverse, they provide a principled alternative to non-maximum suppression techniques for structured prediction, they can act as codebook vectors for quantizing the configuration space, or they can form component centers for mixture model approximation.We present two complementary algorithms for solving the M-modes problem on junction chains. When the underlying graphical model is a simple chain, their complexity is polynomial in all relevant quantities. On general junction chains algorithms do not come with worst-case guarantees, but show promising performance in our experiments. Besides the M-modes problem, our techniques are also applicable to a more general class of optimization problems."
Computing the M Most Probable Modes of a Graphical Model,"We introduce the M-modes problem for graphical models: predicting the M label configurations of highest probability that are at the same time local maxima of the probability landscape. M-modes have multiple possible applications: because they are intrinsically diverse, they provide a principled alternative to non-maximum suppression techniques for structured prediction, they can act as codebook vectors for quantizing the configuration space, or they can form component centers for mixture model approximation.We present two complementary algorithms for solving the M-modes problem on junction chains. When the underlying graphical model is a simple chain, their complexity is polynomial in all relevant quantities. On general junction chains algorithms do not come with worst-case guarantees, but show promising performance in our experiments. Besides the M-modes problem, our techniques are also applicable to a more general class of optimization problems."
Computing the M Most Probable Modes of a Graphical Model,"We introduce the M-modes problem for graphical models: predicting the M label configurations of highest probability that are at the same time local maxima of the probability landscape. M-modes have multiple possible applications: because they are intrinsically diverse, they provide a principled alternative to non-maximum suppression techniques for structured prediction, they can act as codebook vectors for quantizing the configuration space, or they can form component centers for mixture model approximation.We present two complementary algorithms for solving the M-modes problem on junction chains. When the underlying graphical model is a simple chain, their complexity is polynomial in all relevant quantities. On general junction chains algorithms do not come with worst-case guarantees, but show promising performance in our experiments. Besides the M-modes problem, our techniques are also applicable to a more general class of optimization problems."
Active MAP Inference in CRFs for Efficient Semantic Segmentation,"Most MAP inference algorithms for CRFs optimize an energy function knowing all the parameters of the potentials. In this note, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to \emph{on-the-fly} select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 and MSRC-21, show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains. "
Active MAP Inference in CRFs for Efficient Semantic Segmentation,"Most MAP inference algorithms for CRFs optimize an energy function knowing all the parameters of the potentials. In this note, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to \emph{on-the-fly} select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 and MSRC-21, show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains. "
Active MAP Inference in CRFs for Efficient Semantic Segmentation,"Most MAP inference algorithms for CRFs optimize an energy function knowing all the parameters of the potentials. In this note, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to \emph{on-the-fly} select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 and MSRC-21, show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains. "
Active MAP Inference in CRFs for Efficient Semantic Segmentation,"Most MAP inference algorithms for CRFs optimize an energy function knowing all the parameters of the potentials. In this note, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to \emph{on-the-fly} select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 and MSRC-21, show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains. "
Active MAP Inference in CRFs for Efficient Semantic Segmentation,"Most MAP inference algorithms for CRFs optimize an energy function knowing all the parameters of the potentials. In this note, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to \emph{on-the-fly} select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 and MSRC-21, show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains. "
Local Importance Weight for Transductive Classification,"This paper proposes a weighting scheme of training data named local importance weighting for transductive classification when training and test distributions are different. Most of the previous efforts to resolve the distribution difference rarely focus on matching joint distributions of input and output, but we show that it is important at least in classification tasks to adjust the difference of joint distributions. To match up the joint distribution of training data with that of test data. the proposed scheme estimates the joint distributions of both training and test data. Since the class labels of test data are unknown, an EM algorithm is used to estimate them and the parameters of the transductive classifier simultaneously. Through a series of experiments, we show that the transductive classifier with the proposed scheme outperforms that with the existing weighting method. "
Local Importance Weight for Transductive Classification,"This paper proposes a weighting scheme of training data named local importance weighting for transductive classification when training and test distributions are different. Most of the previous efforts to resolve the distribution difference rarely focus on matching joint distributions of input and output, but we show that it is important at least in classification tasks to adjust the difference of joint distributions. To match up the joint distribution of training data with that of test data. the proposed scheme estimates the joint distributions of both training and test data. Since the class labels of test data are unknown, an EM algorithm is used to estimate them and the parameters of the transductive classifier simultaneously. Through a series of experiments, we show that the transductive classifier with the proposed scheme outperforms that with the existing weighting method. "
Local Importance Weight for Transductive Classification,"This paper proposes a weighting scheme of training data named local importance weighting for transductive classification when training and test distributions are different. Most of the previous efforts to resolve the distribution difference rarely focus on matching joint distributions of input and output, but we show that it is important at least in classification tasks to adjust the difference of joint distributions. To match up the joint distribution of training data with that of test data. the proposed scheme estimates the joint distributions of both training and test data. Since the class labels of test data are unknown, an EM algorithm is used to estimate them and the parameters of the transductive classifier simultaneously. Through a series of experiments, we show that the transductive classifier with the proposed scheme outperforms that with the existing weighting method. "
Multi-Attribute Sparse Representation With Group Constraints,"A novel multi-attribute sparse representation enforced with group constraints is proposed in this paper. Data with multiple attributes can be represented by individual binary matrices to indicate the group properties for each data sample. Then, these attribute matrices are incorporated into the formulation of $l_1$-minimization. The solution is obtained by jointly considering the data reconstruction error, the sparsity property as well as the group constraints, thus making the basis selection in sparse coding more efficient in term of accuracy. The proposed optimization formulation with group constraints is simple yet very efficient for classification problems with multiple attributes. In addition, it can be derived into a modified sparse coding form so that any $l_1$-minimization solver can be employed in the corresponding optimization problem. We demonstrate the performance of the proposed multi-attribute sparse representation algorithm through experiments on face recognition with different kinds of variations. Experimental results show that the proposed method is very competitive compared to the state-of-the-art methods."
Multi-Attribute Sparse Representation With Group Constraints,"A novel multi-attribute sparse representation enforced with group constraints is proposed in this paper. Data with multiple attributes can be represented by individual binary matrices to indicate the group properties for each data sample. Then, these attribute matrices are incorporated into the formulation of $l_1$-minimization. The solution is obtained by jointly considering the data reconstruction error, the sparsity property as well as the group constraints, thus making the basis selection in sparse coding more efficient in term of accuracy. The proposed optimization formulation with group constraints is simple yet very efficient for classification problems with multiple attributes. In addition, it can be derived into a modified sparse coding form so that any $l_1$-minimization solver can be employed in the corresponding optimization problem. We demonstrate the performance of the proposed multi-attribute sparse representation algorithm through experiments on face recognition with different kinds of variations. Experimental results show that the proposed method is very competitive compared to the state-of-the-art methods."
Multi-Attribute Sparse Representation With Group Constraints,"A novel multi-attribute sparse representation enforced with group constraints is proposed in this paper. Data with multiple attributes can be represented by individual binary matrices to indicate the group properties for each data sample. Then, these attribute matrices are incorporated into the formulation of $l_1$-minimization. The solution is obtained by jointly considering the data reconstruction error, the sparsity property as well as the group constraints, thus making the basis selection in sparse coding more efficient in term of accuracy. The proposed optimization formulation with group constraints is simple yet very efficient for classification problems with multiple attributes. In addition, it can be derived into a modified sparse coding form so that any $l_1$-minimization solver can be employed in the corresponding optimization problem. We demonstrate the performance of the proposed multi-attribute sparse representation algorithm through experiments on face recognition with different kinds of variations. Experimental results show that the proposed method is very competitive compared to the state-of-the-art methods."
Multi-Attribute Sparse Representation With Group Constraints,"A novel multi-attribute sparse representation enforced with group constraints is proposed in this paper. Data with multiple attributes can be represented by individual binary matrices to indicate the group properties for each data sample. Then, these attribute matrices are incorporated into the formulation of $l_1$-minimization. The solution is obtained by jointly considering the data reconstruction error, the sparsity property as well as the group constraints, thus making the basis selection in sparse coding more efficient in term of accuracy. The proposed optimization formulation with group constraints is simple yet very efficient for classification problems with multiple attributes. In addition, it can be derived into a modified sparse coding form so that any $l_1$-minimization solver can be employed in the corresponding optimization problem. We demonstrate the performance of the proposed multi-attribute sparse representation algorithm through experiments on face recognition with different kinds of variations. Experimental results show that the proposed method is very competitive compared to the state-of-the-art methods."
A Polylog Pivot Steps Simplex Algorithm for Classification,"We present a simplex algorithm for linear programming in a linear classification formulation. The paramount complexity parameter in linear classification problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear. This is in contrast to general linear programming, for which no sub-polynomial pivot rule is known. "
A Polylog Pivot Steps Simplex Algorithm for Classification,"We present a simplex algorithm for linear programming in a linear classification formulation. The paramount complexity parameter in linear classification problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear. This is in contrast to general linear programming, for which no sub-polynomial pivot rule is known. "
Order Preserving Hashing for Approximate Nearest Neighbor Search,"In this paper, we study hash-based indexing techniques for approximate nearest neighbor (ANN) search. The principle of searching ANNs is that the points corresponding to the hash codes near that of the query are more similar to the query than those corresponding to the farther hash codes. Motivated by this point, we propose a novel hashing approach, called order preserving hashing, which learns the hash functions by maximizing the alignment between the similarity orders computed from the original space and the hamming space. We also impose the constraint that the points are as uniformly as possible distributed in the buckets. To this end, we formulate the problem of mapping the NN points for a query point into different hash codes as a classifier learning problem in which a nearest neighbor classifier is used based on the hamming distance over the hash codes, and find the hash functions from the aggregated classifier pooled over all the training points. To make the optimization feasible, we develop several techniques, including Sigmoid relaxation, stochastic gradient decent, and active set, to efficiently learn hash functions. Experimental results demonstrate the superiority of our approach over existing state-of-the-art techniques."
Order Preserving Hashing for Approximate Nearest Neighbor Search,"In this paper, we study hash-based indexing techniques for approximate nearest neighbor (ANN) search. The principle of searching ANNs is that the points corresponding to the hash codes near that of the query are more similar to the query than those corresponding to the farther hash codes. Motivated by this point, we propose a novel hashing approach, called order preserving hashing, which learns the hash functions by maximizing the alignment between the similarity orders computed from the original space and the hamming space. We also impose the constraint that the points are as uniformly as possible distributed in the buckets. To this end, we formulate the problem of mapping the NN points for a query point into different hash codes as a classifier learning problem in which a nearest neighbor classifier is used based on the hamming distance over the hash codes, and find the hash functions from the aggregated classifier pooled over all the training points. To make the optimization feasible, we develop several techniques, including Sigmoid relaxation, stochastic gradient decent, and active set, to efficiently learn hash functions. Experimental results demonstrate the superiority of our approach over existing state-of-the-art techniques."
Order Preserving Hashing for Approximate Nearest Neighbor Search,"In this paper, we study hash-based indexing techniques for approximate nearest neighbor (ANN) search. The principle of searching ANNs is that the points corresponding to the hash codes near that of the query are more similar to the query than those corresponding to the farther hash codes. Motivated by this point, we propose a novel hashing approach, called order preserving hashing, which learns the hash functions by maximizing the alignment between the similarity orders computed from the original space and the hamming space. We also impose the constraint that the points are as uniformly as possible distributed in the buckets. To this end, we formulate the problem of mapping the NN points for a query point into different hash codes as a classifier learning problem in which a nearest neighbor classifier is used based on the hamming distance over the hash codes, and find the hash functions from the aggregated classifier pooled over all the training points. To make the optimization feasible, we develop several techniques, including Sigmoid relaxation, stochastic gradient decent, and active set, to efficiently learn hash functions. Experimental results demonstrate the superiority of our approach over existing state-of-the-art techniques."
Order Preserving Hashing for Approximate Nearest Neighbor Search,"In this paper, we study hash-based indexing techniques for approximate nearest neighbor (ANN) search. The principle of searching ANNs is that the points corresponding to the hash codes near that of the query are more similar to the query than those corresponding to the farther hash codes. Motivated by this point, we propose a novel hashing approach, called order preserving hashing, which learns the hash functions by maximizing the alignment between the similarity orders computed from the original space and the hamming space. We also impose the constraint that the points are as uniformly as possible distributed in the buckets. To this end, we formulate the problem of mapping the NN points for a query point into different hash codes as a classifier learning problem in which a nearest neighbor classifier is used based on the hamming distance over the hash codes, and find the hash functions from the aggregated classifier pooled over all the training points. To make the optimization feasible, we develop several techniques, including Sigmoid relaxation, stochastic gradient decent, and active set, to efficiently learn hash functions. Experimental results demonstrate the superiority of our approach over existing state-of-the-art techniques."
Nonparametric Bayesian Microphone Array Processing,"Sound source localization and separation from a mixture of sounds are essential functions for computational auditory scene analysis.The main challenges are designing a unified framework for joint optimization and estimating the sound sources under auditory uncertainties such as reverberation or unknown number of sounds.Since sound source localization and separation are mutually dependent, their simultaneous estimation is required for better and more robust performance. A unified model is presented for sound source localization and separation that is based on a nonparametric Bayesian model.Experiments using simulated and recorded audio mixtures show that a method based on this model achieves state-of-the-art sound source separation quality and has more robust performance on the source number estimation under reverberant environments."
Nonparametric Bayesian Microphone Array Processing,"Sound source localization and separation from a mixture of sounds are essential functions for computational auditory scene analysis.The main challenges are designing a unified framework for joint optimization and estimating the sound sources under auditory uncertainties such as reverberation or unknown number of sounds.Since sound source localization and separation are mutually dependent, their simultaneous estimation is required for better and more robust performance. A unified model is presented for sound source localization and separation that is based on a nonparametric Bayesian model.Experiments using simulated and recorded audio mixtures show that a method based on this model achieves state-of-the-art sound source separation quality and has more robust performance on the source number estimation under reverberant environments."
Nonparametric Bayesian Microphone Array Processing,"Sound source localization and separation from a mixture of sounds are essential functions for computational auditory scene analysis.The main challenges are designing a unified framework for joint optimization and estimating the sound sources under auditory uncertainties such as reverberation or unknown number of sounds.Since sound source localization and separation are mutually dependent, their simultaneous estimation is required for better and more robust performance. A unified model is presented for sound source localization and separation that is based on a nonparametric Bayesian model.Experiments using simulated and recorded audio mixtures show that a method based on this model achieves state-of-the-art sound source separation quality and has more robust performance on the source number estimation under reverberant environments."
Nonparametric Bayesian Microphone Array Processing,"Sound source localization and separation from a mixture of sounds are essential functions for computational auditory scene analysis.The main challenges are designing a unified framework for joint optimization and estimating the sound sources under auditory uncertainties such as reverberation or unknown number of sounds.Since sound source localization and separation are mutually dependent, their simultaneous estimation is required for better and more robust performance. A unified model is presented for sound source localization and separation that is based on a nonparametric Bayesian model.Experiments using simulated and recorded audio mixtures show that a method based on this model achieves state-of-the-art sound source separation quality and has more robust performance on the source number estimation under reverberant environments."
Nonconvex Relaxation Approaches to Robust Matrix Recovery,"Motivated by the recent developments of nonconvex penalties in sparsity modeling, we propose a nonconvex optimization model for handing the low-rank matrix recovery problem.Different from the famous robust principal component analysis (RPCA), we suggest recovering low-rank and sparse matrices via a nonconvex loss function and a nonconvex penalty instead of the convex ones.The advantages of the nonconvex approach lie in its stronger robustness.To solve the model, we devise a majorization-minimization augmented Lagrange multiplier (MM-ALM) algorithm which finds the local optimal solutions of the proposed nonconvex model.We also provide two efficient strategies to speedup MM-ALM, which make the running time comparable with the state-of-the-art algorithms solving RPCA.Finally, the experimental results demonstrate the superiority of our nonconvex approach over RPCA in terms of matrix recovery accuracy."
Nonconvex Relaxation Approaches to Robust Matrix Recovery,"Motivated by the recent developments of nonconvex penalties in sparsity modeling, we propose a nonconvex optimization model for handing the low-rank matrix recovery problem.Different from the famous robust principal component analysis (RPCA), we suggest recovering low-rank and sparse matrices via a nonconvex loss function and a nonconvex penalty instead of the convex ones.The advantages of the nonconvex approach lie in its stronger robustness.To solve the model, we devise a majorization-minimization augmented Lagrange multiplier (MM-ALM) algorithm which finds the local optimal solutions of the proposed nonconvex model.We also provide two efficient strategies to speedup MM-ALM, which make the running time comparable with the state-of-the-art algorithms solving RPCA.Finally, the experimental results demonstrate the superiority of our nonconvex approach over RPCA in terms of matrix recovery accuracy."
Fast multiple-part based object detection using KD-Ferns,"Part-based models are currently considered state-of-the-art for object detection due to their ability to represent large appearance variations. Furthermore, using a large number of parts with diverse appearances can improve classification accuracy. The computational cost of the detection stage in existing methods has been a limitation for adopting such models in real-time applications. In addition, since the computation time grows linearly with the number of parts, most methods are limited to using several parts only. Our first contribution is the ``KD-Ferns'' algorithm for approximate nearest neighbor search which allows to compare each image location to only a subset of the model parts. This allows scaling the number of parts in the model. Our second contribution is a new algorithm for object detection which is an efficient variant of the ``Feature Synthesis'' (FS) method[1]. The FS uses multiple object parts for detection and is among state-of-the-art methods on human detection benchmarks but suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses learned classifiers with hundreds of parts in a coarse-to-fine strategy, achieving significantly faster object detection compared to existing part based methods. The AFS uses the ``KD-Ferns'' algorithm, and spatially sparse part locations, to reduce the required computation. We evaluate the AFS on the INRIA and Caltech pedestrian detection benchmarks. In this evaluation, the AFS is among the state-of-the-art methods in detection accuracy and is also the fastest method for close range pedestrians, reaching nearly 10 frames per-second on $640\times 480$ images. The AFS is to our best knowledge the first part-based object detection method capable of running in real-time."
Fast multiple-part based object detection using KD-Ferns,"Part-based models are currently considered state-of-the-art for object detection due to their ability to represent large appearance variations. Furthermore, using a large number of parts with diverse appearances can improve classification accuracy. The computational cost of the detection stage in existing methods has been a limitation for adopting such models in real-time applications. In addition, since the computation time grows linearly with the number of parts, most methods are limited to using several parts only. Our first contribution is the ``KD-Ferns'' algorithm for approximate nearest neighbor search which allows to compare each image location to only a subset of the model parts. This allows scaling the number of parts in the model. Our second contribution is a new algorithm for object detection which is an efficient variant of the ``Feature Synthesis'' (FS) method[1]. The FS uses multiple object parts for detection and is among state-of-the-art methods on human detection benchmarks but suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses learned classifiers with hundreds of parts in a coarse-to-fine strategy, achieving significantly faster object detection compared to existing part based methods. The AFS uses the ``KD-Ferns'' algorithm, and spatially sparse part locations, to reduce the required computation. We evaluate the AFS on the INRIA and Caltech pedestrian detection benchmarks. In this evaluation, the AFS is among the state-of-the-art methods in detection accuracy and is also the fastest method for close range pedestrians, reaching nearly 10 frames per-second on $640\times 480$ images. The AFS is to our best knowledge the first part-based object detection method capable of running in real-time."
Fast multiple-part based object detection using KD-Ferns,"Part-based models are currently considered state-of-the-art for object detection due to their ability to represent large appearance variations. Furthermore, using a large number of parts with diverse appearances can improve classification accuracy. The computational cost of the detection stage in existing methods has been a limitation for adopting such models in real-time applications. In addition, since the computation time grows linearly with the number of parts, most methods are limited to using several parts only. Our first contribution is the ``KD-Ferns'' algorithm for approximate nearest neighbor search which allows to compare each image location to only a subset of the model parts. This allows scaling the number of parts in the model. Our second contribution is a new algorithm for object detection which is an efficient variant of the ``Feature Synthesis'' (FS) method[1]. The FS uses multiple object parts for detection and is among state-of-the-art methods on human detection benchmarks but suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses learned classifiers with hundreds of parts in a coarse-to-fine strategy, achieving significantly faster object detection compared to existing part based methods. The AFS uses the ``KD-Ferns'' algorithm, and spatially sparse part locations, to reduce the required computation. We evaluate the AFS on the INRIA and Caltech pedestrian detection benchmarks. In this evaluation, the AFS is among the state-of-the-art methods in detection accuracy and is also the fastest method for close range pedestrians, reaching nearly 10 frames per-second on $640\times 480$ images. The AFS is to our best knowledge the first part-based object detection method capable of running in real-time."
Recursive Deep Learning on 3D Point Clouds,"Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a novel model based on sparse and recursive autoencoders (RAE) for learning both features and object categories from raw 3D point clouds as well as standard images. The model differs from previous RAE models in that it fixes the tree structures and includes short-circuit connections from all tree nodes to the final classifier. This allows the model to take into consideration both low-level features as well as global features of the object. Using our fully learned architecture, we achieve state of the art performance on a standard RGB-D object recognition dataset, rivaling random forest classifiers on hand-designed features such as SIFT and spin images. Our method is very fast and can classify 71 images in 1 second on a standard desktop machine in Matlab. This is possible because the method only requires 16 matrix multiplications to classify each image into one of 51 household objects."
Recursive Deep Learning on 3D Point Clouds,"Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a novel model based on sparse and recursive autoencoders (RAE) for learning both features and object categories from raw 3D point clouds as well as standard images. The model differs from previous RAE models in that it fixes the tree structures and includes short-circuit connections from all tree nodes to the final classifier. This allows the model to take into consideration both low-level features as well as global features of the object. Using our fully learned architecture, we achieve state of the art performance on a standard RGB-D object recognition dataset, rivaling random forest classifiers on hand-designed features such as SIFT and spin images. Our method is very fast and can classify 71 images in 1 second on a standard desktop machine in Matlab. This is possible because the method only requires 16 matrix multiplications to classify each image into one of 51 household objects."
Matching Human Actions in Videos,"Matching human motion in images and videos is one of the most fundamental problems in computer vision. In this paper, we attempt to extend the traditional appearance-similarity-based feature matching to action-similarity-based matching. The matching method we proposed is independent of appearances, camera views, and robust to the pace and speed of the specific human action. Our action similarity measure is based on a novel covariance descriptor for encoding the point motion trajectories of people. We encode each point trajectory into an optical flow covariance matrix, which is symmetry and positive definite (SPD). Using the metric of SPD matrix space, we can identify the similarity correspondence of any pairs of trajectories. We also introduce a subspace alignment algorithm, which allows us to perform dense matching between two trajectory set in a low-dimensional subspace. The resulting matching provides us a set of dense point-point correspondences between two people in videos, who are conducting the same action in the sampled video clips. The action-similarity-based human matching can provide a new view/appearance independent approach for video registration, human tracking, and action recognition."
Matching Human Actions in Videos,"Matching human motion in images and videos is one of the most fundamental problems in computer vision. In this paper, we attempt to extend the traditional appearance-similarity-based feature matching to action-similarity-based matching. The matching method we proposed is independent of appearances, camera views, and robust to the pace and speed of the specific human action. Our action similarity measure is based on a novel covariance descriptor for encoding the point motion trajectories of people. We encode each point trajectory into an optical flow covariance matrix, which is symmetry and positive definite (SPD). Using the metric of SPD matrix space, we can identify the similarity correspondence of any pairs of trajectories. We also introduce a subspace alignment algorithm, which allows us to perform dense matching between two trajectory set in a low-dimensional subspace. The resulting matching provides us a set of dense point-point correspondences between two people in videos, who are conducting the same action in the sampled video clips. The action-similarity-based human matching can provide a new view/appearance independent approach for video registration, human tracking, and action recognition."
Semi-Supervised Domain Adaptation with Non-parametric Copulas,"A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model across different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques."
Semi-Supervised Domain Adaptation with Non-parametric Copulas,"A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model across different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques."
Linked Tensor/Tucker Decomposition and its Application for Multi-block Group Tensor Analysis,"In this paper we propose a new algorithm for flexible group tensor analysis model called the linked tensor/Tucker decomposition (LTD).The LTD can decompose given multiple tensors into common factor matrices, individual factor matrices, and individual core tensors, simultaneously.The LTD provides a novel application called the linked multiway principal component analysis (LMPCA) by addition of orthogonal constraints.This method is derived by the alternating least squares (ALS) algorithm.Furthermore, we conducted experiments of this model for face reconstruction and denoising and EEG classification to demonstrate its advantages."
Linked Tensor/Tucker Decomposition and its Application for Multi-block Group Tensor Analysis,"In this paper we propose a new algorithm for flexible group tensor analysis model called the linked tensor/Tucker decomposition (LTD).The LTD can decompose given multiple tensors into common factor matrices, individual factor matrices, and individual core tensors, simultaneously.The LTD provides a novel application called the linked multiway principal component analysis (LMPCA) by addition of orthogonal constraints.This method is derived by the alternating least squares (ALS) algorithm.Furthermore, we conducted experiments of this model for face reconstruction and denoising and EEG classification to demonstrate its advantages."
Linked Tensor/Tucker Decomposition and its Application for Multi-block Group Tensor Analysis,"In this paper we propose a new algorithm for flexible group tensor analysis model called the linked tensor/Tucker decomposition (LTD).The LTD can decompose given multiple tensors into common factor matrices, individual factor matrices, and individual core tensors, simultaneously.The LTD provides a novel application called the linked multiway principal component analysis (LMPCA) by addition of orthogonal constraints.This method is derived by the alternating least squares (ALS) algorithm.Furthermore, we conducted experiments of this model for face reconstruction and denoising and EEG classification to demonstrate its advantages."
Gated Local Metric Nearest Neighbor Classi?cation,The large margin nearest neighbor (LMNN) algorithm learns a global metric to improve the accuracy of the k-nearest neighbor classi?er. We propose a novel variant with a gating function that divides up the input space into regions and learns a localized distance metric in each using a low-rank approximation. The proposed method is experimented over real data sets and compared with LMNN with good results in terms of higher accuracy and better visualization.
Gated Local Metric Nearest Neighbor Classi?cation,The large margin nearest neighbor (LMNN) algorithm learns a global metric to improve the accuracy of the k-nearest neighbor classi?er. We propose a novel variant with a gating function that divides up the input space into regions and learns a localized distance metric in each using a low-rank approximation. The proposed method is experimented over real data sets and compared with LMNN with good results in terms of higher accuracy and better visualization.
On Detecting Multiple Simultaneous Change-points in High Dimensional Time Series,"This paper studies the detection of multiple simultaneous (systematic) change points for high-dimensional nonstantionary time series data. The analytic framework used is based on the standard and adaptive fused group lasso method, where the mixed $L_{2,1}$ norms in the penalty are either uniform or re-weighted by data-dependent weights. This paper shows that, under appropriate conditions, this approach is $L_2$ consistent and, by adopting the data-dependent weights, could correctly select the change points with probability approaching unity. It quantifies the conditions on the interplay among the averaged (over different dimensions) minimum magnitude of structural changes, the number of change points and the number of observations for consistently discovering the change points. The performance of this approach is illustrated via an analysis of a large panel of U.S. economic and financial time series data over the past $50$ years. "
On Detecting Multiple Simultaneous Change-points in High Dimensional Time Series,"This paper studies the detection of multiple simultaneous (systematic) change points for high-dimensional nonstantionary time series data. The analytic framework used is based on the standard and adaptive fused group lasso method, where the mixed $L_{2,1}$ norms in the penalty are either uniform or re-weighted by data-dependent weights. This paper shows that, under appropriate conditions, this approach is $L_2$ consistent and, by adopting the data-dependent weights, could correctly select the change points with probability approaching unity. It quantifies the conditions on the interplay among the averaged (over different dimensions) minimum magnitude of structural changes, the number of change points and the number of observations for consistently discovering the change points. The performance of this approach is illustrated via an analysis of a large panel of U.S. economic and financial time series data over the past $50$ years. "
k-Nearest Neighbor Transform,"In this paper, we proposes the exact Euclidean k-nearest neighbor transform (kNNT) for binary images.Similar to Distance Transform (DT) finding the closest feature point for each point of an image, kNNT assigns each point in the binary image with its k-nearest feature points.Therefore, kNNT extends the space of DT which is just the extreme case of kNNT when k=1.We propose a linear time algorithm to compute kNNT in O(Nk^2) time with O(Nk) space where N is the size of the image.The query method to answer the kNN query is to find the result in a lookup table in time O(k).The KNNT is suitable for an extremely large number of queries, and thus an efficient method for many applications in which the distance transform is applied."
Gossip-based distributed stochastic bandit algorithms,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called \emph{exploitation-exploration dilemma} in various bandit setups. But significantly less effort has been spent on adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines or peer-to-peer (P2P) environment, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms are available in each peer. In every iteration each peer can pull one arm independently, and then some limited communication is possible with a few random other peers.  The algorithm we chose to adapt is the \Algo{$\epsilon$-GREEDY} policy that was shown by Auer et. al.~\cite{AuCeFi02} to play a suboptimal arm with probability $\bigO(1/t)$ in iteration $t$, and thereby to achieve a regret of $\bigO(\log t)$. This is optimal up to some constant factors~\cite{LaRo85}. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability that a suboptimal arm is played at a peer in iteration $t = \Omega( \log N )$ is $\bigO(1/(Nt))$, where $N$ denotes the number of peers. This result, as mentioned above, means that we achieve the best speedup that is theoretically possible~\cite{LaRo85}. The restriction $t = \Omega(\log N)$ is unavoidable in adapting such algorithms to P2P networks.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
Gossip-based distributed stochastic bandit algorithms,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called \emph{exploitation-exploration dilemma} in various bandit setups. But significantly less effort has been spent on adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines or peer-to-peer (P2P) environment, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms are available in each peer. In every iteration each peer can pull one arm independently, and then some limited communication is possible with a few random other peers.  The algorithm we chose to adapt is the \Algo{$\epsilon$-GREEDY} policy that was shown by Auer et. al.~\cite{AuCeFi02} to play a suboptimal arm with probability $\bigO(1/t)$ in iteration $t$, and thereby to achieve a regret of $\bigO(\log t)$. This is optimal up to some constant factors~\cite{LaRo85}. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability that a suboptimal arm is played at a peer in iteration $t = \Omega( \log N )$ is $\bigO(1/(Nt))$, where $N$ denotes the number of peers. This result, as mentioned above, means that we achieve the best speedup that is theoretically possible~\cite{LaRo85}. The restriction $t = \Omega(\log N)$ is unavoidable in adapting such algorithms to P2P networks.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
Gossip-based distributed stochastic bandit algorithms,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called \emph{exploitation-exploration dilemma} in various bandit setups. But significantly less effort has been spent on adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines or peer-to-peer (P2P) environment, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms are available in each peer. In every iteration each peer can pull one arm independently, and then some limited communication is possible with a few random other peers.  The algorithm we chose to adapt is the \Algo{$\epsilon$-GREEDY} policy that was shown by Auer et. al.~\cite{AuCeFi02} to play a suboptimal arm with probability $\bigO(1/t)$ in iteration $t$, and thereby to achieve a regret of $\bigO(\log t)$. This is optimal up to some constant factors~\cite{LaRo85}. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability that a suboptimal arm is played at a peer in iteration $t = \Omega( \log N )$ is $\bigO(1/(Nt))$, where $N$ denotes the number of peers. This result, as mentioned above, means that we achieve the best speedup that is theoretically possible~\cite{LaRo85}. The restriction $t = \Omega(\log N)$ is unavoidable in adapting such algorithms to P2P networks.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
Gossip-based distributed stochastic bandit algorithms,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called \emph{exploitation-exploration dilemma} in various bandit setups. But significantly less effort has been spent on adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines or peer-to-peer (P2P) environment, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms are available in each peer. In every iteration each peer can pull one arm independently, and then some limited communication is possible with a few random other peers.  The algorithm we chose to adapt is the \Algo{$\epsilon$-GREEDY} policy that was shown by Auer et. al.~\cite{AuCeFi02} to play a suboptimal arm with probability $\bigO(1/t)$ in iteration $t$, and thereby to achieve a regret of $\bigO(\log t)$. This is optimal up to some constant factors~\cite{LaRo85}. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability that a suboptimal arm is played at a peer in iteration $t = \Omega( \log N )$ is $\bigO(1/(Nt))$, where $N$ denotes the number of peers. This result, as mentioned above, means that we achieve the best speedup that is theoretically possible~\cite{LaRo85}. The restriction $t = \Omega(\log N)$ is unavoidable in adapting such algorithms to P2P networks.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
Gossip-based distributed stochastic bandit algorithms,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called \emph{exploitation-exploration dilemma} in various bandit setups. But significantly less effort has been spent on adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines or peer-to-peer (P2P) environment, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms are available in each peer. In every iteration each peer can pull one arm independently, and then some limited communication is possible with a few random other peers.  The algorithm we chose to adapt is the \Algo{$\epsilon$-GREEDY} policy that was shown by Auer et. al.~\cite{AuCeFi02} to play a suboptimal arm with probability $\bigO(1/t)$ in iteration $t$, and thereby to achieve a regret of $\bigO(\log t)$. This is optimal up to some constant factors~\cite{LaRo85}. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability that a suboptimal arm is played at a peer in iteration $t = \Omega( \log N )$ is $\bigO(1/(Nt))$, where $N$ denotes the number of peers. This result, as mentioned above, means that we achieve the best speedup that is theoretically possible~\cite{LaRo85}. The restriction $t = \Omega(\log N)$ is unavoidable in adapting such algorithms to P2P networks.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
Gossip-based distributed stochastic bandit algorithms,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called \emph{exploitation-exploration dilemma} in various bandit setups. But significantly less effort has been spent on adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines or peer-to-peer (P2P) environment, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms are available in each peer. In every iteration each peer can pull one arm independently, and then some limited communication is possible with a few random other peers.  The algorithm we chose to adapt is the \Algo{$\epsilon$-GREEDY} policy that was shown by Auer et. al.~\cite{AuCeFi02} to play a suboptimal arm with probability $\bigO(1/t)$ in iteration $t$, and thereby to achieve a regret of $\bigO(\log t)$. This is optimal up to some constant factors~\cite{LaRo85}. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability that a suboptimal arm is played at a peer in iteration $t = \Omega( \log N )$ is $\bigO(1/(Nt))$, where $N$ denotes the number of peers. This result, as mentioned above, means that we achieve the best speedup that is theoretically possible~\cite{LaRo85}. The restriction $t = \Omega(\log N)$ is unavoidable in adapting such algorithms to P2P networks.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
Learning Modewise Independent Components from Tensor Data via Multilinear Mixing Model,"Independent component analysis (ICA) is a popular unsupervised learning method. This paper extends it to multilinear modewise ICA (MMICA) for tensors and explores two architectures in learning and recognition. MMICA models tensor data as mixtures generated from modewise source matrices that encode statistically independent information. It operates on much lower dimension than ICA and its compact representations require much fewer parameters to estimate. We embed ICA into the multilinear principal component analysis framework to solve for each source matrix alternatively with a few iterations. Then we obtain mixing tensors through regularized inverses of the source matrices. Simulations on synthetic data demonstrate that MMICA can estimate hidden sources from structured tensor data. Moreover, in face recognition experiments, it outperforms competing solutions, while being particularly effective with Architecture II due to sparser and more structured bases."
Learning Modewise Independent Components from Tensor Data via Multilinear Mixing Model,"Independent component analysis (ICA) is a popular unsupervised learning method. This paper extends it to multilinear modewise ICA (MMICA) for tensors and explores two architectures in learning and recognition. MMICA models tensor data as mixtures generated from modewise source matrices that encode statistically independent information. It operates on much lower dimension than ICA and its compact representations require much fewer parameters to estimate. We embed ICA into the multilinear principal component analysis framework to solve for each source matrix alternatively with a few iterations. Then we obtain mixing tensors through regularized inverses of the source matrices. Simulations on synthetic data demonstrate that MMICA can estimate hidden sources from structured tensor data. Moreover, in face recognition experiments, it outperforms competing solutions, while being particularly effective with Architecture II due to sparser and more structured bases."
Identification of Recurrent Patterns in the Activation of Brain Networks,"Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks."
Identification of Recurrent Patterns in the Activation of Brain Networks,"Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks."
Identification of Recurrent Patterns in the Activation of Brain Networks,"Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks."
Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning,"Learning from experience how to manipulate an environment in a goal-directedmanner is one of the central challenges in research on autonomous agents. Inthe case of object manipulation, efficient learning and planning should exploit theunderlying relational structure of manipulation problems and combine geometricstate descriptions with abstract symbolic representations. When appropriate sym-bols are not predefined they need to be learned from geometric data. In this paperwe present an approach for learning symbolic relational abstractions of geometricfeatures such that these symbols enable to learn abstract transition models and touse them for goal-directed planning of motor primitive sequences. This is framedas an optimization problem, where a loss function evaluates how predictive thelearned symbols are for the effects of motor primitives as well as for reward. Theapproach is embedded in a full-fledged symbolic relational model-based reinforce-ment learning setting, where both the symbols as well as the abstract transitionand reward models are learned from experience. We quantitatively compare theapproach to simpler baselines in an object manipulation task and demonstrate iton a real-world robot."
Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning,"Learning from experience how to manipulate an environment in a goal-directedmanner is one of the central challenges in research on autonomous agents. Inthe case of object manipulation, efficient learning and planning should exploit theunderlying relational structure of manipulation problems and combine geometricstate descriptions with abstract symbolic representations. When appropriate sym-bols are not predefined they need to be learned from geometric data. In this paperwe present an approach for learning symbolic relational abstractions of geometricfeatures such that these symbols enable to learn abstract transition models and touse them for goal-directed planning of motor primitive sequences. This is framedas an optimization problem, where a loss function evaluates how predictive thelearned symbols are for the effects of motor primitives as well as for reward. Theapproach is embedded in a full-fledged symbolic relational model-based reinforce-ment learning setting, where both the symbols as well as the abstract transitionand reward models are learned from experience. We quantitatively compare theapproach to simpler baselines in an object manipulation task and demonstrate iton a real-world robot."
Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning,"Learning from experience how to manipulate an environment in a goal-directedmanner is one of the central challenges in research on autonomous agents. Inthe case of object manipulation, efficient learning and planning should exploit theunderlying relational structure of manipulation problems and combine geometricstate descriptions with abstract symbolic representations. When appropriate sym-bols are not predefined they need to be learned from geometric data. In this paperwe present an approach for learning symbolic relational abstractions of geometricfeatures such that these symbols enable to learn abstract transition models and touse them for goal-directed planning of motor primitive sequences. This is framedas an optimization problem, where a loss function evaluates how predictive thelearned symbols are for the effects of motor primitives as well as for reward. Theapproach is embedded in a full-fledged symbolic relational model-based reinforce-ment learning setting, where both the symbols as well as the abstract transitionand reward models are learned from experience. We quantitatively compare theapproach to simpler baselines in an object manipulation task and demonstrate iton a real-world robot."
Variational EM for Sound-Source Separation and Localization with Probabilistic Piecewise Affine Mapping,"We address the problem of sound-source separation and localization in real-world conditions with just two microphones. Separation and localization tasks are solved within a unified formulation that uses a probabilistic piecewise affine mapping. While the parameters of the direct mapping are learned during a training stage that uses sources emitting white noise, the inverse mapping is estimated using a variational EM formulation. The proposed algorithm can deal with reverberations and natural sound sources such as speech data which are known to yield sparse spectrograms, and is able to locate multiple sources both in azimuth and in elevation. Extensive experiments with real data show that the method compares positively with several other recently proposed methods."
Variational EM for Sound-Source Separation and Localization with Probabilistic Piecewise Affine Mapping,"We address the problem of sound-source separation and localization in real-world conditions with just two microphones. Separation and localization tasks are solved within a unified formulation that uses a probabilistic piecewise affine mapping. While the parameters of the direct mapping are learned during a training stage that uses sources emitting white noise, the inverse mapping is estimated using a variational EM formulation. The proposed algorithm can deal with reverberations and natural sound sources such as speech data which are known to yield sparse spectrograms, and is able to locate multiple sources both in azimuth and in elevation. Extensive experiments with real data show that the method compares positively with several other recently proposed methods."
Variational EM for Sound-Source Separation and Localization with Probabilistic Piecewise Affine Mapping,"We address the problem of sound-source separation and localization in real-world conditions with just two microphones. Separation and localization tasks are solved within a unified formulation that uses a probabilistic piecewise affine mapping. While the parameters of the direct mapping are learned during a training stage that uses sources emitting white noise, the inverse mapping is estimated using a variational EM formulation. The proposed algorithm can deal with reverberations and natural sound sources such as speech data which are known to yield sparse spectrograms, and is able to locate multiple sources both in azimuth and in elevation. Extensive experiments with real data show that the method compares positively with several other recently proposed methods."
Spike-Timing-Dependent Plasticity and Mutual Information Maximization for a Hetero-Associative Memory Model,"We explored optimal neural implementations for executing a computation in the hippocampal CA1 network and interpreted the computational roles of spike-timing-dependent plasticity (STDP). We propose an approach consisting of bottom-up and top-down steps in order to clarify physical limits to computations in the neural implementation. In the bottom-up step, hypothesizing that the hippocampal CA1 network computationally works as the hetero-associative memory, we formulate a minimum model of temporal memory functions as a phase reduction model composed of a STDP window function and a phase response curve (PRC). Next, we analytically derive a macroscopic reliability index of memory functions defined as the mutual information between a stored phase memory pattern and a network output. In the top-down step, by maximizing the mutual information, we derive pairs of STDP window functions and PRCs optimally functioning as a hetero-associative memory. We use PRCs of the hippocampal CA1 pyramidal neurons recorded in vitro, and under the constraint of measured PRCs, we search for a set of optimal STDP window functions. The theoretically derived set of these window functions qualitatively conforms to the various STDPs reported previously."
Spike-Timing-Dependent Plasticity and Mutual Information Maximization for a Hetero-Associative Memory Model,"We explored optimal neural implementations for executing a computation in the hippocampal CA1 network and interpreted the computational roles of spike-timing-dependent plasticity (STDP). We propose an approach consisting of bottom-up and top-down steps in order to clarify physical limits to computations in the neural implementation. In the bottom-up step, hypothesizing that the hippocampal CA1 network computationally works as the hetero-associative memory, we formulate a minimum model of temporal memory functions as a phase reduction model composed of a STDP window function and a phase response curve (PRC). Next, we analytically derive a macroscopic reliability index of memory functions defined as the mutual information between a stored phase memory pattern and a network output. In the top-down step, by maximizing the mutual information, we derive pairs of STDP window functions and PRCs optimally functioning as a hetero-associative memory. We use PRCs of the hippocampal CA1 pyramidal neurons recorded in vitro, and under the constraint of measured PRCs, we search for a set of optimal STDP window functions. The theoretically derived set of these window functions qualitatively conforms to the various STDPs reported previously."
Spike-Timing-Dependent Plasticity and Mutual Information Maximization for a Hetero-Associative Memory Model,"We explored optimal neural implementations for executing a computation in the hippocampal CA1 network and interpreted the computational roles of spike-timing-dependent plasticity (STDP). We propose an approach consisting of bottom-up and top-down steps in order to clarify physical limits to computations in the neural implementation. In the bottom-up step, hypothesizing that the hippocampal CA1 network computationally works as the hetero-associative memory, we formulate a minimum model of temporal memory functions as a phase reduction model composed of a STDP window function and a phase response curve (PRC). Next, we analytically derive a macroscopic reliability index of memory functions defined as the mutual information between a stored phase memory pattern and a network output. In the top-down step, by maximizing the mutual information, we derive pairs of STDP window functions and PRCs optimally functioning as a hetero-associative memory. We use PRCs of the hippocampal CA1 pyramidal neurons recorded in vitro, and under the constraint of measured PRCs, we search for a set of optimal STDP window functions. The theoretically derived set of these window functions qualitatively conforms to the various STDPs reported previously."
Learning Hamming Clique Potentials for Higher Order CRFs,"This paper investigates Conditional Random Fields (CRF) clique potentials that are invariant to permutations of label variables within the clique. We propose a general formulation for potential functions of this kind in terms of the Hamming distance from an absolute labeling ? an assignment of all clique variables to the same class. We present an analytical treatment as to which constituent functions of the proposed clique cost facilitate learning them from data through Likelihood Maximization and describe how to learn a non-parametric form of it. Experimental results demonstrate i) how some of the existing higher order clique costs can be either rewritten or approximated by the proposed formulation, and, ii) the advantage of learning the potentials, both for pairwise and higher order cliques, fortwo different applications."
Learning Hamming Clique Potentials for Higher Order CRFs,"This paper investigates Conditional Random Fields (CRF) clique potentials that are invariant to permutations of label variables within the clique. We propose a general formulation for potential functions of this kind in terms of the Hamming distance from an absolute labeling ? an assignment of all clique variables to the same class. We present an analytical treatment as to which constituent functions of the proposed clique cost facilitate learning them from data through Likelihood Maximization and describe how to learn a non-parametric form of it. Experimental results demonstrate i) how some of the existing higher order clique costs can be either rewritten or approximated by the proposed formulation, and, ii) the advantage of learning the potentials, both for pairwise and higher order cliques, fortwo different applications."
Learning Hamming Clique Potentials for Higher Order CRFs,"This paper investigates Conditional Random Fields (CRF) clique potentials that are invariant to permutations of label variables within the clique. We propose a general formulation for potential functions of this kind in terms of the Hamming distance from an absolute labeling ? an assignment of all clique variables to the same class. We present an analytical treatment as to which constituent functions of the proposed clique cost facilitate learning them from data through Likelihood Maximization and describe how to learn a non-parametric form of it. Experimental results demonstrate i) how some of the existing higher order clique costs can be either rewritten or approximated by the proposed formulation, and, ii) the advantage of learning the potentials, both for pairwise and higher order cliques, fortwo different applications."
Tangent Bundle Manifold Learning via Grassmannian eigenmaps,"The goal of Manifold Learning (ML) is, given a set of data points sampled from an unknown nonlinear low-dimensional Data manifold embedded in a high-dimensional input space, to construct an Empirical manifold approximating the Data manifold. We propose an extension of ML, called Tangent Bundle ML (TBML), in which proximity not only between the Data and Empirical manifolds but also between their tangent spaces is required. We present a new algorithm, Grassmannian Eigenmaps, that solves TBML problems and also gives new solution for the ML."
Tangent Bundle Manifold Learning via Grassmannian eigenmaps,"The goal of Manifold Learning (ML) is, given a set of data points sampled from an unknown nonlinear low-dimensional Data manifold embedded in a high-dimensional input space, to construct an Empirical manifold approximating the Data manifold. We propose an extension of ML, called Tangent Bundle ML (TBML), in which proximity not only between the Data and Empirical manifolds but also between their tangent spaces is required. We present a new algorithm, Grassmannian Eigenmaps, that solves TBML problems and also gives new solution for the ML."
Density-Difference Estimation,"We address the problem of estimating the difference betweentwo probability densities.A naive approach is a two-step procedure of first estimating two densities separatelyand then computing their difference.However, such a two-step procedure does not necessarily work wellbecause the first step is performed without regard to the second stepand thus a small error incurred in the first stage can cause a big error in the second stage.In this paper, we propose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities.We derive a non-parametric finite-sample error boundfor the proposed single-shot density-difference estimatorand show that it achieves the optimal convergence rate.We experimentally demonstrate the usefulness of the proposed methodin class-prior estimation and change-point detection."
Density-Difference Estimation,"We address the problem of estimating the difference betweentwo probability densities.A naive approach is a two-step procedure of first estimating two densities separatelyand then computing their difference.However, such a two-step procedure does not necessarily work wellbecause the first step is performed without regard to the second stepand thus a small error incurred in the first stage can cause a big error in the second stage.In this paper, we propose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities.We derive a non-parametric finite-sample error boundfor the proposed single-shot density-difference estimatorand show that it achieves the optimal convergence rate.We experimentally demonstrate the usefulness of the proposed methodin class-prior estimation and change-point detection."
Density-Difference Estimation,"We address the problem of estimating the difference betweentwo probability densities.A naive approach is a two-step procedure of first estimating two densities separatelyand then computing their difference.However, such a two-step procedure does not necessarily work wellbecause the first step is performed without regard to the second stepand thus a small error incurred in the first stage can cause a big error in the second stage.In this paper, we propose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities.We derive a non-parametric finite-sample error boundfor the proposed single-shot density-difference estimatorand show that it achieves the optimal convergence rate.We experimentally demonstrate the usefulness of the proposed methodin class-prior estimation and change-point detection."
Density-Difference Estimation,"We address the problem of estimating the difference betweentwo probability densities.A naive approach is a two-step procedure of first estimating two densities separatelyand then computing their difference.However, such a two-step procedure does not necessarily work wellbecause the first step is performed without regard to the second stepand thus a small error incurred in the first stage can cause a big error in the second stage.In this paper, we propose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities.We derive a non-parametric finite-sample error boundfor the proposed single-shot density-difference estimatorand show that it achieves the optimal convergence rate.We experimentally demonstrate the usefulness of the proposed methodin class-prior estimation and change-point detection."
Sharp analysis of low-rank kernel matrix approximations,"We consider supervised learning problems within the positive-definite kernel framework, such as kernel ridge regression, kernel logistic regression or the support vector machine. With  kernels leading to infinite-dimensional feature spaces, a common practical limiting difficulty is the necessity of computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic in the number of observations n, i.e., O(n^2). Low-rank approximations of the kernel matrix are often considered as they allow the reduction of  running time complexities  to O(p^2 n), where p is the rank of the approximation. The practicality of such methods thus depends on the required rank p. In this paper, we show that for approximations based on a random subset of columns of the original kernel matrix, the rank p may be chosen to be linear in the degrees of freedom associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have sub-quadratic running time complexity, but provably exhibit the same predictive performance than existing algorithms."
Online Learning via Optimizing the Variational Inequalities,"A wide variety of learning problems can be posed in the framework of convex optimization. Many efficient algorithms have been developed based on solving the induced optimization problems. However, there exists a gap between the theoretically unbeatable convergence rate and practically efficient learning speed. In this paper, we cast the regularized learning problem as a variational inequality (VI). Then, we solve the induced VI using the alternating direction method of multipliers (ADMM) in an online setting. For general convex problems, this new formulation enables our stochastic ADMM to achieve an $O(1/t)$ VI-convergence rate. The experiments demonstrate that the stochastic ADMM has almost the same performance as the state-of-the-art online algorithms but its $O(1/t)$ VI-convergence rate is capable of tightly characterizing the real learning speed."
Online Learning via Optimizing the Variational Inequalities,"A wide variety of learning problems can be posed in the framework of convex optimization. Many efficient algorithms have been developed based on solving the induced optimization problems. However, there exists a gap between the theoretically unbeatable convergence rate and practically efficient learning speed. In this paper, we cast the regularized learning problem as a variational inequality (VI). Then, we solve the induced VI using the alternating direction method of multipliers (ADMM) in an online setting. For general convex problems, this new formulation enables our stochastic ADMM to achieve an $O(1/t)$ VI-convergence rate. The experiments demonstrate that the stochastic ADMM has almost the same performance as the state-of-the-art online algorithms but its $O(1/t)$ VI-convergence rate is capable of tightly characterizing the real learning speed."
Online Learned Discriminative Hidden Structural Part Model for Visual Tracking,"We present a discriminative hidden structural part-based approach for visual trackingwithout any prior assumptions about the target and scenarios. Unlike otherweak-constrained or manual labeling part generation strategy in the previous partbasedtrackers, the state (e.g. position, width and height) of each part is consideredas the latent variable in our model and is inferred automatically online withthe dual objective functions. The two objective functions respectively encode theappearance variations of the target and separate the target from the backgroundwith the max-margin. Specifically, the constraints between parts are integratedin graph model through the dynamically constructed pair-wise Markov RandomField (MRF). The part-based Support Vector Machine (SVM) and Reverse JumpMarkov Chain Monte Carlo (RJMCMC) algorithm is adopted to complete thelatent variable inference task. The experimental results on various challengingdatabase demonstrate the learned part-based tracker outperforms other state-ofthe-art trackers (both bounding-box-based as well as part-based)."
Online Learned Discriminative Hidden Structural Part Model for Visual Tracking,"We present a discriminative hidden structural part-based approach for visual trackingwithout any prior assumptions about the target and scenarios. Unlike otherweak-constrained or manual labeling part generation strategy in the previous partbasedtrackers, the state (e.g. position, width and height) of each part is consideredas the latent variable in our model and is inferred automatically online withthe dual objective functions. The two objective functions respectively encode theappearance variations of the target and separate the target from the backgroundwith the max-margin. Specifically, the constraints between parts are integratedin graph model through the dynamically constructed pair-wise Markov RandomField (MRF). The part-based Support Vector Machine (SVM) and Reverse JumpMarkov Chain Monte Carlo (RJMCMC) algorithm is adopted to complete thelatent variable inference task. The experimental results on various challengingdatabase demonstrate the learned part-based tracker outperforms other state-ofthe-art trackers (both bounding-box-based as well as part-based)."
Online Learned Discriminative Hidden Structural Part Model for Visual Tracking,"We present a discriminative hidden structural part-based approach for visual trackingwithout any prior assumptions about the target and scenarios. Unlike otherweak-constrained or manual labeling part generation strategy in the previous partbasedtrackers, the state (e.g. position, width and height) of each part is consideredas the latent variable in our model and is inferred automatically online withthe dual objective functions. The two objective functions respectively encode theappearance variations of the target and separate the target from the backgroundwith the max-margin. Specifically, the constraints between parts are integratedin graph model through the dynamically constructed pair-wise Markov RandomField (MRF). The part-based Support Vector Machine (SVM) and Reverse JumpMarkov Chain Monte Carlo (RJMCMC) algorithm is adopted to complete thelatent variable inference task. The experimental results on various challengingdatabase demonstrate the learned part-based tracker outperforms other state-ofthe-art trackers (both bounding-box-based as well as part-based)."
Online Learned Discriminative Hidden Structural Part Model for Visual Tracking,"We present a discriminative hidden structural part-based approach for visual trackingwithout any prior assumptions about the target and scenarios. Unlike otherweak-constrained or manual labeling part generation strategy in the previous partbasedtrackers, the state (e.g. position, width and height) of each part is consideredas the latent variable in our model and is inferred automatically online withthe dual objective functions. The two objective functions respectively encode theappearance variations of the target and separate the target from the backgroundwith the max-margin. Specifically, the constraints between parts are integratedin graph model through the dynamically constructed pair-wise Markov RandomField (MRF). The part-based Support Vector Machine (SVM) and Reverse JumpMarkov Chain Monte Carlo (RJMCMC) algorithm is adopted to complete thelatent variable inference task. The experimental results on various challengingdatabase demonstrate the learned part-based tracker outperforms other state-ofthe-art trackers (both bounding-box-based as well as part-based)."
Online Learned Discriminative Hidden Structural Part Model for Visual Tracking,"We present a discriminative hidden structural part-based approach for visual trackingwithout any prior assumptions about the target and scenarios. Unlike otherweak-constrained or manual labeling part generation strategy in the previous partbasedtrackers, the state (e.g. position, width and height) of each part is consideredas the latent variable in our model and is inferred automatically online withthe dual objective functions. The two objective functions respectively encode theappearance variations of the target and separate the target from the backgroundwith the max-margin. Specifically, the constraints between parts are integratedin graph model through the dynamically constructed pair-wise Markov RandomField (MRF). The part-based Support Vector Machine (SVM) and Reverse JumpMarkov Chain Monte Carlo (RJMCMC) algorithm is adopted to complete thelatent variable inference task. The experimental results on various challengingdatabase demonstrate the learned part-based tracker outperforms other state-ofthe-art trackers (both bounding-box-based as well as part-based)."
Variational Inference for Crowdsourcing,"Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al, while our MF method is closely related to a commonly used EM algorithm. In both case, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state of art algorithms based on more complicated modeling assumptions."
Variational Inference for Crowdsourcing,"Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al, while our MF method is closely related to a commonly used EM algorithm. In both case, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state of art algorithms based on more complicated modeling assumptions."
Variational Inference for Crowdsourcing,"Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al, while our MF method is closely related to a commonly used EM algorithm. In both case, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state of art algorithms based on more complicated modeling assumptions."
VLSI Implementation of a Coupled MRF Model Using Pulse-coupled Phase Oscillators,"This paper proposes efficient pixel-parallel image processing using a pulse-coupled phase oscillator model and its VLSI implementation. A processing unit which corresponds to a pixel of an image transmits spike pulses to other units, and updates its own analog state value at timing when spikes come from other units. From a VLSI implementation point of view, this mechanism is suitable to very low power operation because analog buffers are unnecessary for data transmission. On the basis of this model, we have designed and fabricated a VLSI image processor chip that performs a coupled Markov random field (MRF) model for image region segmentation. Very low power VLSI design has been achieved by combination of an analog oscillator and digital coupling function generator circuits with time-domain computation. The performance of this oscillator-based image processor chip can be superior to the existing digital processors.  Experiments using the fabricated chip show successful image region segmentation in 1D and 2D images. "
VLSI Implementation of a Coupled MRF Model Using Pulse-coupled Phase Oscillators,"This paper proposes efficient pixel-parallel image processing using a pulse-coupled phase oscillator model and its VLSI implementation. A processing unit which corresponds to a pixel of an image transmits spike pulses to other units, and updates its own analog state value at timing when spikes come from other units. From a VLSI implementation point of view, this mechanism is suitable to very low power operation because analog buffers are unnecessary for data transmission. On the basis of this model, we have designed and fabricated a VLSI image processor chip that performs a coupled Markov random field (MRF) model for image region segmentation. Very low power VLSI design has been achieved by combination of an analog oscillator and digital coupling function generator circuits with time-domain computation. The performance of this oscillator-based image processor chip can be superior to the existing digital processors.  Experiments using the fabricated chip show successful image region segmentation in 1D and 2D images. "
VLSI Implementation of a Coupled MRF Model Using Pulse-coupled Phase Oscillators,"This paper proposes efficient pixel-parallel image processing using a pulse-coupled phase oscillator model and its VLSI implementation. A processing unit which corresponds to a pixel of an image transmits spike pulses to other units, and updates its own analog state value at timing when spikes come from other units. From a VLSI implementation point of view, this mechanism is suitable to very low power operation because analog buffers are unnecessary for data transmission. On the basis of this model, we have designed and fabricated a VLSI image processor chip that performs a coupled Markov random field (MRF) model for image region segmentation. Very low power VLSI design has been achieved by combination of an analog oscillator and digital coupling function generator circuits with time-domain computation. The performance of this oscillator-based image processor chip can be superior to the existing digital processors.  Experiments using the fabricated chip show successful image region segmentation in 1D and 2D images. "
VLSI Implementation of a Coupled MRF Model Using Pulse-coupled Phase Oscillators,"This paper proposes efficient pixel-parallel image processing using a pulse-coupled phase oscillator model and its VLSI implementation. A processing unit which corresponds to a pixel of an image transmits spike pulses to other units, and updates its own analog state value at timing when spikes come from other units. From a VLSI implementation point of view, this mechanism is suitable to very low power operation because analog buffers are unnecessary for data transmission. On the basis of this model, we have designed and fabricated a VLSI image processor chip that performs a coupled Markov random field (MRF) model for image region segmentation. Very low power VLSI design has been achieved by combination of an analog oscillator and digital coupling function generator circuits with time-domain computation. The performance of this oscillator-based image processor chip can be superior to the existing digital processors.  Experiments using the fabricated chip show successful image region segmentation in 1D and 2D images. "
Convex Variational Image Restoration with Histogram Priors,"We present a novel variational approach to image restoration (e.g., denoising, inpainting, labeling.) that enables to complement established variational approaches with a histogram-based prior enforcing closeness of the solution to some given empirical measure. By minimizing a single objective function, the approach utilizes simultaneously two quite different sources of information forrestoration: spatial context in terms of some smoothness prior and non-spatial statistics in terms of the novel prior utilizing the Wasserstein distance between probability measures. We study the combination of the functional lifting technique with two different relaxations of the histogram prior and derive a jointly convex variational approach. Mathematical equivalence of both relaxations and optimality certificates are established. Numerical experimentsusing the basic total-variation based denoising approach as a case study demonstrate our novel regularization approach."
Convex Variational Image Restoration with Histogram Priors,"We present a novel variational approach to image restoration (e.g., denoising, inpainting, labeling.) that enables to complement established variational approaches with a histogram-based prior enforcing closeness of the solution to some given empirical measure. By minimizing a single objective function, the approach utilizes simultaneously two quite different sources of information forrestoration: spatial context in terms of some smoothness prior and non-spatial statistics in terms of the novel prior utilizing the Wasserstein distance between probability measures. We study the combination of the functional lifting technique with two different relaxations of the histogram prior and derive a jointly convex variational approach. Mathematical equivalence of both relaxations and optimality certificates are established. Numerical experimentsusing the basic total-variation based denoising approach as a case study demonstrate our novel regularization approach."
MCMC for continuous-time discrete-state systems,"We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process, while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler, and show its advantages."
MCMC for continuous-time discrete-state systems,"We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process, while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler, and show its advantages."
A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling,"The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session."
A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling,"The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session."
A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling,"The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session."
A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling,"The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session."
Learning about Canonical Views from Internet Image Collections,"Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or ?canonical? view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views?We start by manually finding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to find the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories."
Learning about Canonical Views from Internet Image Collections,"Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or ?canonical? view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views?We start by manually finding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to find the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories."
Context-Aware Wide-Area Activity Recognition,"In this paper, rather than modeling activities in videos individually, we propose a structural representation that jointly models related activities with both motion and context information. This is motivated from the observations that activities related in space and time rarely occur independently and can serve as context for each other. The spatial layout of activities and their sequential patterns provide useful cues for the understanding of activities. Given initial classifying information generated by a base classifier, our model aims to improve recognition accuracy by jointly modeling related activities using these preliminary results and various context features. The proposed model automatically captures and weights motion and context patterns for each activity class, as well as groups of them, from sets of predefined attributes during the learning process. Then the learned model is used to generate globally optimum labels for activities in the testing videos. We show how to learn the model parameter via an unconstrained convex optimization problem and how to predict the correct labels for a testing instance consisting of multiple activities. We show promising results on the VIRAT Ground Dataset that demonstrates the benefit of joint modeling and recognizing contextual activities in a wide-area scene."
Context-Aware Wide-Area Activity Recognition,"In this paper, rather than modeling activities in videos individually, we propose a structural representation that jointly models related activities with both motion and context information. This is motivated from the observations that activities related in space and time rarely occur independently and can serve as context for each other. The spatial layout of activities and their sequential patterns provide useful cues for the understanding of activities. Given initial classifying information generated by a base classifier, our model aims to improve recognition accuracy by jointly modeling related activities using these preliminary results and various context features. The proposed model automatically captures and weights motion and context patterns for each activity class, as well as groups of them, from sets of predefined attributes during the learning process. Then the learned model is used to generate globally optimum labels for activities in the testing videos. We show how to learn the model parameter via an unconstrained convex optimization problem and how to predict the correct labels for a testing instance consisting of multiple activities. We show promising results on the VIRAT Ground Dataset that demonstrates the benefit of joint modeling and recognizing contextual activities in a wide-area scene."
Context-Aware Wide-Area Activity Recognition,"In this paper, rather than modeling activities in videos individually, we propose a structural representation that jointly models related activities with both motion and context information. This is motivated from the observations that activities related in space and time rarely occur independently and can serve as context for each other. The spatial layout of activities and their sequential patterns provide useful cues for the understanding of activities. Given initial classifying information generated by a base classifier, our model aims to improve recognition accuracy by jointly modeling related activities using these preliminary results and various context features. The proposed model automatically captures and weights motion and context patterns for each activity class, as well as groups of them, from sets of predefined attributes during the learning process. Then the learned model is used to generate globally optimum labels for activities in the testing videos. We show how to learn the model parameter via an unconstrained convex optimization problem and how to predict the correct labels for a testing instance consisting of multiple activities. We show promising results on the VIRAT Ground Dataset that demonstrates the benefit of joint modeling and recognizing contextual activities in a wide-area scene."
Linking Heterogeneous Input Spaces with Pivots for Multi-Task Learning,"Most existing works on multi-task learning (MTL) assume the same input space for different tasks. In this paper, we address a general setting where different tasks have heterogeneous input spaces. This setting has a lot of potential applications, yet it poses new algorithmic challenges - how can we link seemingly uncorrelatedtasks to mutually boost their learning performance?Our key observation is that in many real applications, there might exist some correspondence among the inputs of different tasks, which is referred to as pivots. Forsuch applications, we first propose a learning scheme for multiple tasks and analyze its generalization performance. Then we focus on the problems where only a limited number of the pivots are available, and propose a general frameworkto leverage the pivot information. The idea is to map the heterogeneous input spaces to a common space, and construct a single prediction model in this space for all the tasks. We further propose an effective optimization algorithm to find both the mappings and the prediction model. Experimental results demonstrate its effectiveness, especially with very limited number of pivots."
Linking Heterogeneous Input Spaces with Pivots for Multi-Task Learning,"Most existing works on multi-task learning (MTL) assume the same input space for different tasks. In this paper, we address a general setting where different tasks have heterogeneous input spaces. This setting has a lot of potential applications, yet it poses new algorithmic challenges - how can we link seemingly uncorrelatedtasks to mutually boost their learning performance?Our key observation is that in many real applications, there might exist some correspondence among the inputs of different tasks, which is referred to as pivots. Forsuch applications, we first propose a learning scheme for multiple tasks and analyze its generalization performance. Then we focus on the problems where only a limited number of the pivots are available, and propose a general frameworkto leverage the pivot information. The idea is to map the heterogeneous input spaces to a common space, and construct a single prediction model in this space for all the tasks. We further propose an effective optimization algorithm to find both the mappings and the prediction model. Experimental results demonstrate its effectiveness, especially with very limited number of pivots."
Linking Heterogeneous Input Spaces with Pivots for Multi-Task Learning,"Most existing works on multi-task learning (MTL) assume the same input space for different tasks. In this paper, we address a general setting where different tasks have heterogeneous input spaces. This setting has a lot of potential applications, yet it poses new algorithmic challenges - how can we link seemingly uncorrelatedtasks to mutually boost their learning performance?Our key observation is that in many real applications, there might exist some correspondence among the inputs of different tasks, which is referred to as pivots. Forsuch applications, we first propose a learning scheme for multiple tasks and analyze its generalization performance. Then we focus on the problems where only a limited number of the pivots are available, and propose a general frameworkto leverage the pivot information. The idea is to map the heterogeneous input spaces to a common space, and construct a single prediction model in this space for all the tasks. We further propose an effective optimization algorithm to find both the mappings and the prediction model. Experimental results demonstrate its effectiveness, especially with very limited number of pivots."
Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data,"We propose an efficient, generalized, nonparametric, statistical Kolmogorov-Smirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efficient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods."
Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data,"We propose an efficient, generalized, nonparametric, statistical Kolmogorov-Smirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efficient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods."
Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data,"We propose an efficient, generalized, nonparametric, statistical Kolmogorov-Smirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efficient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods."
A Nonparametric Bayesian Classifier under a Mixture Loss Function,"Many classification problems can be conveniently formulated in terms of Bayesian mixture prior models. The mixture prior structure lends itself especially well for adapting to varying degrees of sparsity. Typically, parametric assumptions are made about the components of the mixture priors. In the following, we propose a parametric and a nonparametric classification procedures using a mixture prior Bayesian approach for a risk function that combines misclassification loss and an $L_2$ penalty. While the parametric procedure is closer to traditional approaches,in simulations, we show that the nonparametric classifier typically outperforms it when the parametric prior is misspecified; the two procedures have comparable performance even when the shape of the parametric prior is specified correctly.  We illustrate the properties of the two classifiers on a publicly available gene expression dataset."
TRaNce: Trace Norm for Ranking,"Learning to rank objects, in order of their perceived importance, finds diverse applications in several domains. Typically, the goal of ranking is to learn a real valued function that induces an ordering over the entire object space. In this paper, we formulate query specific ranking as a matrix completion problem: given the relevance values for a (partial) collection of objects over a set of queries, expressed in terms of a matrix, our task is to predict the relevance of the remaining objects by filling in the missing entries of the matrix. Specifically, we develop a matrix factorization based framework, TRaNce, for learning to rank. To our knowledge, TRaNce is the first technique to investigate the applicability of the trace-norm for ranking.  We provide a rigorous theoretical analysis encompassing generalization bounds (including stability and consistency) for the proposed methodology. We also provide experimental evidence to corroborate the efficacy of our technique."
TRaNce: Trace Norm for Ranking,"Learning to rank objects, in order of their perceived importance, finds diverse applications in several domains. Typically, the goal of ranking is to learn a real valued function that induces an ordering over the entire object space. In this paper, we formulate query specific ranking as a matrix completion problem: given the relevance values for a (partial) collection of objects over a set of queries, expressed in terms of a matrix, our task is to predict the relevance of the remaining objects by filling in the missing entries of the matrix. Specifically, we develop a matrix factorization based framework, TRaNce, for learning to rank. To our knowledge, TRaNce is the first technique to investigate the applicability of the trace-norm for ranking.  We provide a rigorous theoretical analysis encompassing generalization bounds (including stability and consistency) for the proposed methodology. We also provide experimental evidence to corroborate the efficacy of our technique."
TRaNce: Trace Norm for Ranking,"Learning to rank objects, in order of their perceived importance, finds diverse applications in several domains. Typically, the goal of ranking is to learn a real valued function that induces an ordering over the entire object space. In this paper, we formulate query specific ranking as a matrix completion problem: given the relevance values for a (partial) collection of objects over a set of queries, expressed in terms of a matrix, our task is to predict the relevance of the remaining objects by filling in the missing entries of the matrix. Specifically, we develop a matrix factorization based framework, TRaNce, for learning to rank. To our knowledge, TRaNce is the first technique to investigate the applicability of the trace-norm for ranking.  We provide a rigorous theoretical analysis encompassing generalization bounds (including stability and consistency) for the proposed methodology. We also provide experimental evidence to corroborate the efficacy of our technique."
Displacement Determination by Motion Compensation,Motion determination from an image sequence has been an important and challenging problem in computer vision and remote sensing applications. A fully constrained nonlinear system of equations combing the Displacement Vector Invariant (DVI) equation for displacement determination from an image sequence are proposed without approximation and imposing any additional constraint and assumption. An adaptive framework for solving the nonlinear system of equations has been developed. This work is to seek motion fields that are consistent with the physical observation because the observation may not be consistent with the physical motion in a featureless image sequence. The estimated displacement field is based on a single minimized target function that leads to optimized motion-compensated predictions and interpolations in a wide class of applications of the motion-compensated compression without any penalty parameters. Experimental tests on synthetic and natural image sequences are presented. Applications of motion-compensated interpolation are also demonstrated.
Newton-Like Methods for Sparse Inverse Covariance Estimation,"We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimationproblem. Comparisons with the method implemented in the QUIC software package are presented."
Newton-Like Methods for Sparse Inverse Covariance Estimation,"We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimationproblem. Comparisons with the method implemented in the QUIC software package are presented."
Learning to Align from Scratch,"  Unsupervised joint alignment of images has been demonstrated to  improve performance on recognition tasks such as face verification.  Such alignment reduces undesired variability due to factors such as  pose, while only requiring weak supervision in the form of poorly  aligned examples.  However, prior work on unsupervised alignment of  complex, real world images has required the careful selection of  feature representation based on hand-crafted image descriptors, in  order to achieve an appropriate, smooth optimization landscape.  In this paper, we instead propose a novel combination of  unsupervised joint alignment with unsupervised feature learning.  Specifically, we incorporate deep learning into the {\em congealing}  alignment framework.  Through deep learning, we obtain features that  can represent the image at differing resolutions based on network  depth, and that are tuned to the statistics of the specific data  being aligned.  In addition, we modify the learning algorithm for  the restricted Boltzmann machine by incorporating a group sparsity  penalty, leading to a topographic organization on the learned  filters and improving subsequent alignment results.  We apply our method to the Labeled Faces in the Wild database  (LFW). Using the aligned images produced by our proposed  unsupervised algorithm, we achieve a significantly higher accuracy  in face verification than obtained using the original face images,  prior work in unsupervised alignment, and prior work in supervised  alignment.  We also match the accuracy for the best available, but  unpublished method."
Learning to Align from Scratch,"  Unsupervised joint alignment of images has been demonstrated to  improve performance on recognition tasks such as face verification.  Such alignment reduces undesired variability due to factors such as  pose, while only requiring weak supervision in the form of poorly  aligned examples.  However, prior work on unsupervised alignment of  complex, real world images has required the careful selection of  feature representation based on hand-crafted image descriptors, in  order to achieve an appropriate, smooth optimization landscape.  In this paper, we instead propose a novel combination of  unsupervised joint alignment with unsupervised feature learning.  Specifically, we incorporate deep learning into the {\em congealing}  alignment framework.  Through deep learning, we obtain features that  can represent the image at differing resolutions based on network  depth, and that are tuned to the statistics of the specific data  being aligned.  In addition, we modify the learning algorithm for  the restricted Boltzmann machine by incorporating a group sparsity  penalty, leading to a topographic organization on the learned  filters and improving subsequent alignment results.  We apply our method to the Labeled Faces in the Wild database  (LFW). Using the aligned images produced by our proposed  unsupervised algorithm, we achieve a significantly higher accuracy  in face verification than obtained using the original face images,  prior work in unsupervised alignment, and prior work in supervised  alignment.  We also match the accuracy for the best available, but  unpublished method."
Learning to Align from Scratch,"  Unsupervised joint alignment of images has been demonstrated to  improve performance on recognition tasks such as face verification.  Such alignment reduces undesired variability due to factors such as  pose, while only requiring weak supervision in the form of poorly  aligned examples.  However, prior work on unsupervised alignment of  complex, real world images has required the careful selection of  feature representation based on hand-crafted image descriptors, in  order to achieve an appropriate, smooth optimization landscape.  In this paper, we instead propose a novel combination of  unsupervised joint alignment with unsupervised feature learning.  Specifically, we incorporate deep learning into the {\em congealing}  alignment framework.  Through deep learning, we obtain features that  can represent the image at differing resolutions based on network  depth, and that are tuned to the statistics of the specific data  being aligned.  In addition, we modify the learning algorithm for  the restricted Boltzmann machine by incorporating a group sparsity  penalty, leading to a topographic organization on the learned  filters and improving subsequent alignment results.  We apply our method to the Labeled Faces in the Wild database  (LFW). Using the aligned images produced by our proposed  unsupervised algorithm, we achieve a significantly higher accuracy  in face verification than obtained using the original face images,  prior work in unsupervised alignment, and prior work in supervised  alignment.  We also match the accuracy for the best available, but  unpublished method."
Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints,"Recent spiking network models of Bayesian inference and unsupervised learningfrequently assume either inputs to arrive in a special format or employ complex computations inneuronal activation functions and synaptic plasticity rules. Here we show in arigorous mathematical treatment how homeostatic processes,which have previously received little attention in this context, can overcome common theoretical limitationsand facilitate the neural implementation and performance of existing models.In particular, we show that homeostatic plasticity can be understood as the enforcement of a 'balancing'posterior constraint during probabilistic inference and learning with Expectation Maximization.We link homeostatic dynamics to the theory of variational inference, and show that, as a side effect, nontrivial terms which typically appear during probabilistic inference in a largeclass of models drop out.We demonstrate the feasibility of our approach in a spiking Winner-Take-All architecture of Bayesian inference and learning,and discuss general properties of the resulting network dynamics. Finally, we sketch how the mathematical framework can be extended to richer, recurrent network architectures.Altogether, our theory provides a novel perspective on the interplay of homeostaticprocesses and synaptic plasticity in cortical microcircuits, pointing to an essential computational roleof homeostasis during probabilistic inference and learning in spiking networks."
Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints,"Recent spiking network models of Bayesian inference and unsupervised learningfrequently assume either inputs to arrive in a special format or employ complex computations inneuronal activation functions and synaptic plasticity rules. Here we show in arigorous mathematical treatment how homeostatic processes,which have previously received little attention in this context, can overcome common theoretical limitationsand facilitate the neural implementation and performance of existing models.In particular, we show that homeostatic plasticity can be understood as the enforcement of a 'balancing'posterior constraint during probabilistic inference and learning with Expectation Maximization.We link homeostatic dynamics to the theory of variational inference, and show that, as a side effect, nontrivial terms which typically appear during probabilistic inference in a largeclass of models drop out.We demonstrate the feasibility of our approach in a spiking Winner-Take-All architecture of Bayesian inference and learning,and discuss general properties of the resulting network dynamics. Finally, we sketch how the mathematical framework can be extended to richer, recurrent network architectures.Altogether, our theory provides a novel perspective on the interplay of homeostaticprocesses and synaptic plasticity in cortical microcircuits, pointing to an essential computational roleof homeostasis during probabilistic inference and learning in spiking networks."
Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints,"Recent spiking network models of Bayesian inference and unsupervised learningfrequently assume either inputs to arrive in a special format or employ complex computations inneuronal activation functions and synaptic plasticity rules. Here we show in arigorous mathematical treatment how homeostatic processes,which have previously received little attention in this context, can overcome common theoretical limitationsand facilitate the neural implementation and performance of existing models.In particular, we show that homeostatic plasticity can be understood as the enforcement of a 'balancing'posterior constraint during probabilistic inference and learning with Expectation Maximization.We link homeostatic dynamics to the theory of variational inference, and show that, as a side effect, nontrivial terms which typically appear during probabilistic inference in a largeclass of models drop out.We demonstrate the feasibility of our approach in a spiking Winner-Take-All architecture of Bayesian inference and learning,and discuss general properties of the resulting network dynamics. Finally, we sketch how the mathematical framework can be extended to richer, recurrent network architectures.Altogether, our theory provides a novel perspective on the interplay of homeostaticprocesses and synaptic plasticity in cortical microcircuits, pointing to an essential computational roleof homeostasis during probabilistic inference and learning in spiking networks."
Multiclass Semi-Supervised Learning on Graphs,"We present a graph-based variational algorithm for multiclassclassification of high dimensional data. The variational energy is basedon a diffuse interface model, and we introduce an alternative measure ofsmoothness appropriate for the multiclass segmentation problem. Wedemonstrate that the multiclass diffuse interface model outperformsclassical spectral clustering methods, and that it obtains resultscompetitive with the state of the art among other graph-based algorithms."
Discriminative Hidden Kalman Filters,"In on-line classification, a real-time decision needs to be made based on signals observed so far. Generative models such as switching Kalman filters are good at describing signals but not classifying them. Discriminative approaches such as conditional random fields offer better classification performance. Nevertheless, they often require pre-defined features. We propose a discriminative hidden Kalman filter that jointly learns features and the classifier based on a novel discriminative training criterion. The variational Bayesian algorithm is employed for optimization. An extension to handle multi-class problems is also presented. "
Discriminative Hidden Kalman Filters,"In on-line classification, a real-time decision needs to be made based on signals observed so far. Generative models such as switching Kalman filters are good at describing signals but not classifying them. Discriminative approaches such as conditional random fields offer better classification performance. Nevertheless, they often require pre-defined features. We propose a discriminative hidden Kalman filter that jointly learns features and the classifier based on a novel discriminative training criterion. The variational Bayesian algorithm is employed for optimization. An extension to handle multi-class problems is also presented. "
Generalized Conformal Prediction for Functional Data,"Conformal prediction is an approach to constructing distribution free predictions. In this paper we apply conformal method to develop a class of exploratory tools for functional data. We first describe a new conformal method and use it to construct simultaneous prediction bands for functional data. Then we use conformal methods with pseudo density to find anomalies, median sets, prototypes, and conformal cluster trees. In developing these tools, we give the first distribution free, finite sample valid prediction sets and prediction bands for functional data. Our construction also features a novel implementation of conformal prediction that greatly enhances the flexibility and computation efficiency of this general method."
Generalized Conformal Prediction for Functional Data,"Conformal prediction is an approach to constructing distribution free predictions. In this paper we apply conformal method to develop a class of exploratory tools for functional data. We first describe a new conformal method and use it to construct simultaneous prediction bands for functional data. Then we use conformal methods with pseudo density to find anomalies, median sets, prototypes, and conformal cluster trees. In developing these tools, we give the first distribution free, finite sample valid prediction sets and prediction bands for functional data. Our construction also features a novel implementation of conformal prediction that greatly enhances the flexibility and computation efficiency of this general method."
Generalized Conformal Prediction for Functional Data,"Conformal prediction is an approach to constructing distribution free predictions. In this paper we apply conformal method to develop a class of exploratory tools for functional data. We first describe a new conformal method and use it to construct simultaneous prediction bands for functional data. Then we use conformal methods with pseudo density to find anomalies, median sets, prototypes, and conformal cluster trees. In developing these tools, we give the first distribution free, finite sample valid prediction sets and prediction bands for functional data. Our construction also features a novel implementation of conformal prediction that greatly enhances the flexibility and computation efficiency of this general method."
Workflows for Computer Vision: Open Publication and Reproducibility of Experiments,"The inability to reproduce computational research is a rapidly growing area of concern in computer vision. In this paper, we incorporate a structured, end-to-end analysis methodology, based on workflows, to easily and automatically allow for standardized replication and testing of state-of-the-art models, inter-operability of heterogeneous codebases, and incorporation of novel algorithms. We demonstrate the utility of our approach by introducing a novel computer vision dataset and conducting an in-depth, comparative analysis of state-of-the-art methods on the new Atomic Pair Actions dataset using workflows. This allows us to re-use pre-existing workflows as well as incorporating new algorithms developed in heterogeneous codebases. The entire framework, including the workflows and the dataset, is then exported as web objects which can be executed via the web, or downloaded and imported into a compatible workflow system, by any user to re-create the full analysis or to change/extend the workflows as desired. In addition, we make the full dataset (the videos, their associated tracks with ground truth, and metadata) and all exported workflows widely available to the research community both as openly accessible web objects."
Workflows for Computer Vision: Open Publication and Reproducibility of Experiments,"The inability to reproduce computational research is a rapidly growing area of concern in computer vision. In this paper, we incorporate a structured, end-to-end analysis methodology, based on workflows, to easily and automatically allow for standardized replication and testing of state-of-the-art models, inter-operability of heterogeneous codebases, and incorporation of novel algorithms. We demonstrate the utility of our approach by introducing a novel computer vision dataset and conducting an in-depth, comparative analysis of state-of-the-art methods on the new Atomic Pair Actions dataset using workflows. This allows us to re-use pre-existing workflows as well as incorporating new algorithms developed in heterogeneous codebases. The entire framework, including the workflows and the dataset, is then exported as web objects which can be executed via the web, or downloaded and imported into a compatible workflow system, by any user to re-create the full analysis or to change/extend the workflows as desired. In addition, we make the full dataset (the videos, their associated tracks with ground truth, and metadata) and all exported workflows widely available to the research community both as openly accessible web objects."
Sparse Manifold Alignment,"Previous approaches to manifold alignment are based on solving a (generalized) eigenvector problem. We propose a least squares formulation of a class of manifold alignment approaches, which has the potential of scaling better to real-world data sets. Furthermore, the least-squares formulation enables various regularization techniques to be readily incorporated to improve model sparsity and generalization ability. In particular, it enables using the $l_1$ norm regularization  framework to make previous manifold alignment algorithms more robust. The new approach can prune domain-dependent features automatically helping to improve transfer learning. This extension significantly broadens the scope of manifold alignment techniques  and leads to faster algorithms. We present detailed experiments to illustrate the approach using the domains of cross-lingual information retrieval and social network analysis."
Sparse Manifold Alignment,"Previous approaches to manifold alignment are based on solving a (generalized) eigenvector problem. We propose a least squares formulation of a class of manifold alignment approaches, which has the potential of scaling better to real-world data sets. Furthermore, the least-squares formulation enables various regularization techniques to be readily incorporated to improve model sparsity and generalization ability. In particular, it enables using the $l_1$ norm regularization  framework to make previous manifold alignment algorithms more robust. The new approach can prune domain-dependent features automatically helping to improve transfer learning. This extension significantly broadens the scope of manifold alignment techniques  and leads to faster algorithms. We present detailed experiments to illustrate the approach using the domains of cross-lingual information retrieval and social network analysis."
Sparse Manifold Alignment,"Previous approaches to manifold alignment are based on solving a (generalized) eigenvector problem. We propose a least squares formulation of a class of manifold alignment approaches, which has the potential of scaling better to real-world data sets. Furthermore, the least-squares formulation enables various regularization techniques to be readily incorporated to improve model sparsity and generalization ability. In particular, it enables using the $l_1$ norm regularization  framework to make previous manifold alignment algorithms more robust. The new approach can prune domain-dependent features automatically helping to improve transfer learning. This extension significantly broadens the scope of manifold alignment techniques  and leads to faster algorithms. We present detailed experiments to illustrate the approach using the domains of cross-lingual information retrieval and social network analysis."
Sparse Manifold Alignment,"Previous approaches to manifold alignment are based on solving a (generalized) eigenvector problem. We propose a least squares formulation of a class of manifold alignment approaches, which has the potential of scaling better to real-world data sets. Furthermore, the least-squares formulation enables various regularization techniques to be readily incorporated to improve model sparsity and generalization ability. In particular, it enables using the $l_1$ norm regularization  framework to make previous manifold alignment algorithms more robust. The new approach can prune domain-dependent features automatically helping to improve transfer learning. This extension significantly broadens the scope of manifold alignment techniques  and leads to faster algorithms. We present detailed experiments to illustrate the approach using the domains of cross-lingual information retrieval and social network analysis."
Foveated Search Models That Learn Eye Movements,"This paper presents foveated search models that learn where to fixate inorder to improve perceptual performance on a simple visual search task. In particular, we combine models of eye movements during search with a perceptual learning model and a machine learning method for a task where the observer has to search for a target and then give a yes/no decision about its presence. We report simulation results for various eye movement models and learning methods. Foveated eye movements include maximum a posteriori (MAP), ideal searcher,random and systematic exploration models. We evaluate Bayesian and on-linelearning algorithms. Simulation results show that for the simple tasks the computationally tractable MAP model can approximate the ideal searcher irrespectiveof the learning algorithm. We also compare model performance (constrained toa human visibility map) to that of a human observer on the same task. Both human improvements of perceptual performance and convergence of eye movements towards the target location across sessions suggest that humans are not using either a random or systematic saccadic exploration or an on-line learning algorithm. Yet, humans are less efficient than the MAP eye movement model with Bayesianlearning."
Foveated Search Models That Learn Eye Movements,"This paper presents foveated search models that learn where to fixate inorder to improve perceptual performance on a simple visual search task. In particular, we combine models of eye movements during search with a perceptual learning model and a machine learning method for a task where the observer has to search for a target and then give a yes/no decision about its presence. We report simulation results for various eye movement models and learning methods. Foveated eye movements include maximum a posteriori (MAP), ideal searcher,random and systematic exploration models. We evaluate Bayesian and on-linelearning algorithms. Simulation results show that for the simple tasks the computationally tractable MAP model can approximate the ideal searcher irrespectiveof the learning algorithm. We also compare model performance (constrained toa human visibility map) to that of a human observer on the same task. Both human improvements of perceptual performance and convergence of eye movements towards the target location across sessions suggest that humans are not using either a random or systematic saccadic exploration or an on-line learning algorithm. Yet, humans are less efficient than the MAP eye movement model with Bayesianlearning."
Anatomy-guided Discovery of Large-scale Consistent Connectivity-based Cortical Landmarks,"Establishment of structural and functional correspondences across different brains and populations is one of the most fundamental issues in the brain imaging field. Recently, several multimodal DTI/fMRI studies have demonstrated that consistent white matter fiber connection patterns can predict brain function and represent common brain architectures across individuals and populations. Although a variety of approaches have been proposed to discover large-scale cortical landmarks with common structural connection profiles, the rich anatomical information such as gyral/sulcal folding patterns and structural connection pattern homogeneity have not been incorporated into existing DTI/fMRI studies yet. This paper presents a novel anatomy-guided discovery framework that defines and optimizes a dense map of cortical landmarks that possess group-wise consistent anatomical and connectional profiles. This framework integrates reliable and rich anatomical information for landmark initialization, optimization and prediction, which are formulated and solved as an energy minimization problem. Validation results based on fMRI data demonstrate that these landmarks are producible, predictable and exhibit accurate structural and functional correspondences across individuals and populations, offering a universal and individualized brain reference system for neuroimaging research."
Anatomy-guided Discovery of Large-scale Consistent Connectivity-based Cortical Landmarks,"Establishment of structural and functional correspondences across different brains and populations is one of the most fundamental issues in the brain imaging field. Recently, several multimodal DTI/fMRI studies have demonstrated that consistent white matter fiber connection patterns can predict brain function and represent common brain architectures across individuals and populations. Although a variety of approaches have been proposed to discover large-scale cortical landmarks with common structural connection profiles, the rich anatomical information such as gyral/sulcal folding patterns and structural connection pattern homogeneity have not been incorporated into existing DTI/fMRI studies yet. This paper presents a novel anatomy-guided discovery framework that defines and optimizes a dense map of cortical landmarks that possess group-wise consistent anatomical and connectional profiles. This framework integrates reliable and rich anatomical information for landmark initialization, optimization and prediction, which are formulated and solved as an energy minimization problem. Validation results based on fMRI data demonstrate that these landmarks are producible, predictable and exhibit accurate structural and functional correspondences across individuals and populations, offering a universal and individualized brain reference system for neuroimaging research."
Anatomy-guided Discovery of Large-scale Consistent Connectivity-based Cortical Landmarks,"Establishment of structural and functional correspondences across different brains and populations is one of the most fundamental issues in the brain imaging field. Recently, several multimodal DTI/fMRI studies have demonstrated that consistent white matter fiber connection patterns can predict brain function and represent common brain architectures across individuals and populations. Although a variety of approaches have been proposed to discover large-scale cortical landmarks with common structural connection profiles, the rich anatomical information such as gyral/sulcal folding patterns and structural connection pattern homogeneity have not been incorporated into existing DTI/fMRI studies yet. This paper presents a novel anatomy-guided discovery framework that defines and optimizes a dense map of cortical landmarks that possess group-wise consistent anatomical and connectional profiles. This framework integrates reliable and rich anatomical information for landmark initialization, optimization and prediction, which are formulated and solved as an energy minimization problem. Validation results based on fMRI data demonstrate that these landmarks are producible, predictable and exhibit accurate structural and functional correspondences across individuals and populations, offering a universal and individualized brain reference system for neuroimaging research."
Anatomy-guided Discovery of Large-scale Consistent Connectivity-based Cortical Landmarks,"Establishment of structural and functional correspondences across different brains and populations is one of the most fundamental issues in the brain imaging field. Recently, several multimodal DTI/fMRI studies have demonstrated that consistent white matter fiber connection patterns can predict brain function and represent common brain architectures across individuals and populations. Although a variety of approaches have been proposed to discover large-scale cortical landmarks with common structural connection profiles, the rich anatomical information such as gyral/sulcal folding patterns and structural connection pattern homogeneity have not been incorporated into existing DTI/fMRI studies yet. This paper presents a novel anatomy-guided discovery framework that defines and optimizes a dense map of cortical landmarks that possess group-wise consistent anatomical and connectional profiles. This framework integrates reliable and rich anatomical information for landmark initialization, optimization and prediction, which are formulated and solved as an energy minimization problem. Validation results based on fMRI data demonstrate that these landmarks are producible, predictable and exhibit accurate structural and functional correspondences across individuals and populations, offering a universal and individualized brain reference system for neuroimaging research."
Anatomy-guided Discovery of Large-scale Consistent Connectivity-based Cortical Landmarks,"Establishment of structural and functional correspondences across different brains and populations is one of the most fundamental issues in the brain imaging field. Recently, several multimodal DTI/fMRI studies have demonstrated that consistent white matter fiber connection patterns can predict brain function and represent common brain architectures across individuals and populations. Although a variety of approaches have been proposed to discover large-scale cortical landmarks with common structural connection profiles, the rich anatomical information such as gyral/sulcal folding patterns and structural connection pattern homogeneity have not been incorporated into existing DTI/fMRI studies yet. This paper presents a novel anatomy-guided discovery framework that defines and optimizes a dense map of cortical landmarks that possess group-wise consistent anatomical and connectional profiles. This framework integrates reliable and rich anatomical information for landmark initialization, optimization and prediction, which are formulated and solved as an energy minimization problem. Validation results based on fMRI data demonstrate that these landmarks are producible, predictable and exhibit accurate structural and functional correspondences across individuals and populations, offering a universal and individualized brain reference system for neuroimaging research."
Anatomy-guided Discovery of Large-scale Consistent Connectivity-based Cortical Landmarks,"Establishment of structural and functional correspondences across different brains and populations is one of the most fundamental issues in the brain imaging field. Recently, several multimodal DTI/fMRI studies have demonstrated that consistent white matter fiber connection patterns can predict brain function and represent common brain architectures across individuals and populations. Although a variety of approaches have been proposed to discover large-scale cortical landmarks with common structural connection profiles, the rich anatomical information such as gyral/sulcal folding patterns and structural connection pattern homogeneity have not been incorporated into existing DTI/fMRI studies yet. This paper presents a novel anatomy-guided discovery framework that defines and optimizes a dense map of cortical landmarks that possess group-wise consistent anatomical and connectional profiles. This framework integrates reliable and rich anatomical information for landmark initialization, optimization and prediction, which are formulated and solved as an energy minimization problem. Validation results based on fMRI data demonstrate that these landmarks are producible, predictable and exhibit accurate structural and functional correspondences across individuals and populations, offering a universal and individualized brain reference system for neuroimaging research."
Cluster-Based Active Learning to Address the Class Imbalance and Cold Start Problems,"Active learning (AL) has been used to improve the performance for supervised learning (SL) functions by selecting the training instances to be labeled.  However, there are two open problems which can actually make AL worse than passive learning:  class imbalance and cold start.  First, AL tends to ignore the minority label in the training instances making it unlikely that the function will ever predict the minority label.  Second, the quality for the instances selected depend on those previously selected.  This makes AL very sensitive to the order instances are selected for training.  To address this latter problem, cluster-based AL has been proposed, but they assume that instances in the same cluster have the same label, which is often untrue.  Further, cluster-based AL does not currently address class imbalance.  Therefore, we propose a novel new cluster-based AL powered by the Boundary of Use (BoU) framework which focuses on finding clusters which contain similar instances with multiple labels.  These BoU clusters are designed to capture the area around the decision boundary containing instances which are most informative for AL.  Our experiments, using 21 UCI datasets and four real-world datasets, show that cluster-based AL powered by the BoU framework improves test accuracy using three different SL systems.  Our approach is also more adept at addressing the two open problems than cluster-based AL."
A Framework for Enhancing Repair Mechanisms for Supervised Learning,"Repair mechanisms such as feature selection and noise correction have been used to improve the performance for supervised learning (SL) functions.  Furthermore, active learning has been used to identify cost-effective instances and could also be considered as a repair mechanism?removing less useful instances from training.  All these mechanisms have been enhanced with clustering to allow for more specific and localized repairs?hereby known as cluster-based repair?rather than performing repair on all the training data (i.e., universal repair).  However, traditional clustering?grouping similar instances with the same label?can make it difficult to know (for certain) whether a cluster still needs to be repaired or left alone.  Subsequently, repairs applied to these clusters can result in unnecessary repairs and overfitting which ultimately reduce function performance.  Therefore, we propose a novel framework called the Boundary of Use (BoU) framework which repairs clusters where there is a mixture of instances correctly and incorrectly labeled by the function and leaves correct clusters?no need for repair?and incorrect clusters?not repairable?alone.  Our experiments, using 21 UCI datasets, show that the cluster-based repair powered by the BoU framework reduces unnecessary repairs and overfitting and outperform traditional cluster-based repair and universal repair approaches on nine (repair mechanism ? SL system) configurations."
Topology Constraints in Graphical Models,"Graphical models are a very useful tool to describe and understand natural phenomena, from gene expression to climate change and social interactions. The topological structure of these graphs/networks is a fundamental part of the analysis, and in many cases the main goal of the study. However, little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data.In this work we propose extensions to the basic joint regression model for network estimation, which explicitly incorporate graph-topological constraints into the corresponding optimization approach. The first proposed extension includes an eigenvector centrality constraint, thereby promoting this important prior topological property. The second developed extension promotes the formation of certain motifs, triangle-shaped ones in particular, which are known to exist for example in genetic regulatory networks. The presentation of the underlying formulations, which serve as examples of the introduction of topological constraints in network estimation, is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge."
Topology Constraints in Graphical Models,"Graphical models are a very useful tool to describe and understand natural phenomena, from gene expression to climate change and social interactions. The topological structure of these graphs/networks is a fundamental part of the analysis, and in many cases the main goal of the study. However, little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data.In this work we propose extensions to the basic joint regression model for network estimation, which explicitly incorporate graph-topological constraints into the corresponding optimization approach. The first proposed extension includes an eigenvector centrality constraint, thereby promoting this important prior topological property. The second developed extension promotes the formation of certain motifs, triangle-shaped ones in particular, which are known to exist for example in genetic regulatory networks. The presentation of the underlying formulations, which serve as examples of the introduction of topological constraints in network estimation, is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge."
Fast Exact Gaussian Process Regression for Separable Covariance Functions,"Gaussian process regression (GPR) is a powerful non-linear technique for Bayesian inference and prediction. One drawback is its computational complexity, requiring O($n^3$) flops for both prediction and hyperparameter estimation over $n$ training points due to the need for inverting a covariance matrix. We show that when the problem has the structure that its covariance function is separable in the case of noisy observations, the costs can non-trivially be reduced to a potentially sub-quadratic function of $n$. Separable covariance functions commonly occur in spatial-spatial and temporal-spatial regression problems. Our method is demonstrated on problems in image resampling, denoising, and spherical-temporal kriging applications."
Fast Exact Gaussian Process Regression for Separable Covariance Functions,"Gaussian process regression (GPR) is a powerful non-linear technique for Bayesian inference and prediction. One drawback is its computational complexity, requiring O($n^3$) flops for both prediction and hyperparameter estimation over $n$ training points due to the need for inverting a covariance matrix. We show that when the problem has the structure that its covariance function is separable in the case of noisy observations, the costs can non-trivially be reduced to a potentially sub-quadratic function of $n$. Separable covariance functions commonly occur in spatial-spatial and temporal-spatial regression problems. Our method is demonstrated on problems in image resampling, denoising, and spherical-temporal kriging applications."
Hierarchical Identification of Leaves,"There is massive diversity among deformable biological objects such as cells and plants, including a large number of genetic categories and a large variation in appearance within categories. We present a novel method for determining the species of botanical objects from scanned images. We focus on leaves but the strategy is generic, based on a hierarchical representation of latent variables called identification keys which embody domain knowledge about taxonomy and landmarks. Classification proceeds systematically from coarse-grained to fine-grained characterizations. First, keys are estimated, one at a time, starting with landmarks and proceeding to the genus, and finally the individual species is identified. Each step is conditional on previous estimates. Two other main ingredients are multiple key-based local coordinate systems and likelihood ratios of discriminant scores. We obtain the best performance to date on several databases of scanned simple leaves."
Scalable Heterogeneous Transfer Ranking,"Learning to rank aims to automatically learn a ranking function with point-wise, pair-wise, or list-wise cost functions and has been demonstrated to achieve superior performance in many information retrieval applications. It is known that the ranking problem usually requires significantly more labeling efforts per task. However, in practical application we can often obtain abundant labeled documents in popular languages but very few or even no labeled documents in less popular languages.  In this paper, we propose to study the problem of heterogeneous transfer ranking, a transfer learning problem with heterogeneous features in order to utilize the rich labeled data in popular languages to help the ranking task in less popular languages. We develop a large-margin algorithm, namely LM-HTR, to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We analyze the theoretical bound of the prediction loss and develop fast algorithms via stochastic gradient descent so that our model can be scalable to large-scale applications. Experiment results on four application datasets demonstrate the advantages of our algorithms over other state-of-the-art methods."
Cosegmentation with Subspace Constraints ? Identifying Shared Structure in Related Image Sets,"We develop new algorithms to analyze and exploit the joint subspace structure of a set of related images to facilitate the process of concurrent segmentation of a large set of images. Most existing approaches for this problem are either limited to extracting a single similar object across the given image set or do not scale wellto a large number of images containing multiple objects varying at different scales. One of the goals of this paper is to show that various desirable properties of such an algorithm (ability to handle multiple images with multiple objects showing arbitary scale variations) can be cast elegantly using simple constructs from linearalgebra: this signi?cantly extends the operating range of such methods. While intuitive, this formulation leads to a hard optimization problem where one must perform the image segmentation task together with appropriate constraints which enforce desired algebraic regularity (e.g., common subspace structure). We propose ef?cient iterative algorithms (with small computational requirements) whose key steps reduce to objective functions solvable by max-?ow and/or nearly closed form identities. We study the qualitative, theoretical, and empirical properties of the method,and present results on benchmark datasets."
"Learning Feature Hierarchies from Recurring Coincidences in Images, Videos and Audios","Models for learning feature hierarchies in an unsupervised and online manner are not only imperative for real world machine perception applications but also helps to understand how the brain works. In this paper, we present a multilayered feedforward neural network model that learns a hierarchy of overcomplete and sparse feature dictionaries. A neuron is modeled as an integrate-and-fire neuron and is connected to its neighbors in its own layer as well as the layers above it by lateral and feedforward connections respectively. As we ascend up the layers, the spatial and/or temporal receptive field sizes of neurons increase. The feedforward weights are learnt from repeating spatial and temporal coincident patterns to encode features. We use the learning algorithm, without any alteration, for learning multiple layers of features from images, videos, and audios with no preprocessing. When applied to edge-detected images, the neurons learn to respond to edges/bars in different orientations at the first layer (similar to simple cells in primary visual cortex or V1), and combinations of these edges that depict parts of shapes or whole generic shapes in higher layers. When applied to action videos, the second layer neurons develop orientation- and direction-selective receptive fields (similar to complex cells in V1). When applied to speech data, the model learns a hierarchy of features that can be used to reconstruct unseen speech."
"Learning Feature Hierarchies from Recurring Coincidences in Images, Videos and Audios","Models for learning feature hierarchies in an unsupervised and online manner are not only imperative for real world machine perception applications but also helps to understand how the brain works. In this paper, we present a multilayered feedforward neural network model that learns a hierarchy of overcomplete and sparse feature dictionaries. A neuron is modeled as an integrate-and-fire neuron and is connected to its neighbors in its own layer as well as the layers above it by lateral and feedforward connections respectively. As we ascend up the layers, the spatial and/or temporal receptive field sizes of neurons increase. The feedforward weights are learnt from repeating spatial and temporal coincident patterns to encode features. We use the learning algorithm, without any alteration, for learning multiple layers of features from images, videos, and audios with no preprocessing. When applied to edge-detected images, the neurons learn to respond to edges/bars in different orientations at the first layer (similar to simple cells in primary visual cortex or V1), and combinations of these edges that depict parts of shapes or whole generic shapes in higher layers. When applied to action videos, the second layer neurons develop orientation- and direction-selective receptive fields (similar to complex cells in V1). When applied to speech data, the model learns a hierarchy of features that can be used to reconstruct unseen speech."
"Learning Feature Hierarchies from Recurring Coincidences in Images, Videos and Audios","Models for learning feature hierarchies in an unsupervised and online manner are not only imperative for real world machine perception applications but also helps to understand how the brain works. In this paper, we present a multilayered feedforward neural network model that learns a hierarchy of overcomplete and sparse feature dictionaries. A neuron is modeled as an integrate-and-fire neuron and is connected to its neighbors in its own layer as well as the layers above it by lateral and feedforward connections respectively. As we ascend up the layers, the spatial and/or temporal receptive field sizes of neurons increase. The feedforward weights are learnt from repeating spatial and temporal coincident patterns to encode features. We use the learning algorithm, without any alteration, for learning multiple layers of features from images, videos, and audios with no preprocessing. When applied to edge-detected images, the neurons learn to respond to edges/bars in different orientations at the first layer (similar to simple cells in primary visual cortex or V1), and combinations of these edges that depict parts of shapes or whole generic shapes in higher layers. When applied to action videos, the second layer neurons develop orientation- and direction-selective receptive fields (similar to complex cells in V1). When applied to speech data, the model learns a hierarchy of features that can be used to reconstruct unseen speech."
Stratified Tree Search: A Novel Suboptimal Heuristic Search Algorithm,"Traditional heuristic search algorithms use the ranking of states a heuristic function provides to guide the search. In this paper---with the object of improving suboptimality and runtime of search algorithms when only weak heuristics are available---, we present Stratified Tree Search (STS), a novel suboptimal heuristic search algorithm that uses a heuristic function to make a partition of the state space to guide search. We call the partition used by STS a type system. STS assumes that nodes of the same type will lead to solutions of the same cost. Thus, STS expands only one node of each type in every level of search. We empirically evaluated STS in heuristic search domains. Our results show that STS can find solutions of lower suboptimality in less time than standard heuristic search algorithms for finding suboptimal solutions."
A Polynomial Mechanism for Resource Allocation under Price Rigidities,"Computational issues on trading mechanisms for resource allocation have been studies intensively in the AI literature. The literature has focused almost entirely on economic models without price rigidities. However, it is ubiquitous in the real world that prices of items are not completely flexible but restricted to some admissible interval price rigidities for some economic or political reasons. This paper studies the computational issues ofdynamic mechanisms for selling multiple indivisible items under price rigidities. We propose a polynomial algorithm that can be used to find over-demanded sets of items, and then introduce a dynamic mechanism with rationing to discover constrained Walrasian equilibria under price rigidities in polynomial time."
Robust L1 Normalized Cut and Symmetric Nonnegative Matrix Factorization,"Spectral clustering is widely used in practice. However existing formulation is prone to large errors. In this paper, we first transform it into a matrix decompositionproblem and then propose a robust formulation using L1-norm. This leads to L1 normalized cut and L1 symmetric nonnegative matrix factorization (SNMF) models. We derive computational algorithms for L1 normalized cut. We also providevery efficient updating rules for L1 SNMF with rigorous convergence analysis. Extensive experiments on 6 datasets with significant data corruption/occlusionshow that L1 normalized cut and L1 SNMF provide consistently better clustering results as compared to standard methods and other L1-type functionals."
Multiview Spectral Clustering via Pareto Optimization,"Traditionally, the input of spectral clustering is limited to single-view data. However, many real-world datasets come with multiple heterogeneous feature sets, which provide multiple views of the same data. Such datasets include scientific data (fMRI scans of different individuals), social data (different types of connections between people), web data (multi-type data), and so on. How to optimally combine knowledge from multiple views to help spectral clustering find a better partition remains a developing area. Previous work formulates the problem as a single objective function to optimize, typically by combining the views under a compatibility assumption and requiring the users to decide the importance of each view a priori. In this work, we propose a multi-objective formulation and show how to solve it using Pareto optimization. The Pareto frontier captures all possible good cuts without requiring the users to set the ``correct'' parameter. The effectiveness of our approach is justified by both theoretical analysis and empirical results on benchmark datasets."
Multiview Spectral Clustering via Pareto Optimization,"Traditionally, the input of spectral clustering is limited to single-view data. However, many real-world datasets come with multiple heterogeneous feature sets, which provide multiple views of the same data. Such datasets include scientific data (fMRI scans of different individuals), social data (different types of connections between people), web data (multi-type data), and so on. How to optimally combine knowledge from multiple views to help spectral clustering find a better partition remains a developing area. Previous work formulates the problem as a single objective function to optimize, typically by combining the views under a compatibility assumption and requiring the users to decide the importance of each view a priori. In this work, we propose a multi-objective formulation and show how to solve it using Pareto optimization. The Pareto frontier captures all possible good cuts without requiring the users to set the ``correct'' parameter. The effectiveness of our approach is justified by both theoretical analysis and empirical results on benchmark datasets."
Multiview Spectral Clustering via Pareto Optimization,"Traditionally, the input of spectral clustering is limited to single-view data. However, many real-world datasets come with multiple heterogeneous feature sets, which provide multiple views of the same data. Such datasets include scientific data (fMRI scans of different individuals), social data (different types of connections between people), web data (multi-type data), and so on. How to optimally combine knowledge from multiple views to help spectral clustering find a better partition remains a developing area. Previous work formulates the problem as a single objective function to optimize, typically by combining the views under a compatibility assumption and requiring the users to decide the importance of each view a priori. In this work, we propose a multi-objective formulation and show how to solve it using Pareto optimization. The Pareto frontier captures all possible good cuts without requiring the users to set the ``correct'' parameter. The effectiveness of our approach is justified by both theoretical analysis and empirical results on benchmark datasets."
A New Formulation for Deep Neural Net Optimization,"Deep neural nets are very difficult to train, even with parallel computers, because of the ill-conditioned nature of their objective function, which involves a deeply nested mapping from inputs to outputs. We propose a new formulation to optimize deep nets that directly addresses the ill-conditioning problem, based on an idea of auxiliary variables. This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. We solve the constrained problem with a quadratic-penalty approach using alternating optimization over the weights and the auxiliary coordinates. This procedure decouples into many independent subproblems and allows a trivial parallelization. In experiments using autoencoders of varying depth with image and speech data, we show our algorithm far outperforms all leading methods for deep net optimization."
A New Formulation for Deep Neural Net Optimization,"Deep neural nets are very difficult to train, even with parallel computers, because of the ill-conditioned nature of their objective function, which involves a deeply nested mapping from inputs to outputs. We propose a new formulation to optimize deep nets that directly addresses the ill-conditioning problem, based on an idea of auxiliary variables. This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. We solve the constrained problem with a quadratic-penalty approach using alternating optimization over the weights and the auxiliary coordinates. This procedure decouples into many independent subproblems and allows a trivial parallelization. In experiments using autoencoders of varying depth with image and speech data, we show our algorithm far outperforms all leading methods for deep net optimization."
L$^{\natural}$-CCCP : L$^{\natural}$-Concave Convex Procedure,"L$^{\natural}$-Convexity is a discrete counterpart of convexity in a continuous function. In this paper, we propose L$^{\natural}$-CCCP (L$^{\natural}$-ConCave Convex Procedure); an approximation algorithm for minimizing the difference of two L$^{\natural}$-convex functions, which can formulate any discrete function optimization. L$^{\natural}$-CCCP is basically a discrete analog of CCCP (ConCave Convex Procedure) for D.C.\@ programing problems. L$^{\natural}$-CCCP is performed as a terminating iterative procedure, each of whose iterations can be computed in the same order as submodular minimization. We~describe the implementation of L$^{\natural}$-CCCP and prove termination at a stationary point. Moreover, we describe an application of L$^{\natural}$-CCCP to multi-label energy minimization with empirical examples."
L$^{\natural}$-CCCP : L$^{\natural}$-Concave Convex Procedure,"L$^{\natural}$-Convexity is a discrete counterpart of convexity in a continuous function. In this paper, we propose L$^{\natural}$-CCCP (L$^{\natural}$-ConCave Convex Procedure); an approximation algorithm for minimizing the difference of two L$^{\natural}$-convex functions, which can formulate any discrete function optimization. L$^{\natural}$-CCCP is basically a discrete analog of CCCP (ConCave Convex Procedure) for D.C.\@ programing problems. L$^{\natural}$-CCCP is performed as a terminating iterative procedure, each of whose iterations can be computed in the same order as submodular minimization. We~describe the implementation of L$^{\natural}$-CCCP and prove termination at a stationary point. Moreover, we describe an application of L$^{\natural}$-CCCP to multi-label energy minimization with empirical examples."
"Combinatorial Multi-Armed Bandit: General Framework, Results and Applications","We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where multiple arms with unknown distributions organized in some unknown combinatorial structure to form super arms, each of which is a unit of play in each round. After a super arm is played, the outcome of related individual arms is revealed, and a reward for the super arm is given. The reward function only needs to satisfy a couple of mild assumptions and is otherwise unknown. Instead of knowing the specifics of the problem instance, we assume the availability of a computation oracle that takes the means of the distributions of arms and outputs a super arm that generates an $\alpha$-approximation of the optimal expected reward. The objective of a CMAB algorithm is to minimize {\em $\alpha$-approximation regret}, which is the difference in total expected reward between the $\alpha$ fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide two algorithms, CUCB and $\varepsilon_n$-greedy, and show that they achieve low regret of $O(\log n)$, where $n$ is the number of rounds played. We then demonstrate applications of our CMAB framework with several problem instances, namely probabilistic maximum coverage (PMC), social influence maximization, and matching bandit. The results on the first two problems are new, while the result on the last one matches the regret of a previous work that designed specifically for this problem."
"Combinatorial Multi-Armed Bandit: General Framework, Results and Applications","We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where multiple arms with unknown distributions organized in some unknown combinatorial structure to form super arms, each of which is a unit of play in each round. After a super arm is played, the outcome of related individual arms is revealed, and a reward for the super arm is given. The reward function only needs to satisfy a couple of mild assumptions and is otherwise unknown. Instead of knowing the specifics of the problem instance, we assume the availability of a computation oracle that takes the means of the distributions of arms and outputs a super arm that generates an $\alpha$-approximation of the optimal expected reward. The objective of a CMAB algorithm is to minimize {\em $\alpha$-approximation regret}, which is the difference in total expected reward between the $\alpha$ fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide two algorithms, CUCB and $\varepsilon_n$-greedy, and show that they achieve low regret of $O(\log n)$, where $n$ is the number of rounds played. We then demonstrate applications of our CMAB framework with several problem instances, namely probabilistic maximum coverage (PMC), social influence maximization, and matching bandit. The results on the first two problems are new, while the result on the last one matches the regret of a previous work that designed specifically for this problem."
"Combinatorial Multi-Armed Bandit: General Framework, Results and Applications","We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where multiple arms with unknown distributions organized in some unknown combinatorial structure to form super arms, each of which is a unit of play in each round. After a super arm is played, the outcome of related individual arms is revealed, and a reward for the super arm is given. The reward function only needs to satisfy a couple of mild assumptions and is otherwise unknown. Instead of knowing the specifics of the problem instance, we assume the availability of a computation oracle that takes the means of the distributions of arms and outputs a super arm that generates an $\alpha$-approximation of the optimal expected reward. The objective of a CMAB algorithm is to minimize {\em $\alpha$-approximation regret}, which is the difference in total expected reward between the $\alpha$ fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide two algorithms, CUCB and $\varepsilon_n$-greedy, and show that they achieve low regret of $O(\log n)$, where $n$ is the number of rounds played. We then demonstrate applications of our CMAB framework with several problem instances, namely probabilistic maximum coverage (PMC), social influence maximization, and matching bandit. The results on the first two problems are new, while the result on the last one matches the regret of a previous work that designed specifically for this problem."
"Combinatorial Multi-Armed Bandit: General Framework, Results and Applications","We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where multiple arms with unknown distributions organized in some unknown combinatorial structure to form super arms, each of which is a unit of play in each round. After a super arm is played, the outcome of related individual arms is revealed, and a reward for the super arm is given. The reward function only needs to satisfy a couple of mild assumptions and is otherwise unknown. Instead of knowing the specifics of the problem instance, we assume the availability of a computation oracle that takes the means of the distributions of arms and outputs a super arm that generates an $\alpha$-approximation of the optimal expected reward. The objective of a CMAB algorithm is to minimize {\em $\alpha$-approximation regret}, which is the difference in total expected reward between the $\alpha$ fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide two algorithms, CUCB and $\varepsilon_n$-greedy, and show that they achieve low regret of $O(\log n)$, where $n$ is the number of rounds played. We then demonstrate applications of our CMAB framework with several problem instances, namely probabilistic maximum coverage (PMC), social influence maximization, and matching bandit. The results on the first two problems are new, while the result on the last one matches the regret of a previous work that designed specifically for this problem."
High Dimensional Transelliptical Graphical Models,"We advocate the use of a new distribution family--the transelliptical--for robust inference of high dimensional graphical models. The transelliptical family is an extension of the nonparanormal family proposed by Liu et al. (2009). Just as the nonparanormal extends the normal by transforming the variables using univariate functions, the transelliptical extends the elliptical family in the same way. We propose a nonparametric rank-based regularization estimator which achieves the parametric rates of convergence for both graph recovery and parameter estimation. Such a result suggests that the extra robustness and flexibility obtained by the semiparametric transelliptical modeling incurs almost no efficiency loss. Thorough numerical experiments are provided to back up our theory."
High Dimensional Transelliptical Graphical Models,"We advocate the use of a new distribution family--the transelliptical--for robust inference of high dimensional graphical models. The transelliptical family is an extension of the nonparanormal family proposed by Liu et al. (2009). Just as the nonparanormal extends the normal by transforming the variables using univariate functions, the transelliptical extends the elliptical family in the same way. We propose a nonparametric rank-based regularization estimator which achieves the parametric rates of convergence for both graph recovery and parameter estimation. Such a result suggests that the extra robustness and flexibility obtained by the semiparametric transelliptical modeling incurs almost no efficiency loss. Thorough numerical experiments are provided to back up our theory."
Adaptive Segmental Alignment for Improved Sequence Classification,"Traditional pairwise sequence alignment is based on matching individual samples from two sequences, under time monotonicity constraints. However, in some instances matching segments of points may be preferred and can result in increased noise robustness. This paper presents an approach to segmental sequence alignment based on adaptive pairwise segmentation. We introduce a distance metric between segments based on average pairwise distances, which addresses deficiencies of prior approaches. We then present a modified pair-HMM that incorporates the proposed distance metric  and uses it to devise an efficient algorithm to jointly segment and align the two sequences. Our results demonstrate that this new measure of sequence similarity can lead to improved classification performance, while being resilient to noise, on a variety of problems, from EEG to motion sequence classification."
Adaptive Segmental Alignment for Improved Sequence Classification,"Traditional pairwise sequence alignment is based on matching individual samples from two sequences, under time monotonicity constraints. However, in some instances matching segments of points may be preferred and can result in increased noise robustness. This paper presents an approach to segmental sequence alignment based on adaptive pairwise segmentation. We introduce a distance metric between segments based on average pairwise distances, which addresses deficiencies of prior approaches. We then present a modified pair-HMM that incorporates the proposed distance metric  and uses it to devise an efficient algorithm to jointly segment and align the two sequences. Our results demonstrate that this new measure of sequence similarity can lead to improved classification performance, while being resilient to noise, on a variety of problems, from EEG to motion sequence classification."
Kernel Latent SVM for Visual Recognition,"Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning."
Kernel Latent SVM for Visual Recognition,"Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning."
Learning Partially Observable Models Using Temporally Abstract  Decision Trees,"This paper introduces timeline trees, which are partial models of partially observable environments. Timeline trees are given some specific predictions to make and learn a decision tree over history. The main idea of timeline trees is to use temporally abstract features to identify and split on features of key events, spread arbitrarily far apart in the past (whereas previous decision-tree-based methods have been limited to a finite suffix of history). Experiments demonstrate that timeline trees can learn to make high quality predictions in complex, partially observable environments with high-dimensional observations (e.g. an arcade game)."
Tumor Gene Expression Purification Using Infinite Mixture Topic Models,"There is significant interest in using gene expression measurements to aid in the personalization of medical treatment.  The presence of significant normal tissue contamination in tumor samples makes it difficult to use tumor expression measurements to predict clinical variables and treatment response.  We propose a probabilistic method, TMMPure, to infer the expression profile of the cancerous tissue using a modified topic model that contains a hierarchical Dirichlet process prior on the cancer profiles. We demonstrate that TMMpure is able to infer the expression profile of cancerous tissue and improves the power of predictive models for clinical variables using expression profiles. "
Tumor Gene Expression Purification Using Infinite Mixture Topic Models,"There is significant interest in using gene expression measurements to aid in the personalization of medical treatment.  The presence of significant normal tissue contamination in tumor samples makes it difficult to use tumor expression measurements to predict clinical variables and treatment response.  We propose a probabilistic method, TMMPure, to infer the expression profile of the cancerous tissue using a modified topic model that contains a hierarchical Dirichlet process prior on the cancer profiles. We demonstrate that TMMpure is able to infer the expression profile of cancerous tissue and improves the power of predictive models for clinical variables using expression profiles. "
Manifold Alignment Preserving Global Geometry,"This paper proposes a novel algorithm for manifold alignment preserving global geometry. This approach constructs mapping functions that project data instances from different input domains to a new lower-dimensional space, simultaneously matching the instances in correspondence and preserving global distances between instances within the original domains. In contrast to previous approaches, which are largely based on preserving local geometry, the proposed approach is suited to applications where the global manifold geometry needs to be respected. We evaluate the effectiveness of our algorithm for transfer learning in two real-world cross-lingual information retrieval tasks."
Manifold Alignment Preserving Global Geometry,"This paper proposes a novel algorithm for manifold alignment preserving global geometry. This approach constructs mapping functions that project data instances from different input domains to a new lower-dimensional space, simultaneously matching the instances in correspondence and preserving global distances between instances within the original domains. In contrast to previous approaches, which are largely based on preserving local geometry, the proposed approach is suited to applications where the global manifold geometry needs to be respected. We evaluate the effectiveness of our algorithm for transfer learning in two real-world cross-lingual information retrieval tasks."
A simple algorithm for learning a globally optimal chain event graph from data,"Chain event graphs are a model family particularly suited for asymmetric causal discrete domains. This paper describes a simple algorithm for exact learning of chain event graphs from multivariate data. While the exact algorithm is slow, it allows reasonably fast approximations and provides clues for implementing more scalable heuristic algorithms."
A simple algorithm for learning a globally optimal chain event graph from data,"Chain event graphs are a model family particularly suited for asymmetric causal discrete domains. This paper describes a simple algorithm for exact learning of chain event graphs from multivariate data. While the exact algorithm is slow, it allows reasonably fast approximations and provides clues for implementing more scalable heuristic algorithms."
A scalable direct formulation for multi-class boosting,"We present a scalable and effective classification model for multi-class boosting classification. In [15], a direct formulation for multi-class boosting was introduced. Unlike many existing multi-class boosting algorithms, which rely on output coding matrices, the approach in [15] directly maximizes the multi-class margin. The major problem of [15] is its extremely high computational complexity, which hampers its application on real-world problems. In this work, we propose a scalable and simple stage-wise boosting method, which also directly maximizes the multi-class margin. Our approach has several advantages: (1) it is simple and computationally efficient to train. The approach can speed up the training time by more than two orders of magnitude without sacrificing the classification accuracy. (2) Like traditional AdaBoost, it is parameter free and empirically demonstrates excellent generalization performance. In contrast, one has to tune the regularization parameter for the multi-class boosting of [15]. Experimental results on challenging multi-class machine learning and vision tasks demonstrate that the proposed approach substantially improves the convergence rate and accuracy of the final visual detector at no additional computational cost."
Regularized Off-Policy TD-Learning,"We present a novel $l_1$ regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization.A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm."
Regularized Off-Policy TD-Learning,"We present a novel $l_1$ regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization.A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm."
Guided Network Discovery in fMRI Data via Constrained Tensor Analysis ,We investigate the problem of network discovery which involves simplifying spatio-temporal data into nodes and edges. Such problems naturally exist in fMRI scans of human subjects which consist of activations of thousands of voxels over time with the aim to simplify them into the underlying cognitive network being used. We propose supervised and semi-supervised variations of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least squares solver that is easily implementable. For real fMRI data our approach can reproduce well known results in neurology regarding the default mode network in resting state healthy and Alzheimer affected individuals. We show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive scores both by itself and with additional clinical information.
Link Prediction via Ranking with a Multiple Membership  Nonparametric Bayesian Model,"Link prediction in complex networks has found applications in a wide range of real-world domains involving relational data.  The goal is to predict some hidden relations between individuals based on the observed relations.  Existing models are unsatisfactory when more general multiple membership in latent groups can be found in the network data.  Taking the nonparametric Bayesian approach, we propose a multiple membership latent group model for link prediction.  Besides, we argue that existing performance evaluation methods for link prediction, which regard it as a binary classification problem, do not satisfy the nature of the problem.  As another contribution of this work, we propose a new evaluation method by regarding link prediction as ranking.  Based on this new evaluation method, we compare the proposed model with two related state-of-the-art models and find that the proposed model can learn more compact structure from the network data."
Link Prediction via Ranking with a Multiple Membership  Nonparametric Bayesian Model,"Link prediction in complex networks has found applications in a wide range of real-world domains involving relational data.  The goal is to predict some hidden relations between individuals based on the observed relations.  Existing models are unsatisfactory when more general multiple membership in latent groups can be found in the network data.  Taking the nonparametric Bayesian approach, we propose a multiple membership latent group model for link prediction.  Besides, we argue that existing performance evaluation methods for link prediction, which regard it as a binary classification problem, do not satisfy the nature of the problem.  As another contribution of this work, we propose a new evaluation method by regarding link prediction as ranking.  Based on this new evaluation method, we compare the proposed model with two related state-of-the-art models and find that the proposed model can learn more compact structure from the network data."
Modeling Visual Clutter Using Parametric Proto-object Segmentation,"Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects defined by superpixel similarity in intensity, color, and texture, features. We introduce a novel parametric method of merging superpixels using mixtures of Weibull distributions of edge weights, then take the normalized number of proto-objects following partitioning as our estimate of clutter. We validated the model using clutter ratings of 90 images (SUN Dataset) obtained from humans, and showed that our method not only predicted clutter extremely well (r=0.76, p<0.001), but also outperformed other clutter prediction methods."
Modeling Visual Clutter Using Parametric Proto-object Segmentation,"Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects defined by superpixel similarity in intensity, color, and texture, features. We introduce a novel parametric method of merging superpixels using mixtures of Weibull distributions of edge weights, then take the normalized number of proto-objects following partitioning as our estimate of clutter. We validated the model using clutter ratings of 90 images (SUN Dataset) obtained from humans, and showed that our method not only predicted clutter extremely well (r=0.76, p<0.001), but also outperformed other clutter prediction methods."
Modeling Visual Clutter Using Parametric Proto-object Segmentation,"Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects defined by superpixel similarity in intensity, color, and texture, features. We introduce a novel parametric method of merging superpixels using mixtures of Weibull distributions of edge weights, then take the normalized number of proto-objects following partitioning as our estimate of clutter. We validated the model using clutter ratings of 90 images (SUN Dataset) obtained from humans, and showed that our method not only predicted clutter extremely well (r=0.76, p<0.001), but also outperformed other clutter prediction methods."
A System for Predicting Action Content On-Line and in Real Time before Action Onset in Humans ? an Intracranial Study,"The ability to predict action content from neural signals in real time before action onset has been long sought in the neuroscientific study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a ?matching-pennies? game against either the experimenter or a computer. In each trial, subjects were given a 5s countdown, after which they had to raise their left or right hand immediately as the ?go? signal appeared on a computer screen. They won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The working hypothesis of this experiment was that neural precursors of the subjects? decisions precede action onset and potentially also the awareness of the decision to move, and that these signals could be detected in intracranial local field potentials (LFP).We found that low-frequency LFP signals from a combination of 10 channels, especially bilateral anterior cingulate cortex and supplementary motor area, were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5s before the go signal with 68?3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 channels simultaneously, and tested it on retrospective data from 6 patients. On average, we could predict the correct hand choice in 80% of the trials, which rose to 90% correct if we let the system drop about 1/3 of the trials on which it was less confident. Our system demonstrates ? for the first time ? the feasibility of accurately predicting a binary action in real time for patients with intracranial recordings, well before the action occurs."
A System for Predicting Action Content On-Line and in Real Time before Action Onset in Humans ? an Intracranial Study,"The ability to predict action content from neural signals in real time before action onset has been long sought in the neuroscientific study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a ?matching-pennies? game against either the experimenter or a computer. In each trial, subjects were given a 5s countdown, after which they had to raise their left or right hand immediately as the ?go? signal appeared on a computer screen. They won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The working hypothesis of this experiment was that neural precursors of the subjects? decisions precede action onset and potentially also the awareness of the decision to move, and that these signals could be detected in intracranial local field potentials (LFP).We found that low-frequency LFP signals from a combination of 10 channels, especially bilateral anterior cingulate cortex and supplementary motor area, were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5s before the go signal with 68?3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 channels simultaneously, and tested it on retrospective data from 6 patients. On average, we could predict the correct hand choice in 80% of the trials, which rose to 90% correct if we let the system drop about 1/3 of the trials on which it was less confident. Our system demonstrates ? for the first time ? the feasibility of accurately predicting a binary action in real time for patients with intracranial recordings, well before the action occurs."
A System for Predicting Action Content On-Line and in Real Time before Action Onset in Humans ? an Intracranial Study,"The ability to predict action content from neural signals in real time before action onset has been long sought in the neuroscientific study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a ?matching-pennies? game against either the experimenter or a computer. In each trial, subjects were given a 5s countdown, after which they had to raise their left or right hand immediately as the ?go? signal appeared on a computer screen. They won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The working hypothesis of this experiment was that neural precursors of the subjects? decisions precede action onset and potentially also the awareness of the decision to move, and that these signals could be detected in intracranial local field potentials (LFP).We found that low-frequency LFP signals from a combination of 10 channels, especially bilateral anterior cingulate cortex and supplementary motor area, were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5s before the go signal with 68?3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 channels simultaneously, and tested it on retrospective data from 6 patients. On average, we could predict the correct hand choice in 80% of the trials, which rose to 90% correct if we let the system drop about 1/3 of the trials on which it was less confident. Our system demonstrates ? for the first time ? the feasibility of accurately predicting a binary action in real time for patients with intracranial recordings, well before the action occurs."
A System for Predicting Action Content On-Line and in Real Time before Action Onset in Humans ? an Intracranial Study,"The ability to predict action content from neural signals in real time before action onset has been long sought in the neuroscientific study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a ?matching-pennies? game against either the experimenter or a computer. In each trial, subjects were given a 5s countdown, after which they had to raise their left or right hand immediately as the ?go? signal appeared on a computer screen. They won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The working hypothesis of this experiment was that neural precursors of the subjects? decisions precede action onset and potentially also the awareness of the decision to move, and that these signals could be detected in intracranial local field potentials (LFP).We found that low-frequency LFP signals from a combination of 10 channels, especially bilateral anterior cingulate cortex and supplementary motor area, were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5s before the go signal with 68?3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 channels simultaneously, and tested it on retrospective data from 6 patients. On average, we could predict the correct hand choice in 80% of the trials, which rose to 90% correct if we let the system drop about 1/3 of the trials on which it was less confident. Our system demonstrates ? for the first time ? the feasibility of accurately predicting a binary action in real time for patients with intracranial recordings, well before the action occurs."
"Constrain, Train, Validate and Explain: A Classifier for Mission-Critical Applications","Classifiers used in mission-critical applications, where misclassification errors incurhigh costs, should be robust to training-set artifacts, such as insufficient ormisrepresentative coverage, as well as high levels of missing values. As such,they are required to support intensive designer-control, and a range of validationprocedures that go beyond cross-validation. For such applications, we advocatethe use of a family of classifiers that employ a factored model of the posteriorclass probabilities. These classifiers are simple, interpretable, allow their designersto enforce a variety of domain-specific constraints, and can handle missingdata both during training and at prediction time. Such classifiers are also capableof explaining their decisions in terms of the basic measured quantities."
"Constrain, Train, Validate and Explain: A Classifier for Mission-Critical Applications","Classifiers used in mission-critical applications, where misclassification errors incurhigh costs, should be robust to training-set artifacts, such as insufficient ormisrepresentative coverage, as well as high levels of missing values. As such,they are required to support intensive designer-control, and a range of validationprocedures that go beyond cross-validation. For such applications, we advocatethe use of a family of classifiers that employ a factored model of the posteriorclass probabilities. These classifiers are simple, interpretable, allow their designersto enforce a variety of domain-specific constraints, and can handle missingdata both during training and at prediction time. Such classifiers are also capableof explaining their decisions in terms of the basic measured quantities."
Emergence of Flexible Prediction-Based Discrete Decision Making and Continuous Motion Generation through Actor-Q-Learning,"In this paper, through the learning of invisible-target-capturing task by Actor-Q-learning with a recurrent neural network (RNN), the followings are shown.(1) Prediction-based continuous motions that should be varied complicatedly by several factors can be acquired only from rewards and punishments through reinforcement learning (RL) with a RNN.(2) Actor-Q-learning that is a RL method for both discrete decision(action) making and continuous motion generation works in a task other than active perception-and-recognition. It can also learn prediction-required action and motion by using a RNN.(3) As initial connection weights in RNN, local and regular connection and positive self-feedback connection improve the performance of learning with vision-like sensor inputs.(4) Two problem-solving strategies one of which is selectively used can be acquired in just one neural network as a parallel processing-and-learning system only from rewards and punishments through RL. The strategy-switching caused by situation changes also emerges without any explicit switching element or any prior knowledge."
Bayesian Estimation for Partially Observed MRFs,Bayesian estimation in Markov random fields is very hard due to the intractability of the partition function. The introduction of hidden units makes the situation even worse due to the presence of potentially very many modes in the posterior distribution. For the first time we propose a comprehensive procedure to approximate the evidence of partially observed MRFs based on the Laplace approximation. We also introduce a number of approximate MCMC-based methods for comparison but find that the Laplace approximation significantly outperforms these.
Bayesian Estimation for Partially Observed MRFs,Bayesian estimation in Markov random fields is very hard due to the intractability of the partition function. The introduction of hidden units makes the situation even worse due to the presence of potentially very many modes in the posterior distribution. For the first time we propose a comprehensive procedure to approximate the evidence of partially observed MRFs based on the Laplace approximation. We also introduce a number of approximate MCMC-based methods for comparison but find that the Laplace approximation significantly outperforms these.
Searching for objects driven by context,"The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired.We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image. Their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set.In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy."
Searching for objects driven by context,"The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired.We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image. Their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set.In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy."
Searching for objects driven by context,"The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired.We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image. Their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set.In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy."
Time-Dependent Subset Mining via iHMM,"When we analyze a time series data with large cardinality of item sets (or, high-dimensional feature vectors), it is often the case that a pile of non-informativeitems disturbs mining of hidden dynamic patterns behind the data. In such cases, we need to find and extract a tiny portion of informative items that are related(informative) to the change of hidden patterns. In this paper, we address the problem of modeling time series data that are characterized by the large cardinalityof item sets and sparseness of related items among them, where the relatedness of items are time-dependent. We propose an extension of the infinite Hidden Markov Model so that only related items of each hidden state be automatically selected. We can improve estimation of hidden states by excluding unrelated (non-informative) items and focusing on the latent structure analysis of extracted related items. Combined with the nonparametric Bayes approach, the proposed model simultaneously selects related items and finds hidden states without knowing thenumber of states beforehand. The proposed model is experimentally verified and we show that both relatedness and time dependency are important for mining related items of time series data."
Time-Dependent Subset Mining via iHMM,"When we analyze a time series data with large cardinality of item sets (or, high-dimensional feature vectors), it is often the case that a pile of non-informativeitems disturbs mining of hidden dynamic patterns behind the data. In such cases, we need to find and extract a tiny portion of informative items that are related(informative) to the change of hidden patterns. In this paper, we address the problem of modeling time series data that are characterized by the large cardinalityof item sets and sparseness of related items among them, where the relatedness of items are time-dependent. We propose an extension of the infinite Hidden Markov Model so that only related items of each hidden state be automatically selected. We can improve estimation of hidden states by excluding unrelated (non-informative) items and focusing on the latent structure analysis of extracted related items. Combined with the nonparametric Bayes approach, the proposed model simultaneously selects related items and finds hidden states without knowing thenumber of states beforehand. The proposed model is experimentally verified and we show that both relatedness and time dependency are important for mining related items of time series data."
Time-Dependent Subset Mining via iHMM,"When we analyze a time series data with large cardinality of item sets (or, high-dimensional feature vectors), it is often the case that a pile of non-informativeitems disturbs mining of hidden dynamic patterns behind the data. In such cases, we need to find and extract a tiny portion of informative items that are related(informative) to the change of hidden patterns. In this paper, we address the problem of modeling time series data that are characterized by the large cardinalityof item sets and sparseness of related items among them, where the relatedness of items are time-dependent. We propose an extension of the infinite Hidden Markov Model so that only related items of each hidden state be automatically selected. We can improve estimation of hidden states by excluding unrelated (non-informative) items and focusing on the latent structure analysis of extracted related items. Combined with the nonparametric Bayes approach, the proposed model simultaneously selects related items and finds hidden states without knowing thenumber of states beforehand. The proposed model is experimentally verified and we show that both relatedness and time dependency are important for mining related items of time series data."
Stationary Component Analysis for Video Classification,"Low-dimensional modelling of videos is key to the success of efficient video classification algorithms.From a signal processing perspective, video data is a mixture of stationary and non-stationary components.While videos belonging to the same class share the same stationary components,they may differ in the non-stationary ones.Existing video classification methods based on dimensionality reductiontypically fail to explicitly separate stationary parts of the signal from non-stationary ones.As a consequence, the low-dimensionality representation of the videos contains information that is not class-specific,and thus irrelevant for classification.We propose an approach to video modelling that overcomes this issue by explicitly separatingstationary from non-stationary parts of the signal.To this end, we make use of Stationary Subspace Analysis,which factorises multivariate time series into stationary and non-stationary components.Video classification can be then performed based only on the relevant, stationary part of the video signal.We demonstrate the benefits of our approach over state-of-the-art techniques on the tasks of near-duplicate video detection,dynamic texture classification and scene recognition.Our study shows that modelling videos with their stationary components not only dramatically improves recognition accuracy,but also yields a significant advantage in computational cost of classification over existing methods."
Stationary Component Analysis for Video Classification,"Low-dimensional modelling of videos is key to the success of efficient video classification algorithms.From a signal processing perspective, video data is a mixture of stationary and non-stationary components.While videos belonging to the same class share the same stationary components,they may differ in the non-stationary ones.Existing video classification methods based on dimensionality reductiontypically fail to explicitly separate stationary parts of the signal from non-stationary ones.As a consequence, the low-dimensionality representation of the videos contains information that is not class-specific,and thus irrelevant for classification.We propose an approach to video modelling that overcomes this issue by explicitly separatingstationary from non-stationary parts of the signal.To this end, we make use of Stationary Subspace Analysis,which factorises multivariate time series into stationary and non-stationary components.Video classification can be then performed based only on the relevant, stationary part of the video signal.We demonstrate the benefits of our approach over state-of-the-art techniques on the tasks of near-duplicate video detection,dynamic texture classification and scene recognition.Our study shows that modelling videos with their stationary components not only dramatically improves recognition accuracy,but also yields a significant advantage in computational cost of classification over existing methods."
Stationary Component Analysis for Video Classification,"Low-dimensional modelling of videos is key to the success of efficient video classification algorithms.From a signal processing perspective, video data is a mixture of stationary and non-stationary components.While videos belonging to the same class share the same stationary components,they may differ in the non-stationary ones.Existing video classification methods based on dimensionality reductiontypically fail to explicitly separate stationary parts of the signal from non-stationary ones.As a consequence, the low-dimensionality representation of the videos contains information that is not class-specific,and thus irrelevant for classification.We propose an approach to video modelling that overcomes this issue by explicitly separatingstationary from non-stationary parts of the signal.To this end, we make use of Stationary Subspace Analysis,which factorises multivariate time series into stationary and non-stationary components.Video classification can be then performed based only on the relevant, stationary part of the video signal.We demonstrate the benefits of our approach over state-of-the-art techniques on the tasks of near-duplicate video detection,dynamic texture classification and scene recognition.Our study shows that modelling videos with their stationary components not only dramatically improves recognition accuracy,but also yields a significant advantage in computational cost of classification over existing methods."
Stationary Component Analysis for Video Classification,"Low-dimensional modelling of videos is key to the success of efficient video classification algorithms.From a signal processing perspective, video data is a mixture of stationary and non-stationary components.While videos belonging to the same class share the same stationary components,they may differ in the non-stationary ones.Existing video classification methods based on dimensionality reductiontypically fail to explicitly separate stationary parts of the signal from non-stationary ones.As a consequence, the low-dimensionality representation of the videos contains information that is not class-specific,and thus irrelevant for classification.We propose an approach to video modelling that overcomes this issue by explicitly separatingstationary from non-stationary parts of the signal.To this end, we make use of Stationary Subspace Analysis,which factorises multivariate time series into stationary and non-stationary components.Video classification can be then performed based only on the relevant, stationary part of the video signal.We demonstrate the benefits of our approach over state-of-the-art techniques on the tasks of near-duplicate video detection,dynamic texture classification and scene recognition.Our study shows that modelling videos with their stationary components not only dramatically improves recognition accuracy,but also yields a significant advantage in computational cost of classification over existing methods."
Stationary Component Analysis for Video Classification,"Low-dimensional modelling of videos is key to the success of efficient video classification algorithms.From a signal processing perspective, video data is a mixture of stationary and non-stationary components.While videos belonging to the same class share the same stationary components,they may differ in the non-stationary ones.Existing video classification methods based on dimensionality reductiontypically fail to explicitly separate stationary parts of the signal from non-stationary ones.As a consequence, the low-dimensionality representation of the videos contains information that is not class-specific,and thus irrelevant for classification.We propose an approach to video modelling that overcomes this issue by explicitly separatingstationary from non-stationary parts of the signal.To this end, we make use of Stationary Subspace Analysis,which factorises multivariate time series into stationary and non-stationary components.Video classification can be then performed based only on the relevant, stationary part of the video signal.We demonstrate the benefits of our approach over state-of-the-art techniques on the tasks of near-duplicate video detection,dynamic texture classification and scene recognition.Our study shows that modelling videos with their stationary components not only dramatically improves recognition accuracy,but also yields a significant advantage in computational cost of classification over existing methods."
Timely Object Recognition,"In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the eminent PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains $66\%$ better AP than a random ordering, and $14\%$ better performance than an intelligent baseline. On the timeliness measure, our method obtains at least $11\%$ better performance. Our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning."
Multi-Domain Manifold Learning for Interaction Prediction,"Despite of the high dimensionality in many machine learning problems, data distributions in these high dimensional spaces usually span on certain low dimensional manifolds. In the last few years, extensive research efforts have been devoted to the utilization of manifold property on high dimensional data, e.g. dimension reduction methods preserving local structures of the manifolds. Motivated by the successes of these studies, we extend the manifold learning problem from single domain to multiple domain, especially on learning cross-domain interactive pairs. While many real-world applications (e.g. automatic image annotation) can be modelled as multi-domain interaction prediction, existing solutions to manifold learning fail to fully exploit the manifold property of the distributions. In this paper, we propose a general framework to bridge the gap, taking both manifold structures and known interaction/non-interaction information into account. To overcome the challenges of domain scaling and information inconsistency, we formulate the problem with Semidefinite Programming(SDP), including new constraints to improve the robustness of the learning procedure. A variety of optimization techniques are also designed to enhance the scalability of the problem solver. Effectiveness of the method is evaluated by experiments with two different tasks, including drug prediction and image annotation."
Multi-Domain Manifold Learning for Interaction Prediction,"Despite of the high dimensionality in many machine learning problems, data distributions in these high dimensional spaces usually span on certain low dimensional manifolds. In the last few years, extensive research efforts have been devoted to the utilization of manifold property on high dimensional data, e.g. dimension reduction methods preserving local structures of the manifolds. Motivated by the successes of these studies, we extend the manifold learning problem from single domain to multiple domain, especially on learning cross-domain interactive pairs. While many real-world applications (e.g. automatic image annotation) can be modelled as multi-domain interaction prediction, existing solutions to manifold learning fail to fully exploit the manifold property of the distributions. In this paper, we propose a general framework to bridge the gap, taking both manifold structures and known interaction/non-interaction information into account. To overcome the challenges of domain scaling and information inconsistency, we formulate the problem with Semidefinite Programming(SDP), including new constraints to improve the robustness of the learning procedure. A variety of optimization techniques are also designed to enhance the scalability of the problem solver. Effectiveness of the method is evaluated by experiments with two different tasks, including drug prediction and image annotation."
Multi-Domain Manifold Learning for Interaction Prediction,"Despite of the high dimensionality in many machine learning problems, data distributions in these high dimensional spaces usually span on certain low dimensional manifolds. In the last few years, extensive research efforts have been devoted to the utilization of manifold property on high dimensional data, e.g. dimension reduction methods preserving local structures of the manifolds. Motivated by the successes of these studies, we extend the manifold learning problem from single domain to multiple domain, especially on learning cross-domain interactive pairs. While many real-world applications (e.g. automatic image annotation) can be modelled as multi-domain interaction prediction, existing solutions to manifold learning fail to fully exploit the manifold property of the distributions. In this paper, we propose a general framework to bridge the gap, taking both manifold structures and known interaction/non-interaction information into account. To overcome the challenges of domain scaling and information inconsistency, we formulate the problem with Semidefinite Programming(SDP), including new constraints to improve the robustness of the learning procedure. A variety of optimization techniques are also designed to enhance the scalability of the problem solver. Effectiveness of the method is evaluated by experiments with two different tasks, including drug prediction and image annotation."
Multi-Domain Manifold Learning for Interaction Prediction,"Despite of the high dimensionality in many machine learning problems, data distributions in these high dimensional spaces usually span on certain low dimensional manifolds. In the last few years, extensive research efforts have been devoted to the utilization of manifold property on high dimensional data, e.g. dimension reduction methods preserving local structures of the manifolds. Motivated by the successes of these studies, we extend the manifold learning problem from single domain to multiple domain, especially on learning cross-domain interactive pairs. While many real-world applications (e.g. automatic image annotation) can be modelled as multi-domain interaction prediction, existing solutions to manifold learning fail to fully exploit the manifold property of the distributions. In this paper, we propose a general framework to bridge the gap, taking both manifold structures and known interaction/non-interaction information into account. To overcome the challenges of domain scaling and information inconsistency, we formulate the problem with Semidefinite Programming(SDP), including new constraints to improve the robustness of the learning procedure. A variety of optimization techniques are also designed to enhance the scalability of the problem solver. Effectiveness of the method is evaluated by experiments with two different tasks, including drug prediction and image annotation."
Model Selection in Markov Reward Processes,"Algorithms for solving Markov reward processes almost always start with the assumption that the state space is known. In this work we address the problem of how to use data to choose from a set of candidate discrete state spaces, where these spaces are constructed by a problem dependent domain expert. We discuss the difference between our proposed framework and the classical maximum likelihood framework, and give an example for such a criterion to fail. We propose alternative criterion and prove that it is consistent if the models are identifiable in an appropriate sense."
Model Selection in Markov Reward Processes,"Algorithms for solving Markov reward processes almost always start with the assumption that the state space is known. In this work we address the problem of how to use data to choose from a set of candidate discrete state spaces, where these spaces are constructed by a problem dependent domain expert. We discuss the difference between our proposed framework and the classical maximum likelihood framework, and give an example for such a criterion to fail. We propose alternative criterion and prove that it is consistent if the models are identifiable in an appropriate sense."
Nonparanormal Belief Propagation (NPBP),"The empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances. Yet, for continuous non-Gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models.  In this work we present Nonparanormal BP  for performing efficient inference on distributions parameterized by  a Gaussian copulas network and any univariate marginals. For  tree structured networks, our approach is guaranteed to be exact for  this powerful class of non-Gaussian models. Importantly, the method  is as efficient as standard Gaussian BP, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used."
Nonparanormal Belief Propagation (NPBP),"The empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances. Yet, for continuous non-Gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models.  In this work we present Nonparanormal BP  for performing efficient inference on distributions parameterized by  a Gaussian copulas network and any univariate marginals. For  tree structured networks, our approach is guaranteed to be exact for  this powerful class of non-Gaussian models. Importantly, the method  is as efficient as standard Gaussian BP, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used."
Kernelized Bayesian Matrix Factorization,"We extend kernelized matrix factorization with a full-Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (a) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas full-Bayesian treatments are not computationally feasible in the earlier approaches. (b) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our proposed method outperforms alternatives in predicting drug-protein interactions on two data sets."
Kernelized Bayesian Matrix Factorization,"We extend kernelized matrix factorization with a full-Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (a) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas full-Bayesian treatments are not computationally feasible in the earlier approaches. (b) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our proposed method outperforms alternatives in predicting drug-protein interactions on two data sets."
Kernelized Bayesian Matrix Factorization,"We extend kernelized matrix factorization with a full-Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (a) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas full-Bayesian treatments are not computationally feasible in the earlier approaches. (b) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our proposed method outperforms alternatives in predicting drug-protein interactions on two data sets."
Fast Convolutional Sparse Coding,"Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many signals (e.g. visual and acoustic) however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimisation problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and augmented Lagrange multipliers (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence. "
Fast Convolutional Sparse Coding,"Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many signals (e.g. visual and acoustic) however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimisation problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and augmented Lagrange multipliers (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence. "
Learning Non-Linear Sub-Spaces using K-RBMs,"The overall complexity in building descriptive or discriminative models is shared between the features derived from raw data and the models that use these features as inputs. Simple features require complex models while more sophisticated features require simpler models to achieve the same level of model quality. Learning semantically richer features is, therefore, the key to building simpler, more interpretable, and more accurate models. In domains such as images, where the data (image patches) might lie in multiple non-linear manifolds, feature learning becomes even more important. In this paper, we propose a framework that uses K Restricted Boltzmann Machines (K-RBMs) to learn non-linear manifolds in the raw image space. We solve the coupled problem of finding the right non-linear manifolds in the input space and associating image patches with those manifolds in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that such a framework outperforms the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks."
Learning Non-Linear Sub-Spaces using K-RBMs,"The overall complexity in building descriptive or discriminative models is shared between the features derived from raw data and the models that use these features as inputs. Simple features require complex models while more sophisticated features require simpler models to achieve the same level of model quality. Learning semantically richer features is, therefore, the key to building simpler, more interpretable, and more accurate models. In domains such as images, where the data (image patches) might lie in multiple non-linear manifolds, feature learning becomes even more important. In this paper, we propose a framework that uses K Restricted Boltzmann Machines (K-RBMs) to learn non-linear manifolds in the raw image space. We solve the coupled problem of finding the right non-linear manifolds in the input space and associating image patches with those manifolds in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that such a framework outperforms the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks."
Learning Non-Linear Sub-Spaces using K-RBMs,"The overall complexity in building descriptive or discriminative models is shared between the features derived from raw data and the models that use these features as inputs. Simple features require complex models while more sophisticated features require simpler models to achieve the same level of model quality. Learning semantically richer features is, therefore, the key to building simpler, more interpretable, and more accurate models. In domains such as images, where the data (image patches) might lie in multiple non-linear manifolds, feature learning becomes even more important. In this paper, we propose a framework that uses K Restricted Boltzmann Machines (K-RBMs) to learn non-linear manifolds in the raw image space. We solve the coupled problem of finding the right non-linear manifolds in the input space and associating image patches with those manifolds in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that such a framework outperforms the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks."
A New Fast Stochastic Bayesian Learning Automata: a Machine Learning Perspective,"One of the drawbacks of Learning Automata is having a relatively slow rate of convergence, thus the main challenge of Learning Automata theory is designing faster learning algorithms. In this paper, we propose a new fast learning algorithm from a machine learning perspective. The key idea is that the estimator which estimates the probability of stochastic environment rewarding each action, is considered as reconstructing Bernoulli distribution from sequential data, and is formalized based on exponential conjugate family which enables our designed Learning Automata having relatively simple format and hence easy to be implemented. We emphasize that this approach is quite generous and applicable to existing estimator based Learning Automata. Meanwhile,the -optimality of the proposed Learn-ing Automata referred to as Generalized Bayesian Stochastic Estimator Learning Automata is also presented. Extensive experimental results on benchmark environments demonstrate our proposed learning scheme is faster than the LA state of the art."
An Analysis on Ensembles of Neural Networks: a Motivation Behind Low Dimensional Networks,"The accuracy of ensembles have been shown to depend on the base classifier's accuracy and diversity. However, choosing good base classifiers to compose an ensemble is still a difficult task. The base classifier's accuracy-diversity trade-off composes a complex space to search. Moreover, searching for all possible classifiers is naturally unfeasible and some assumptions have to be taken. In this context, we present a motivation behind low dimensional classifiers. The experiments are conducted on varying levels of difficulty (time constraints and addition of irrelevant variables) and different datasets. Based on these experiments, it is verified that an evolutionary ensemble made of a certain portion of low dimensional neural networks can be more robust and accurate than high dimensional ones. Throughout the experiments the influences of diversity, base classifier's accuracy as well as the effects of the ensemble and evolution are analyzed and empirically evaluated. Thus, the use of low dimensional base classifiers is justified as a reasonable assumption for the construction of ensembles, which enables ensembles to achieve the maximum observed accuracy as well as surpass high dimensional ones in robustness and learning speed."
An Analysis on Ensembles of Neural Networks: a Motivation Behind Low Dimensional Networks,"The accuracy of ensembles have been shown to depend on the base classifier's accuracy and diversity. However, choosing good base classifiers to compose an ensemble is still a difficult task. The base classifier's accuracy-diversity trade-off composes a complex space to search. Moreover, searching for all possible classifiers is naturally unfeasible and some assumptions have to be taken. In this context, we present a motivation behind low dimensional classifiers. The experiments are conducted on varying levels of difficulty (time constraints and addition of irrelevant variables) and different datasets. Based on these experiments, it is verified that an evolutionary ensemble made of a certain portion of low dimensional neural networks can be more robust and accurate than high dimensional ones. Throughout the experiments the influences of diversity, base classifier's accuracy as well as the effects of the ensemble and evolution are analyzed and empirically evaluated. Thus, the use of low dimensional base classifiers is justified as a reasonable assumption for the construction of ensembles, which enables ensembles to achieve the maximum observed accuracy as well as surpass high dimensional ones in robustness and learning speed."
An Analysis on Ensembles of Neural Networks: a Motivation Behind Low Dimensional Networks,"The accuracy of ensembles have been shown to depend on the base classifier's accuracy and diversity. However, choosing good base classifiers to compose an ensemble is still a difficult task. The base classifier's accuracy-diversity trade-off composes a complex space to search. Moreover, searching for all possible classifiers is naturally unfeasible and some assumptions have to be taken. In this context, we present a motivation behind low dimensional classifiers. The experiments are conducted on varying levels of difficulty (time constraints and addition of irrelevant variables) and different datasets. Based on these experiments, it is verified that an evolutionary ensemble made of a certain portion of low dimensional neural networks can be more robust and accurate than high dimensional ones. Throughout the experiments the influences of diversity, base classifier's accuracy as well as the effects of the ensemble and evolution are analyzed and empirically evaluated. Thus, the use of low dimensional base classifiers is justified as a reasonable assumption for the construction of ensembles, which enables ensembles to achieve the maximum observed accuracy as well as surpass high dimensional ones in robustness and learning speed."
Identifying Unique Features in Latent Generative Models:  Medicinally-Induced fMRI Networks in Buproprion Trials,"Machine learning and feature evaluation algorithms often assume a direct correspondence of features across observations, with the features themselves well-established and directly measurable. Oftentimes though the true number and form of the features are unknown, and observations are merely a permuted assortment of distorted variables. Four-dimensional neuroimaging data, such as fMRI, can be understood as a collection of brain networks, operating over time.  These networks vary both within and across subjects'  brain scans, and not all brain networks operate during a given scan. Because of this, it is non-trivial to identify generative, consistent features that are unique to certain treatment conditions.  We present a method for learning the true form, uniqueness, and function of latent generative features in situations where the observations are a permuted selection of distorted features, obtained from a higher-level generative set of possible parents. Two competing basis sets are used to model the observed features within the desired treatment condition, and the distribution of the model fits are used to compute a posterior probability for whether the observed feature originated from the set of generative features common across all treatment conditions, or whether that feature exists only within that treatment condition. The observed unique features are then used to create a new generative set that is specific to the treatment condition.  We illustrate these methods by identifying the treatment networks associated with the drug buproprion during a smoking-cessation study, obtaining possible brain networks that exist only post-treatment for subjects who took the medication, but for none of the other treatment groups."
Enhanced statistical rankings via targeted data collection,"We study the dependence of the statistical ranking problem on the available pairwise data and propose a framework for which additional data may be collected to increase the informativeness of the ranking. Given a graph where vertices represent alternatives and pairwise comparison data is given on the edges, the \emph{statistical ranking problem} is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. For each edge $ij$, the pairwise comparison data consists of $w_{ij}$ comparisons between the alternatives $i$ and $j$ and  mean preference, written $y_{ij}$. Our goal, given an existing pairwise comparison dataset, $(w,y)$, is to augment this dataset, denoted $(\tilde w,\tilde y)$, so that  the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximally increases the informativeness of the ranking. Since there is a tradeoff between the amount of pairwise data to be ranked and the informativeness of the ranking, we constrain the total number of additional pairwise comparisons $\|\tilde{w}- w\|_{1}$. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding edge weights such that the $\tilde{w}$-weighted graph Laplacian has large second eigenvalue. This reduction of the data collection problem to spectral graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. "
Enhanced statistical rankings via targeted data collection,"We study the dependence of the statistical ranking problem on the available pairwise data and propose a framework for which additional data may be collected to increase the informativeness of the ranking. Given a graph where vertices represent alternatives and pairwise comparison data is given on the edges, the \emph{statistical ranking problem} is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. For each edge $ij$, the pairwise comparison data consists of $w_{ij}$ comparisons between the alternatives $i$ and $j$ and  mean preference, written $y_{ij}$. Our goal, given an existing pairwise comparison dataset, $(w,y)$, is to augment this dataset, denoted $(\tilde w,\tilde y)$, so that  the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximally increases the informativeness of the ranking. Since there is a tradeoff between the amount of pairwise data to be ranked and the informativeness of the ranking, we constrain the total number of additional pairwise comparisons $\|\tilde{w}- w\|_{1}$. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding edge weights such that the $\tilde{w}$-weighted graph Laplacian has large second eigenvalue. This reduction of the data collection problem to spectral graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. "
Enhanced statistical rankings via targeted data collection,"We study the dependence of the statistical ranking problem on the available pairwise data and propose a framework for which additional data may be collected to increase the informativeness of the ranking. Given a graph where vertices represent alternatives and pairwise comparison data is given on the edges, the \emph{statistical ranking problem} is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. For each edge $ij$, the pairwise comparison data consists of $w_{ij}$ comparisons between the alternatives $i$ and $j$ and  mean preference, written $y_{ij}$. Our goal, given an existing pairwise comparison dataset, $(w,y)$, is to augment this dataset, denoted $(\tilde w,\tilde y)$, so that  the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximally increases the informativeness of the ranking. Since there is a tradeoff between the amount of pairwise data to be ranked and the informativeness of the ranking, we constrain the total number of additional pairwise comparisons $\|\tilde{w}- w\|_{1}$. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding edge weights such that the $\tilde{w}$-weighted graph Laplacian has large second eigenvalue. This reduction of the data collection problem to spectral graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. "
Collaborative Filtering with Hybrid Restricted Boltzmann Machines,"We propose an novel framework for collaborative filtering based on RestrictedBoltzmann Machines (RBM), which extends previous RBM-based approaches inseveral important directions. First, we work with numbers as opposed to usingcategorical variables, which allows us to take the natural order between user-itemratings into account, e.g., we consider predicting a rating of 4 when the actual rat-ing is 5 to be better than predicting a rating of 2. More importantly, while previousRBM research modeled the correlation between item ratings only, we model bothuser-user and item-item correlations in a unified framework. Finally, we explorethe potential of combining the original training data with data generated by theRBM model itself in a bootstrapping fashion. The evaluation on the MovieLensdataset shows that our extended RBM model yields results that rival the predictionquality of the best previously-proposed collaborative filtering algorithms."
Collaborative Filtering with Hybrid Restricted Boltzmann Machines,"We propose an novel framework for collaborative filtering based on RestrictedBoltzmann Machines (RBM), which extends previous RBM-based approaches inseveral important directions. First, we work with numbers as opposed to usingcategorical variables, which allows us to take the natural order between user-itemratings into account, e.g., we consider predicting a rating of 4 when the actual rat-ing is 5 to be better than predicting a rating of 2. More importantly, while previousRBM research modeled the correlation between item ratings only, we model bothuser-user and item-item correlations in a unified framework. Finally, we explorethe potential of combining the original training data with data generated by theRBM model itself in a bootstrapping fashion. The evaluation on the MovieLensdataset shows that our extended RBM model yields results that rival the predictionquality of the best previously-proposed collaborative filtering algorithms."
Local Learning with Local Patch Dissimilarity for Image Classification,"A new dissimilarity measure for images based on patches, called Local Patch Dissimilarity (LPD), was recently introduced in [1]. It was inspired from rank distance which is a distance measure for strings.There are many other patch-based techniques used in image processing. Patches contain contextual information and have advantages in terms of generalization. But most patch-based algorithms are heavy to compute with our current machines [2]. Despite LPD is also heavy computational, it has very promising results in image processing and works very well in image classification. In this paper we turn to unconventional learning methods to avoid the problem of the higher computational time on large sets of images. We conduct several experiments on large datasets using methods such as k-NN with filtering and local learning. All methods are based on LPD. The obtained results come to support the previous work of [1].  "
Local Learning with Local Patch Dissimilarity for Image Classification,"A new dissimilarity measure for images based on patches, called Local Patch Dissimilarity (LPD), was recently introduced in [1]. It was inspired from rank distance which is a distance measure for strings.There are many other patch-based techniques used in image processing. Patches contain contextual information and have advantages in terms of generalization. But most patch-based algorithms are heavy to compute with our current machines [2]. Despite LPD is also heavy computational, it has very promising results in image processing and works very well in image classification. In this paper we turn to unconventional learning methods to avoid the problem of the higher computational time on large sets of images. We conduct several experiments on large datasets using methods such as k-NN with filtering and local learning. All methods are based on LPD. The obtained results come to support the previous work of [1].  "
Revenue Maximization for Groupon-Style Websites by Dynamic Deal Allocation,"Groupon-style (GS) websites are very popular in electronic commerce and online shopping nowadays.  They provide one or multiple deals with significant discount at their websites every day. We study how to dynamically allocate the user traffic of a GS website to different deals to maximize its revenue. We first consider a simple scenario in which the GS website shows one deal to each visitor, and derive an efficient algorithm to find the optimal allocation. We then consider the scenario of showing multiple deals to each visitor, and propose an efficient algorithm that can achieve a 1/4 approximation of the optimal allocation in the worst case. With these results, we demonstrate that carefully designed allocation algorithms can help a GS website improve its revenue by better leveraging its user traffic."
Revenue Maximization for Groupon-Style Websites by Dynamic Deal Allocation,"Groupon-style (GS) websites are very popular in electronic commerce and online shopping nowadays.  They provide one or multiple deals with significant discount at their websites every day. We study how to dynamically allocate the user traffic of a GS website to different deals to maximize its revenue. We first consider a simple scenario in which the GS website shows one deal to each visitor, and derive an efficient algorithm to find the optimal allocation. We then consider the scenario of showing multiple deals to each visitor, and propose an efficient algorithm that can achieve a 1/4 approximation of the optimal allocation in the worst case. With these results, we demonstrate that carefully designed allocation algorithms can help a GS website improve its revenue by better leveraging its user traffic."
Identifying Stationary Behavior in Externally Driven Environments,"  In real-world settings, environments and sensory data are typically  subject to change over time. Sensor readings are a mixture of  changes due to control on actors of a system and resulting changes  of its internal state but might also contain changes in the  properties of the dynamical system itself. Such changes are caused  by faults, wear, adjusted system configuration, or other permanent  changes. The task of separating these two sources is hard because  the resulting effects in measurable signals might live on similar  time scales, moreover the irrelevant signal components can have  significant internal structure. We present a new approach based on  Stationary Subspace Analysis to detect changes in such complex  scenarios reliably. We demonstrate its capabilities on synthetic  data. Finally, we consider a real-world application, a gas turbine  simulation where we detect changes of internal parameters under  heavily varying external conditions caused by transient operation."
Identifying Stationary Behavior in Externally Driven Environments,"  In real-world settings, environments and sensory data are typically  subject to change over time. Sensor readings are a mixture of  changes due to control on actors of a system and resulting changes  of its internal state but might also contain changes in the  properties of the dynamical system itself. Such changes are caused  by faults, wear, adjusted system configuration, or other permanent  changes. The task of separating these two sources is hard because  the resulting effects in measurable signals might live on similar  time scales, moreover the irrelevant signal components can have  significant internal structure. We present a new approach based on  Stationary Subspace Analysis to detect changes in such complex  scenarios reliably. We demonstrate its capabilities on synthetic  data. Finally, we consider a real-world application, a gas turbine  simulation where we detect changes of internal parameters under  heavily varying external conditions caused by transient operation."
Identifying Stationary Behavior in Externally Driven Environments,"  In real-world settings, environments and sensory data are typically  subject to change over time. Sensor readings are a mixture of  changes due to control on actors of a system and resulting changes  of its internal state but might also contain changes in the  properties of the dynamical system itself. Such changes are caused  by faults, wear, adjusted system configuration, or other permanent  changes. The task of separating these two sources is hard because  the resulting effects in measurable signals might live on similar  time scales, moreover the irrelevant signal components can have  significant internal structure. We present a new approach based on  Stationary Subspace Analysis to detect changes in such complex  scenarios reliably. We demonstrate its capabilities on synthetic  data. Finally, we consider a real-world application, a gas turbine  simulation where we detect changes of internal parameters under  heavily varying external conditions caused by transient operation."
Identifying Stationary Behavior in Externally Driven Environments,"  In real-world settings, environments and sensory data are typically  subject to change over time. Sensor readings are a mixture of  changes due to control on actors of a system and resulting changes  of its internal state but might also contain changes in the  properties of the dynamical system itself. Such changes are caused  by faults, wear, adjusted system configuration, or other permanent  changes. The task of separating these two sources is hard because  the resulting effects in measurable signals might live on similar  time scales, moreover the irrelevant signal components can have  significant internal structure. We present a new approach based on  Stationary Subspace Analysis to detect changes in such complex  scenarios reliably. We demonstrate its capabilities on synthetic  data. Finally, we consider a real-world application, a gas turbine  simulation where we detect changes of internal parameters under  heavily varying external conditions caused by transient operation."
Identifying Stationary Behavior in Externally Driven Environments,"  In real-world settings, environments and sensory data are typically  subject to change over time. Sensor readings are a mixture of  changes due to control on actors of a system and resulting changes  of its internal state but might also contain changes in the  properties of the dynamical system itself. Such changes are caused  by faults, wear, adjusted system configuration, or other permanent  changes. The task of separating these two sources is hard because  the resulting effects in measurable signals might live on similar  time scales, moreover the irrelevant signal components can have  significant internal structure. We present a new approach based on  Stationary Subspace Analysis to detect changes in such complex  scenarios reliably. We demonstrate its capabilities on synthetic  data. Finally, we consider a real-world application, a gas turbine  simulation where we detect changes of internal parameters under  heavily varying external conditions caused by transient operation."
Fast variational inference for stochastic differential equations,We introduce a Gaussian variational mean field approximation for inference in continuous time stochastic differential equations. This approach allows us to express the variational free energy as a functional of the marginal moments of the approximating Gaussian process. A restriction of moments to piecewise polynomial functions over time makes the complexity of approximate inference for stochastic differential equation models comparable to that of  discrete time hidden Markov models. We demonstrate the algorithm on state and parameter estimation for nonlinear problems with up to forty state variables. 
"Exploring Distinct, Alike and Identical Concepts for Transfer Learning","Transfer learning targets at leveraging the knowledge from source domains with different distributions, to train accurate models for test data coming from target domains. In spite of the different distributions in raw word features, which degrade the performance of traditional machine learning algorithms, the high level concepts(e.g., word clusters) are more appropriate for classification. Most previous works assume that the source and target domains share the same set of concepts, however, different domains may contain the distinct concepts. Along this line, we explore three types of concepts, namely distinct, alike and identical concepts for transfer learning and propose a generative model DAIC. Then an EM algorithm is developed to solve the optimization problem. Furthermore, we design two types of transfer learning tasks. One is the newly constructed classification problem, in which the distinct concepts may exist, and the other is the traditional transfer learning task. Finally,systemic experiments demonstrate the effectiveness of our model. It is worth mentioning that our model can gain 47.67% average accuracy improvement over the traditional machine learning algorithm, i.e., Logistic Regression, on the much more challenging transfer learning tasks."
"Exploring Distinct, Alike and Identical Concepts for Transfer Learning","Transfer learning targets at leveraging the knowledge from source domains with different distributions, to train accurate models for test data coming from target domains. In spite of the different distributions in raw word features, which degrade the performance of traditional machine learning algorithms, the high level concepts(e.g., word clusters) are more appropriate for classification. Most previous works assume that the source and target domains share the same set of concepts, however, different domains may contain the distinct concepts. Along this line, we explore three types of concepts, namely distinct, alike and identical concepts for transfer learning and propose a generative model DAIC. Then an EM algorithm is developed to solve the optimization problem. Furthermore, we design two types of transfer learning tasks. One is the newly constructed classification problem, in which the distinct concepts may exist, and the other is the traditional transfer learning task. Finally,systemic experiments demonstrate the effectiveness of our model. It is worth mentioning that our model can gain 47.67% average accuracy improvement over the traditional machine learning algorithm, i.e., Logistic Regression, on the much more challenging transfer learning tasks."
"Exploring Distinct, Alike and Identical Concepts for Transfer Learning","Transfer learning targets at leveraging the knowledge from source domains with different distributions, to train accurate models for test data coming from target domains. In spite of the different distributions in raw word features, which degrade the performance of traditional machine learning algorithms, the high level concepts(e.g., word clusters) are more appropriate for classification. Most previous works assume that the source and target domains share the same set of concepts, however, different domains may contain the distinct concepts. Along this line, we explore three types of concepts, namely distinct, alike and identical concepts for transfer learning and propose a generative model DAIC. Then an EM algorithm is developed to solve the optimization problem. Furthermore, we design two types of transfer learning tasks. One is the newly constructed classification problem, in which the distinct concepts may exist, and the other is the traditional transfer learning task. Finally,systemic experiments demonstrate the effectiveness of our model. It is worth mentioning that our model can gain 47.67% average accuracy improvement over the traditional machine learning algorithm, i.e., Logistic Regression, on the much more challenging transfer learning tasks."
"Exploring Distinct, Alike and Identical Concepts for Transfer Learning","Transfer learning targets at leveraging the knowledge from source domains with different distributions, to train accurate models for test data coming from target domains. In spite of the different distributions in raw word features, which degrade the performance of traditional machine learning algorithms, the high level concepts(e.g., word clusters) are more appropriate for classification. Most previous works assume that the source and target domains share the same set of concepts, however, different domains may contain the distinct concepts. Along this line, we explore three types of concepts, namely distinct, alike and identical concepts for transfer learning and propose a generative model DAIC. Then an EM algorithm is developed to solve the optimization problem. Furthermore, we design two types of transfer learning tasks. One is the newly constructed classification problem, in which the distinct concepts may exist, and the other is the traditional transfer learning task. Finally,systemic experiments demonstrate the effectiveness of our model. It is worth mentioning that our model can gain 47.67% average accuracy improvement over the traditional machine learning algorithm, i.e., Logistic Regression, on the much more challenging transfer learning tasks."
"Exploring Distinct, Alike and Identical Concepts for Transfer Learning","Transfer learning targets at leveraging the knowledge from source domains with different distributions, to train accurate models for test data coming from target domains. In spite of the different distributions in raw word features, which degrade the performance of traditional machine learning algorithms, the high level concepts(e.g., word clusters) are more appropriate for classification. Most previous works assume that the source and target domains share the same set of concepts, however, different domains may contain the distinct concepts. Along this line, we explore three types of concepts, namely distinct, alike and identical concepts for transfer learning and propose a generative model DAIC. Then an EM algorithm is developed to solve the optimization problem. Furthermore, we design two types of transfer learning tasks. One is the newly constructed classification problem, in which the distinct concepts may exist, and the other is the traditional transfer learning task. Finally,systemic experiments demonstrate the effectiveness of our model. It is worth mentioning that our model can gain 47.67% average accuracy improvement over the traditional machine learning algorithm, i.e., Logistic Regression, on the much more challenging transfer learning tasks."
Maximal Deviations of Incomplete U-statistics ?with Applications to Empirical Risk Sampling,"It is the goal of this paper to extend the Empirical Risk Minimization paradigm, from a practical perspective, to the situation where a natural estimate of the risk is of the form of a $K$-sample $U$-statistics, as it is the case in the $K$-partite ranking problem for instance. Indeed, the numerical computation of the empirical risk is hardly feasible if not infeasible, even for moderate samples sizes. Precisely, it involves averaging $O(n^{d_1+\ldots+d_K})$ terms, when considering a $U$-statistic of degrees $(d_1,\;\ldots,\; d_K)$ based on samples of sizes proportional to $n$. We propose here to consider a drastically simpler Monte-Carlo version of the empirical risk based on $O(n)$ terms solely, which can be viewed as an \textit{incomplete generalized $U$-statistic}, and prove that, remarkably, the approximation stage does not damage the ERM procedure and yields a learning rate of order $O_{\mathbb{P}}(1/\sqrt{n})$.Beyond a preliminary theoretical analysis guaranteeing the validity of this approach, numerical experiments are displayed for illustrative purpose."
Maximal Deviations of Incomplete U-statistics ?with Applications to Empirical Risk Sampling,"It is the goal of this paper to extend the Empirical Risk Minimization paradigm, from a practical perspective, to the situation where a natural estimate of the risk is of the form of a $K$-sample $U$-statistics, as it is the case in the $K$-partite ranking problem for instance. Indeed, the numerical computation of the empirical risk is hardly feasible if not infeasible, even for moderate samples sizes. Precisely, it involves averaging $O(n^{d_1+\ldots+d_K})$ terms, when considering a $U$-statistic of degrees $(d_1,\;\ldots,\; d_K)$ based on samples of sizes proportional to $n$. We propose here to consider a drastically simpler Monte-Carlo version of the empirical risk based on $O(n)$ terms solely, which can be viewed as an \textit{incomplete generalized $U$-statistic}, and prove that, remarkably, the approximation stage does not damage the ERM procedure and yields a learning rate of order $O_{\mathbb{P}}(1/\sqrt{n})$.Beyond a preliminary theoretical analysis guaranteeing the validity of this approach, numerical experiments are displayed for illustrative purpose."
Maximal Deviations of Incomplete U-statistics ?with Applications to Empirical Risk Sampling,"It is the goal of this paper to extend the Empirical Risk Minimization paradigm, from a practical perspective, to the situation where a natural estimate of the risk is of the form of a $K$-sample $U$-statistics, as it is the case in the $K$-partite ranking problem for instance. Indeed, the numerical computation of the empirical risk is hardly feasible if not infeasible, even for moderate samples sizes. Precisely, it involves averaging $O(n^{d_1+\ldots+d_K})$ terms, when considering a $U$-statistic of degrees $(d_1,\;\ldots,\; d_K)$ based on samples of sizes proportional to $n$. We propose here to consider a drastically simpler Monte-Carlo version of the empirical risk based on $O(n)$ terms solely, which can be viewed as an \textit{incomplete generalized $U$-statistic}, and prove that, remarkably, the approximation stage does not damage the ERM procedure and yields a learning rate of order $O_{\mathbb{P}}(1/\sqrt{n})$.Beyond a preliminary theoretical analysis guaranteeing the validity of this approach, numerical experiments are displayed for illustrative purpose."
Memory Constraint Online Multitask Classification,"We investigate online kernel algorithms which simultaneously processmultiple classification tasks while a fixed constraint is imposed onthe size of their active sets. We focus in particular on the design ofalgorithms that can efficiently deal with problems where the number oftasks is extremely high and the task data are large scale.  Two newprojection-based algorithms are introduced to efficiently tackle thoseissues while presenting different trade offs on how the availablememory is managed with respect to the prior information about thelearning tasks.  Theoretically sound budget algorithms are devised bycoupling the Randomized Budget Perceptron and the Forgetron algorithmswith the multitask kernel.  We show how the two seemingly contrastingproperties of learning from multiple tasks and keeping a constantmemory footprint can be balanced, and how the sharing of the availablespace among different tasks is automatically taken care of. We proposeand discuss new insights on the multitask kernel. Experiments showthat online kernel multitask algorithms running on a budget canefficiently tackle real world learning problems involving multipletasks."
Memory Constraint Online Multitask Classification,"We investigate online kernel algorithms which simultaneously processmultiple classification tasks while a fixed constraint is imposed onthe size of their active sets. We focus in particular on the design ofalgorithms that can efficiently deal with problems where the number oftasks is extremely high and the task data are large scale.  Two newprojection-based algorithms are introduced to efficiently tackle thoseissues while presenting different trade offs on how the availablememory is managed with respect to the prior information about thelearning tasks.  Theoretically sound budget algorithms are devised bycoupling the Randomized Budget Perceptron and the Forgetron algorithmswith the multitask kernel.  We show how the two seemingly contrastingproperties of learning from multiple tasks and keeping a constantmemory footprint can be balanced, and how the sharing of the availablespace among different tasks is automatically taken care of. We proposeand discuss new insights on the multitask kernel. Experiments showthat online kernel multitask algorithms running on a budget canefficiently tackle real world learning problems involving multipletasks."
Toward Optimal Uniform Stratification for Stratified Monte-Carlo Integration,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite budget n of noisy evaluations to the function. We tackle in this paper the problem of stratifying the domain in an efficient way. More precisely, it is interesting to refine the partition of the domain as much as possible, to have more flexibility on where to sample. On the other hand, having a (too) refined stratification is not optimal, since the more refined it is, the more difficult it is to estimate the variance of the noise and the variations of the function, in each stratum. We provide in this paper an algorithm, Adapt-MC-UCB, that is almost as efficient (up to a constant) as the algorithm MC-UCB (introduced in [Carpentier and Munos, 2011]) run on the best uniform partition defined in a hierarchical partitioning of the domain."
Toward Optimal Uniform Stratification for Stratified Monte-Carlo Integration,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite budget n of noisy evaluations to the function. We tackle in this paper the problem of stratifying the domain in an efficient way. More precisely, it is interesting to refine the partition of the domain as much as possible, to have more flexibility on where to sample. On the other hand, having a (too) refined stratification is not optimal, since the more refined it is, the more difficult it is to estimate the variance of the noise and the variations of the function, in each stratum. We provide in this paper an algorithm, Adapt-MC-UCB, that is almost as efficient (up to a constant) as the algorithm MC-UCB (introduced in [Carpentier and Munos, 2011]) run on the best uniform partition defined in a hierarchical partitioning of the domain."
Phase vs. amplitude ? learning from subjective image quality assessment,"In frequency-based representation of images, phase and amplitude convey complementary information. Phase has been regarded as dominating the image appearance, however power (amplitude) spectra have recently been found useful for image classification. In the primary visual cortex (V1), simple cells are sensitive to and thereby encode the phase of visual stimuli, while complex cells, being majority in V1, show phase-invariance and encode the energy (magnitude) of simple cells? spikes. In this paper, we attempt to quantitatively exploit the relative importance of phase and amplitude to visual perception by learning from subjective image quality assessment. We designed an image quality metric based on the weighted combination of the amplitude and phase errors, and determined the weights so as to maximize the prediction accuracy of the metric over subjectively- rated databases, where a joint optimization over multiple databases strengthened the reliability of weights. The results confirm that: 1) both the phase and the amplitude are necessary for image quality assessment; 2) the amplitude becomes more important at the finest image scale while the phase dominates at the coarser scale. Moreover, the multiplicative combination of the amplitude and phase errors plausibly interprets the visual perception on negative images."
Phase vs. amplitude ? learning from subjective image quality assessment,"In frequency-based representation of images, phase and amplitude convey complementary information. Phase has been regarded as dominating the image appearance, however power (amplitude) spectra have recently been found useful for image classification. In the primary visual cortex (V1), simple cells are sensitive to and thereby encode the phase of visual stimuli, while complex cells, being majority in V1, show phase-invariance and encode the energy (magnitude) of simple cells? spikes. In this paper, we attempt to quantitatively exploit the relative importance of phase and amplitude to visual perception by learning from subjective image quality assessment. We designed an image quality metric based on the weighted combination of the amplitude and phase errors, and determined the weights so as to maximize the prediction accuracy of the metric over subjectively- rated databases, where a joint optimization over multiple databases strengthened the reliability of weights. The results confirm that: 1) both the phase and the amplitude are necessary for image quality assessment; 2) the amplitude becomes more important at the finest image scale while the phase dominates at the coarser scale. Moreover, the multiplicative combination of the amplitude and phase errors plausibly interprets the visual perception on negative images."
Phase vs. amplitude ? learning from subjective image quality assessment,"In frequency-based representation of images, phase and amplitude convey complementary information. Phase has been regarded as dominating the image appearance, however power (amplitude) spectra have recently been found useful for image classification. In the primary visual cortex (V1), simple cells are sensitive to and thereby encode the phase of visual stimuli, while complex cells, being majority in V1, show phase-invariance and encode the energy (magnitude) of simple cells? spikes. In this paper, we attempt to quantitatively exploit the relative importance of phase and amplitude to visual perception by learning from subjective image quality assessment. We designed an image quality metric based on the weighted combination of the amplitude and phase errors, and determined the weights so as to maximize the prediction accuracy of the metric over subjectively- rated databases, where a joint optimization over multiple databases strengthened the reliability of weights. The results confirm that: 1) both the phase and the amplitude are necessary for image quality assessment; 2) the amplitude becomes more important at the finest image scale while the phase dominates at the coarser scale. Moreover, the multiplicative combination of the amplitude and phase errors plausibly interprets the visual perception on negative images."
Phase vs. amplitude ? learning from subjective image quality assessment,"In frequency-based representation of images, phase and amplitude convey complementary information. Phase has been regarded as dominating the image appearance, however power (amplitude) spectra have recently been found useful for image classification. In the primary visual cortex (V1), simple cells are sensitive to and thereby encode the phase of visual stimuli, while complex cells, being majority in V1, show phase-invariance and encode the energy (magnitude) of simple cells? spikes. In this paper, we attempt to quantitatively exploit the relative importance of phase and amplitude to visual perception by learning from subjective image quality assessment. We designed an image quality metric based on the weighted combination of the amplitude and phase errors, and determined the weights so as to maximize the prediction accuracy of the metric over subjectively- rated databases, where a joint optimization over multiple databases strengthened the reliability of weights. The results confirm that: 1) both the phase and the amplitude are necessary for image quality assessment; 2) the amplitude becomes more important at the finest image scale while the phase dominates at the coarser scale. Moreover, the multiplicative combination of the amplitude and phase errors plausibly interprets the visual perception on negative images."
A Spectral Algorithm for Latent Dirichlet Allocation,"The problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that the observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem involving the estimation of  topic probability vectors (the distribution of words under each topic), when only the words are observed and the corresponding topics are hidden.We provide a simple and an efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which can be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order observed moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on $k \times k$ matrices, where $k$ is the number of latent factors (e.g., the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$)."
A Spectral Algorithm for Latent Dirichlet Allocation,"The problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that the observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem involving the estimation of  topic probability vectors (the distribution of words under each topic), when only the words are observed and the corresponding topics are hidden.We provide a simple and an efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which can be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order observed moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on $k \times k$ matrices, where $k$ is the number of latent factors (e.g., the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$)."
A Spectral Algorithm for Latent Dirichlet Allocation,"The problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that the observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem involving the estimation of  topic probability vectors (the distribution of words under each topic), when only the words are observed and the corresponding topics are hidden.We provide a simple and an efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which can be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order observed moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on $k \times k$ matrices, where $k$ is the number of latent factors (e.g., the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$)."
A Spectral Algorithm for Latent Dirichlet Allocation,"The problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that the observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem involving the estimation of  topic probability vectors (the distribution of words under each topic), when only the words are observed and the corresponding topics are hidden.We provide a simple and an efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which can be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order observed moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on $k \times k$ matrices, where $k$ is the number of latent factors (e.g., the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$)."
A Spectral Algorithm for Latent Dirichlet Allocation,"The problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that the observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem involving the estimation of  topic probability vectors (the distribution of words under each topic), when only the words are observed and the corresponding topics are hidden.We provide a simple and an efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which can be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order observed moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on $k \times k$ matrices, where $k$ is the number of latent factors (e.g., the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$)."
Identification of Consistent Brain Networks via Maximization of Predictability of Functional Connectivity from Structural Connectivity,"Recent studies have suggested that structural brain connectivity is strongly correlated with functional connectivity. However, it is still largely unknown what brain networks best possibly exhibit such close structural/functional connectivity relationship and how this close relationship can guide the identification of brain networks. This paper presents a novel framework to infer brain networks that are consistent across multiple neuroimaging modalities and across individuals. Our basic premise is that the predictability of functional connectivity from structural connectivity within each brain network should be maximized, which is formulated by and solved via a novel feedback-regulated multi-view spectral clustering algorithm. We applied and tested the proposed algorithm on the multimodal structural and functional brain networks of 50 healthy subjects, and obtained promising results. Our validation experiments demonstrated that the derived brain networks are in agreement with current neuroscience knowledge and offer novel insights into the close relationship between brain structure and function."
Identification of Consistent Brain Networks via Maximization of Predictability of Functional Connectivity from Structural Connectivity,"Recent studies have suggested that structural brain connectivity is strongly correlated with functional connectivity. However, it is still largely unknown what brain networks best possibly exhibit such close structural/functional connectivity relationship and how this close relationship can guide the identification of brain networks. This paper presents a novel framework to infer brain networks that are consistent across multiple neuroimaging modalities and across individuals. Our basic premise is that the predictability of functional connectivity from structural connectivity within each brain network should be maximized, which is formulated by and solved via a novel feedback-regulated multi-view spectral clustering algorithm. We applied and tested the proposed algorithm on the multimodal structural and functional brain networks of 50 healthy subjects, and obtained promising results. Our validation experiments demonstrated that the derived brain networks are in agreement with current neuroscience knowledge and offer novel insights into the close relationship between brain structure and function."
Identification of Consistent Brain Networks via Maximization of Predictability of Functional Connectivity from Structural Connectivity,"Recent studies have suggested that structural brain connectivity is strongly correlated with functional connectivity. However, it is still largely unknown what brain networks best possibly exhibit such close structural/functional connectivity relationship and how this close relationship can guide the identification of brain networks. This paper presents a novel framework to infer brain networks that are consistent across multiple neuroimaging modalities and across individuals. Our basic premise is that the predictability of functional connectivity from structural connectivity within each brain network should be maximized, which is formulated by and solved via a novel feedback-regulated multi-view spectral clustering algorithm. We applied and tested the proposed algorithm on the multimodal structural and functional brain networks of 50 healthy subjects, and obtained promising results. Our validation experiments demonstrated that the derived brain networks are in agreement with current neuroscience knowledge and offer novel insights into the close relationship between brain structure and function."
Identification of Consistent Brain Networks via Maximization of Predictability of Functional Connectivity from Structural Connectivity,"Recent studies have suggested that structural brain connectivity is strongly correlated with functional connectivity. However, it is still largely unknown what brain networks best possibly exhibit such close structural/functional connectivity relationship and how this close relationship can guide the identification of brain networks. This paper presents a novel framework to infer brain networks that are consistent across multiple neuroimaging modalities and across individuals. Our basic premise is that the predictability of functional connectivity from structural connectivity within each brain network should be maximized, which is formulated by and solved via a novel feedback-regulated multi-view spectral clustering algorithm. We applied and tested the proposed algorithm on the multimodal structural and functional brain networks of 50 healthy subjects, and obtained promising results. Our validation experiments demonstrated that the derived brain networks are in agreement with current neuroscience knowledge and offer novel insights into the close relationship between brain structure and function."
Identification of Consistent Brain Networks via Maximization of Predictability of Functional Connectivity from Structural Connectivity,"Recent studies have suggested that structural brain connectivity is strongly correlated with functional connectivity. However, it is still largely unknown what brain networks best possibly exhibit such close structural/functional connectivity relationship and how this close relationship can guide the identification of brain networks. This paper presents a novel framework to infer brain networks that are consistent across multiple neuroimaging modalities and across individuals. Our basic premise is that the predictability of functional connectivity from structural connectivity within each brain network should be maximized, which is formulated by and solved via a novel feedback-regulated multi-view spectral clustering algorithm. We applied and tested the proposed algorithm on the multimodal structural and functional brain networks of 50 healthy subjects, and obtained promising results. Our validation experiments demonstrated that the derived brain networks are in agreement with current neuroscience knowledge and offer novel insights into the close relationship between brain structure and function."
Music Generation with Weighted Finite-state Transducers,"We approach the task of musical style imitation by probabilistically modeling the melody and harmony of music pieces in the framework of weighted finite-state transducers (WFSTs), which have been used successfully for probabilistic models in speech and language processing. We divide the generation process into different steps, each performed by inference though transducers. We present a method to imitate local and global structure in the melody of music pieces, and a method for four-part harmonization that models vertical and horizontal structure in the generated harmonization. The weights of our transducers are learned with maximum likelihood estimation from a corpus of music pieces. We compare the predictive power of our models against that of existing approaches."
Music Generation with Weighted Finite-state Transducers,"We approach the task of musical style imitation by probabilistically modeling the melody and harmony of music pieces in the framework of weighted finite-state transducers (WFSTs), which have been used successfully for probabilistic models in speech and language processing. We divide the generation process into different steps, each performed by inference though transducers. We present a method to imitate local and global structure in the melody of music pieces, and a method for four-part harmonization that models vertical and horizontal structure in the generated harmonization. The weights of our transducers are learned with maximum likelihood estimation from a corpus of music pieces. We compare the predictive power of our models against that of existing approaches."
Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs,"Given $\alpha,\epsilon$, we study the time complexity  required to improperly learn a halfspace with misclassification  error rate of at most $(1+\alpha)\,L^*_\gamma + \epsilon$, where  $L^*_\gamma$ is the optimal $\gamma$-margin error rate. For $\alpha  = 1/\gamma$, polynomial time and sample complexity is achievable  using the hinge-loss. For $\alpha = 0$, \cite{ShalevShSr11} showed  that $\poly(1/\gamma)$ time is impossible, while learning is  possible in time $\exp(\tilde{O}(1/\gamma))$.  An immediate  question, which this paper tackles, is what is achievable if $\alpha  \in (0,1/\gamma)$.  We derive positive results interpolating between  the polynomial time for $\alpha = 1/\gamma$ and the exponential  time for $\alpha=0$. In particular, we show that there are cases in  which $\alpha = o(1/\gamma)$ but the problem is still solvable in  polynomial time. Our results naturally extend to the adversarial  online learning model and to the PAC learning with malicious noise  model."
Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs,"Given $\alpha,\epsilon$, we study the time complexity  required to improperly learn a halfspace with misclassification  error rate of at most $(1+\alpha)\,L^*_\gamma + \epsilon$, where  $L^*_\gamma$ is the optimal $\gamma$-margin error rate. For $\alpha  = 1/\gamma$, polynomial time and sample complexity is achievable  using the hinge-loss. For $\alpha = 0$, \cite{ShalevShSr11} showed  that $\poly(1/\gamma)$ time is impossible, while learning is  possible in time $\exp(\tilde{O}(1/\gamma))$.  An immediate  question, which this paper tackles, is what is achievable if $\alpha  \in (0,1/\gamma)$.  We derive positive results interpolating between  the polynomial time for $\alpha = 1/\gamma$ and the exponential  time for $\alpha=0$. In particular, we show that there are cases in  which $\alpha = o(1/\gamma)$ but the problem is still solvable in  polynomial time. Our results naturally extend to the adversarial  online learning model and to the PAC learning with malicious noise  model."
Regularized Mapping to Latent Structures and Its Application to Web Search,"The task of matching data from two heterogeneous domains naturally arises in various areas, for example, in real-world web search. However, to our knowledge, there is no principled approach to learning a matching model. In this paper, we propose a framework for matching heterogeneous objects, which renders a rich family of matching models when different regularizations are enforced. With orthonormal constraints on the mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with $\ell_1$+$\ell_2$ type of regularization,  we obtain a new model called \emph{Regularized Mapping to Latent Structures} (RMLS).  RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization.  As another contribution, we give a generalizationanalysis of this matching framework, and apply it to both PLS and RMLS.  We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods,  while RMLS significantly speeds up the learning process."
Regularized Mapping to Latent Structures and Its Application to Web Search,"The task of matching data from two heterogeneous domains naturally arises in various areas, for example, in real-world web search. However, to our knowledge, there is no principled approach to learning a matching model. In this paper, we propose a framework for matching heterogeneous objects, which renders a rich family of matching models when different regularizations are enforced. With orthonormal constraints on the mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with $\ell_1$+$\ell_2$ type of regularization,  we obtain a new model called \emph{Regularized Mapping to Latent Structures} (RMLS).  RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization.  As another contribution, we give a generalizationanalysis of this matching framework, and apply it to both PLS and RMLS.  We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods,  while RMLS significantly speeds up the learning process."
Regularized Mapping to Latent Structures and Its Application to Web Search,"The task of matching data from two heterogeneous domains naturally arises in various areas, for example, in real-world web search. However, to our knowledge, there is no principled approach to learning a matching model. In this paper, we propose a framework for matching heterogeneous objects, which renders a rich family of matching models when different regularizations are enforced. With orthonormal constraints on the mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with $\ell_1$+$\ell_2$ type of regularization,  we obtain a new model called \emph{Regularized Mapping to Latent Structures} (RMLS).  RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization.  As another contribution, we give a generalizationanalysis of this matching framework, and apply it to both PLS and RMLS.  We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods,  while RMLS significantly speeds up the learning process."
Convex formulations of radius-margin based Support Vector Machines,"We consider Support Vector Machines (SVMs) learned together with linear transformations of the feature spaces on which they are applied. Under this scenario the radius of the smallest data enclosing sphere is no longer ?xed. Therefore optimizing the SVM error bound by considering both the radius and the margin has the potential to deliver a tighter error bound. In this paper we present two novel algorithms: R-SVM^+_{mu} ? a SVM radius-margin based feature selection algorithm, and R-SVM^+ ? a metric learning-based SVM. We derive our algorithms by exploiting a new tighter approximation of the radius and a metric learning interpretation of SVM. Both optimize directly the radius-margin error bound using linear transformations. Unlike almost all existing radius-margin based SVM algorithms which are either non-convex or combinatorial, our algorithms are standard quadratic convex optimization problems with linear or quadratic constraints. We perform a number of experiments on benchmark datasets. R-SVM^+_{mu} exhibits excellent feature selection performance compared to state of the art methods such as SVMRFE; in addition it determines automatically the appropriate sparsity level,unlike most existing feature selection algorithms, which require a sparsity constraint, such as the number of features to select. R-SVM+ has a predictive performance that is signi?cantly better than SVM and other state of the art SVM variants."
Convex formulations of radius-margin based Support Vector Machines,"We consider Support Vector Machines (SVMs) learned together with linear transformations of the feature spaces on which they are applied. Under this scenario the radius of the smallest data enclosing sphere is no longer ?xed. Therefore optimizing the SVM error bound by considering both the radius and the margin has the potential to deliver a tighter error bound. In this paper we present two novel algorithms: R-SVM^+_{mu} ? a SVM radius-margin based feature selection algorithm, and R-SVM^+ ? a metric learning-based SVM. We derive our algorithms by exploiting a new tighter approximation of the radius and a metric learning interpretation of SVM. Both optimize directly the radius-margin error bound using linear transformations. Unlike almost all existing radius-margin based SVM algorithms which are either non-convex or combinatorial, our algorithms are standard quadratic convex optimization problems with linear or quadratic constraints. We perform a number of experiments on benchmark datasets. R-SVM^+_{mu} exhibits excellent feature selection performance compared to state of the art methods such as SVMRFE; in addition it determines automatically the appropriate sparsity level,unlike most existing feature selection algorithms, which require a sparsity constraint, such as the number of features to select. R-SVM+ has a predictive performance that is signi?cantly better than SVM and other state of the art SVM variants."
Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states."
Learning with Marginalized Corrupted Features,"An important goal of machine learning is to develop predictors that are robust to noise in the observations. In this paper, we consider a particular type of observation noise in which features are ``blanked out'' with some probability. Such blank-out noise occurs, \emph{e.g.} when sensors measuring features (temporarily) break down or when particular words related to a topic are not observed in a document. A simple way to train predictors that are robust to such noise is to extend the training data with training examples in which some of the variables are blanked out at random, but such an approach is computationally costly. This paper presents a new approach, called \emph{marginalized corrupted features} (MCF), that trains robust predictors by minimizing the expected value of the loss function under the blank-out noise model. Experimental evaluation of our approach reveals that the resulting predictors are not only more robust to sensors breaking down, but that they also perform substantially better on data with high-dimensional, heavy-tailed features, such as bag-of-words text documents. "
Controlling Transfer For Reinforcement Learning,"Recently, algorithms for transfer learning in reinforcement learning have been proposed that utilize a map between state-action pairs in the target and source tasks. This map is used to suggest possible values for states in the target task based on values of states in the source task. In this paper, we describe a generic transfer algorithm that, given any such map, determines online if the the map is correct and if not limits its use and minimizes {\em negative transfer}. We give bounds on the expected negative transfer and perform experiments to illustrate the usefulness of the algorithm."
Structured sparse coding via group gating,"Multiplicative feature learning models, like the Gated Boltzmann Machine,  are a recent extension of sparse coding for modeling relations and image transformations.  A potential shortcoming of these methods is that they do not allow any given feature to be transformed in multiple different ways, because features are matched in pairs.  We propose a ``group gating'' model that addresses this issue by allowing the re-use of filters.  In the model, features form groups, and multiplicative interactions are allowed between all features within each group. We demonstrate that the group gating extension can lead to improved performance in transformation extraction tasks.  We also show that learning on natural videos leads to filter-groups of similar frequency and orientation and of varying phase, as well as to ``pinwheel'' structures in the case of overlapping groups, providing a  new interpretation of this effect known previously from subspace energy models."
Structured sparse coding via group gating,"Multiplicative feature learning models, like the Gated Boltzmann Machine,  are a recent extension of sparse coding for modeling relations and image transformations.  A potential shortcoming of these methods is that they do not allow any given feature to be transformed in multiple different ways, because features are matched in pairs.  We propose a ``group gating'' model that addresses this issue by allowing the re-use of filters.  In the model, features form groups, and multiplicative interactions are allowed between all features within each group. We demonstrate that the group gating extension can lead to improved performance in transformation extraction tasks.  We also show that learning on natural videos leads to filter-groups of similar frequency and orientation and of varying phase, as well as to ``pinwheel'' structures in the case of overlapping groups, providing a  new interpretation of this effect known previously from subspace energy models."
Directly Optimizing 0-1 Loss for Large-Scale Nonlinear Transductive Multiclass Classification,"We take a new look at graph-based transductive classifiers of min-cut type (MCCs), and we offer a new derivation for them as regularized risk minimizers for non-parametric, discrete-valued function spaces. MCCs directly optimize the expected 0\u20131 loss without need for a convex surrogate loss and they can be naturally formulated in the multi-class situation without need for one-vs-one or one-vs-rest constructions. Nevertheless, they can be trained efficiently (exactly in the two-class case, approximately with factor 2 guarantee in the multi-class case) using discrete energy-minimization techniques. This allows scaling them to large datasets, as we show in experiments on standard computer vision datasets"
Directly Optimizing 0-1 Loss for Large-Scale Nonlinear Transductive Multiclass Classification,"We take a new look at graph-based transductive classifiers of min-cut type (MCCs), and we offer a new derivation for them as regularized risk minimizers for non-parametric, discrete-valued function spaces. MCCs directly optimize the expected 0\u20131 loss without need for a convex surrogate loss and they can be naturally formulated in the multi-class situation without need for one-vs-one or one-vs-rest constructions. Nevertheless, they can be trained efficiently (exactly in the two-class case, approximately with factor 2 guarantee in the multi-class case) using discrete energy-minimization techniques. This allows scaling them to large datasets, as we show in experiments on standard computer vision datasets"
Bayesian Conditional Tensor Factorizations for High-Dimensional Classification,"In many application areas, data are collected on a categorical response and highdimensional categorical features, with the goals being to build a parsimoniousmodel for classification while doing inferences on the important features. By using a carefully-structured Tucker factorization, we define a model that can characterizeany classification function, while facilitating variable selection and modeling of higher-order interactions. Following a Bayesian approach, we propose a Markov chain Monte Carlo algorithm for posterior computation accommodating uncertainty in the features to be included. Under near sparsity assumptions, the posterior distribution for the classification function is shown to achieve close tothe parametric rate of contraction even in ultra high-dimensional settings in which the number of candidate features increases exponentially with sample size. Themethods are illustrated through several applications."
Sample Bias Correction for Regression,"This paper presents a theoretical and empirical study of a discrepancy minimization (DM) sample bias correction algorithm in regression.  We give a theoretical analysis of sample bias correction for kernel ridge regression and prove new and more informative learning guarantees in terms of the \emph{discrepancy} of the empirical distributions. These results provide a strong theoretical support for the use and application of the DM algorithm. We have carried out an extensive empirical analysis of this algorithm both with artificial and real-world data sets and compared it with three other state-of-the-art sample bias correction algorithms applicable in regression. Till now, this algorithm had only been applied to the problem of domain adaptation and it was primarily evaluated for computational efficiency. Here we carry out a thorough comparative study of the algorithm used for the problem of sample bias correction. We report in detail the results of these empirical results which demonstrate the benefits of this algorithm."
Analog readout for optical reservoir computers,"Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers."
Analog readout for optical reservoir computers,"Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers."
Analog readout for optical reservoir computers,"Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers."
Analog readout for optical reservoir computers,"Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers."
Analog readout for optical reservoir computers,"Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers."
Robust Out-of-sample Extension for the Local and Global Consistency Algorithm,"The contribution of this paper is twofold. First, we reformulate the Local and Global Consistency (LGC) algorithm and show how it can be computed in linear time with the number of unlabeled examples combining an approximated eigendecomposition with Woodbury's formula. Last, we provide a robust out-of-sample extension for the LGC algorithm using this novel formulation. We provide a naive approximation to turn the proposed out-of-sample extension feasible for large-scale problems. Experiments on a number of benchmark data sets show the effectiveness of the proposed approach."
Visual Interestingness in Image Sequences,"Interestingness -- The power of attracting or holding one's attention(http://www.thefreedictionary.com/Interestingness, 2012/02/22).Visual attention is guided by experience with similar situations. Such experience can make us surprise and excite when unexpected visual events occur, or make us wait for an interesting upcoming event. Consider for example the image sequences in the figure on the left. The spider in front of the camera or the snow on the lens are examples of events that deviate from the context since they violate the expectations, whereas the egg in the stork's nest is an expected but exciting event that attracts huge attention. In this work we investigate what humans consider as interesting in image sequences and to what extent and why current state-of-the-art computer vision methods can automatically detect such interesting events."
Visual Interestingness in Image Sequences,"Interestingness -- The power of attracting or holding one's attention(http://www.thefreedictionary.com/Interestingness, 2012/02/22).Visual attention is guided by experience with similar situations. Such experience can make us surprise and excite when unexpected visual events occur, or make us wait for an interesting upcoming event. Consider for example the image sequences in the figure on the left. The spider in front of the camera or the snow on the lens are examples of events that deviate from the context since they violate the expectations, whereas the egg in the stork's nest is an expected but exciting event that attracts huge attention. In this work we investigate what humans consider as interesting in image sequences and to what extent and why current state-of-the-art computer vision methods can automatically detect such interesting events."
Visual Interestingness in Image Sequences,"Interestingness -- The power of attracting or holding one's attention(http://www.thefreedictionary.com/Interestingness, 2012/02/22).Visual attention is guided by experience with similar situations. Such experience can make us surprise and excite when unexpected visual events occur, or make us wait for an interesting upcoming event. Consider for example the image sequences in the figure on the left. The spider in front of the camera or the snow on the lens are examples of events that deviate from the context since they violate the expectations, whereas the egg in the stork's nest is an expected but exciting event that attracts huge attention. In this work we investigate what humans consider as interesting in image sequences and to what extent and why current state-of-the-art computer vision methods can automatically detect such interesting events."
Accuracy at the Top,"We introduce a new notion of classification accuracy based on the top $\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines. We define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems. We also present margin-based guarantees for this algorithm based on the $\tau$-quantile of the functions in the hypothesis set. Finally, we report the results of several experiments evaluating the performance of our algorithm. In a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top."
Accuracy at the Top,"We introduce a new notion of classification accuracy based on the top $\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines. We define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems. We also present margin-based guarantees for this algorithm based on the $\tau$-quantile of the functions in the hypothesis set. Finally, we report the results of several experiments evaluating the performance of our algorithm. In a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top."
Minimizing Sparse High-Order Energies by Submodular Vertex-Cover,"Inference on high-order graphical models has become increasingly important in recent years. We consider energies with simple 'sparse'high-order potentials. Previous work in this area uses either specialized message-passing or transforms each high-order potential to the pairwise case.We take a fundamentally different approach, transforming theentire original problem into a comparatively small instance of a submodularvertex-cover problem. These vertex-cover instancescan then be attacked by standard pairwise methods,where they run much faster (4--15 times) and are often more effectivethan on the original problem.We evaluate ourapproach on synthetic data, and we show that our algorithmcan be useful in a fast hierarchical clustering and model estimation framework."
Minimizing Sparse High-Order Energies by Submodular Vertex-Cover,"Inference on high-order graphical models has become increasingly important in recent years. We consider energies with simple 'sparse'high-order potentials. Previous work in this area uses either specialized message-passing or transforms each high-order potential to the pairwise case.We take a fundamentally different approach, transforming theentire original problem into a comparatively small instance of a submodularvertex-cover problem. These vertex-cover instancescan then be attacked by standard pairwise methods,where they run much faster (4--15 times) and are often more effectivethan on the original problem.We evaluate ourapproach on synthetic data, and we show that our algorithmcan be useful in a fast hierarchical clustering and model estimation framework."
Minimizing Sparse High-Order Energies by Submodular Vertex-Cover,"Inference on high-order graphical models has become increasingly important in recent years. We consider energies with simple 'sparse'high-order potentials. Previous work in this area uses either specialized message-passing or transforms each high-order potential to the pairwise case.We take a fundamentally different approach, transforming theentire original problem into a comparatively small instance of a submodularvertex-cover problem. These vertex-cover instancescan then be attacked by standard pairwise methods,where they run much faster (4--15 times) and are often more effectivethan on the original problem.We evaluate ourapproach on synthetic data, and we show that our algorithmcan be useful in a fast hierarchical clustering and model estimation framework."
Learning in Real-Time in Repeated Games,"Despite much progress, state-of-the-art learning algorithms for repeated games still often require thousands of moves to learn effectively, even in two-agent, two-action games.  Our goal is to find algorithms that learn effective strategies in tens of moves against many kinds of associates.  Toward this end, we describe a new method designed to increase the learning speed and proficiency of expert learning algorithms.  We show that this method improves four expert learning algorithms, and produces an algorithm that quickly learns effective strategies in many two-agent, two-action repeated matrix games played against many associates."
On the Relation of Loss-Based Error Bounds to Discriminative Training Criteria,"General Bayes decision theory involves a loss function which is linked to the error rate of the corresponding classification task. Many pattern recognition tasks like automatic speech recognition, part-of-speech tagging, machine translation, and other string (word sequence) recognition tasks use the symbol (word, character, tag, etc.) error rate as evaluation measure based on a non-trivial loss function. Opposed to string recognition tasks, most pattern classification tasks assume classes having no symbol level and are evaluated using the classification error performance measure based on the simple 0-1 loss function (cost 0/1 for correct/false classification). We follow a principled approach which derives discriminative training criteria from bounds on the loss-based error. The derived criteria have a sound property ---- in case of infinite training data the corresponding loss-based error is minimized. Theoretical error optimal bounds and criteria are a novelty for loss-based string recognition tasks. Furthermore, the minimum phoneme error (MPE) criterion, which is the state-of-the-art discriminative training criterion in ASR, is shown to be an approximation to one of the proposed criteria. This result connects MPE to the corresponding loss-based error and gives a better and so far unknown theoretical justification for the practical effectiveness of MPE. The theoretical results are complemented by experiments comparing the novel and state-of-the-art discriminative training criteria on large scale ASR tasks."
On the Relation of Loss-Based Error Bounds to Discriminative Training Criteria,"General Bayes decision theory involves a loss function which is linked to the error rate of the corresponding classification task. Many pattern recognition tasks like automatic speech recognition, part-of-speech tagging, machine translation, and other string (word sequence) recognition tasks use the symbol (word, character, tag, etc.) error rate as evaluation measure based on a non-trivial loss function. Opposed to string recognition tasks, most pattern classification tasks assume classes having no symbol level and are evaluated using the classification error performance measure based on the simple 0-1 loss function (cost 0/1 for correct/false classification). We follow a principled approach which derives discriminative training criteria from bounds on the loss-based error. The derived criteria have a sound property ---- in case of infinite training data the corresponding loss-based error is minimized. Theoretical error optimal bounds and criteria are a novelty for loss-based string recognition tasks. Furthermore, the minimum phoneme error (MPE) criterion, which is the state-of-the-art discriminative training criterion in ASR, is shown to be an approximation to one of the proposed criteria. This result connects MPE to the corresponding loss-based error and gives a better and so far unknown theoretical justification for the practical effectiveness of MPE. The theoretical results are complemented by experiments comparing the novel and state-of-the-art discriminative training criteria on large scale ASR tasks."
On the Relation of Loss-Based Error Bounds to Discriminative Training Criteria,"General Bayes decision theory involves a loss function which is linked to the error rate of the corresponding classification task. Many pattern recognition tasks like automatic speech recognition, part-of-speech tagging, machine translation, and other string (word sequence) recognition tasks use the symbol (word, character, tag, etc.) error rate as evaluation measure based on a non-trivial loss function. Opposed to string recognition tasks, most pattern classification tasks assume classes having no symbol level and are evaluated using the classification error performance measure based on the simple 0-1 loss function (cost 0/1 for correct/false classification). We follow a principled approach which derives discriminative training criteria from bounds on the loss-based error. The derived criteria have a sound property ---- in case of infinite training data the corresponding loss-based error is minimized. Theoretical error optimal bounds and criteria are a novelty for loss-based string recognition tasks. Furthermore, the minimum phoneme error (MPE) criterion, which is the state-of-the-art discriminative training criterion in ASR, is shown to be an approximation to one of the proposed criteria. This result connects MPE to the corresponding loss-based error and gives a better and so far unknown theoretical justification for the practical effectiveness of MPE. The theoretical results are complemented by experiments comparing the novel and state-of-the-art discriminative training criteria on large scale ASR tasks."
On the Relation of Loss-Based Error Bounds to Discriminative Training Criteria,"General Bayes decision theory involves a loss function which is linked to the error rate of the corresponding classification task. Many pattern recognition tasks like automatic speech recognition, part-of-speech tagging, machine translation, and other string (word sequence) recognition tasks use the symbol (word, character, tag, etc.) error rate as evaluation measure based on a non-trivial loss function. Opposed to string recognition tasks, most pattern classification tasks assume classes having no symbol level and are evaluated using the classification error performance measure based on the simple 0-1 loss function (cost 0/1 for correct/false classification). We follow a principled approach which derives discriminative training criteria from bounds on the loss-based error. The derived criteria have a sound property ---- in case of infinite training data the corresponding loss-based error is minimized. Theoretical error optimal bounds and criteria are a novelty for loss-based string recognition tasks. Furthermore, the minimum phoneme error (MPE) criterion, which is the state-of-the-art discriminative training criterion in ASR, is shown to be an approximation to one of the proposed criteria. This result connects MPE to the corresponding loss-based error and gives a better and so far unknown theoretical justification for the practical effectiveness of MPE. The theoretical results are complemented by experiments comparing the novel and state-of-the-art discriminative training criteria on large scale ASR tasks."
State Abstraction in Reinforcement Learning by Eliminating Useless Dimensions,"Q-learning and other linear dynamic learning algorithms are subject to Bellman?s curse of dimensionality for any realistic learning problem. This paper introduces a framework for satisficing state abstraction ? one that reduces state dimensionality, improving convergence and reducing computational and memory resources ? by eliminating useless state dimensions. Statistical parameters that are dependent on the state and Q-values identify the relevance of a given state space to a task space and allow state elements that contribute least to task learning to be discarded. Empirical results of applying state abstraction to a canonical single-agent path planning task and to a more difficult multi-agent foraging problem demonstrate utility of the proposed methods in improving learning convergence and performance in resource-constrained learning problems."
State Abstraction in Reinforcement Learning by Eliminating Useless Dimensions,"Q-learning and other linear dynamic learning algorithms are subject to Bellman?s curse of dimensionality for any realistic learning problem. This paper introduces a framework for satisficing state abstraction ? one that reduces state dimensionality, improving convergence and reducing computational and memory resources ? by eliminating useless state dimensions. Statistical parameters that are dependent on the state and Q-values identify the relevance of a given state space to a task space and allow state elements that contribute least to task learning to be discarded. Empirical results of applying state abstraction to a canonical single-agent path planning task and to a more difficult multi-agent foraging problem demonstrate utility of the proposed methods in improving learning convergence and performance in resource-constrained learning problems."
State Abstraction in Reinforcement Learning by Eliminating Useless Dimensions,"Q-learning and other linear dynamic learning algorithms are subject to Bellman?s curse of dimensionality for any realistic learning problem. This paper introduces a framework for satisficing state abstraction ? one that reduces state dimensionality, improving convergence and reducing computational and memory resources ? by eliminating useless state dimensions. Statistical parameters that are dependent on the state and Q-values identify the relevance of a given state space to a task space and allow state elements that contribute least to task learning to be discarded. Empirical results of applying state abstraction to a canonical single-agent path planning task and to a more difficult multi-agent foraging problem demonstrate utility of the proposed methods in improving learning convergence and performance in resource-constrained learning problems."
Near-optimal Batch Mode Active Learning and Stochastic Optimization,"Active learning can lead to dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing,  surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance.In this paper, we consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some case, surprisingly, the use of batches only increases the cost by a constant factor independent of the batch size, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on active learning tasks, as well as adaptive influence maximization in social networks."
Near-optimal Batch Mode Active Learning and Stochastic Optimization,"Active learning can lead to dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing,  surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance.In this paper, we consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some case, surprisingly, the use of batches only increases the cost by a constant factor independent of the batch size, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on active learning tasks, as well as adaptive influence maximization in social networks."
Learning mid-level representations by learning to relate viewpoints,"The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks such as stereopsis and motion analysis.  We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to observed transformations.  We show how this allows us to learn pose-invariant features of objects by observing how the objects change.  We also describe a dataset of objects undergoing 3-D transformations, that we use to evaluate the model."
Learning mid-level representations by learning to relate viewpoints,"The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks such as stereopsis and motion analysis.  We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to observed transformations.  We show how this allows us to learn pose-invariant features of objects by observing how the objects change.  We also describe a dataset of objects undergoing 3-D transformations, that we use to evaluate the model."
Multi-Class Classification with Maximum Margin Multiple Kernel,"We present a new algorithm for multi-class classification withmultiple kernels. Our algorithm is based on a natural notion of themulti-class margin of a kernel. We show that larger values ofthis quantity guarantee the existence of an accurate multi-classpredictor and also define a family of multiple kernel algorithms based onthe maximization of the multi-class margin of a kernel.  Wepresent an extensive theoretical analysis in support of our algorithm,including novel multi-class Rademacher complexity margin bounds.Finally, we also report the results of a series of experiments withseveral data sets, including comparisons where we improve upon theperformance of state-of-the-art algorithms both in binary andmulti-class classification with multiple kernels."
Multi-Class Classification with Maximum Margin Multiple Kernel,"We present a new algorithm for multi-class classification withmultiple kernels. Our algorithm is based on a natural notion of themulti-class margin of a kernel. We show that larger values ofthis quantity guarantee the existence of an accurate multi-classpredictor and also define a family of multiple kernel algorithms based onthe maximization of the multi-class margin of a kernel.  Wepresent an extensive theoretical analysis in support of our algorithm,including novel multi-class Rademacher complexity margin bounds.Finally, we also report the results of a series of experiments withseveral data sets, including comparisons where we improve upon theperformance of state-of-the-art algorithms both in binary andmulti-class classification with multiple kernels."
Perfect Dimensionality Recovery by Variational Bayesian PCA,"The variational Bayesian (VB) approach isone of the best tractable approximations to the Bayesian estimation,and it was demonstrated to perform well in many applications.However, its good performance was not fully understood theoretically.For example, VB sometimes produces a sparse solution,which is regarded as a practical advantage of VB,but such sparsity is hardly observed in the rigorous Bayesian estimation.In this paper, we focus on probabilistic PCA andgive more theoretical insight into the empirical success of VB.More specifically, for the situation where the noise variance is unknown,we derive a sufficient condition for perfect recovery of the true PCAdimensionalityin the large-scale limitwhen the size of an observed matrixgoes to infinity with its column-row ratio fixed.In our analysis, we obtain bounds for a noise variance estimatorand simple closed-form solutions for other parameters,which themselves are actually very useful for better implementation of VB-PCA."
Perfect Dimensionality Recovery by Variational Bayesian PCA,"The variational Bayesian (VB) approach isone of the best tractable approximations to the Bayesian estimation,and it was demonstrated to perform well in many applications.However, its good performance was not fully understood theoretically.For example, VB sometimes produces a sparse solution,which is regarded as a practical advantage of VB,but such sparsity is hardly observed in the rigorous Bayesian estimation.In this paper, we focus on probabilistic PCA andgive more theoretical insight into the empirical success of VB.More specifically, for the situation where the noise variance is unknown,we derive a sufficient condition for perfect recovery of the true PCAdimensionalityin the large-scale limitwhen the size of an observed matrixgoes to infinity with its column-row ratio fixed.In our analysis, we obtain bounds for a noise variance estimatorand simple closed-form solutions for other parameters,which themselves are actually very useful for better implementation of VB-PCA."
Perfect Dimensionality Recovery by Variational Bayesian PCA,"The variational Bayesian (VB) approach isone of the best tractable approximations to the Bayesian estimation,and it was demonstrated to perform well in many applications.However, its good performance was not fully understood theoretically.For example, VB sometimes produces a sparse solution,which is regarded as a practical advantage of VB,but such sparsity is hardly observed in the rigorous Bayesian estimation.In this paper, we focus on probabilistic PCA andgive more theoretical insight into the empirical success of VB.More specifically, for the situation where the noise variance is unknown,we derive a sufficient condition for perfect recovery of the true PCAdimensionalityin the large-scale limitwhen the size of an observed matrixgoes to infinity with its column-row ratio fixed.In our analysis, we obtain bounds for a noise variance estimatorand simple closed-form solutions for other parameters,which themselves are actually very useful for better implementation of VB-PCA."
Mirror Descent Meets Fixed Share (and feels no regret),"Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters."
Mirror Descent Meets Fixed Share (and feels no regret),"Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters."
Mirror Descent Meets Fixed Share (and feels no regret),"Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters."
Mirror Descent Meets Fixed Share (and feels no regret),"Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters."
Downsampling a neighborhood graph,"Given a large neighborhood graph $G$, we would like to downsample it to some smaller graph $G'$ with much fewer vertices. This downsampling procedure should ``keep the geometry in the graph invariant''. To this end we define the notion of geometry-preserving downsampling. We then introduce a simple downsampling procedure that is based on a uniform subsample of vertices that is connected based on shortest path distances in the original graph. We prove that this procedure is geometry-preserving if it is applied to random geometric graphs like $k$-nearest neighbor graphs or $\eps$-graphs. We also show that some other popular downsampling algorithms are not geometry-preserving. "
Downsampling a neighborhood graph,"Given a large neighborhood graph $G$, we would like to downsample it to some smaller graph $G'$ with much fewer vertices. This downsampling procedure should ``keep the geometry in the graph invariant''. To this end we define the notion of geometry-preserving downsampling. We then introduce a simple downsampling procedure that is based on a uniform subsample of vertices that is connected based on shortest path distances in the original graph. We prove that this procedure is geometry-preserving if it is applied to random geometric graphs like $k$-nearest neighbor graphs or $\eps$-graphs. We also show that some other popular downsampling algorithms are not geometry-preserving. "
Mining Frequent Sub-hypergraphs in an Uncertain Hypergraph for Knowledge Transfer,"The knowledge transfer learning can generalize across domains where the types of objects and variables are different. Previous studies ignored the connectivity and creativity of domain knowledge. Thus, these studies just transfer knowledge from a source domain to a target domain, and they do not fully utilize the knowledge from other domains. To address this problem, we proposed a method, called Multi-domain second order knowledge integration (MSKI), for integrating, hybridizing and creating new knowledge, which is formalized into an uncertain hypergraph. Then, we proposed a method to mining frequent sub-hypergraph from the uncertain hypergraph (MFS-UHG). Actually, the frequent sub-hypergraphs are pivot knowledge, which have to be transferred with high priority. We embed the pivot knowledge in the progress of MLN structure learning. The experimental evaluation on four domain datasets shows that our method outperforms state-of-the-art MLN-based transfer learning."
Indexed Optimization: Learning Ramp-Loss SVM in Sublinear Time,"Multidimensional indexing has been frequently used for sublinear-time nearest neighbor search in various applications. In this paper, we demonstrate how this technique can be integrated into learning problem with sublinear sparsity like ramp-loss SVM. We propose an outlier-free convex-relaxation for ramp-loss SVM and an indexed optimization algorithm which solves large-scale problem in sublinear-time even when data cannot fit into memory. We compare our algorithm with state-of-the-art linear hinge-loss solver and ramp-loss solver in both sufficient and limited memory conditions, where our algorithm not only learns several times faster but achieves more accurate result on noisy and large-scale datasets."
Indexed Optimization: Learning Ramp-Loss SVM in Sublinear Time,"Multidimensional indexing has been frequently used for sublinear-time nearest neighbor search in various applications. In this paper, we demonstrate how this technique can be integrated into learning problem with sublinear sparsity like ramp-loss SVM. We propose an outlier-free convex-relaxation for ramp-loss SVM and an indexed optimization algorithm which solves large-scale problem in sublinear-time even when data cannot fit into memory. We compare our algorithm with state-of-the-art linear hinge-loss solver and ramp-loss solver in both sufficient and limited memory conditions, where our algorithm not only learns several times faster but achieves more accurate result on noisy and large-scale datasets."
Indexed Optimization: Learning Ramp-Loss SVM in Sublinear Time,"Multidimensional indexing has been frequently used for sublinear-time nearest neighbor search in various applications. In this paper, we demonstrate how this technique can be integrated into learning problem with sublinear sparsity like ramp-loss SVM. We propose an outlier-free convex-relaxation for ramp-loss SVM and an indexed optimization algorithm which solves large-scale problem in sublinear-time even when data cannot fit into memory. We compare our algorithm with state-of-the-art linear hinge-loss solver and ramp-loss solver in both sufficient and limited memory conditions, where our algorithm not only learns several times faster but achieves more accurate result on noisy and large-scale datasets."
Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
Sparse Additive Matrix Factorization for Robust PCA,"Principal component analysis (PCA) can be regarded as approximating adata matrix witha low-rank one by imposing sparsity on its singular values,and its robust variant further captures sparse noise.In this paper, we extend such sparse matrix learning methods,and propose a novel framework called sparse additive matrix factorization(SAMF).SAMF systematically inducesvarious types of sparsityby the so-called model-induced regularization in the Bayesian framework.We  propose an iterative algorithm calledthe mean update (MU) for the variational Bayesian approximation to SAMF, which gives the global optimal solution for a large subset of parameters in each step.We demonstrate the usefulness of our  methodon artificial dataand the foreground/background video separation."
Sparse Additive Matrix Factorization for Robust PCA,"Principal component analysis (PCA) can be regarded as approximating adata matrix witha low-rank one by imposing sparsity on its singular values,and its robust variant further captures sparse noise.In this paper, we extend such sparse matrix learning methods,and propose a novel framework called sparse additive matrix factorization(SAMF).SAMF systematically inducesvarious types of sparsityby the so-called model-induced regularization in the Bayesian framework.We  propose an iterative algorithm calledthe mean update (MU) for the variational Bayesian approximation to SAMF, which gives the global optimal solution for a large subset of parameters in each step.We demonstrate the usefulness of our  methodon artificial dataand the foreground/background video separation."
Novelty Detection with Extreme Function Theory,"We introduce extreme function theory as a novel method by which probabilistic novelty detection may be performed over functions, where the functions are represented by time-series of (potentially multivariate) discrete observations. We set the method within the framework of Gaussian processes (GP), which offers a convenient means of constructing a distribution over functions. Whereas conventional novelty detection methods aim to identify individually extreme data points, w.r.t. a model of normality constructed using examples of ?normal? data points, the proposed method aims to identify extreme functions, w.r.t. a model of normality constructed using examples of ?normal? functions, where those functions are represented by time-series of observations."
Design of Nonlinear Phase Oscillators with Custom Limit Cycle Shape and Convergence Behavior,In this contribution we present a general way to design low-dimensional nonlinear phase oscillators with arbitrary limit cycle shape and global asymptotic stability. We show examples for which the solution of the nonlinear oscillator with an arbitrary limit cycle shape can be obtained in closed form. The elegance of the oscillator allows for easy extension of the basic system to incorporate other interesting properties such as custom convergence behavior. Numerical simulation is used to show the properties of the proposed oscillator. We also demonstrate two example applications of the introduced oscillator: 1) We show how to couple a number of these oscillators to create a multidimensional Central Pattern Generator. We use this Central Pattern Generator to encode human kinematics as a dynamical system.  2) We present how an adaptive landscape shaping rule can be written for this oscillator to reduce the tracking error when controlling periodic movements of a mechanical system with a simple and suboptimal P-controller.
Design of Nonlinear Phase Oscillators with Custom Limit Cycle Shape and Convergence Behavior,In this contribution we present a general way to design low-dimensional nonlinear phase oscillators with arbitrary limit cycle shape and global asymptotic stability. We show examples for which the solution of the nonlinear oscillator with an arbitrary limit cycle shape can be obtained in closed form. The elegance of the oscillator allows for easy extension of the basic system to incorporate other interesting properties such as custom convergence behavior. Numerical simulation is used to show the properties of the proposed oscillator. We also demonstrate two example applications of the introduced oscillator: 1) We show how to couple a number of these oscillators to create a multidimensional Central Pattern Generator. We use this Central Pattern Generator to encode human kinematics as a dynamical system.  2) We present how an adaptive landscape shaping rule can be written for this oscillator to reduce the tracking error when controlling periodic movements of a mechanical system with a simple and suboptimal P-controller.
Design of Nonlinear Phase Oscillators with Custom Limit Cycle Shape and Convergence Behavior,In this contribution we present a general way to design low-dimensional nonlinear phase oscillators with arbitrary limit cycle shape and global asymptotic stability. We show examples for which the solution of the nonlinear oscillator with an arbitrary limit cycle shape can be obtained in closed form. The elegance of the oscillator allows for easy extension of the basic system to incorporate other interesting properties such as custom convergence behavior. Numerical simulation is used to show the properties of the proposed oscillator. We also demonstrate two example applications of the introduced oscillator: 1) We show how to couple a number of these oscillators to create a multidimensional Central Pattern Generator. We use this Central Pattern Generator to encode human kinematics as a dynamical system.  2) We present how an adaptive landscape shaping rule can be written for this oscillator to reduce the tracking error when controlling periodic movements of a mechanical system with a simple and suboptimal P-controller.
Convex Shape Priors for Isometry-Invariant Variational Image Segmentation,"Convex relaxations of variational approaches to image segmentation constitute an active field of research. Convex state-of-the-art functionals penalize segmentation boundaries in terms of length or curvature, whereas variational approaches that take into account more specific shape knowledge suffer from non-convexity, thus requiring careful initializations to converge. Moreover, invariant shape comparison is only implicitly achieved by additionally optimizing over transformation parameters, aggravating issues of non-convexity.This paper combines for the first time convex state-of-the-art variational segmentation in terms of continuous cuts with a variational shape prior, based on shape representation by metric structures that enables fully invariant shape comparison and matching, while preserving convexity of the overall variational approach. Thus standard convex programming techniques can be applied to variational segmentation enhanced by invariant shape priors."
Convex Shape Priors for Isometry-Invariant Variational Image Segmentation,"Convex relaxations of variational approaches to image segmentation constitute an active field of research. Convex state-of-the-art functionals penalize segmentation boundaries in terms of length or curvature, whereas variational approaches that take into account more specific shape knowledge suffer from non-convexity, thus requiring careful initializations to converge. Moreover, invariant shape comparison is only implicitly achieved by additionally optimizing over transformation parameters, aggravating issues of non-convexity.This paper combines for the first time convex state-of-the-art variational segmentation in terms of continuous cuts with a variational shape prior, based on shape representation by metric structures that enables fully invariant shape comparison and matching, while preserving convexity of the overall variational approach. Thus standard convex programming techniques can be applied to variational segmentation enhanced by invariant shape priors."
Boundary Preserving Distance Metric for Superpixelization ,"Superpixel is a continuous pixel region sharing similar characteristics, such as color, texture, etc. In recent years, it has been widely accepted as an alternative to physically-defined grid pixels to aid and enhance further image analysis in various applications. In this paper, a generalized superpixelization algorithm with boundary preserving distance metric is proposed. It outperforms state-of-the-art superpixelization algorithms in three aspects: 1. It can be generalized to both color and highly-textured images; 2. Generated superpixels are more compact with less number of superpixels; 3. Less computational complexity and memoryconsumption."
Boundary Preserving Distance Metric for Superpixelization ,"Superpixel is a continuous pixel region sharing similar characteristics, such as color, texture, etc. In recent years, it has been widely accepted as an alternative to physically-defined grid pixels to aid and enhance further image analysis in various applications. In this paper, a generalized superpixelization algorithm with boundary preserving distance metric is proposed. It outperforms state-of-the-art superpixelization algorithms in three aspects: 1. It can be generalized to both color and highly-textured images; 2. Generated superpixels are more compact with less number of superpixels; 3. Less computational complexity and memoryconsumption."
Boundary Preserving Distance Metric for Superpixelization ,"Superpixel is a continuous pixel region sharing similar characteristics, such as color, texture, etc. In recent years, it has been widely accepted as an alternative to physically-defined grid pixels to aid and enhance further image analysis in various applications. In this paper, a generalized superpixelization algorithm with boundary preserving distance metric is proposed. It outperforms state-of-the-art superpixelization algorithms in three aspects: 1. It can be generalized to both color and highly-textured images; 2. Generated superpixels are more compact with less number of superpixels; 3. Less computational complexity and memoryconsumption."
Boundary Preserving Distance Metric for Superpixelization ,"Superpixel is a continuous pixel region sharing similar characteristics, such as color, texture, etc. In recent years, it has been widely accepted as an alternative to physically-defined grid pixels to aid and enhance further image analysis in various applications. In this paper, a generalized superpixelization algorithm with boundary preserving distance metric is proposed. It outperforms state-of-the-art superpixelization algorithms in three aspects: 1. It can be generalized to both color and highly-textured images; 2. Generated superpixels are more compact with less number of superpixels; 3. Less computational complexity and memoryconsumption."
Boundary Preserving Distance Metric for Superpixelization ,"Superpixel is a continuous pixel region sharing similar characteristics, such as color, texture, etc. In recent years, it has been widely accepted as an alternative to physically-defined grid pixels to aid and enhance further image analysis in various applications. In this paper, a generalized superpixelization algorithm with boundary preserving distance metric is proposed. It outperforms state-of-the-art superpixelization algorithms in three aspects: 1. It can be generalized to both color and highly-textured images; 2. Generated superpixels are more compact with less number of superpixels; 3. Less computational complexity and memoryconsumption."
Boundary Preserving Distance Metric for Superpixelization ,"Superpixel is a continuous pixel region sharing similar characteristics, such as color, texture, etc. In recent years, it has been widely accepted as an alternative to physically-defined grid pixels to aid and enhance further image analysis in various applications. In this paper, a generalized superpixelization algorithm with boundary preserving distance metric is proposed. It outperforms state-of-the-art superpixelization algorithms in three aspects: 1. It can be generalized to both color and highly-textured images; 2. Generated superpixels are more compact with less number of superpixels; 3. Less computational complexity and memoryconsumption."
Near-optimal Differentially Private Principal Components,"Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension.  Many current data sets of interest contain private or sensitive information about individuals.  Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs.  Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs.  In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output.  We demonstrate that on real data, there this a large performance gap between the existing methods and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling."
Inverse Reinforcement Learning: An Alternative Definition,"This paper discusses the foundations of the definition of the Inverse Reinforcement Learning (IRL) problem. Given a finite-Markov Decision Process (MDP) without reward function and an expert policy, IRL is usually stated as the problem of finding a reward function for which the expert policy is optimal. This problem is clearly ill-posed as the zero-reward function is always solution. Thus, in order to give a more mature definition of the IRL problem, we introduce the notions of set-policy and optimality for a set-policy. These notions allow us to create a particular partition of the space of reward functions for which each part corresponds to a unique set-policy and inversely. Moreover, each set-policy is optimal for each reward of its corresponding part of the partition and only them. Thanks to this partition, we give a new and well-posed definition of IRL. Based on this framework, we introduce a new algorithm."
Inverse Reinforcement Learning: An Alternative Definition,"This paper discusses the foundations of the definition of the Inverse Reinforcement Learning (IRL) problem. Given a finite-Markov Decision Process (MDP) without reward function and an expert policy, IRL is usually stated as the problem of finding a reward function for which the expert policy is optimal. This problem is clearly ill-posed as the zero-reward function is always solution. Thus, in order to give a more mature definition of the IRL problem, we introduce the notions of set-policy and optimality for a set-policy. These notions allow us to create a particular partition of the space of reward functions for which each part corresponds to a unique set-policy and inversely. Moreover, each set-policy is optimal for each reward of its corresponding part of the partition and only them. Thanks to this partition, we give a new and well-posed definition of IRL. Based on this framework, we introduce a new algorithm."
Inverse Reinforcement Learning: An Alternative Definition,"This paper discusses the foundations of the definition of the Inverse Reinforcement Learning (IRL) problem. Given a finite-Markov Decision Process (MDP) without reward function and an expert policy, IRL is usually stated as the problem of finding a reward function for which the expert policy is optimal. This problem is clearly ill-posed as the zero-reward function is always solution. Thus, in order to give a more mature definition of the IRL problem, we introduce the notions of set-policy and optimality for a set-policy. These notions allow us to create a particular partition of the space of reward functions for which each part corresponds to a unique set-policy and inversely. Moreover, each set-policy is optimal for each reward of its corresponding part of the partition and only them. Thanks to this partition, we give a new and well-posed definition of IRL. Based on this framework, we introduce a new algorithm."
Random function priors for exchangeable graphs and arrays,"A fundamental problem in the analysis of relational data---graphs, matrices or higher-dimensional arrays---is to extract a summary of the common structure underlying relations between individual entities. A successful approach is latent variable modeling, which summarizes this structure as an embedding into a suitable latent space. Results in probability theory, due to Aldous, Hoover and Kallenberg, show that relational data satisfying an exchangeability property can be represented in terms of a random measurable function. In a Bayesian model, this function constitutes the natural model parameter, and we discuss how available latent variable models can be classified according to how they implicitly approximate this parameter. We obtain a flexible yet simple model for relational data by representing the  parameter function as a Gaussian process. Efficient inference draws on the large available arsenal of Gaussian process algorithms; sparse approximations prove particularly useful. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases."
Random function priors for exchangeable graphs and arrays,"A fundamental problem in the analysis of relational data---graphs, matrices or higher-dimensional arrays---is to extract a summary of the common structure underlying relations between individual entities. A successful approach is latent variable modeling, which summarizes this structure as an embedding into a suitable latent space. Results in probability theory, due to Aldous, Hoover and Kallenberg, show that relational data satisfying an exchangeability property can be represented in terms of a random measurable function. In a Bayesian model, this function constitutes the natural model parameter, and we discuss how available latent variable models can be classified according to how they implicitly approximate this parameter. We obtain a flexible yet simple model for relational data by representing the  parameter function as a Gaussian process. Efficient inference draws on the large available arsenal of Gaussian process algorithms; sparse approximations prove particularly useful. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases."
Random function priors for exchangeable graphs and arrays,"A fundamental problem in the analysis of relational data---graphs, matrices or higher-dimensional arrays---is to extract a summary of the common structure underlying relations between individual entities. A successful approach is latent variable modeling, which summarizes this structure as an embedding into a suitable latent space. Results in probability theory, due to Aldous, Hoover and Kallenberg, show that relational data satisfying an exchangeability property can be represented in terms of a random measurable function. In a Bayesian model, this function constitutes the natural model parameter, and we discuss how available latent variable models can be classified according to how they implicitly approximate this parameter. We obtain a flexible yet simple model for relational data by representing the  parameter function as a Gaussian process. Efficient inference draws on the large available arsenal of Gaussian process algorithms; sparse approximations prove particularly useful. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases."
Multi-Source Sensing with Group Reliablity Ranking,"Prediction and decision-making tasks are often negatively impacted by the low-quality of the data collected from multiple sensing sources. In order to address these issues, we study the joint ranking of the reliability of data sources and infer their true values from their observations. Data sources are very often highly interdependent. Clearly, this interdependence can play an important role in the reliability estimation process as well. This is achieved by latent grouping of the dependent sources, and analyzing the underlying reliability at this level. Different groups usually contain conflicting observations from one other. In order to resolve such conflicts and improve the data quality, we assume that the observations from each group are not equal, and can be ranked based on their reliability. For this purpose, we impose a prior for the group reliability in a Markov chain structure, and rank the group reliability levels of the observations in a Bayesian inference framework. The highly ranked groups in the chain provide more information about data objects, and thus play a more important role in the true value inference. Finally, we demonstrate the effectiveness of the proposed approach on two real data sets."
Multi-Source Sensing with Group Reliablity Ranking,"Prediction and decision-making tasks are often negatively impacted by the low-quality of the data collected from multiple sensing sources. In order to address these issues, we study the joint ranking of the reliability of data sources and infer their true values from their observations. Data sources are very often highly interdependent. Clearly, this interdependence can play an important role in the reliability estimation process as well. This is achieved by latent grouping of the dependent sources, and analyzing the underlying reliability at this level. Different groups usually contain conflicting observations from one other. In order to resolve such conflicts and improve the data quality, we assume that the observations from each group are not equal, and can be ranked based on their reliability. For this purpose, we impose a prior for the group reliability in a Markov chain structure, and rank the group reliability levels of the observations in a Bayesian inference framework. The highly ranked groups in the chain provide more information about data objects, and thus play a more important role in the true value inference. Finally, we demonstrate the effectiveness of the proposed approach on two real data sets."
Multi-Source Sensing with Group Reliablity Ranking,"Prediction and decision-making tasks are often negatively impacted by the low-quality of the data collected from multiple sensing sources. In order to address these issues, we study the joint ranking of the reliability of data sources and infer their true values from their observations. Data sources are very often highly interdependent. Clearly, this interdependence can play an important role in the reliability estimation process as well. This is achieved by latent grouping of the dependent sources, and analyzing the underlying reliability at this level. Different groups usually contain conflicting observations from one other. In order to resolve such conflicts and improve the data quality, we assume that the observations from each group are not equal, and can be ranked based on their reliability. For this purpose, we impose a prior for the group reliability in a Markov chain structure, and rank the group reliability levels of the observations in a Bayesian inference framework. The highly ranked groups in the chain provide more information about data objects, and thus play a more important role in the true value inference. Finally, we demonstrate the effectiveness of the proposed approach on two real data sets."
Multi-Source Sensing with Group Reliablity Ranking,"Prediction and decision-making tasks are often negatively impacted by the low-quality of the data collected from multiple sensing sources. In order to address these issues, we study the joint ranking of the reliability of data sources and infer their true values from their observations. Data sources are very often highly interdependent. Clearly, this interdependence can play an important role in the reliability estimation process as well. This is achieved by latent grouping of the dependent sources, and analyzing the underlying reliability at this level. Different groups usually contain conflicting observations from one other. In order to resolve such conflicts and improve the data quality, we assume that the observations from each group are not equal, and can be ranked based on their reliability. For this purpose, we impose a prior for the group reliability in a Markov chain structure, and rank the group reliability levels of the observations in a Bayesian inference framework. The highly ranked groups in the chain provide more information about data objects, and thus play a more important role in the true value inference. Finally, we demonstrate the effectiveness of the proposed approach on two real data sets."
Learning a reward function from demonstrations: a cascaded supervised learning approach,"Discovering the reward function optimized by an expert in an MDP, usually referred to as the IRL problem, is addressed in this paper. The proposed generic model-free contribution consists in cascading a classification and a regression steps to produce a non-trivial reward function. We prove the expert policy to be near optimal with respect to this reward. The algorithm does not require solving any direct reinforcement learning problem (unlike most of IRL algorithms). With the help of some heuristics, it can be instantiated to solely rely on transitions from the expert (no need to sample trajectories according to other policies). All of this is illustrated through a standard benchmark."
Learning a reward function from demonstrations: a cascaded supervised learning approach,"Discovering the reward function optimized by an expert in an MDP, usually referred to as the IRL problem, is addressed in this paper. The proposed generic model-free contribution consists in cascading a classification and a regression steps to produce a non-trivial reward function. We prove the expert policy to be near optimal with respect to this reward. The algorithm does not require solving any direct reinforcement learning problem (unlike most of IRL algorithms). With the help of some heuristics, it can be instantiated to solely rely on transitions from the expert (no need to sample trajectories according to other policies). All of this is illustrated through a standard benchmark."
Learning a reward function from demonstrations: a cascaded supervised learning approach,"Discovering the reward function optimized by an expert in an MDP, usually referred to as the IRL problem, is addressed in this paper. The proposed generic model-free contribution consists in cascading a classification and a regression steps to produce a non-trivial reward function. We prove the expert policy to be near optimal with respect to this reward. The algorithm does not require solving any direct reinforcement learning problem (unlike most of IRL algorithms). With the help of some heuristics, it can be instantiated to solely rely on transitions from the expert (no need to sample trajectories according to other policies). All of this is illustrated through a standard benchmark."
Learning a reward function from demonstrations: a cascaded supervised learning approach,"Discovering the reward function optimized by an expert in an MDP, usually referred to as the IRL problem, is addressed in this paper. The proposed generic model-free contribution consists in cascading a classification and a regression steps to produce a non-trivial reward function. We prove the expert policy to be near optimal with respect to this reward. The algorithm does not require solving any direct reinforcement learning problem (unlike most of IRL algorithms). With the help of some heuristics, it can be instantiated to solely rely on transitions from the expert (no need to sample trajectories according to other policies). All of this is illustrated through a standard benchmark."
Inverse Reinforcement Learning through Structured Classification,"This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called featureexpectation of the expert as the parameterization of the score function of a multiclass classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving a single time the direct RL problem. Moreover, up to the use of some heuristic, it may work with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator."
Inverse Reinforcement Learning through Structured Classification,"This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called featureexpectation of the expert as the parameterization of the score function of a multiclass classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving a single time the direct RL problem. Moreover, up to the use of some heuristic, it may work with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator."
Inverse Reinforcement Learning through Structured Classification,"This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called featureexpectation of the expert as the parameterization of the score function of a multiclass classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving a single time the direct RL problem. Moreover, up to the use of some heuristic, it may work with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator."
Inverse Reinforcement Learning through Structured Classification,"This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called featureexpectation of the expert as the parameterization of the score function of a multiclass classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving a single time the direct RL problem. Moreover, up to the use of some heuristic, it may work with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator."
Classification with Asymmetric Label Noise,"We consider the problem of binary classification when the training labels are noisy. Previous theoretical work on this problem assumes that the two classes are separable, or that the label noise is independent of the class label. We present a general framework that allows for overlapping class-conditional distributions (so that the true labels are not deterministic) and that accommodates asymmetric label noise. This problem is motivated by several applications including nuclear particle classification, where background radiation and other environmental factors make it impossible to obtain pure training samples. We first identify a necessary and sufficient condition that makes the unknown noise proportions identifiable. Under this assumption, we describe estimation of the Type I and Type II errors, and use these estimates to design a classification rule that is consistent with respect to the minmax criterion. Our approach depends critically on recent results for mixture proportion estimation, which is the problem of estimating the percentage of one distribution that is present in another."
Episodic Risk-Sensitive Actor-Critic,"We present an episodic risk-sensitive actor-critic algorithm that is suitable for stochastic, continuous, and high-dimensional systems with policy-dependent cost variance. We generalize the simple stochastic gradient descent update to the risk-sensitive case, derive the minimum variance baseline, and show that, under certain conditions, it leads to an unbiased estimate of the gradient of the risk-sensitive objective. We show that the local critic structure used in the update can be exploited to interweave offline and online search to select local greedy policies or quickly change risk sensitivity. Our experiments include learning to lift a heavy, liquid-filled bottle with a dynamically balancing mobile manipulator and online learning of stiffnesses for fall bracing after very large impacts. "
Episodic Risk-Sensitive Actor-Critic,"We present an episodic risk-sensitive actor-critic algorithm that is suitable for stochastic, continuous, and high-dimensional systems with policy-dependent cost variance. We generalize the simple stochastic gradient descent update to the risk-sensitive case, derive the minimum variance baseline, and show that, under certain conditions, it leads to an unbiased estimate of the gradient of the risk-sensitive objective. We show that the local critic structure used in the update can be exploited to interweave offline and online search to select local greedy policies or quickly change risk sensitivity. Our experiments include learning to lift a heavy, liquid-filled bottle with a dynamically balancing mobile manipulator and online learning of stiffnesses for fall bracing after very large impacts. "
Episodic Risk-Sensitive Actor-Critic,"We present an episodic risk-sensitive actor-critic algorithm that is suitable for stochastic, continuous, and high-dimensional systems with policy-dependent cost variance. We generalize the simple stochastic gradient descent update to the risk-sensitive case, derive the minimum variance baseline, and show that, under certain conditions, it leads to an unbiased estimate of the gradient of the risk-sensitive objective. We show that the local critic structure used in the update can be exploited to interweave offline and online search to select local greedy policies or quickly change risk sensitivity. Our experiments include learning to lift a heavy, liquid-filled bottle with a dynamically balancing mobile manipulator and online learning of stiffnesses for fall bracing after very large impacts. "
Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics,"Non-linear dynamical systems (DS) have been used extensively for building generative models of human behavior. Its applications range from modeling brain dynamics  to encoding motor commands. Many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space. Although these enable the robots to react against sudden perturbations without any re-planning, the motions are always directed towards a single target. In this work, we focus on combining several such DS with distinct attractors, resulting in a multi-stable DS. We show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object. While exploiting multiple attractors provides more flexibility in recovering from unseen perturbations, it also increases the complexity of the underlying learning problem. Here we present the Augmented-SVM (A-SVM) model which inherits region partitioning ability of the well known SVM classifier and is augmented with novel constraints derived from the individual DS. The new constraints modify the original SVM dual whose optimal solution then results in a new class of support vectors (SV). These new SV ensure that the resulting multi-stable DS incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction. We show, via implementations on a simulated 10 degrees of freedom mobile robotic platform, that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations. "
Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics,"Non-linear dynamical systems (DS) have been used extensively for building generative models of human behavior. Its applications range from modeling brain dynamics  to encoding motor commands. Many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space. Although these enable the robots to react against sudden perturbations without any re-planning, the motions are always directed towards a single target. In this work, we focus on combining several such DS with distinct attractors, resulting in a multi-stable DS. We show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object. While exploiting multiple attractors provides more flexibility in recovering from unseen perturbations, it also increases the complexity of the underlying learning problem. Here we present the Augmented-SVM (A-SVM) model which inherits region partitioning ability of the well known SVM classifier and is augmented with novel constraints derived from the individual DS. The new constraints modify the original SVM dual whose optimal solution then results in a new class of support vectors (SV). These new SV ensure that the resulting multi-stable DS incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction. We show, via implementations on a simulated 10 degrees of freedom mobile robotic platform, that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations. "
Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search,"Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sampled-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration."
Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search,"Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sampled-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration."
Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search,"Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sampled-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration."
Mining the brain with a theory of visual attention,"We propose a new target objective for BCI systems in which a parametric model of early visual perception follows the EEG decoder stage. This approach enables the supervised extraction of EEG components that jointly predict behavioral responses. We analyze the pre-stimulus EEG activity from a letter-recognition task using two EEG decoders running in stereo, and detect distinct components of the EEG that predict separable attentional parameters on a single-trial level."
Dimensionality Dependent PAC-Bayes Margin Bound,"Margin is one of the most important concepts in machine learning. Previous margin bounds, both for SVM and for boosting, are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or infinite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors fixed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of finite dimension; and the two bounds tend to be equivalent as the dimension goes to infinity. In addition, we show that the VC bound for linear classifiers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classifiers."
Dimensionality Dependent PAC-Bayes Margin Bound,"Margin is one of the most important concepts in machine learning. Previous margin bounds, both for SVM and for boosting, are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or infinite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors fixed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of finite dimension; and the two bounds tend to be equivalent as the dimension goes to infinity. In addition, we show that the VC bound for linear classifiers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classifiers."
Fast Exact Search in Hamming Space Using Anchors,"Recently there are a surge of approaches to learn a compact binary code representation of the data for large-scale nearest neighbor search. Once the binary codes are obtained, these methods usually employ the linear scan or hash table lookup to search in Hamming space, which are not efficient. There are some efforts trying to speed up the search in Hamming space, however, these techniques require specific data structure which consumes a lot of memory. In this paper, we propose a novel method for sub-linear exact search in Hamming space with only linear memory cost. The key idea of our method is reducing the search range using the triangle inequality induced by a small number of anchors, which are the representative samples of the database. An effective data structure is proposed to efficiently reject samples that are unnecessarily calculated. To further improve the performance, a MaxMin algorithm is presented to select the anchors. Both the theoretical analysis and empirical evaluation demonstrate that our approach is not only more efficient than the state-of-the-art methods but also consumes much less memory, which is essential to the large-scale problems."
Fast Exact Search in Hamming Space Using Anchors,"Recently there are a surge of approaches to learn a compact binary code representation of the data for large-scale nearest neighbor search. Once the binary codes are obtained, these methods usually employ the linear scan or hash table lookup to search in Hamming space, which are not efficient. There are some efforts trying to speed up the search in Hamming space, however, these techniques require specific data structure which consumes a lot of memory. In this paper, we propose a novel method for sub-linear exact search in Hamming space with only linear memory cost. The key idea of our method is reducing the search range using the triangle inequality induced by a small number of anchors, which are the representative samples of the database. An effective data structure is proposed to efficiently reject samples that are unnecessarily calculated. To further improve the performance, a MaxMin algorithm is presented to select the anchors. Both the theoretical analysis and empirical evaluation demonstrate that our approach is not only more efficient than the state-of-the-art methods but also consumes much less memory, which is essential to the large-scale problems."
Fast Exact Search in Hamming Space Using Anchors,"Recently there are a surge of approaches to learn a compact binary code representation of the data for large-scale nearest neighbor search. Once the binary codes are obtained, these methods usually employ the linear scan or hash table lookup to search in Hamming space, which are not efficient. There are some efforts trying to speed up the search in Hamming space, however, these techniques require specific data structure which consumes a lot of memory. In this paper, we propose a novel method for sub-linear exact search in Hamming space with only linear memory cost. The key idea of our method is reducing the search range using the triangle inequality induced by a small number of anchors, which are the representative samples of the database. An effective data structure is proposed to efficiently reject samples that are unnecessarily calculated. To further improve the performance, a MaxMin algorithm is presented to select the anchors. Both the theoretical analysis and empirical evaluation demonstrate that our approach is not only more efficient than the state-of-the-art methods but also consumes much less memory, which is essential to the large-scale problems."
Learning via Margin Maximization under Max-Mini Entropy,"This paper presents a new learning framework in the context of classification using nearest neighbors. The learning problem is mathematically formulated as a large margin problem under the principle of max-mini entropy. The margin of a sample is locally defined by its nearest neighbors, which allows the decomposition of any given sophisticated nonlinear separation problem into a series of local and simpler ones. Margin maximization directly leads to better generalizability, and the max-mini entropy principle makes the learned classifier more robust. An iterative algorithm was derived to implement this learning framework.  The convergence analysis of the algorithm is straightforward.  We demonstrate the power of our new approach by comparing it to a couple of widely used classifiers (such as Support Vector Machines, decision tree, na?ve Bayes classifier, k-nearest neighbors, and so on) and several other margin-based local learners (such as RELIEF, Simba, and G-flip, and so on) on a number of UCI and gene expression datasets."
Learning via Margin Maximization under Max-Mini Entropy,"This paper presents a new learning framework in the context of classification using nearest neighbors. The learning problem is mathematically formulated as a large margin problem under the principle of max-mini entropy. The margin of a sample is locally defined by its nearest neighbors, which allows the decomposition of any given sophisticated nonlinear separation problem into a series of local and simpler ones. Margin maximization directly leads to better generalizability, and the max-mini entropy principle makes the learned classifier more robust. An iterative algorithm was derived to implement this learning framework.  The convergence analysis of the algorithm is straightforward.  We demonstrate the power of our new approach by comparing it to a couple of widely used classifiers (such as Support Vector Machines, decision tree, na?ve Bayes classifier, k-nearest neighbors, and so on) and several other margin-based local learners (such as RELIEF, Simba, and G-flip, and so on) on a number of UCI and gene expression datasets."
Local stability and robustness of sparse dictionary learning in the presence of noise,"A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries and noisy signals, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations."
Local stability and robustness of sparse dictionary learning in the presence of noise,"A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries and noisy signals, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations."
Local stability and robustness of sparse dictionary learning in the presence of noise,"A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries and noisy signals, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations."
Unifying Common Multi-Task Multi-Kernel Learning Problems,"Over the past few years, Multi-Kernel Learning (MKL) has received significant attention among data-driven feature selection techniques. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including Multi-Task Learning (MTL). Soving different Multi-Task Multi-Kernel Learning (Multi-Task MKL) formulations usually involves designing algorithms that are tailored to the problem at hand, which is a non-trivial accomplishment. In this paper we present a general Multi-Task MKL framework that subsumes well-known Multi-Task MKL formulations, as well as several important MKL approaches on single-task problems. We then derive a simple algorithm that can solve the unifying framework. To furthermore underline the generality of the proposed framework, we formulate a new learning problem, namely Partially-Shared Common Space (PSCS) Multi-Task MKL, and demonstrate its merits through experimentation."
Unifying Common Multi-Task Multi-Kernel Learning Problems,"Over the past few years, Multi-Kernel Learning (MKL) has received significant attention among data-driven feature selection techniques. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including Multi-Task Learning (MTL). Soving different Multi-Task Multi-Kernel Learning (Multi-Task MKL) formulations usually involves designing algorithms that are tailored to the problem at hand, which is a non-trivial accomplishment. In this paper we present a general Multi-Task MKL framework that subsumes well-known Multi-Task MKL formulations, as well as several important MKL approaches on single-task problems. We then derive a simple algorithm that can solve the unifying framework. To furthermore underline the generality of the proposed framework, we formulate a new learning problem, namely Partially-Shared Common Space (PSCS) Multi-Task MKL, and demonstrate its merits through experimentation."
Error Correction in Learning Using SVMs,"This paper is concerned with learning binary classifiers under adversarial label-noise. We introduce the problem of error correction in learning where the goal is to recover the original clean data from a label-manipulated version of it, given (i)~ no constraints on the adversary other than an upper-bound on the number of errors, and (ii)~some regularity properties for the original data.  We present a simple and practical error-correction algorithm called SubSVMs that essentially learns separate SVMs on several class-balanced, small-size (often log-size), random subsets of the data and then reclassifies the training points using a majority vote. Our analysis reveals the need for the two main ingredients of SubSVMs, namely class-balanced sampling and subsampled bagging. We present experimental results on synthetic as well as benchmark data to demonstrate the effectiveness of our approach. In addition to the primary goal of noise-tolerance, log-size subsampled bagging also yields significant run-time advantages over standard SVMs."
Error Correction in Learning Using SVMs,"This paper is concerned with learning binary classifiers under adversarial label-noise. We introduce the problem of error correction in learning where the goal is to recover the original clean data from a label-manipulated version of it, given (i)~ no constraints on the adversary other than an upper-bound on the number of errors, and (ii)~some regularity properties for the original data.  We present a simple and practical error-correction algorithm called SubSVMs that essentially learns separate SVMs on several class-balanced, small-size (often log-size), random subsets of the data and then reclassifies the training points using a majority vote. Our analysis reveals the need for the two main ingredients of SubSVMs, namely class-balanced sampling and subsampled bagging. We present experimental results on synthetic as well as benchmark data to demonstrate the effectiveness of our approach. In addition to the primary goal of noise-tolerance, log-size subsampled bagging also yields significant run-time advantages over standard SVMs."
Predicting a Neural Spiking Probability Map,"This paper models signals and noise for extracellular neural recording. Although recorded data approximately follow Gaussian distribution, there are slight deviations that are critical for signal detection: a statistical examination of neural data in Hilbert space shows that noise forms an exponential term while signals form a polynomial term. These two terms can be used to estimate a spiking probability map which tells the probability of spike presence in any time window. The predictions of the two terms and the spiking probability map are quantitatively assessed with both animal and synthesized data. To demonstrate the usefulness of the work, a few application examples are presented with quantitative experimental results that have shown improved signal processing reliability. Last, we report an algorithm implementation in a 130nm CMOS process for on-the-fly processing multi-channel neural data."
Predicting a Neural Spiking Probability Map,"This paper models signals and noise for extracellular neural recording. Although recorded data approximately follow Gaussian distribution, there are slight deviations that are critical for signal detection: a statistical examination of neural data in Hilbert space shows that noise forms an exponential term while signals form a polynomial term. These two terms can be used to estimate a spiking probability map which tells the probability of spike presence in any time window. The predictions of the two terms and the spiking probability map are quantitatively assessed with both animal and synthesized data. To demonstrate the usefulness of the work, a few application examples are presented with quantitative experimental results that have shown improved signal processing reliability. Last, we report an algorithm implementation in a 130nm CMOS process for on-the-fly processing multi-channel neural data."
Exact Cheeger-Cuts through Discrete Newton Method,"The Cheeger-cut is one of the most popular formulations for {?it balanced clustering} that has been actively discussed in machine learning. In this paper, we propose an efficient algorithm for finding an exact Cheeger-cut. We formulate the Cheeger-cut as a parametric submodular-minimization problem and apply the discrete Newton method, whose convergence to an optimal solution is guaranteed. We derive the optimality conditions for the sub-problem solved at each iteration and develop an efficient algorithm for this problem, which is calculated as submodular minimization using the maximum-flow method. We also analyze the computational complexity of the proposed algorithm, and consider an extension to multiple clusters although this is no longer solvable as a convergent procedure. The performance of the proposed method is investigated through empirical experiments."
Exact Cheeger-Cuts through Discrete Newton Method,"The Cheeger-cut is one of the most popular formulations for {?it balanced clustering} that has been actively discussed in machine learning. In this paper, we propose an efficient algorithm for finding an exact Cheeger-cut. We formulate the Cheeger-cut as a parametric submodular-minimization problem and apply the discrete Newton method, whose convergence to an optimal solution is guaranteed. We derive the optimality conditions for the sub-problem solved at each iteration and develop an efficient algorithm for this problem, which is calculated as submodular minimization using the maximum-flow method. We also analyze the computational complexity of the proposed algorithm, and consider an extension to multiple clusters although this is no longer solvable as a convergent procedure. The performance of the proposed method is investigated through empirical experiments."
Exact Cheeger-Cuts through Discrete Newton Method,"The Cheeger-cut is one of the most popular formulations for {?it balanced clustering} that has been actively discussed in machine learning. In this paper, we propose an efficient algorithm for finding an exact Cheeger-cut. We formulate the Cheeger-cut as a parametric submodular-minimization problem and apply the discrete Newton method, whose convergence to an optimal solution is guaranteed. We derive the optimality conditions for the sub-problem solved at each iteration and develop an efficient algorithm for this problem, which is calculated as submodular minimization using the maximum-flow method. We also analyze the computational complexity of the proposed algorithm, and consider an extension to multiple clusters although this is no longer solvable as a convergent procedure. The performance of the proposed method is investigated through empirical experiments."
Temporal Abstraction in Reinforcement Learning based on Holonic Concept Clustering and Attentional System,"This paper proposes a new method which extracts bottleneck states automatically based on abstraction concepts for reinforcement learning agents in offline/online manner -incremental-. Generally, the existing mechanisms for creating temporally extended actions need to burdensome of calculations and eventually prone to error. Our approach is built on lines of researches from cognitive science and behavior analysis. We utilized attentional mechanisms as an effective tool to extract bottlenecks. Holonic concept clustering and attentional-system are the motivation and the core of the proposed method. The experimentations confirmed that the proposed method is able to identify bottlenecks with more precision and thereupon the speed of learning and the ability of knowledge transferring are improved significantly. Also, the time complexity of the proposed method is considerably less in comparison to other similar methods."
Temporal Abstraction in Reinforcement Learning based on Holonic Concept Clustering and Attentional System,"This paper proposes a new method which extracts bottleneck states automatically based on abstraction concepts for reinforcement learning agents in offline/online manner -incremental-. Generally, the existing mechanisms for creating temporally extended actions need to burdensome of calculations and eventually prone to error. Our approach is built on lines of researches from cognitive science and behavior analysis. We utilized attentional mechanisms as an effective tool to extract bottlenecks. Holonic concept clustering and attentional-system are the motivation and the core of the proposed method. The experimentations confirmed that the proposed method is able to identify bottlenecks with more precision and thereupon the speed of learning and the ability of knowledge transferring are improved significantly. Also, the time complexity of the proposed method is considerably less in comparison to other similar methods."
Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs,"Graphical model selection refers to theproblem of estimating the unknown graph structure givenobservations at the nodes in the model. We consider achallenging instance of this problem when some of thenodes are latent or hidden.  We  characterize  conditionsfor tractable graph estimation and develop efficient methods with provable guarantees. We consider the class of Ising models Markov on  locally tree-likegraphs, which are in the regime of correlationdecay. We  propose an efficient method for graph estimation, andestablish its structural consistency when the number of samples$n$ scales as $n = \Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where $\theta_{\min}$ is theminimum edge potential, $\delta$ is the depth (i.e.,distance from a hidden node to the nearest  observed nodes),and $\eta$ is a parameter which depends on the minimum andmaximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides  flexibility to control  the number of latent variables and the cycle lengths in the output graph.  We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound  on sample requirements.  "
Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs,"Graphical model selection refers to theproblem of estimating the unknown graph structure givenobservations at the nodes in the model. We consider achallenging instance of this problem when some of thenodes are latent or hidden.  We  characterize  conditionsfor tractable graph estimation and develop efficient methods with provable guarantees. We consider the class of Ising models Markov on  locally tree-likegraphs, which are in the regime of correlationdecay. We  propose an efficient method for graph estimation, andestablish its structural consistency when the number of samples$n$ scales as $n = \Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where $\theta_{\min}$ is theminimum edge potential, $\delta$ is the depth (i.e.,distance from a hidden node to the nearest  observed nodes),and $\eta$ is a parameter which depends on the minimum andmaximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides  flexibility to control  the number of latent variables and the cycle lengths in the output graph.  We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound  on sample requirements.  "
Biased and Unbiased Natural Actor-Critics,"We show that NAC-LSTD and eNAC, two popular discounted-reward natural actor-critics, follow biased estimates of the natural policy gradient. We derive the first unbiased discounted-reward natural actor-critics using batch and iterative approaches to gradient estimation and prove their convergence to globally optimal policies for discrete problems and locally optimal policies for continuous problems. We discuss what the bias does to the system and suggest that in some situations it may be desirable."
Bayesian Nonparametric Image  Super-resolution,"Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data.  We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior.  However, this algorithm is not feasible for large-scale data.  To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries  in a fraction of the time needed by the Gibbs sampler. "
Learning Robust Low-Rank Representations,"In this paper we present a comprehensive framework for learning robustlow-rank representations by combining and extending recent ideas forlearning fast sparse coding regressors with structured non-convexoptimization techniques. This approach connects robust principalcomponent analysis (RPCA) with dictionary learning techniques andallows its approximation via trainable encoders. We propose anefficient feed-forward architecture derived from an optimizationalgorithm designed to exactly solve robust low dimensionalprojections. This architecture, in combination with different trainingobjective functions, allows the regressors to be used as onlineapproximants of the exact offline RPCA problem or as RPCA-based neuralnetworks. Simple modifications of these encoders can handlechallenging extensions, such as the inclusion of geometric data transformations.We present several examples with real data from image, audio, and videoprocessing. When used to approximate RPCA, our basic implementationshows several orders of magnitude speedup compared to the exactsolvers with almost no performance degradation. We show the strengthof the inclusion of learning to the RPCA approach on a music source separationapplication, where the encoders outperform the exact RPCA algorithms,which are already reported to produce state-of-the-art results on a benchmarkdatabase. Our preliminary implementation on an iPad showsfaster-than-real-time performancewith minimal latency."
A Max-K-Min Approach for Classification,"Over the past decades, Maximin/Minimax Classifiers have been proven to be of excellent performance in numerous applications. Nevertheless, the Maximin/ Minimax approach, which only considers the most boundary instance of each class, may be sensitive to the outliers/noisy points near the boundary. In this paper, a novel robust Max-K-Min approach for classification is proposed to make the K worst cases best. The original Max-K-Min formulation to maximize K-Min Margin, which concerns the classification confidence of the $K$ worst cases among total $N$ training instances, however, is an optimization problem with $C_N^K$ inequality constrains and is intractable when $N$ and $K$ are relatively large. In order to make the optimization of Max-K-Min approach tractable, a reformulation is adopted, which changes the original Max-K-Min formulation into a compact optimization problem with $2N$ inequality constrains. To verify the performance of Max-K-Min approach, by defining two kinds of K-Min Margin functions, a naive linear Max-K-Min classifier and a kernel Max-K-Min classifier are built respectively for 2-class classification. The extensive experiments on $10$ datasets show that the performance of Max-K-Min classifiers is competitive with the prestigious classifiers of Support Vector Machine (SVM) and Logistic Regression (LR)."
A Max-K-Min Approach for Classification,"Over the past decades, Maximin/Minimax Classifiers have been proven to be of excellent performance in numerous applications. Nevertheless, the Maximin/ Minimax approach, which only considers the most boundary instance of each class, may be sensitive to the outliers/noisy points near the boundary. In this paper, a novel robust Max-K-Min approach for classification is proposed to make the K worst cases best. The original Max-K-Min formulation to maximize K-Min Margin, which concerns the classification confidence of the $K$ worst cases among total $N$ training instances, however, is an optimization problem with $C_N^K$ inequality constrains and is intractable when $N$ and $K$ are relatively large. In order to make the optimization of Max-K-Min approach tractable, a reformulation is adopted, which changes the original Max-K-Min formulation into a compact optimization problem with $2N$ inequality constrains. To verify the performance of Max-K-Min approach, by defining two kinds of K-Min Margin functions, a naive linear Max-K-Min classifier and a kernel Max-K-Min classifier are built respectively for 2-class classification. The extensive experiments on $10$ datasets show that the performance of Max-K-Min classifiers is competitive with the prestigious classifiers of Support Vector Machine (SVM) and Logistic Regression (LR)."
A Max-K-Min Approach for Classification,"Over the past decades, Maximin/Minimax Classifiers have been proven to be of excellent performance in numerous applications. Nevertheless, the Maximin/ Minimax approach, which only considers the most boundary instance of each class, may be sensitive to the outliers/noisy points near the boundary. In this paper, a novel robust Max-K-Min approach for classification is proposed to make the K worst cases best. The original Max-K-Min formulation to maximize K-Min Margin, which concerns the classification confidence of the $K$ worst cases among total $N$ training instances, however, is an optimization problem with $C_N^K$ inequality constrains and is intractable when $N$ and $K$ are relatively large. In order to make the optimization of Max-K-Min approach tractable, a reformulation is adopted, which changes the original Max-K-Min formulation into a compact optimization problem with $2N$ inequality constrains. To verify the performance of Max-K-Min approach, by defining two kinds of K-Min Margin functions, a naive linear Max-K-Min classifier and a kernel Max-K-Min classifier are built respectively for 2-class classification. The extensive experiments on $10$ datasets show that the performance of Max-K-Min classifiers is competitive with the prestigious classifiers of Support Vector Machine (SVM) and Logistic Regression (LR)."
A Max-K-Min Approach for Classification,"Over the past decades, Maximin/Minimax Classifiers have been proven to be of excellent performance in numerous applications. Nevertheless, the Maximin/ Minimax approach, which only considers the most boundary instance of each class, may be sensitive to the outliers/noisy points near the boundary. In this paper, a novel robust Max-K-Min approach for classification is proposed to make the K worst cases best. The original Max-K-Min formulation to maximize K-Min Margin, which concerns the classification confidence of the $K$ worst cases among total $N$ training instances, however, is an optimization problem with $C_N^K$ inequality constrains and is intractable when $N$ and $K$ are relatively large. In order to make the optimization of Max-K-Min approach tractable, a reformulation is adopted, which changes the original Max-K-Min formulation into a compact optimization problem with $2N$ inequality constrains. To verify the performance of Max-K-Min approach, by defining two kinds of K-Min Margin functions, a naive linear Max-K-Min classifier and a kernel Max-K-Min classifier are built respectively for 2-class classification. The extensive experiments on $10$ datasets show that the performance of Max-K-Min classifiers is competitive with the prestigious classifiers of Support Vector Machine (SVM) and Logistic Regression (LR)."
A Max-K-Min Approach for Classification,"Over the past decades, Maximin/Minimax Classifiers have been proven to be of excellent performance in numerous applications. Nevertheless, the Maximin/ Minimax approach, which only considers the most boundary instance of each class, may be sensitive to the outliers/noisy points near the boundary. In this paper, a novel robust Max-K-Min approach for classification is proposed to make the K worst cases best. The original Max-K-Min formulation to maximize K-Min Margin, which concerns the classification confidence of the $K$ worst cases among total $N$ training instances, however, is an optimization problem with $C_N^K$ inequality constrains and is intractable when $N$ and $K$ are relatively large. In order to make the optimization of Max-K-Min approach tractable, a reformulation is adopted, which changes the original Max-K-Min formulation into a compact optimization problem with $2N$ inequality constrains. To verify the performance of Max-K-Min approach, by defining two kinds of K-Min Margin functions, a naive linear Max-K-Min classifier and a kernel Max-K-Min classifier are built respectively for 2-class classification. The extensive experiments on $10$ datasets show that the performance of Max-K-Min classifiers is competitive with the prestigious classifiers of Support Vector Machine (SVM) and Logistic Regression (LR)."
Learning Mixtures of Tree Graphical Models,"We consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with  provable guarantees.   Our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The   sample and computational requirements for our method scale as $\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs."
Learning Mixtures of Tree Graphical Models,"We consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with  provable guarantees.   Our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The   sample and computational requirements for our method scale as $\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs."
Learning Mixtures of Tree Graphical Models,"We consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with  provable guarantees.   Our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The   sample and computational requirements for our method scale as $\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs."
Learning Mixtures of Tree Graphical Models,"We consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with  provable guarantees.   Our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The   sample and computational requirements for our method scale as $\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs."
Break and Conquer: Efficient Correlation Clustering for Image Segmentation,"We present a probabilistic model for image segmentation and an efficient approach to find the best segmentation. The image is first grouped into superpixels and a local information is extracted for each pair of spatially adjacent superpixels. The global optimization problem is then cast as correlation clustering which is knownto be NP hard. This study demonstrates that in many cases, finding the exact global solution is still feasible by exploiting the characteristics of the image segmentationproblem that make it possible to break the problem into subproblems. Each sub-problem corresponds to an automatically detected image part. The reducedcomputational complexity of the proposed optimization algorithm and the improved image segmentation performance are demonstrated on manually annotatedimages."
Break and Conquer: Efficient Correlation Clustering for Image Segmentation,"We present a probabilistic model for image segmentation and an efficient approach to find the best segmentation. The image is first grouped into superpixels and a local information is extracted for each pair of spatially adjacent superpixels. The global optimization problem is then cast as correlation clustering which is knownto be NP hard. This study demonstrates that in many cases, finding the exact global solution is still feasible by exploiting the characteristics of the image segmentationproblem that make it possible to break the problem into subproblems. Each sub-problem corresponds to an automatically detected image part. The reducedcomputational complexity of the proposed optimization algorithm and the improved image segmentation performance are demonstrated on manually annotatedimages."
Hamming Distance Metric Learning,"Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs.  We develop a new loss-augmented inference algorithm that is quadratic in the code length.  We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes."
Hamming Distance Metric Learning,"Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs.  We develop a new loss-augmented inference algorithm that is quadratic in the code length.  We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes."
Hamming Distance Metric Learning,"Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs.  We develop a new loss-augmented inference algorithm that is quadratic in the code length.  We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes."
Beyond Distance Metric: Learning Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community. For example, our method achieves 89.3\% accuracy on the ``Labeled Face in the Wild'' (LFW) dataset, which outperforms the best reported results under the same setting in recent years."
Beyond Distance Metric: Learning Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community. For example, our method achieves 89.3\% accuracy on the ``Labeled Face in the Wild'' (LFW) dataset, which outperforms the best reported results under the same setting in recent years."
Beyond Distance Metric: Learning Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community. For example, our method achieves 89.3\% accuracy on the ``Labeled Face in the Wild'' (LFW) dataset, which outperforms the best reported results under the same setting in recent years."
Beyond Distance Metric: Learning Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community. For example, our method achieves 89.3\% accuracy on the ``Labeled Face in the Wild'' (LFW) dataset, which outperforms the best reported results under the same setting in recent years."
Beyond Distance Metric: Learning Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community. For example, our method achieves 89.3\% accuracy on the ``Labeled Face in the Wild'' (LFW) dataset, which outperforms the best reported results under the same setting in recent years."
Beyond Distance Metric: Learning Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community. For example, our method achieves 89.3\% accuracy on the ``Labeled Face in the Wild'' (LFW) dataset, which outperforms the best reported results under the same setting in recent years."
Symmetry Detection by Distributed Synchrony of Spiking VLSI Neurons,"The detection of geometrical symmetries in visual scenes plays a key role in both animal perception and machine vision. Such types of sophisticated pattern detection can be obtained via spike-to-spike synchrony in recurrent networks of Integrate & Fire (I&F) neurons. To determine the network properties and the conditions required for synchronization we apply a formal contraction theory analysis using weakly coupled oscillator models and show how, under these conditions, the stability of the synchronous state can be guaranteed. These conditions can be experimentally verified through the measurement of the Phase Response Curve (PRC). To demonstrate the reliability of the method and its robustness to noise and parameter variability we used it to implement a bilateral symmetry detection network in analog/digital neuromorphic hardware and applied the system to real-time symmetry detection in response to real-world sensory data, provided by an event-based silicon retina. The silicon neurons quickly synchronize when a symmetric object is aligned with respect to the scene vertical axis. The presence of a symmetric input stimulus is reported by a separate read-out network of I&F neurons used as coincidence detectors. Our results demonstrate how this theory can be used to successfully configure low-power neuromorphic system for robust real-time pattern detection, making a central use of precise spike timing to detect user-specified symmetries present in visual scenes."
Symmetry Detection by Distributed Synchrony of Spiking VLSI Neurons,"The detection of geometrical symmetries in visual scenes plays a key role in both animal perception and machine vision. Such types of sophisticated pattern detection can be obtained via spike-to-spike synchrony in recurrent networks of Integrate & Fire (I&F) neurons. To determine the network properties and the conditions required for synchronization we apply a formal contraction theory analysis using weakly coupled oscillator models and show how, under these conditions, the stability of the synchronous state can be guaranteed. These conditions can be experimentally verified through the measurement of the Phase Response Curve (PRC). To demonstrate the reliability of the method and its robustness to noise and parameter variability we used it to implement a bilateral symmetry detection network in analog/digital neuromorphic hardware and applied the system to real-time symmetry detection in response to real-world sensory data, provided by an event-based silicon retina. The silicon neurons quickly synchronize when a symmetric object is aligned with respect to the scene vertical axis. The presence of a symmetric input stimulus is reported by a separate read-out network of I&F neurons used as coincidence detectors. Our results demonstrate how this theory can be used to successfully configure low-power neuromorphic system for robust real-time pattern detection, making a central use of precise spike timing to detect user-specified symmetries present in visual scenes."
Symmetry Detection by Distributed Synchrony of Spiking VLSI Neurons,"The detection of geometrical symmetries in visual scenes plays a key role in both animal perception and machine vision. Such types of sophisticated pattern detection can be obtained via spike-to-spike synchrony in recurrent networks of Integrate & Fire (I&F) neurons. To determine the network properties and the conditions required for synchronization we apply a formal contraction theory analysis using weakly coupled oscillator models and show how, under these conditions, the stability of the synchronous state can be guaranteed. These conditions can be experimentally verified through the measurement of the Phase Response Curve (PRC). To demonstrate the reliability of the method and its robustness to noise and parameter variability we used it to implement a bilateral symmetry detection network in analog/digital neuromorphic hardware and applied the system to real-time symmetry detection in response to real-world sensory data, provided by an event-based silicon retina. The silicon neurons quickly synchronize when a symmetric object is aligned with respect to the scene vertical axis. The presence of a symmetric input stimulus is reported by a separate read-out network of I&F neurons used as coincidence detectors. Our results demonstrate how this theory can be used to successfully configure low-power neuromorphic system for robust real-time pattern detection, making a central use of precise spike timing to detect user-specified symmetries present in visual scenes."
Symmetry Detection by Distributed Synchrony of Spiking VLSI Neurons,"The detection of geometrical symmetries in visual scenes plays a key role in both animal perception and machine vision. Such types of sophisticated pattern detection can be obtained via spike-to-spike synchrony in recurrent networks of Integrate & Fire (I&F) neurons. To determine the network properties and the conditions required for synchronization we apply a formal contraction theory analysis using weakly coupled oscillator models and show how, under these conditions, the stability of the synchronous state can be guaranteed. These conditions can be experimentally verified through the measurement of the Phase Response Curve (PRC). To demonstrate the reliability of the method and its robustness to noise and parameter variability we used it to implement a bilateral symmetry detection network in analog/digital neuromorphic hardware and applied the system to real-time symmetry detection in response to real-world sensory data, provided by an event-based silicon retina. The silicon neurons quickly synchronize when a symmetric object is aligned with respect to the scene vertical axis. The presence of a symmetric input stimulus is reported by a separate read-out network of I&F neurons used as coincidence detectors. Our results demonstrate how this theory can be used to successfully configure low-power neuromorphic system for robust real-time pattern detection, making a central use of precise spike timing to detect user-specified symmetries present in visual scenes."
Towards Practical Planning to Predict Intentions for Interacting with Self-Interested Agents,"A key challenge in non-cooperative multi-agent systems is that of developing efficient planning algorithms for intelligent agents to perform effectively among boundedly rational, self-interested agents (e.g., humans). The practicality of existing works addressing this challenge is being undermined due to either the restrictive assumptions on other agents' behaviors, the failure in accounting for their rationality, or the prohibitively expensive cost of modeling and predicting their intentions. To boost the practicality of research in this field, we investigate how intention prediction can be efficiently exploited and made practical in planning, thereby leading to efficient intention-aware planning frameworks capable of predicting the intentions of other agents and acting optimally with respect to their predicted intentions. We show that the performance loss incurred by the resulting planning policy is linearly bounded by the error of intention prediction. Empirical evaluations through a series of stochastic games demonstrate that our approach achieves better and more robust performance than the state-of-the-art algorithms."
Towards Practical Planning to Predict Intentions for Interacting with Self-Interested Agents,"A key challenge in non-cooperative multi-agent systems is that of developing efficient planning algorithms for intelligent agents to perform effectively among boundedly rational, self-interested agents (e.g., humans). The practicality of existing works addressing this challenge is being undermined due to either the restrictive assumptions on other agents' behaviors, the failure in accounting for their rationality, or the prohibitively expensive cost of modeling and predicting their intentions. To boost the practicality of research in this field, we investigate how intention prediction can be efficiently exploited and made practical in planning, thereby leading to efficient intention-aware planning frameworks capable of predicting the intentions of other agents and acting optimally with respect to their predicted intentions. We show that the performance loss incurred by the resulting planning policy is linearly bounded by the error of intention prediction. Empirical evaluations through a series of stochastic games demonstrate that our approach achieves better and more robust performance than the state-of-the-art algorithms."
Spiking and saturating dendrites differentially expand single neuron computation capacity.,"The integration of excitatory inputs in dendrites is non-linear: multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input's response taken separately. If this depolarization is bigger than the arithmetic sum, the dendrite is spiking; if the depolarization is smaller, the dendrite is saturating. Decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity, as the neuron then maps onto a two layer neural network, enabling it to compute linearly non-separable Boolean functions (lnBFs). How can these lnBFs be implemented by dendritic architectures in practise? And can saturating dendrites equally expand computational capacity? To adress these questions we use a binary neuron model and Boolean algebra. First, we confirm that spiking dendrites enable a neuron to compute lnBFs using an architecture based on the disjunctive normal form (DNF). Second, we prove that saturating dendrites as well as spiking dendrites also enable a neuron to compute lnBFs using an architecture based on the conjunctive normal form (CNF). Contrary to the DNF-based architecture, a CNF-based architecture leads to a dendritic unit tuning that does not imply the neuron tuning, as has been observed experimentally. Third, we show that one cannot use a DNF-based architecture with saturating dendrites. Consequently, we show that an important family of lnBFs implemented with a CNF-architecture can require an exponential number of saturating dendritic units, whereas the same family implemented with either a DNF-architecture or a CNF-architecture always require a linear number of spiking dendritic unit. This minimization could explain why a neuron spends energetic resources to make its dendrites spike."
Spiking and saturating dendrites differentially expand single neuron computation capacity.,"The integration of excitatory inputs in dendrites is non-linear: multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input's response taken separately. If this depolarization is bigger than the arithmetic sum, the dendrite is spiking; if the depolarization is smaller, the dendrite is saturating. Decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity, as the neuron then maps onto a two layer neural network, enabling it to compute linearly non-separable Boolean functions (lnBFs). How can these lnBFs be implemented by dendritic architectures in practise? And can saturating dendrites equally expand computational capacity? To adress these questions we use a binary neuron model and Boolean algebra. First, we confirm that spiking dendrites enable a neuron to compute lnBFs using an architecture based on the disjunctive normal form (DNF). Second, we prove that saturating dendrites as well as spiking dendrites also enable a neuron to compute lnBFs using an architecture based on the conjunctive normal form (CNF). Contrary to the DNF-based architecture, a CNF-based architecture leads to a dendritic unit tuning that does not imply the neuron tuning, as has been observed experimentally. Third, we show that one cannot use a DNF-based architecture with saturating dendrites. Consequently, we show that an important family of lnBFs implemented with a CNF-architecture can require an exponential number of saturating dendritic units, whereas the same family implemented with either a DNF-architecture or a CNF-architecture always require a linear number of spiking dendritic unit. This minimization could explain why a neuron spends energetic resources to make its dendrites spike."
Spiking and saturating dendrites differentially expand single neuron computation capacity.,"The integration of excitatory inputs in dendrites is non-linear: multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input's response taken separately. If this depolarization is bigger than the arithmetic sum, the dendrite is spiking; if the depolarization is smaller, the dendrite is saturating. Decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity, as the neuron then maps onto a two layer neural network, enabling it to compute linearly non-separable Boolean functions (lnBFs). How can these lnBFs be implemented by dendritic architectures in practise? And can saturating dendrites equally expand computational capacity? To adress these questions we use a binary neuron model and Boolean algebra. First, we confirm that spiking dendrites enable a neuron to compute lnBFs using an architecture based on the disjunctive normal form (DNF). Second, we prove that saturating dendrites as well as spiking dendrites also enable a neuron to compute lnBFs using an architecture based on the conjunctive normal form (CNF). Contrary to the DNF-based architecture, a CNF-based architecture leads to a dendritic unit tuning that does not imply the neuron tuning, as has been observed experimentally. Third, we show that one cannot use a DNF-based architecture with saturating dendrites. Consequently, we show that an important family of lnBFs implemented with a CNF-architecture can require an exponential number of saturating dendritic units, whereas the same family implemented with either a DNF-architecture or a CNF-architecture always require a linear number of spiking dendritic unit. This minimization could explain why a neuron spends energetic resources to make its dendrites spike."
Probability Distribution Estimation by Aggregated One-Class SVMs,"In this paper, we present a probability distribution estimation algorithm using aggregated One-Class-SVM (AOC-SVM) classifiers.Our work is along the line of generative and discriminative learning, which has been a long standing problem in machine learning.We combine the nice property of one-class-SVM with the idea of ensemble learning and divide-and-conquer.Given a set of training samples, we draw a random subset each time and train a corresponding one-class SVM classifier;the final probability distribution is a normalized ensemble of one-class SVMs;for each randomly drawn subset, one alternative is to obtain a randomized tree with a one-class SVM classifier learned on each leaf node.Compared with the traditional density estimation (generative) algorithms, our method has less restriction in model assumptionsand it is efficient to learn;compared with discriminative approaches, our algorithm approximates the in-class distribution withoutthe ``negative'' examples. We provide justification to our approach and show encouraging experimental results."
Probability Distribution Estimation by Aggregated One-Class SVMs,"In this paper, we present a probability distribution estimation algorithm using aggregated One-Class-SVM (AOC-SVM) classifiers.Our work is along the line of generative and discriminative learning, which has been a long standing problem in machine learning.We combine the nice property of one-class-SVM with the idea of ensemble learning and divide-and-conquer.Given a set of training samples, we draw a random subset each time and train a corresponding one-class SVM classifier;the final probability distribution is a normalized ensemble of one-class SVMs;for each randomly drawn subset, one alternative is to obtain a randomized tree with a one-class SVM classifier learned on each leaf node.Compared with the traditional density estimation (generative) algorithms, our method has less restriction in model assumptionsand it is efficient to learn;compared with discriminative approaches, our algorithm approximates the in-class distribution withoutthe ``negative'' examples. We provide justification to our approach and show encouraging experimental results."
EM-based Wall Fitting in Visually Extracted Noisy Point Clouds for UAV Exploration,"We introduce a robust and efficient technique for room fitting in noisy 2D point clouds. Our method simplifies the exploration problem for UAVs because it estimates the room geometry from visual SLAM feature locations. We use a single camera SLAM implementation, which generates features that roughly indicate the wall positions. To uncover these latent locations, we created a layered model that incorporates a sampling model and prior knowledge about walls. The Expectation Maximization algorithm for Gaussian Mixtures is extended to solve the problem, given this model. We demonstrate our technique on a popular low-cost quadcopter platform and show that it outperforms various alternative techniques."
EM-based Wall Fitting in Visually Extracted Noisy Point Clouds for UAV Exploration,"We introduce a robust and efficient technique for room fitting in noisy 2D point clouds. Our method simplifies the exploration problem for UAVs because it estimates the room geometry from visual SLAM feature locations. We use a single camera SLAM implementation, which generates features that roughly indicate the wall positions. To uncover these latent locations, we created a layered model that incorporates a sampling model and prior knowledge about walls. The Expectation Maximization algorithm for Gaussian Mixtures is extended to solve the problem, given this model. We demonstrate our technique on a popular low-cost quadcopter platform and show that it outperforms various alternative techniques."
EM-based Wall Fitting in Visually Extracted Noisy Point Clouds for UAV Exploration,"We introduce a robust and efficient technique for room fitting in noisy 2D point clouds. Our method simplifies the exploration problem for UAVs because it estimates the room geometry from visual SLAM feature locations. We use a single camera SLAM implementation, which generates features that roughly indicate the wall positions. To uncover these latent locations, we created a layered model that incorporates a sampling model and prior knowledge about walls. The Expectation Maximization algorithm for Gaussian Mixtures is extended to solve the problem, given this model. We demonstrate our technique on a popular low-cost quadcopter platform and show that it outperforms various alternative techniques."
EM-based Wall Fitting in Visually Extracted Noisy Point Clouds for UAV Exploration,"We introduce a robust and efficient technique for room fitting in noisy 2D point clouds. Our method simplifies the exploration problem for UAVs because it estimates the room geometry from visual SLAM feature locations. We use a single camera SLAM implementation, which generates features that roughly indicate the wall positions. To uncover these latent locations, we created a layered model that incorporates a sampling model and prior knowledge about walls. The Expectation Maximization algorithm for Gaussian Mixtures is extended to solve the problem, given this model. We demonstrate our technique on a popular low-cost quadcopter platform and show that it outperforms various alternative techniques."
Nonparametric Bayesian Clustering via Infinite Warped Mixture Models,"We introduce a flexible class of mixture models for clustering and density estimation. Our model allows clustering of non-linearly-separable data, produces a potentially low-dimensional latent representation, automatically infers the number of clusters, and produces a density estimate. Our approach makes use of two tools from Bayesian nonparametrics: a Dirichlet process mixture model to allow an unbounded number of clusters, and a Gaussian process warping function to allow each cluster to have a complex shape. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, and performs much better than infinite Gaussian mixture models at discovering meaningful clusters."
Nonparametric Bayesian Clustering via Infinite Warped Mixture Models,"We introduce a flexible class of mixture models for clustering and density estimation. Our model allows clustering of non-linearly-separable data, produces a potentially low-dimensional latent representation, automatically infers the number of clusters, and produces a density estimate. Our approach makes use of two tools from Bayesian nonparametrics: a Dirichlet process mixture model to allow an unbounded number of clusters, and a Gaussian process warping function to allow each cluster to have a complex shape. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, and performs much better than infinite Gaussian mixture models at discovering meaningful clusters."
Clustering by Nonnegative Matrix Factorization Using Graph Random Walk,"Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity."
Clustering by Nonnegative Matrix Factorization Using Graph Random Walk,"Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity."
Clustering by Nonnegative Matrix Factorization Using Graph Random Walk,"Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity."
Clustering by Nonnegative Matrix Factorization Using Graph Random Walk,"Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity."
Clustering by Nonnegative Matrix Factorization Using Graph Random Walk,"Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity."
Multi-Label Classification with Relevance Ordering,"In many real multi-label tasks, it is often the case that the ordering of relevant labels for an example is important, while that of irrelevant labels is meaningless. Such a problem, however, could not be addressed by existing multi-label learning or label ranking approaches, because the former usually ignores the ordering among relevant labels while the latter often fails to explicitly distinguish relevance from irrelevance and involves more consideration on ordering irrelevant labels. Considering that there is no adequate criterion available, in this paper, we propose PRO Loss, a new criterion which concerns the ordering of only relevant labels and classification of all the labels. We then propose ProSVM that optimizes PRO Loss efficiently with the use of alternating direction method of multipliers. We further improve efficiency with an upper approximation that reduces the number of constraints from O(T^2) to O(T), where T is the number of labels. Experiments show that our proposals not only perform superior to state-of-the-art approaches on PRO Loss, but also achieve highly competitive performance on existing evaluation criteria."
Multi-Label Classification with Relevance Ordering,"In many real multi-label tasks, it is often the case that the ordering of relevant labels for an example is important, while that of irrelevant labels is meaningless. Such a problem, however, could not be addressed by existing multi-label learning or label ranking approaches, because the former usually ignores the ordering among relevant labels while the latter often fails to explicitly distinguish relevance from irrelevance and involves more consideration on ordering irrelevant labels. Considering that there is no adequate criterion available, in this paper, we propose PRO Loss, a new criterion which concerns the ordering of only relevant labels and classification of all the labels. We then propose ProSVM that optimizes PRO Loss efficiently with the use of alternating direction method of multipliers. We further improve efficiency with an upper approximation that reduces the number of constraints from O(T^2) to O(T), where T is the number of labels. Experiments show that our proposals not only perform superior to state-of-the-art approaches on PRO Loss, but also achieve highly competitive performance on existing evaluation criteria."
Delay Compensation with Dynamical Synapses,"Time delay is pervasive in neural information processing. To achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system. In the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner. The state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents. The parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network. We also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude."
Delay Compensation with Dynamical Synapses,"Time delay is pervasive in neural information processing. To achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system. In the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner. The state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents. The parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network. We also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude."
An Anytime Algorithm for Exact Bayesian Network Structure Learning,"Learning a Bayesian network structure that optimizes a scoring function for a given dataset is NP-hard. In recent years, several exact algorithms have been developed for learning optimal Bayesian network structures. Most of these algorithms only find a solution at the end of the search, so they fail to find any solution if stopped early for some reason, e.g., out of time or memory. We present a novel anytime algorithm that also guarantees to find an optimal Bayesian network structure upon completion. We use a sparse representation for storing search information which often reduces the memory requirements by several orders of magnitude. The algorithm improves the runtime to find optimal network structures up to 100 times compared to existing state of the art methods. It is also shown to have excellent anytime behavior and often finds better network structures faster than existing local search techniques and other anytime exact algorithms."
An Anytime Algorithm for Exact Bayesian Network Structure Learning,"Learning a Bayesian network structure that optimizes a scoring function for a given dataset is NP-hard. In recent years, several exact algorithms have been developed for learning optimal Bayesian network structures. Most of these algorithms only find a solution at the end of the search, so they fail to find any solution if stopped early for some reason, e.g., out of time or memory. We present a novel anytime algorithm that also guarantees to find an optimal Bayesian network structure upon completion. We use a sparse representation for storing search information which often reduces the memory requirements by several orders of magnitude. The algorithm improves the runtime to find optimal network structures up to 100 times compared to existing state of the art methods. It is also shown to have excellent anytime behavior and often finds better network structures faster than existing local search techniques and other anytime exact algorithms."
Real-time online denoising and speaker identification by learning low-rank non-negative sparse models,"In this paper we present a new framework for real time speech denoising under non-stationary noise. We first propose a regularized version of nonnnegative matrix factorization (NMF) for modeling time-frequency representations of speech signals in which the spectral frames are decomposed as sparse linear combinations of atoms of an undercomplete dictionary. This regularization minimizes an upper-bound of the nuclear norm of the reconstruction. The proposed model outperforms standard NMF in speech denoising experiments and reduces the sensitivity of the obtained results with respect to the size of the dictionary. The main contribution consists of combining this model with recent developments in fast regressors for approximating sparse codes, to produce efficient feed-forward architectures that approximate the output of the exact algorithms with low latency and a fraction of the complexity. Incorporating dictionary update and elements of discriminative learning makes the proposed architecture full-featured low-rank non-negative sparse models, significantly outperforming exact NMF algorithms. We present several experiments in speech denoising and speaker identification in the presence of non-stationary noise that show successful results and the potential of the framework."
Real-time online denoising and speaker identification by learning low-rank non-negative sparse models,"In this paper we present a new framework for real time speech denoising under non-stationary noise. We first propose a regularized version of nonnnegative matrix factorization (NMF) for modeling time-frequency representations of speech signals in which the spectral frames are decomposed as sparse linear combinations of atoms of an undercomplete dictionary. This regularization minimizes an upper-bound of the nuclear norm of the reconstruction. The proposed model outperforms standard NMF in speech denoising experiments and reduces the sensitivity of the obtained results with respect to the size of the dictionary. The main contribution consists of combining this model with recent developments in fast regressors for approximating sparse codes, to produce efficient feed-forward architectures that approximate the output of the exact algorithms with low latency and a fraction of the complexity. Incorporating dictionary update and elements of discriminative learning makes the proposed architecture full-featured low-rank non-negative sparse models, significantly outperforming exact NMF algorithms. We present several experiments in speech denoising and speaker identification in the presence of non-stationary noise that show successful results and the potential of the framework."
An Efficient Feature Selection Algorithm,"Many computer vision and medical imaging problems are faced with learning classifiers from large datasets, with millions of observations and features. In this paper we propose a novel algorithm for variable selection and learning on such datasets, coming from the field of penalized likelihood optimization. We pose the learning problem as a constrained penalized likelihood optimization and introduce a suboptimal algorithm that gradually removes variables based on a criterion and a schedule. The approach is generic, allowing the use of any differentiable prior on the coefficients. Experiments on real and synthetic data show that the proposed method outperforms Logitboost and L1 penalized methods for both variable selection and prediction while being computationally faster. "
A dynamic excitatory-inhibitory network in a VLSI chip for spiking information reregistration,"Inhibitory synapse is an important component both in physiology and artificial neural network, which has been widely investigated and used. A typical inhibitorysynapse in very large scale integrated (VLSI) circuit is simplified from related research and applied in a VLSI chip for spike train reregistration. The spike trainreregistration network is derived from a neural network model for sensory map realignment for network adaptation. In this paper, we introduce the design of spiketrain registration in CMOS circuit and analyze the performance of the inhibitory network in it, which shows representative characters for the firing rate of inhibitedneuron and information transmission in circuit compared to math model."
Random Projections for Support Vector Machines,"Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be a data matrix of rank $\rho$, representing $n$ points in $\mathbb{R}^d$. The linear support vector machine constructs a hyperplane separator that maximizes the  1-norm soft margin. We develop a new \emph{oblivious} dimension reduction technique which is precomputed and can be applied to any input matrix \math{\mathbf{X}}. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \math{\epsilon}-\emph{relative error}, ensuring comparable generalization as in the original space. We present extensive experiments in support of the theory."
ImageNet Classification with Deep Convolutional Neural Networks,"We trained a large, deep convolutional neural network to classifythe 1.3 million high-resolution images in the LSVRC-2010 ImageNettraining set into the 1000 different classes. On the test data, weachieved top-1 and top-5 error rates of 39.7\% and 18.9\% which isconsiderably better than the previous state-of-the-art results. Theneural network, which has 60 million parameters and 500,000 neurons,consists of five convolutional layers, some of which are followedby max-pooling layers, and two globally connected layers with a final1000-way softmax. To make training faster, we used non-saturatingneurons and a very efficient GPU implementation of convolutional nets.To reduce overfitting in the globally connected layers we employeda new regularization method that proved to be very effective. "
ImageNet Classification with Deep Convolutional Neural Networks,"We trained a large, deep convolutional neural network to classifythe 1.3 million high-resolution images in the LSVRC-2010 ImageNettraining set into the 1000 different classes. On the test data, weachieved top-1 and top-5 error rates of 39.7\% and 18.9\% which isconsiderably better than the previous state-of-the-art results. Theneural network, which has 60 million parameters and 500,000 neurons,consists of five convolutional layers, some of which are followedby max-pooling layers, and two globally connected layers with a final1000-way softmax. To make training faster, we used non-saturatingneurons and a very efficient GPU implementation of convolutional nets.To reduce overfitting in the globally connected layers we employeda new regularization method that proved to be very effective. "
ImageNet Classification with Deep Convolutional Neural Networks,"We trained a large, deep convolutional neural network to classifythe 1.3 million high-resolution images in the LSVRC-2010 ImageNettraining set into the 1000 different classes. On the test data, weachieved top-1 and top-5 error rates of 39.7\% and 18.9\% which isconsiderably better than the previous state-of-the-art results. Theneural network, which has 60 million parameters and 500,000 neurons,consists of five convolutional layers, some of which are followedby max-pooling layers, and two globally connected layers with a final1000-way softmax. To make training faster, we used non-saturatingneurons and a very efficient GPU implementation of convolutional nets.To reduce overfitting in the globally connected layers we employeda new regularization method that proved to be very effective. "
A Study of Image Statistics through Higher-Order Interactions and Higher-Order Statistics,"  Nature image statistics is essential to the understanding of the  biological vision system, the Bayesian prior for image analysis,  texture analysis and synthesis, object detection, and scene  classification. The prior arts have mainly focused on the pairwise  interactions (e.g., correlation and 2-D joint histogram), and the  first and second order statistics (e.g. power spectrum and PCA). In  this paper, we use the multivariate (normalized) disjoint  information as a tool to capture the higher-order interactions  between two or more image subbands or transform coefficients. The  multivariate disjoint information satisfies the properties of  identity, non-negativity, symmetry, degeneracy, independence, upper  bounds, generalized triangle inequality, and simplex inequality. It  captures the higher-order interactions between an arbitrary number  of variables simultaneously, without simplification to the pairwise  interactions. In addition, the higher-order statistics, such as  bispectrum, are computed on the intensity images, wavelet  coefficients, and the disjoint information map. Preliminary results  based on the higher-order interactions and statistics are  demonstrated to differentiate the nature and man-made scenes."
A Study of Image Statistics through Higher-Order Interactions and Higher-Order Statistics,"  Nature image statistics is essential to the understanding of the  biological vision system, the Bayesian prior for image analysis,  texture analysis and synthesis, object detection, and scene  classification. The prior arts have mainly focused on the pairwise  interactions (e.g., correlation and 2-D joint histogram), and the  first and second order statistics (e.g. power spectrum and PCA). In  this paper, we use the multivariate (normalized) disjoint  information as a tool to capture the higher-order interactions  between two or more image subbands or transform coefficients. The  multivariate disjoint information satisfies the properties of  identity, non-negativity, symmetry, degeneracy, independence, upper  bounds, generalized triangle inequality, and simplex inequality. It  captures the higher-order interactions between an arbitrary number  of variables simultaneously, without simplification to the pairwise  interactions. In addition, the higher-order statistics, such as  bispectrum, are computed on the intensity images, wavelet  coefficients, and the disjoint information map. Preliminary results  based on the higher-order interactions and statistics are  demonstrated to differentiate the nature and man-made scenes."
Rates for Inductive Learning of Compositional Models,"Compositional Models are widely used in Computer Vision as they exhibit strong expressive power by generating a combinatorial number of configurations with a small number of components. However, the literature is still missing a theoretical understanding of why compositional models are better than flat representations, despite empirical evidence that compositional models need fewer training examples. In this paper we try to give the first theoretical answers in this direction. We focus on AND/OR Graph (AOG) models used in recent literature for representing objects, scenes and events, and bring the following contributions. First, we analyze the capacity of the space of AND/OR graphs, obtaining PAC (Probable and Approximately Correct) bounds for the number of training examples necessary to guarantee with a given certainty that the model learned has a given accuracy. We analyze both supervised and unsupervised learning approaches. Second, we observe that part localization leads to a reduction in the number of training examples required. Finally, we perform experiments for unsupervised learning of  AND/OR Graphs and part templates for objects and compare the theoretical bounds with the practical learning rates."
Rates for Inductive Learning of Compositional Models,"Compositional Models are widely used in Computer Vision as they exhibit strong expressive power by generating a combinatorial number of configurations with a small number of components. However, the literature is still missing a theoretical understanding of why compositional models are better than flat representations, despite empirical evidence that compositional models need fewer training examples. In this paper we try to give the first theoretical answers in this direction. We focus on AND/OR Graph (AOG) models used in recent literature for representing objects, scenes and events, and bring the following contributions. First, we analyze the capacity of the space of AND/OR graphs, obtaining PAC (Probable and Approximately Correct) bounds for the number of training examples necessary to guarantee with a given certainty that the model learned has a given accuracy. We analyze both supervised and unsupervised learning approaches. Second, we observe that part localization leads to a reduction in the number of training examples required. Finally, we perform experiments for unsupervised learning of  AND/OR Graphs and part templates for objects and compare the theoretical bounds with the practical learning rates."
Rates for Inductive Learning of Compositional Models,"Compositional Models are widely used in Computer Vision as they exhibit strong expressive power by generating a combinatorial number of configurations with a small number of components. However, the literature is still missing a theoretical understanding of why compositional models are better than flat representations, despite empirical evidence that compositional models need fewer training examples. In this paper we try to give the first theoretical answers in this direction. We focus on AND/OR Graph (AOG) models used in recent literature for representing objects, scenes and events, and bring the following contributions. First, we analyze the capacity of the space of AND/OR graphs, obtaining PAC (Probable and Approximately Correct) bounds for the number of training examples necessary to guarantee with a given certainty that the model learned has a given accuracy. We analyze both supervised and unsupervised learning approaches. Second, we observe that part localization leads to a reduction in the number of training examples required. Finally, we perform experiments for unsupervised learning of  AND/OR Graphs and part templates for objects and compare the theoretical bounds with the practical learning rates."
Training sparse natural image models with a fast Gibbs sampler of an extended state space,"We present a new learning strategy based on an efficient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models."
Training sparse natural image models with a fast Gibbs sampler of an extended state space,"We present a new learning strategy based on an efficient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models."
A Bayesian Approach for Policy Learning from Trajectory Preference Queries,"We consider the problem of learning control policies via trajectory preference queries to an expert. In particular,the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates thepreferred trajectory. The agent's goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problemwe propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries.Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and thatactive query selection can be substantially more efficient than random selection."
A Mixed-Initiative Approach to Solving  Correspondence Problems,"Finding correspondences among objects in different images is a critical problem in computer vision. Correspondence procedures can fail when faced with deformations, occlusions, and differences in lighting and zoom levels across images. We present a methodology for augmenting correspondence matching algorithms with a means for triaging the focus of attention and effort in assisting the automated matching. The mix of human and automated initiatives is guided by a speci?c computation of the expected value of resolving correspondence uncertainties. We introduce this measure and show how it can be used to guide efforts by human taggers. We explore the value of the approach with experiments on benchmark data. "
A Mixed-Initiative Approach to Solving  Correspondence Problems,"Finding correspondences among objects in different images is a critical problem in computer vision. Correspondence procedures can fail when faced with deformations, occlusions, and differences in lighting and zoom levels across images. We present a methodology for augmenting correspondence matching algorithms with a means for triaging the focus of attention and effort in assisting the automated matching. The mix of human and automated initiatives is guided by a speci?c computation of the expected value of resolving correspondence uncertainties. We introduce this measure and show how it can be used to guide efforts by human taggers. We explore the value of the approach with experiments on benchmark data. "
A Mixed-Initiative Approach to Solving  Correspondence Problems,"Finding correspondences among objects in different images is a critical problem in computer vision. Correspondence procedures can fail when faced with deformations, occlusions, and differences in lighting and zoom levels across images. We present a methodology for augmenting correspondence matching algorithms with a means for triaging the focus of attention and effort in assisting the automated matching. The mix of human and automated initiatives is guided by a speci?c computation of the expected value of resolving correspondence uncertainties. We introduce this measure and show how it can be used to guide efforts by human taggers. We explore the value of the approach with experiments on benchmark data. "
Evaluation of clustering stability using leave-one-out approach,"Currently, there are many clustering algorithms for the case of a known numberof clusters. Typically, clustering is a result of optimisation a quality criterionor iterative process. How to assess the quality of clustering obtained by somemethod? Is the data clustering corresponding to the objective reality or just astopping criterion of the method is made and obtained some partition? In thispaper, a practical approach and the general criterion based on an assessment ofthe stability of clustering are proposed. For the well-known clustering methods,efficient algorithms for computing the stability criterion according to the trainingset are obtained. We give illustrative examples."
ML Confidential: Machine Learning on Encrypted Data,"We demonstrate that by using a recently proposed somewhat homomorphic encryption (SHE) scheme it is possible to delegate the execution of a machine learning (ML) algorithm to a compute service while retaining confidentiality of the training and test data. Since the computational complexity of the SHE scheme depends primarily on the number of multiplications to be carried out on the encrypted data, we devise a new class of machine learning algorithms in which the algorithm's predictions viewed as functions of the input data can be expressed as polynomials of bounded degree. We propose confidential ML algorithms for binary classification based on polynomial approximations to least-squares solutions obtained by a small number of gradient descent steps. We present experimental validation of the confidential ML pipeline and discuss the trade-offs regarding computational complexity, prediction accuracy and cryptographic security."
ML Confidential: Machine Learning on Encrypted Data,"We demonstrate that by using a recently proposed somewhat homomorphic encryption (SHE) scheme it is possible to delegate the execution of a machine learning (ML) algorithm to a compute service while retaining confidentiality of the training and test data. Since the computational complexity of the SHE scheme depends primarily on the number of multiplications to be carried out on the encrypted data, we devise a new class of machine learning algorithms in which the algorithm's predictions viewed as functions of the input data can be expressed as polynomials of bounded degree. We propose confidential ML algorithms for binary classification based on polynomial approximations to least-squares solutions obtained by a small number of gradient descent steps. We present experimental validation of the confidential ML pipeline and discuss the trade-offs regarding computational complexity, prediction accuracy and cryptographic security."
ML Confidential: Machine Learning on Encrypted Data,"We demonstrate that by using a recently proposed somewhat homomorphic encryption (SHE) scheme it is possible to delegate the execution of a machine learning (ML) algorithm to a compute service while retaining confidentiality of the training and test data. Since the computational complexity of the SHE scheme depends primarily on the number of multiplications to be carried out on the encrypted data, we devise a new class of machine learning algorithms in which the algorithm's predictions viewed as functions of the input data can be expressed as polynomials of bounded degree. We propose confidential ML algorithms for binary classification based on polynomial approximations to least-squares solutions obtained by a small number of gradient descent steps. We present experimental validation of the confidential ML pipeline and discuss the trade-offs regarding computational complexity, prediction accuracy and cryptographic security."
Efficient Dimensionality Reduction for  Canonical Correlation Analysis,"We present the first sub-cubic time algorithm for Canonical Correlation Analysis. Given a pair of tall-and-thin matrices, our algorithm first employs a randomized dimensionality reduction transform to reduce the size of the input matrices, and then applies a standard SVD-based method to compute the canonical correlations. We prove that our algorithm computes an accurate approximation to the canonical correlations with high probability, and with asymptotic running times much better than the standard algorithm. We also show that our algorithm beats the standard algorithm in practice by 30-40% even on fairly small matrices."
Efficient Dimensionality Reduction for  Canonical Correlation Analysis,"We present the first sub-cubic time algorithm for Canonical Correlation Analysis. Given a pair of tall-and-thin matrices, our algorithm first employs a randomized dimensionality reduction transform to reduce the size of the input matrices, and then applies a standard SVD-based method to compute the canonical correlations. We prove that our algorithm computes an accurate approximation to the canonical correlations with high probability, and with asymptotic running times much better than the standard algorithm. We also show that our algorithm beats the standard algorithm in practice by 30-40% even on fairly small matrices."
Efficient Dimensionality Reduction for  Canonical Correlation Analysis,"We present the first sub-cubic time algorithm for Canonical Correlation Analysis. Given a pair of tall-and-thin matrices, our algorithm first employs a randomized dimensionality reduction transform to reduce the size of the input matrices, and then applies a standard SVD-based method to compute the canonical correlations. We prove that our algorithm computes an accurate approximation to the canonical correlations with high probability, and with asymptotic running times much better than the standard algorithm. We also show that our algorithm beats the standard algorithm in practice by 30-40% even on fairly small matrices."
Composite Discriminant Factor Analysis,"We propose a linear dimensionality reduction method, Composite Discriminant Factor (CDF) analysis, that searches for a discriminative but compact feature subspace that can be used as input to classifiers that suffer from problems such as multi-collinearity or the curse of dimensionality.  The subspace selected by CDF maximizes the performance of the entire classification pipeline, and is chosen from a set of candidate subspaces that are each discriminative by various local measures, such as covariance between input features and output labels or the margin between positive and negative samples.  Our method is based on Partial Least Squares (PLS) analysis, and can be viewed as a generalization of the PLS1 algorithm, designed to increase discrimination in classification tasks.  While our experiments focus on improvements to object detection, a computer vision task that often involves high dimensional features and benefits from fast linear approaches, we also demonstrate our approach on machine learning datasets from the UCI Machine Learning repository."
Composite Discriminant Factor Analysis,"We propose a linear dimensionality reduction method, Composite Discriminant Factor (CDF) analysis, that searches for a discriminative but compact feature subspace that can be used as input to classifiers that suffer from problems such as multi-collinearity or the curse of dimensionality.  The subspace selected by CDF maximizes the performance of the entire classification pipeline, and is chosen from a set of candidate subspaces that are each discriminative by various local measures, such as covariance between input features and output labels or the margin between positive and negative samples.  Our method is based on Partial Least Squares (PLS) analysis, and can be viewed as a generalization of the PLS1 algorithm, designed to increase discrimination in classification tasks.  While our experiments focus on improvements to object detection, a computer vision task that often involves high dimensional features and benefits from fast linear approaches, we also demonstrate our approach on machine learning datasets from the UCI Machine Learning repository."
Composite Discriminant Factor Analysis,"We propose a linear dimensionality reduction method, Composite Discriminant Factor (CDF) analysis, that searches for a discriminative but compact feature subspace that can be used as input to classifiers that suffer from problems such as multi-collinearity or the curse of dimensionality.  The subspace selected by CDF maximizes the performance of the entire classification pipeline, and is chosen from a set of candidate subspaces that are each discriminative by various local measures, such as covariance between input features and output labels or the margin between positive and negative samples.  Our method is based on Partial Least Squares (PLS) analysis, and can be viewed as a generalization of the PLS1 algorithm, designed to increase discrimination in classification tasks.  While our experiments focus on improvements to object detection, a computer vision task that often involves high dimensional features and benefits from fast linear approaches, we also demonstrate our approach on machine learning datasets from the UCI Machine Learning repository."
Composite Discriminant Factor Analysis,"We propose a linear dimensionality reduction method, Composite Discriminant Factor (CDF) analysis, that searches for a discriminative but compact feature subspace that can be used as input to classifiers that suffer from problems such as multi-collinearity or the curse of dimensionality.  The subspace selected by CDF maximizes the performance of the entire classification pipeline, and is chosen from a set of candidate subspaces that are each discriminative by various local measures, such as covariance between input features and output labels or the margin between positive and negative samples.  Our method is based on Partial Least Squares (PLS) analysis, and can be viewed as a generalization of the PLS1 algorithm, designed to increase discrimination in classification tasks.  While our experiments focus on improvements to object detection, a computer vision task that often involves high dimensional features and benefits from fast linear approaches, we also demonstrate our approach on machine learning datasets from the UCI Machine Learning repository."
Submodular Function Maximization with Higher-Order Priors for Video Segmentation,"We propose a video segmentation algorithm by maximizing a submodular objective function subject to a matroid constraint. This function is similar to standard energy function in computer vision problems with unary terms, pairwise terms from Potts model, and a higher-order term. We show that the standard Potts model prior, which becomes non-submodular for multi-label problems, still induces a submodular function in a maximization framework. We propose a new higher-order prior to enforce consistency in the appearance histograms both spatially and temporally across the video. The matroid constraint leads to a simple greedy algorithm with a performance bound of $\frac{1}{2}$. We further develop a branch and bound procedure to improve the solution. We evaluate the proposed algorithm on a standard video segmentation dataset and achieve favorable performance."
Submodular Function Maximization with Higher-Order Priors for Video Segmentation,"We propose a video segmentation algorithm by maximizing a submodular objective function subject to a matroid constraint. This function is similar to standard energy function in computer vision problems with unary terms, pairwise terms from Potts model, and a higher-order term. We show that the standard Potts model prior, which becomes non-submodular for multi-label problems, still induces a submodular function in a maximization framework. We propose a new higher-order prior to enforce consistency in the appearance histograms both spatially and temporally across the video. The matroid constraint leads to a simple greedy algorithm with a performance bound of $\frac{1}{2}$. We further develop a branch and bound procedure to improve the solution. We evaluate the proposed algorithm on a standard video segmentation dataset and achieve favorable performance."
Submodular Function Maximization with Higher-Order Priors for Video Segmentation,"We propose a video segmentation algorithm by maximizing a submodular objective function subject to a matroid constraint. This function is similar to standard energy function in computer vision problems with unary terms, pairwise terms from Potts model, and a higher-order term. We show that the standard Potts model prior, which becomes non-submodular for multi-label problems, still induces a submodular function in a maximization framework. We propose a new higher-order prior to enforce consistency in the appearance histograms both spatially and temporally across the video. The matroid constraint leads to a simple greedy algorithm with a performance bound of $\frac{1}{2}$. We further develop a branch and bound procedure to improve the solution. We evaluate the proposed algorithm on a standard video segmentation dataset and achieve favorable performance."
Submodular Function Maximization with Higher-Order Priors for Video Segmentation,"We propose a video segmentation algorithm by maximizing a submodular objective function subject to a matroid constraint. This function is similar to standard energy function in computer vision problems with unary terms, pairwise terms from Potts model, and a higher-order term. We show that the standard Potts model prior, which becomes non-submodular for multi-label problems, still induces a submodular function in a maximization framework. We propose a new higher-order prior to enforce consistency in the appearance histograms both spatially and temporally across the video. The matroid constraint leads to a simple greedy algorithm with a performance bound of $\frac{1}{2}$. We further develop a branch and bound procedure to improve the solution. We evaluate the proposed algorithm on a standard video segmentation dataset and achieve favorable performance."
Linear and Nonlinear Predictive Coding using Biologically Plausible Neuronal Circuits,"Predictive coding has previously been proposed as a useful model of redundancy reduction in sensory systems and served as a foundation for a normative theory of sensory processing. However, its discussion has been restricted to an abstract mathematical algorithm without a mechanistic implementation. In this paper, we demonstrate that simplified linear neurons in biologically plausible circuits can implement optimal predictive coding. Further, the addition of a neuron-like rectilinear nonlinearity into a feedback inhibitory neuronal circuit approximates the response of predictive filters optimized for various stimulus statistics, without the need to vary any parameters. This nonlinear network, therefore, provides a mechanism for dynamic gain control, which can operate in sensory neurons on time scales much faster than adaptation requiring synaptic plasticity."
Linear and Nonlinear Predictive Coding using Biologically Plausible Neuronal Circuits,"Predictive coding has previously been proposed as a useful model of redundancy reduction in sensory systems and served as a foundation for a normative theory of sensory processing. However, its discussion has been restricted to an abstract mathematical algorithm without a mechanistic implementation. In this paper, we demonstrate that simplified linear neurons in biologically plausible circuits can implement optimal predictive coding. Further, the addition of a neuron-like rectilinear nonlinearity into a feedback inhibitory neuronal circuit approximates the response of predictive filters optimized for various stimulus statistics, without the need to vary any parameters. This nonlinear network, therefore, provides a mechanism for dynamic gain control, which can operate in sensory neurons on time scales much faster than adaptation requiring synaptic plasticity."
GenDeR: A Generic Diversified Ranking Algorithm,"Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling,product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a wide range of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm."
GenDeR: A Generic Diversified Ranking Algorithm,"Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling,product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a wide range of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm."
GenDeR: A Generic Diversified Ranking Algorithm,"Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling,product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a wide range of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm."
A kernel independence test for time series, Testing for independence between random variables when samples are not independent identically distributed is a complex issue. Here we provide a solution to this problem when random variables a sampled from stationary time-series of arbitrary objects. To achieve this goal we define a test based on the statistics of the cross-spectral density operator induced by positive definite kernels. The performance of our test is compared to using i.i.d assumptions and show the interest of this approach for testing dependency in complex dynamic systems.
A kernel independence test for time series, Testing for independence between random variables when samples are not independent identically distributed is a complex issue. Here we provide a solution to this problem when random variables a sampled from stationary time-series of arbitrary objects. To achieve this goal we define a test based on the statistics of the cross-spectral density operator induced by positive definite kernels. The performance of our test is compared to using i.i.d assumptions and show the interest of this approach for testing dependency in complex dynamic systems.
A kernel independence test for time series, Testing for independence between random variables when samples are not independent identically distributed is a complex issue. Here we provide a solution to this problem when random variables a sampled from stationary time-series of arbitrary objects. To achieve this goal we define a test based on the statistics of the cross-spectral density operator induced by positive definite kernels. The performance of our test is compared to using i.i.d assumptions and show the interest of this approach for testing dependency in complex dynamic systems.
Scalarized Multi-Objective Reinforcement Learning: Novel Design Techniques,"In this paper, we use techniques from multi-objective optimization (MO), such as scalarization functions, to explore MO environments with reinforcement learning. A general framework for multi-objective reinforcement learning (MORL) is proposed. We call this class of algorithms scalarized MORL. A novel instance of scalarized MORL that uses the Chebyshev scalarization function is suggested. We experimentally compare scalarized MORL on three multi-objective simulation environments with two and three objectives. We show that the Chebyshev scalarization algorithm significantly outperforms MORLs using the linear scalarization function. This conclusion is in concordance with state-of-the-art off-line multi-objective optimization, proving our correct use of multi-objective techniques in the reinforcement learning field."
Scalarized Multi-Objective Reinforcement Learning: Novel Design Techniques,"In this paper, we use techniques from multi-objective optimization (MO), such as scalarization functions, to explore MO environments with reinforcement learning. A general framework for multi-objective reinforcement learning (MORL) is proposed. We call this class of algorithms scalarized MORL. A novel instance of scalarized MORL that uses the Chebyshev scalarization function is suggested. We experimentally compare scalarized MORL on three multi-objective simulation environments with two and three objectives. We show that the Chebyshev scalarization algorithm significantly outperforms MORLs using the linear scalarization function. This conclusion is in concordance with state-of-the-art off-line multi-objective optimization, proving our correct use of multi-objective techniques in the reinforcement learning field."
Scalarized Multi-Objective Reinforcement Learning: Novel Design Techniques,"In this paper, we use techniques from multi-objective optimization (MO), such as scalarization functions, to explore MO environments with reinforcement learning. A general framework for multi-objective reinforcement learning (MORL) is proposed. We call this class of algorithms scalarized MORL. A novel instance of scalarized MORL that uses the Chebyshev scalarization function is suggested. We experimentally compare scalarized MORL on three multi-objective simulation environments with two and three objectives. We show that the Chebyshev scalarization algorithm significantly outperforms MORLs using the linear scalarization function. This conclusion is in concordance with state-of-the-art off-line multi-objective optimization, proving our correct use of multi-objective techniques in the reinforcement learning field."
Online Learning of Rotations Using Geometric Structures,"This paper provides a solution to online learning for rotations by employing exponentials of sparse antisymmetric matrices. The method performs similarly to Riemannian gradient based methods but is derived using simple matrix algebraic techniques. We first show that a general optimization problem with a rotation constraint can be transformed into an equivalent problem in the space of antisymmetric matrices. An efficient approach is then introduced to iteratively solve the problem using antisymmetric matrices with one or more nonzero columns and an equal number of nonzero rows. Specially, we show that it is sufficient to employ antisymmetric matrices with only one nonzero column and row. Fast implementation is also presented to simplify the computation of sparse antisymmetric matrix exponentials involved in the algorithm. Experimental results obtained by using a variety of loss functions indicate that the algorithm converges quickly and estimates unknown rotations accurately."
Flexible Shift-Invariant Locality and Globality Preserving Projections,"In machine learning, the dimension reduction methods have commonly been used as a principled way to understand the high-dimensional data. To solve the out-of-sample problem, local preserving projection (LPP) was proposed and applied to many applications. However, LPP suffers two crucial deficiencies: 1) the LPP loses shift invariance property which is an important property of embedding methods; 2) the rigid linear embedding is used as constraint, which often inhibits the optimal manifold structures finding. To overcome these two important problems, we propose a novel flexible shift invariant locality and globality preserving projection method, which utilizes a newly defined graph Laplacian to make the projection shift invariant. Meanwhile, the relaxed embedding is introduced to allow the finding of more optimal manifold structures. We derive the new optimization algorithm to solve the proposed objective with rigorously proved global convergence. Extensive experiments have been performed on the machine learning benchmark data sets. In all empirical results, our method shows promising results."
Flexible Shift-Invariant Locality and Globality Preserving Projections,"In machine learning, the dimension reduction methods have commonly been used as a principled way to understand the high-dimensional data. To solve the out-of-sample problem, local preserving projection (LPP) was proposed and applied to many applications. However, LPP suffers two crucial deficiencies: 1) the LPP loses shift invariance property which is an important property of embedding methods; 2) the rigid linear embedding is used as constraint, which often inhibits the optimal manifold structures finding. To overcome these two important problems, we propose a novel flexible shift invariant locality and globality preserving projection method, which utilizes a newly defined graph Laplacian to make the projection shift invariant. Meanwhile, the relaxed embedding is introduced to allow the finding of more optimal manifold structures. We derive the new optimization algorithm to solve the proposed objective with rigorously proved global convergence. Extensive experiments have been performed on the machine learning benchmark data sets. In all empirical results, our method shows promising results."
On Multilabel Classification and Ranking with Partial Feedback,"We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show $O(T^{1/2}\log T)$ regret bounds, which improve in several ways on the existing results.We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance."
On Multilabel Classification and Ranking with Partial Feedback,"We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show $O(T^{1/2}\log T)$ regret bounds, which improve in several ways on the existing results.We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance."
FAsT-Match: Fast Affine Template Matching,"FAsT-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. The smoother the image, the sparser the set of affine transformations we evaluate. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth these lead to a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We report quantitative results on a large data set of 9,500 images. To the best of our knowledge, this is the first template matching algorithm, which can handle arbitrary 2D affine transformations."
FAsT-Match: Fast Affine Template Matching,"FAsT-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. The smoother the image, the sparser the set of affine transformations we evaluate. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth these lead to a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We report quantitative results on a large data set of 9,500 images. To the best of our knowledge, this is the first template matching algorithm, which can handle arbitrary 2D affine transformations."
FAsT-Match: Fast Affine Template Matching,"FAsT-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. The smoother the image, the sparser the set of affine transformations we evaluate. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth these lead to a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We report quantitative results on a large data set of 9,500 images. To the best of our knowledge, this is the first template matching algorithm, which can handle arbitrary 2D affine transformations."
FAsT-Match: Fast Affine Template Matching,"FAsT-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. The smoother the image, the sparser the set of affine transformations we evaluate. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth these lead to a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We report quantitative results on a large data set of 9,500 images. To the best of our knowledge, this is the first template matching algorithm, which can handle arbitrary 2D affine transformations."
Adaptive Universal Linear Filtering,"We consider the problem of online estimation of an arbitrary real-valued signal corrupted by zero-mean noise using  linear estimators. The estimator is required to iteratively predict the underlying signal  based on the current and several last noisy observations, and its performance is measured by the mean-square-error. We design and analyse an algorithm for this task which achieves logarithmic adaptive regret against the best linear filter in hindsight. This bound is asymptotically tight, and resolves the question of \cite{MWPaper}. Furthermore, the algorithm runs in linear time in terms of the number of filter coefficients. Previous constructions required at least quadratic time. "
Adaptive Universal Linear Filtering,"We consider the problem of online estimation of an arbitrary real-valued signal corrupted by zero-mean noise using  linear estimators. The estimator is required to iteratively predict the underlying signal  based on the current and several last noisy observations, and its performance is measured by the mean-square-error. We design and analyse an algorithm for this task which achieves logarithmic adaptive regret against the best linear filter in hindsight. This bound is asymptotically tight, and resolves the question of \cite{MWPaper}. Furthermore, the algorithm runs in linear time in terms of the number of filter coefficients. Previous constructions required at least quadratic time. "
Dictionary Training with Side Information,"Recently, learning with side information, which incorporates information contained in the training data but not available in the testing phase to guide the learning process, attracts great attention in machine learning field. In this work, we propose a discriminative dictionary learning method that involves side information. In particular, we introduce a new soft constraint derived from side information and combine it with the reconstruction error and the classification error to form a unified objective function. The optimal solution to the objective function is efficiently obtained using the K-SVD algorithm. Our algorithm learns the dictionary and an optimal linear classifier jointly. We apply the proposed method to two pattern recognition problems, namely low resolution expression recognition and face recognition, where it demonstrates the effectiveness of the proposed method in classification performance."
Discriminative Restricted Boltzmann Machines for Regression,"The paper explores a variant of a discriminative Restricted Boltzmann Machine (RBM) model that performs regression, by defining an RBM with Gaussian output variables and conditioning on the data. The resulting model is intractable in general, but we derive a practical mean-field approximation to the predictive distribution of the model. We investigate two learning algorithms for discriminative RBM regressors, viz. one based on contrastive mean-field learning and one based on contrastive divergences. Experimental evaluation of the new discriminative RBM for regression illustrates the potential of the discriminative RBM regressor, in particular, for learning problems in which the data is high-dimensional."
Correlations strike back (again): the case of associative memory retrieval,"It has long been recognised that statistical dependencies in neuronalactivity need to be taken into account when decoding stimuli encoded ina neural population. Less studied, though equally pernicious, is theneed to take account of dependencies between synaptic weights whendecoding stimuli previously encoded in an auto-associative memory. Weshow that activity-dependent learning generically produces suchcorrelations, and failing to take them into account in the dynamics ofmemory retrieval leads to catastrophically poor recall. We deriveoptimal network dynamics for recall in the face of synaptic correlationscaused by a range of synaptic plasticity rules. These dynamics involvewell-studied circuit motifs, such as forms of feedback inhibition andexperimentally observed dendritic nonlinearities. We therefore show howassuaging an old enemy leads to a novel functional account of keybiophysical features of the neural substrate."
Correlations strike back (again): the case of associative memory retrieval,"It has long been recognised that statistical dependencies in neuronalactivity need to be taken into account when decoding stimuli encoded ina neural population. Less studied, though equally pernicious, is theneed to take account of dependencies between synaptic weights whendecoding stimuli previously encoded in an auto-associative memory. Weshow that activity-dependent learning generically produces suchcorrelations, and failing to take them into account in the dynamics ofmemory retrieval leads to catastrophically poor recall. We deriveoptimal network dynamics for recall in the face of synaptic correlationscaused by a range of synaptic plasticity rules. These dynamics involvewell-studied circuit motifs, such as forms of feedback inhibition andexperimentally observed dendritic nonlinearities. We therefore show howassuaging an old enemy leads to a novel functional account of keybiophysical features of the neural substrate."
A spectral learning framework for graphical models,"This work draws on the previous works on spectral algorithm(\cite{Hsu:COLT09-long},\cite{DBLP:conf/alt/BaillyHD10},\cite{DBLP:conf/icml/SongSGS10},\cite{DBLP:conf/pkdd/BalleQC11}). We propose an extension of the\emph{Hidden Markov Models}, called \emph{Graphical Weighted Models  (GWM)}, whose purpose is to model distributions over labeledgraphs. We expose the spectral algorithm for GWM, which generalizesthe previous ones for sequences and trees. We show that this algorithmis \emph{consistant}, and we provide bounds for the statisticalconvergence for the parameters estimate and for the learned distribution."
High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning,"Joint sparsity regularization in multi-task learning has attracted much attentionin recent years. The traditional convex formulation employs the group Lasso relaxationto achieve joint sparsity across tasks. Although this approach leads to asimple convex formulation, we argue in this paper that the quadratic regularizerinduced by the group Lasso formulation is suboptimal. To remedy this problem,we view jointly sparse multi-task learning as a specialized random effects model,and derive a convex relaxation approach that involves two steps. The first steplearns the covariance matrix of the coefficients using a convex formulation whichwe refer to as sparse covariance coding; the second step solves a ridge regressionproblem with a sparse quadratic regularizer based on the covariance matrix obtainedin the first step. It is shown that this approach produces an asymptoticallyoptimal quadratic regularizer in the multitask learning setting if the number oftasks approaches infinity. Experimental results demonstrate that the convex formulationobtained via the proposed model significantly outperforms group Lasso."
Extending Fano's Inequality: Bounds on Balanced Error Rate and F-Score Using Conditional Entropy,"Fano's inequality lower bounds the probability of transmission errorthrough a noisy communication channel. Applied to classification problems, it provides a lower bound on the Bayes error and motivates the use of information theory in machine learning. In practice, we are often interested in more than just error rate;two popular measures are balanced error rate (BER) and F-score(also known as $F_1$-measure). In this work, we prove that the ``Bayes value'' of BER and F-score can be obtained by tuning a threshold on the posterior probability of the positive class. It is well known that a threshold of 0.5, i.e., where the posteriors cross, will minimize the error rate---the similar optimalthresholds are derived for BER and F-score. We then use a general defintion of conditional entropy that includes Shannon as a special case to derive lower/upper bounds on the Bayes BER and F-score, extending Fano's result. Our results show that the lower bound on the Bayes F-score is monotonically increasing with conditional entropy, meaning that when the entropy (amount of uncertainty) about class label becomes smaller, the performance of classifiers as measured by F-score could possibly degenerate. We discuss the underlying reason for this anti-intuitive phenomenon via a toy example and argue that the ratio of conditional entropy and the probability of the positive class might be a more accurate indicator for F-score."
Extending Fano's Inequality: Bounds on Balanced Error Rate and F-Score Using Conditional Entropy,"Fano's inequality lower bounds the probability of transmission errorthrough a noisy communication channel. Applied to classification problems, it provides a lower bound on the Bayes error and motivates the use of information theory in machine learning. In practice, we are often interested in more than just error rate;two popular measures are balanced error rate (BER) and F-score(also known as $F_1$-measure). In this work, we prove that the ``Bayes value'' of BER and F-score can be obtained by tuning a threshold on the posterior probability of the positive class. It is well known that a threshold of 0.5, i.e., where the posteriors cross, will minimize the error rate---the similar optimalthresholds are derived for BER and F-score. We then use a general defintion of conditional entropy that includes Shannon as a special case to derive lower/upper bounds on the Bayes BER and F-score, extending Fano's result. Our results show that the lower bound on the Bayes F-score is monotonically increasing with conditional entropy, meaning that when the entropy (amount of uncertainty) about class label becomes smaller, the performance of classifiers as measured by F-score could possibly degenerate. We discuss the underlying reason for this anti-intuitive phenomenon via a toy example and argue that the ratio of conditional entropy and the probability of the positive class might be a more accurate indicator for F-score."
Extending Fano's Inequality: Bounds on Balanced Error Rate and F-Score Using Conditional Entropy,"Fano's inequality lower bounds the probability of transmission errorthrough a noisy communication channel. Applied to classification problems, it provides a lower bound on the Bayes error and motivates the use of information theory in machine learning. In practice, we are often interested in more than just error rate;two popular measures are balanced error rate (BER) and F-score(also known as $F_1$-measure). In this work, we prove that the ``Bayes value'' of BER and F-score can be obtained by tuning a threshold on the posterior probability of the positive class. It is well known that a threshold of 0.5, i.e., where the posteriors cross, will minimize the error rate---the similar optimalthresholds are derived for BER and F-score. We then use a general defintion of conditional entropy that includes Shannon as a special case to derive lower/upper bounds on the Bayes BER and F-score, extending Fano's result. Our results show that the lower bound on the Bayes F-score is monotonically increasing with conditional entropy, meaning that when the entropy (amount of uncertainty) about class label becomes smaller, the performance of classifiers as measured by F-score could possibly degenerate. We discuss the underlying reason for this anti-intuitive phenomenon via a toy example and argue that the ratio of conditional entropy and the probability of the positive class might be a more accurate indicator for F-score."
Extending Fano's Inequality: Bounds on Balanced Error Rate and F-Score Using Conditional Entropy,"Fano's inequality lower bounds the probability of transmission errorthrough a noisy communication channel. Applied to classification problems, it provides a lower bound on the Bayes error and motivates the use of information theory in machine learning. In practice, we are often interested in more than just error rate;two popular measures are balanced error rate (BER) and F-score(also known as $F_1$-measure). In this work, we prove that the ``Bayes value'' of BER and F-score can be obtained by tuning a threshold on the posterior probability of the positive class. It is well known that a threshold of 0.5, i.e., where the posteriors cross, will minimize the error rate---the similar optimalthresholds are derived for BER and F-score. We then use a general defintion of conditional entropy that includes Shannon as a special case to derive lower/upper bounds on the Bayes BER and F-score, extending Fano's result. Our results show that the lower bound on the Bayes F-score is monotonically increasing with conditional entropy, meaning that when the entropy (amount of uncertainty) about class label becomes smaller, the performance of classifiers as measured by F-score could possibly degenerate. We discuss the underlying reason for this anti-intuitive phenomenon via a toy example and argue that the ratio of conditional entropy and the probability of the positive class might be a more accurate indicator for F-score."
The Mondrian hidden Markov model,"This paper discusses a novel extension of hidden Markov models that can capture clusters embedded in transitions between the hidden states. In our model, the state-transition matrices are viewed as representation of relational data reflecting a network structure between the hidden states. We specifically present a nonparametric Bayesian approach to the proposed state-space model whose network structure is represented by a Mondrian Process-based relational model. We describe an inference scheme, and also show an application of the proposed model to music signal analysis."
The Mondrian hidden Markov model,"This paper discusses a novel extension of hidden Markov models that can capture clusters embedded in transitions between the hidden states. In our model, the state-transition matrices are viewed as representation of relational data reflecting a network structure between the hidden states. We specifically present a nonparametric Bayesian approach to the proposed state-space model whose network structure is represented by a Mondrian Process-based relational model. We describe an inference scheme, and also show an application of the proposed model to music signal analysis."
The Mondrian hidden Markov model,"This paper discusses a novel extension of hidden Markov models that can capture clusters embedded in transitions between the hidden states. In our model, the state-transition matrices are viewed as representation of relational data reflecting a network structure between the hidden states. We specifically present a nonparametric Bayesian approach to the proposed state-space model whose network structure is represented by a Mondrian Process-based relational model. We describe an inference scheme, and also show an application of the proposed model to music signal analysis."
The Mondrian hidden Markov model,"This paper discusses a novel extension of hidden Markov models that can capture clusters embedded in transitions between the hidden states. In our model, the state-transition matrices are viewed as representation of relational data reflecting a network structure between the hidden states. We specifically present a nonparametric Bayesian approach to the proposed state-space model whose network structure is represented by a Mondrian Process-based relational model. We describe an inference scheme, and also show an application of the proposed model to music signal analysis."
The Mondrian hidden Markov model,"This paper discusses a novel extension of hidden Markov models that can capture clusters embedded in transitions between the hidden states. In our model, the state-transition matrices are viewed as representation of relational data reflecting a network structure between the hidden states. We specifically present a nonparametric Bayesian approach to the proposed state-space model whose network structure is represented by a Mondrian Process-based relational model. We describe an inference scheme, and also show an application of the proposed model to music signal analysis."
Causal discovery with scale-mixture model for spatiotemporal variance dependencies,"In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be representedas a function of the direct causes themselves. However, in many real world problems, there are significant dependencies in the variances or energies, which indicatesthat causality may possibly take place on the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporalvariance dependencies to represent a specific type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneousand temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR).We prove the identifiability of this model under the non-Gaussian assumption on the innovation processes. Wealso propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthesis and real world data areconducted to show the applicability of the proposed model and algorithms."
Causal discovery with scale-mixture model for spatiotemporal variance dependencies,"In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be representedas a function of the direct causes themselves. However, in many real world problems, there are significant dependencies in the variances or energies, which indicatesthat causality may possibly take place on the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporalvariance dependencies to represent a specific type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneousand temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR).We prove the identifiability of this model under the non-Gaussian assumption on the innovation processes. Wealso propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthesis and real world data areconducted to show the applicability of the proposed model and algorithms."
Causal discovery with scale-mixture model for spatiotemporal variance dependencies,"In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be representedas a function of the direct causes themselves. However, in many real world problems, there are significant dependencies in the variances or energies, which indicatesthat causality may possibly take place on the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporalvariance dependencies to represent a specific type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneousand temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR).We prove the identifiability of this model under the non-Gaussian assumption on the innovation processes. Wealso propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthesis and real world data areconducted to show the applicability of the proposed model and algorithms."
Fast Gaussian Process Uncertainty Approximation for Novelty Detection,"Gaussian processes offer the advantage of calculating the classification uncertainty in terms of predictive variance associated with the classification result.This is especially useful to spot samples of previously unseen classes known as novelty detection.However, the Gaussian process framework suffers from high computational complexity.Hence, we propose an approximation of the Gaussian process predictive variance leading to rigorous speedups. While the accuracy in novelty detection tasks remains the same or even increases, the complexity of both learning and testing the classification model as wellas the memory demand decreases by one magnitude with respect to the number of training samples involved.We also verify that our approximation is an upper bound for the exact variance and present bounds for the approximation."
Fast Gaussian Process Uncertainty Approximation for Novelty Detection,"Gaussian processes offer the advantage of calculating the classification uncertainty in terms of predictive variance associated with the classification result.This is especially useful to spot samples of previously unseen classes known as novelty detection.However, the Gaussian process framework suffers from high computational complexity.Hence, we propose an approximation of the Gaussian process predictive variance leading to rigorous speedups. While the accuracy in novelty detection tasks remains the same or even increases, the complexity of both learning and testing the classification model as wellas the memory demand decreases by one magnitude with respect to the number of training samples involved.We also verify that our approximation is an upper bound for the exact variance and present bounds for the approximation."
Fast Gaussian Process Uncertainty Approximation for Novelty Detection,"Gaussian processes offer the advantage of calculating the classification uncertainty in terms of predictive variance associated with the classification result.This is especially useful to spot samples of previously unseen classes known as novelty detection.However, the Gaussian process framework suffers from high computational complexity.Hence, we propose an approximation of the Gaussian process predictive variance leading to rigorous speedups. While the accuracy in novelty detection tasks remains the same or even increases, the complexity of both learning and testing the classification model as wellas the memory demand decreases by one magnitude with respect to the number of training samples involved.We also verify that our approximation is an upper bound for the exact variance and present bounds for the approximation."
Fast Gaussian Process Uncertainty Approximation for Novelty Detection,"Gaussian processes offer the advantage of calculating the classification uncertainty in terms of predictive variance associated with the classification result.This is especially useful to spot samples of previously unseen classes known as novelty detection.However, the Gaussian process framework suffers from high computational complexity.Hence, we propose an approximation of the Gaussian process predictive variance leading to rigorous speedups. While the accuracy in novelty detection tasks remains the same or even increases, the complexity of both learning and testing the classification model as wellas the memory demand decreases by one magnitude with respect to the number of training samples involved.We also verify that our approximation is an upper bound for the exact variance and present bounds for the approximation."
Early Active Learning via Robust Representation and Structured Sparsity,"Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. The robust sparse representation loss function is utilized to reduce the effect of outliers and the structural sparsity regularization is adopted to find the most representative samples during the sparse representations. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost. We rigorously prove the global convergence of our solutions. Empirical results on multiple benchmark data sets show the promising results of our method."
Early Active Learning via Robust Representation and Structured Sparsity,"Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. The robust sparse representation loss function is utilized to reduce the effect of outliers and the structural sparsity regularization is adopted to find the most representative samples during the sparse representations. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost. We rigorously prove the global convergence of our solutions. Empirical results on multiple benchmark data sets show the promising results of our method."
Robust Rank-$k$ Matrix Completion,"Many applications can be formulated as reconstructing a matrix $M$ from noisy observations of a small, random subset of its entries. Much recent work has focused on the assumption that the data matrix has low rank and used its nuclear norm to approximate the rank. However, recent research casts doubts about this category of approach since such yielded solution could be indeed not low rank and unstable for practical applications. In this paper, we explicitly seek a matrix of \emph{exact} rank. Moreover, our method is robust to outlying or corrupted observations. We optimize the objective function in an alternative and asymptotic convergent manner, based on a combination of ancillary variables and augmented Lagrangian methods (ALM). We perform extensive experiments on three real world data sets and all empirical results demonstrate the effectiveness of our method."
Robust Rank-$k$ Matrix Completion,"Many applications can be formulated as reconstructing a matrix $M$ from noisy observations of a small, random subset of its entries. Much recent work has focused on the assumption that the data matrix has low rank and used its nuclear norm to approximate the rank. However, recent research casts doubts about this category of approach since such yielded solution could be indeed not low rank and unstable for practical applications. In this paper, we explicitly seek a matrix of \emph{exact} rank. Moreover, our method is robust to outlying or corrupted observations. We optimize the objective function in an alternative and asymptotic convergent manner, based on a combination of ancillary variables and augmented Lagrangian methods (ALM). We perform extensive experiments on three real world data sets and all empirical results demonstrate the effectiveness of our method."
Bayesian Games for Adversarial Regression Problems,"We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary's objective; instead, any knowledge of the learner about parameters of the adversary's goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression."
Bayesian Games for Adversarial Regression Problems,"We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary's objective; instead, any knowledge of the learner about parameters of the adversary's goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression."
Bayesian Games for Adversarial Regression Problems,"We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary's objective; instead, any knowledge of the learner about parameters of the adversary's goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression."
Bayesian Games for Adversarial Regression Problems,"We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary's objective; instead, any knowledge of the learner about parameters of the adversary's goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression."
Online Optimization in Markov Decision Processes with Dynamic Uncertainty,"Markov Decision Processes (MDPs) are used to model many important and practical problems involving optimization under uncertainty. In online decision making this uncertainty may lead to an algorithm making irreversible sub-optimal decisions. Fortunately, in many natural scenarios some short-term `lookahead' is available into future rewards. Thus, the information available to the online algorithm changes from time step to time step as more information is revealed, leading to what we call \textit{dynamic} uncertainty. To study this class of problems we define a framework of adaptive optimization with dynamic uncertainty and provide online algorithms, based on discounting of future rewards, to handle time varying uncertainty in future reward structures and values. We derive theoretical bounds and establish that discounting is, in some sense, a method to effectively de-randomize against possible futures. We analyze the performance of our algorithms in terms of both regret and competitive ratio, under varying dynamics of uncertainty. In particular, we define a notion of \textit{effective uncertainty} that captures the accuracy of information, when it becomes available and how usable it is, and show how this plays a crucial role in our analyses."
Online Optimization in Markov Decision Processes with Dynamic Uncertainty,"Markov Decision Processes (MDPs) are used to model many important and practical problems involving optimization under uncertainty. In online decision making this uncertainty may lead to an algorithm making irreversible sub-optimal decisions. Fortunately, in many natural scenarios some short-term `lookahead' is available into future rewards. Thus, the information available to the online algorithm changes from time step to time step as more information is revealed, leading to what we call \textit{dynamic} uncertainty. To study this class of problems we define a framework of adaptive optimization with dynamic uncertainty and provide online algorithms, based on discounting of future rewards, to handle time varying uncertainty in future reward structures and values. We derive theoretical bounds and establish that discounting is, in some sense, a method to effectively de-randomize against possible futures. We analyze the performance of our algorithms in terms of both regret and competitive ratio, under varying dynamics of uncertainty. In particular, we define a notion of \textit{effective uncertainty} that captures the accuracy of information, when it becomes available and how usable it is, and show how this plays a crucial role in our analyses."
Online Optimization in Markov Decision Processes with Dynamic Uncertainty,"Markov Decision Processes (MDPs) are used to model many important and practical problems involving optimization under uncertainty. In online decision making this uncertainty may lead to an algorithm making irreversible sub-optimal decisions. Fortunately, in many natural scenarios some short-term `lookahead' is available into future rewards. Thus, the information available to the online algorithm changes from time step to time step as more information is revealed, leading to what we call \textit{dynamic} uncertainty. To study this class of problems we define a framework of adaptive optimization with dynamic uncertainty and provide online algorithms, based on discounting of future rewards, to handle time varying uncertainty in future reward structures and values. We derive theoretical bounds and establish that discounting is, in some sense, a method to effectively de-randomize against possible futures. We analyze the performance of our algorithms in terms of both regret and competitive ratio, under varying dynamics of uncertainty. In particular, we define a notion of \textit{effective uncertainty} that captures the accuracy of information, when it becomes available and how usable it is, and show how this plays a crucial role in our analyses."
Probabilistically Sampled Forests,"Random subspaces are the key idea in random forests.  While the optimal size s of random subspaces can significantly improve classification accuracy, a suboptimal choice can be detrimental, especially in high-dimensional feature spaces, which typically contain only a few discriminative features among a majority of (nearly) uninformative ones.  As to overcome this problem, we propose an alternative to the subspace method: learning a forest by probabilistically sampling trees from the Boltzmann distribution at temperature T.  This approach includes sampling from the posterior distribution as a special case for T=1.  An increased temperature T>1 serves as a conceptually well-established measure for the additional randomness introduced into the learning process.  Moreover, this approach also suggests a novel relationship between the bootstrap and the random subspace method.  Apart from this, in our experiments on UCI data sets, we also found improvements on the practical side: classification accuracy does not tend to suffer as much from a suboptimal T compared to a suboptimal s in high-dimensional feature spaces."
"A Mechanism of Generating Joint plans for Self-interested Agents, and by the Agents","Generating joint plans for multiple self-interested agents is one of the most challenging problems in AI, since complications arise when each agent brings into a multi-agent system its personal abilities and utilities. Some fully centralized approaches (which require agents to fully reveal their private information) have been proposed for the plan synthesis problem in the literature. However, in the real world, private information exists widely, andit is unacceptable for a self--interested agent to reveal its private information. In this paper, we define a class ofmulti--agent planning problems, in which self--interested agents' values are private information, and the agents are ready to cooperate with each other in order to cost efficiently achieve their individual goals. We further propose a semi--distributed mechanism to deal with this kind of problems. In this mechanism, the involved agents will bargain with each other to reach an agreement, and do not need to reveal their private information. We show that this agreement is a possible joint plan which is Pareto optimal and entails minimal concessions."
"Natural Images, Gaussian Mixtures and Dead Leaves","Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components --- including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models."
"Natural Images, Gaussian Mixtures and Dead Leaves","Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components --- including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models."
Shape Priors for Weakly Labeled Segmentation,"In this paper we tackle the problem of weakly labeled image segmentation.  Towards this goal, we propose a novel generative model of segmentation based on transformed hierarchical Pitman-Yor processes, where we augment each object class with a shape prior.  Our model exploits weakly label data, as it does not require a training set composed of  pixel-wise annotations. Instead, it learns appearance models for each object using  as labels only bounding boxes around the object of interest as well as a  shape prior. We demonstrate the effectiveness of our approach on the PASCAL 2010 dataset and show that we outperformed a set of baselines, improving $8\%$ absolute error over the unsupervised version of our model as well as $9\%$ over the detector, which is theinput to our approach. Importantly, our approach performs similarly to fully-supervised approaches.  "
Sparse coding for multitask and transfer learning,"We present an extension of sparse coding to the problems of multitask and transfer learning. The central assumption of the method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Preliminary experiments indicate the advantage of the sparse multitask coding method over single task learning and a previous method based on orthogonal and dense representation of the tasks."
Sparse coding for multitask and transfer learning,"We present an extension of sparse coding to the problems of multitask and transfer learning. The central assumption of the method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Preliminary experiments indicate the advantage of the sparse multitask coding method over single task learning and a previous method based on orthogonal and dense representation of the tasks."
Sparse coding for multitask and transfer learning,"We present an extension of sparse coding to the problems of multitask and transfer learning. The central assumption of the method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Preliminary experiments indicate the advantage of the sparse multitask coding method over single task learning and a previous method based on orthogonal and dense representation of the tasks."
Grounded Language Acquisition: A Cognitive Approach,"We detail a cognitively inspired primitive model for embodied language acquisition for an artificial agent with limited perceptual multi-modal input capabilities. We formalize the meaning space through unsupervisedly learned perceptual feature-classes, and demonstrate that noun/verb/relational linguistic elements can be acquired through similar correlation with the respective meaning spaces. Finally, through unsupervised syntactical knowledge discovery, we further refine the assigned labels and acquire argument structures to syntactical structure mapping for verbs/relations in a cognitive grammar framework.  "
Grounded Language Acquisition: A Cognitive Approach,"We detail a cognitively inspired primitive model for embodied language acquisition for an artificial agent with limited perceptual multi-modal input capabilities. We formalize the meaning space through unsupervisedly learned perceptual feature-classes, and demonstrate that noun/verb/relational linguistic elements can be acquired through similar correlation with the respective meaning spaces. Finally, through unsupervised syntactical knowledge discovery, we further refine the assigned labels and acquire argument structures to syntactical structure mapping for verbs/relations in a cognitive grammar framework.  "
Dual-Space Analysis of the Sparse Linear Model,"Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients.  These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters.  Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation.  The underlying cost functions can be related via a dual-space framework from Wipf et al. (2011), which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space.  This perspective is useful because some analyses or extensions are more conducive to development in one space or the other.  Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit.  As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I.  In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II.  For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular L1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations."
Fast Forward Feature Selection for Support Vector Machines,"When creating a pattern recognition system, the set of used features has typically a strong influence on the performance of the selected classifier. Automatic feature selection algorithms aim to select the best features of a given dataset. Whereas filter approaches do not take the target algorithm into account, wrapper approaches evaluate the target algorithm for each considered feature subset. Therefore, filter approaches are typically faster but wrapper approaches may deliver a higher performance.In this paper, we present an approach for reducing the run-time of the common wrapper approach forward selection for Support Vector Machines (SVM). The number of required evaluations of the SVM classifier is reduced by using experience knowledge gained during past feature selection runs.We evaluated the approach on 22 real world datasets and compared the results with state-of-the-art wrapper and filter approaches as well as one embedded method according to performance and run-time. The results show that the presented approach reaches the accuracy of traditional wrapper approaches requiring significantly less evaluations of the target algorithm. Moreover, the presented approach achieves statistically significant better results than the filter approaches."
Fast Forward Feature Selection for Support Vector Machines,"When creating a pattern recognition system, the set of used features has typically a strong influence on the performance of the selected classifier. Automatic feature selection algorithms aim to select the best features of a given dataset. Whereas filter approaches do not take the target algorithm into account, wrapper approaches evaluate the target algorithm for each considered feature subset. Therefore, filter approaches are typically faster but wrapper approaches may deliver a higher performance.In this paper, we present an approach for reducing the run-time of the common wrapper approach forward selection for Support Vector Machines (SVM). The number of required evaluations of the SVM classifier is reduced by using experience knowledge gained during past feature selection runs.We evaluated the approach on 22 real world datasets and compared the results with state-of-the-art wrapper and filter approaches as well as one embedded method according to performance and run-time. The results show that the presented approach reaches the accuracy of traditional wrapper approaches requiring significantly less evaluations of the target algorithm. Moreover, the presented approach achieves statistically significant better results than the filter approaches."
Fast Forward Feature Selection for Support Vector Machines,"When creating a pattern recognition system, the set of used features has typically a strong influence on the performance of the selected classifier. Automatic feature selection algorithms aim to select the best features of a given dataset. Whereas filter approaches do not take the target algorithm into account, wrapper approaches evaluate the target algorithm for each considered feature subset. Therefore, filter approaches are typically faster but wrapper approaches may deliver a higher performance.In this paper, we present an approach for reducing the run-time of the common wrapper approach forward selection for Support Vector Machines (SVM). The number of required evaluations of the SVM classifier is reduced by using experience knowledge gained during past feature selection runs.We evaluated the approach on 22 real world datasets and compared the results with state-of-the-art wrapper and filter approaches as well as one embedded method according to performance and run-time. The results show that the presented approach reaches the accuracy of traditional wrapper approaches requiring significantly less evaluations of the target algorithm. Moreover, the presented approach achieves statistically significant better results than the filter approaches."
Active Comparison of Prediction Models,"We address the problem of comparing the risks of two given predictive models - for instance, a baseline model and a challenger - as confidently as possible on a fixed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values."
Active Comparison of Prediction Models,"We address the problem of comparing the risks of two given predictive models - for instance, a baseline model and a challenger - as confidently as possible on a fixed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values."
Active Comparison of Prediction Models,"We address the problem of comparing the risks of two given predictive models - for instance, a baseline model and a challenger - as confidently as possible on a fixed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values."
Low-Rank Affine Subspace Clustering,"We consider the problem of clustering high-dimensional data lying approximately in a union of affine subspaces. State-of-the-art methods solve this problem in two stages: learning an affinity matrix and applying spectral clustering. However, each stage is solved using a different optimization criteria. In this paper, we propose a unified approach in which we solve jointly for an affinity matrix and the segmentation of the data. We pose this problem as a rank minimization problem, which we solve using an alternating direction method, where the optimal solution at each iteration can be computed in closed form. Experiments on synthetic and real data demonstrate the superiority of our approach over state-of-the-art methods."
Picking the Low-Hanging Fruit First with Non-Convex Learning to Rank,"  In many learning to rank applications the main goal is to boost  precision at least at low recall. This can usually be done by  focusing the learning on the ``low-hanging fruit'', that is the  documents that are easier to model. In principle, this can be  obtained automatically by optimising appropriate ranking losses such  as AP that reward precision. However, we show that existing convex  surrogates of these losses, as used in practice by many  learning-to-rank methods as a convenient optimisation tool, may fail  to do so to the point that simple binary SVM can be competitive with  more advanced approaches.  We then show how to modify the latter to  better capture easy sub-populations in the training, improving  precision as part of a non-convex optimisation step. We demonstrate  this algorithm on a retrieval task in computer vision, including the  stage-wise construction of a mixture of linear models."
Picking the Low-Hanging Fruit First with Non-Convex Learning to Rank,"  In many learning to rank applications the main goal is to boost  precision at least at low recall. This can usually be done by  focusing the learning on the ``low-hanging fruit'', that is the  documents that are easier to model. In principle, this can be  obtained automatically by optimising appropriate ranking losses such  as AP that reward precision. However, we show that existing convex  surrogates of these losses, as used in practice by many  learning-to-rank methods as a convenient optimisation tool, may fail  to do so to the point that simple binary SVM can be competitive with  more advanced approaches.  We then show how to modify the latter to  better capture easy sub-populations in the training, improving  precision as part of a non-convex optimisation step. We demonstrate  this algorithm on a retrieval task in computer vision, including the  stage-wise construction of a mixture of linear models."
Incremental Online Boosting using Product of Experts,"We develop a novel incremental online algorithm for boosting based on a Product of Experts. The Product of Experts formulation allows for the first time, to devise rules to add experts to the ensemble thus incrementally adapt the model complexity of the ensemble along with learning in an online fashion. In this paper, we illustrate the efficacy of the online PoEBoost by comparing it with the previous approaches to online boosting on benchmark datasets."
Incremental Online Boosting using Product of Experts,"We develop a novel incremental online algorithm for boosting based on a Product of Experts. The Product of Experts formulation allows for the first time, to devise rules to add experts to the ensemble thus incrementally adapt the model complexity of the ensemble along with learning in an online fashion. In this paper, we illustrate the efficacy of the online PoEBoost by comparing it with the previous approaches to online boosting on benchmark datasets."
Incremental Online Boosting using Product of Experts,"We develop a novel incremental online algorithm for boosting based on a Product of Experts. The Product of Experts formulation allows for the first time, to devise rules to add experts to the ensemble thus incrementally adapt the model complexity of the ensemble along with learning in an online fashion. In this paper, we illustrate the efficacy of the online PoEBoost by comparing it with the previous approaches to online boosting on benchmark datasets."
Maximal Margin Learning Vector Quantisation,Kernel Generalised Learning Vector Quantisation (KGLVQ) was proposed to extend Generalised Learning Vector Quantisation into the kernel feature space to deal with complex class boundaries and thus yield promising performance for complex classification tasks in pattern recognition. However KGLVQ does not follow the maximal margin principle which is crucial for kernel-based learning methods. In this paper we propose a maximal margin approach to Kernel Generalised Learning Vector Quantisation algorithm which inherits the merits of KGLVQ and follows the maximal margin principle to favour the generalisation capability. Experiments performed on the well-known data sets available in UCI repository show promising classification results for the proposed method.
Maximal Margin Learning Vector Quantisation,Kernel Generalised Learning Vector Quantisation (KGLVQ) was proposed to extend Generalised Learning Vector Quantisation into the kernel feature space to deal with complex class boundaries and thus yield promising performance for complex classification tasks in pattern recognition. However KGLVQ does not follow the maximal margin principle which is crucial for kernel-based learning methods. In this paper we propose a maximal margin approach to Kernel Generalised Learning Vector Quantisation algorithm which inherits the merits of KGLVQ and follows the maximal margin principle to favour the generalisation capability. Experiments performed on the well-known data sets available in UCI repository show promising classification results for the proposed method.
Maximal Margin Learning Vector Quantisation,Kernel Generalised Learning Vector Quantisation (KGLVQ) was proposed to extend Generalised Learning Vector Quantisation into the kernel feature space to deal with complex class boundaries and thus yield promising performance for complex classification tasks in pattern recognition. However KGLVQ does not follow the maximal margin principle which is crucial for kernel-based learning methods. In this paper we propose a maximal margin approach to Kernel Generalised Learning Vector Quantisation algorithm which inherits the merits of KGLVQ and follows the maximal margin principle to favour the generalisation capability. Experiments performed on the well-known data sets available in UCI repository show promising classification results for the proposed method.
Maximal Margin Learning Vector Quantisation,Kernel Generalised Learning Vector Quantisation (KGLVQ) was proposed to extend Generalised Learning Vector Quantisation into the kernel feature space to deal with complex class boundaries and thus yield promising performance for complex classification tasks in pattern recognition. However KGLVQ does not follow the maximal margin principle which is crucial for kernel-based learning methods. In this paper we propose a maximal margin approach to Kernel Generalised Learning Vector Quantisation algorithm which inherits the merits of KGLVQ and follows the maximal margin principle to favour the generalisation capability. Experiments performed on the well-known data sets available in UCI repository show promising classification results for the proposed method.
On the Consistency of AUC Optimization,"AUC has been widely used as an evaluation criterion in diverse learning tasks. Many learning approaches have been developed to optimize AUC; however, owing to the non-convexity and discontinuousness of AUC, almost all approaches work with surrogate loss functions. Thus, the consistency of AUC is a crucial issue. In this paper, we theoretically study the asymptotic consistency of learning approaches based on surrogate loss functions and provide a sufficient condition. Based on this result, we prove that the exponential loss and logistic loss are consistent with AUC, whereas the hinge loss is inconsistent. We then derive the {$q$-norm hinge loss} and {general hinge loss} that are consistent with AUC. We also derive the consistent bounds for the exponential loss and logistic loss. Additionally, we obtain the consistent bounds for many surrogate loss functions under non-noisy setting. Finally, we find an equivalence between the exponential surrogate loss of AUC and the exponential surrogate loss of accuracy, leading to a direct consequence that AdaBoost and RankBoost are asymptotically equivalent."
Stereopsis via deep learning,"Estimation of binocular disparity in vision systems is typically based on a matching pipeline and rectification. Estimation of disparity in the brain, in contrast, is widely assumed to be based on the comparison of local phaseinformation from binocular receptive fields. The classic binocular energy model shows that this requires thepresence of local quadrature pairs within the eye which show phase- or position-shifts across the eyes.  While numerous theoretical accounts of stereopsis have been based on these observations, there has been little work on how energy models and depth inference may emerge through learning from the statistics of image pairs.  Here, we describe a probabilistic, deep learning approach to modeling disparity and a methodology for generating binocular training data to estimate model parameters.  We show that within-eye quadrature filters occur as a result of fitting the model to data, and we demonstrate how a three-layer network can learn to infer depth entirely from training data. We also show how training energy models can provide depth cues that are useful for recognition. We also show that pooling over more than two filters leads to richer dependencies between the learned filters. "
Stereopsis via deep learning,"Estimation of binocular disparity in vision systems is typically based on a matching pipeline and rectification. Estimation of disparity in the brain, in contrast, is widely assumed to be based on the comparison of local phaseinformation from binocular receptive fields. The classic binocular energy model shows that this requires thepresence of local quadrature pairs within the eye which show phase- or position-shifts across the eyes.  While numerous theoretical accounts of stereopsis have been based on these observations, there has been little work on how energy models and depth inference may emerge through learning from the statistics of image pairs.  Here, we describe a probabilistic, deep learning approach to modeling disparity and a methodology for generating binocular training data to estimate model parameters.  We show that within-eye quadrature filters occur as a result of fitting the model to data, and we demonstrate how a three-layer network can learn to infer depth entirely from training data. We also show how training energy models can provide depth cues that are useful for recognition. We also show that pooling over more than two filters leads to richer dependencies between the learned filters. "
Common Object Discovery via Relaxed Rank Minimization,"In this paper, we study the problem of discovering a common object within a set of images. We cast this problem as a robust subspace learning problem in presence of overwhelming outliers (negative instances) and corruptions. The additional knowledge of having one positive instance per image allows us to relax this highly combinatorial and prohibitive problem as a convex programming problem. Solving this program allows us to simultaneously identify the common objects in the image and learn a subspace model for its appearance. We give an efficient and effective algorithm based on the Augmented Lagrangian Multiplier method. We provide extensive simulations and experiments to verify the effectiveness of our method. Experimental results on challenging real-world datasets demonstrate significant advantages of our method over the state-of-the-art approaches. The  proposed scheme is not limited to solve object discovery and it can be applied to solve a wide class of high-dimensional combinatorial selection problems."
Common Object Discovery via Relaxed Rank Minimization,"In this paper, we study the problem of discovering a common object within a set of images. We cast this problem as a robust subspace learning problem in presence of overwhelming outliers (negative instances) and corruptions. The additional knowledge of having one positive instance per image allows us to relax this highly combinatorial and prohibitive problem as a convex programming problem. Solving this program allows us to simultaneously identify the common objects in the image and learn a subspace model for its appearance. We give an efficient and effective algorithm based on the Augmented Lagrangian Multiplier method. We provide extensive simulations and experiments to verify the effectiveness of our method. Experimental results on challenging real-world datasets demonstrate significant advantages of our method over the state-of-the-art approaches. The  proposed scheme is not limited to solve object discovery and it can be applied to solve a wide class of high-dimensional combinatorial selection problems."
Common Object Discovery via Relaxed Rank Minimization,"In this paper, we study the problem of discovering a common object within a set of images. We cast this problem as a robust subspace learning problem in presence of overwhelming outliers (negative instances) and corruptions. The additional knowledge of having one positive instance per image allows us to relax this highly combinatorial and prohibitive problem as a convex programming problem. Solving this program allows us to simultaneously identify the common objects in the image and learn a subspace model for its appearance. We give an efficient and effective algorithm based on the Augmented Lagrangian Multiplier method. We provide extensive simulations and experiments to verify the effectiveness of our method. Experimental results on challenging real-world datasets demonstrate significant advantages of our method over the state-of-the-art approaches. The  proposed scheme is not limited to solve object discovery and it can be applied to solve a wide class of high-dimensional combinatorial selection problems."
Common Object Discovery via Relaxed Rank Minimization,"In this paper, we study the problem of discovering a common object within a set of images. We cast this problem as a robust subspace learning problem in presence of overwhelming outliers (negative instances) and corruptions. The additional knowledge of having one positive instance per image allows us to relax this highly combinatorial and prohibitive problem as a convex programming problem. Solving this program allows us to simultaneously identify the common objects in the image and learn a subspace model for its appearance. We give an efficient and effective algorithm based on the Augmented Lagrangian Multiplier method. We provide extensive simulations and experiments to verify the effectiveness of our method. Experimental results on challenging real-world datasets demonstrate significant advantages of our method over the state-of-the-art approaches. The  proposed scheme is not limited to solve object discovery and it can be applied to solve a wide class of high-dimensional combinatorial selection problems."
Learning in deep architectures with folding transformations,"Statistical Learning Theory establishes the consistency of a learning process as an upper bound that holds with certain probability.  We propose a folding transformation paradigm for deep architectures for supervised layer-wise learning that assures the reduction of the complexity of the task to be learned without compromising the learning consistency regardless of the additional VC-dimension due to the depth of the architecture.   We introduce concepts of internal decision making, mapping and shatter complexity that aid in the analysis of a contribution of an individual hidden transformation to the solution of a task in a deep architecture, and apply these methods to investigate the capabilities of the proposed folding transformations. We also provide possible implementation using a neural network and carry out a small test of the architecture's performance on a classification task."
Learning in deep architectures with folding transformations,"Statistical Learning Theory establishes the consistency of a learning process as an upper bound that holds with certain probability.  We propose a folding transformation paradigm for deep architectures for supervised layer-wise learning that assures the reduction of the complexity of the task to be learned without compromising the learning consistency regardless of the additional VC-dimension due to the depth of the architecture.   We introduce concepts of internal decision making, mapping and shatter complexity that aid in the analysis of a contribution of an individual hidden transformation to the solution of a task in a deep architecture, and apply these methods to investigate the capabilities of the proposed folding transformations. We also provide possible implementation using a neural network and carry out a small test of the architecture's performance on a classification task."
Online Regret Bounds for Undiscounted Continuous Reinforcement Learning,"We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside a technical condition to ensure existence of an optimal policy, the only assumptions made are Hoelder continuity of rewards and transition probabilities. "
Online Regret Bounds for Undiscounted Continuous Reinforcement Learning,"We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside a technical condition to ensure existence of an optimal policy, the only assumptions made are Hoelder continuity of rewards and transition probabilities. "
Bayesian Sparse Partial Least Squares,"Born in the bosom of the chemometrics discipline, partial least squares (PLS) is a class of methods that makes use of a set of latent or unobserved variables to model the relation between (typically) two sets of input and output variables, respectively. Several flavors, consisting on how the latent variables or components are computed, have been developed over the last years. In this paper, we propose a Bayesian formulation of PLS along with some extensions. In a nutshell, we provide sparsity at the input space level and an automatic estimation of the optimal number of latent components. We follow the variational approach to infer the parameter distributions. We have successfully tested the proposed methods on electrocorticogram data associated with several motor outputs in monkeys."
Bayesian Sparse Partial Least Squares,"Born in the bosom of the chemometrics discipline, partial least squares (PLS) is a class of methods that makes use of a set of latent or unobserved variables to model the relation between (typically) two sets of input and output variables, respectively. Several flavors, consisting on how the latent variables or components are computed, have been developed over the last years. In this paper, we propose a Bayesian formulation of PLS along with some extensions. In a nutshell, we provide sparsity at the input space level and an automatic estimation of the optimal number of latent components. We follow the variational approach to infer the parameter distributions. We have successfully tested the proposed methods on electrocorticogram data associated with several motor outputs in monkeys."
Bayesian Sparse Partial Least Squares,"Born in the bosom of the chemometrics discipline, partial least squares (PLS) is a class of methods that makes use of a set of latent or unobserved variables to model the relation between (typically) two sets of input and output variables, respectively. Several flavors, consisting on how the latent variables or components are computed, have been developed over the last years. In this paper, we propose a Bayesian formulation of PLS along with some extensions. In a nutshell, we provide sparsity at the input space level and an automatic estimation of the optimal number of latent components. We follow the variational approach to infer the parameter distributions. We have successfully tested the proposed methods on electrocorticogram data associated with several motor outputs in monkeys."
Hybrid Hierarchical Growing Neural Gas for Clustering of 3D Environmental Map,"This paper proposes a method of clustering of 3D environmental map. Our method is based on Growing Cell Structure and Growing Neural Gas. These methods are one of self-organizing neural network based on unsupervised learning First, we propose 3D map building method using Kinect that can get camera image and 3D distance information simultaneously. Next, we propose a method of clustering method based on hybrid hierarchical growing neural gas to cluster 3D environmental map. Finally, we show experimental results of the proposed method and discuss the effectiveness of the proposed method."
Hybrid Hierarchical Growing Neural Gas for Clustering of 3D Environmental Map,"This paper proposes a method of clustering of 3D environmental map. Our method is based on Growing Cell Structure and Growing Neural Gas. These methods are one of self-organizing neural network based on unsupervised learning First, we propose 3D map building method using Kinect that can get camera image and 3D distance information simultaneously. Next, we propose a method of clustering method based on hybrid hierarchical growing neural gas to cluster 3D environmental map. Finally, we show experimental results of the proposed method and discuss the effectiveness of the proposed method."
Efficient Inference for Robust GP Regression on Highly Contaminated Data,"We consider robust non-parametric regression on data where the scale of the outlier errors is larger than the amplitude of the oscillation of the inlier manifold. This phenomenon appears frequently in warp estimation for scattered data interpolation, where outliers are incorrect data assignments, and the scale of the outlier errors is usually larger than the warping effects of the underlying interpolant. In this context, we propose a fast inference algorithm for robust Gaussian Process regression which takes advantage of the above condition. Our idea is to putatively fit a simple parametric model to capture the global trend of the inlier distribution \emph{relative} to the outliers, and then guide the Gaussian Process inference using statistics collected from the fitted parametric model. Compared to standard inference algorithms, this combination of parametric and non-parametric techniques achieves significantly higher efficiency with less tendency to converge to local minima."
Networks of rate neurons with no propagation of chaos: when mean-field theory fails,"We show how to compute analytically the correlation between pairs of neurons in a linear stochastic network described by rate equations. We prove that correlation depends on the eigenvalues of the connectivity matrix and that in special cases it isn?t negligible (no propagation of chaos), preventing us from applying mean-field theories to describe the activity of the network."
Networks of rate neurons with no propagation of chaos: when mean-field theory fails,"We show how to compute analytically the correlation between pairs of neurons in a linear stochastic network described by rate equations. We prove that correlation depends on the eigenvalues of the connectivity matrix and that in special cases it isn?t negligible (no propagation of chaos), preventing us from applying mean-field theories to describe the activity of the network."
Generalization Bounds for Domain Adaptation,"In this paper, we provide a new framework to study the generalization bound of thelearning process for domain adaptation. Without loss of generality, we considertwo kinds of representative domain adaptation settings: one is domain adaptationwith multiple sources and the other is domain adaptation combining sourceand target data. In particular, we introduce two quantities that capture the inherentcharacteristics of domains. For either kind of domain adaptation, basedon the two quantities, we then develop the specific Hoeffding-type deviation inequalityand symmetrization inequality to achieve the corresponding generalizationbound based on the uniform entropy number. By using the resultant generalizationbound, we analyze the asymptotic convergence and the rate of convergenceof the learning process for such kind of domain adaptation. Meanwhile, we discussthe factors that affect the asymptotic behavior of the learning process. Thenumerical experiments support our results."
Generalization Bounds for Domain Adaptation,"In this paper, we provide a new framework to study the generalization bound of thelearning process for domain adaptation. Without loss of generality, we considertwo kinds of representative domain adaptation settings: one is domain adaptationwith multiple sources and the other is domain adaptation combining sourceand target data. In particular, we introduce two quantities that capture the inherentcharacteristics of domains. For either kind of domain adaptation, basedon the two quantities, we then develop the specific Hoeffding-type deviation inequalityand symmetrization inequality to achieve the corresponding generalizationbound based on the uniform entropy number. By using the resultant generalizationbound, we analyze the asymptotic convergence and the rate of convergenceof the learning process for such kind of domain adaptation. Meanwhile, we discussthe factors that affect the asymptotic behavior of the learning process. Thenumerical experiments support our results."
Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning,"One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called \textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency."
Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning,"One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called \textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency."
fMRI Based Localization of EEG ,"This work introduces a novel EEG/fMRI integration approach that attempts to improve the spatial resolution of EEG using fMRI data, based on concurrent EEG/fMRI recordings. Advanced signal processing and machine learning, are used to improve EEG spatial resolution for specific regions based on simultaneous fMRI activity measurements of these areas.  We concentrate on demonstrating improved EEG localization in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model which is based on time/frequency representation of EEG data can predict the amygdala activity significantly better than traditional theta/alpha activity.  The important outcome is that with the proposed framework, the activity of sub-cortical regions can be analyzed with higher temporal resolution than obtained by fMRI. "
fMRI Based Localization of EEG ,"This work introduces a novel EEG/fMRI integration approach that attempts to improve the spatial resolution of EEG using fMRI data, based on concurrent EEG/fMRI recordings. Advanced signal processing and machine learning, are used to improve EEG spatial resolution for specific regions based on simultaneous fMRI activity measurements of these areas.  We concentrate on demonstrating improved EEG localization in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model which is based on time/frequency representation of EEG data can predict the amygdala activity significantly better than traditional theta/alpha activity.  The important outcome is that with the proposed framework, the activity of sub-cortical regions can be analyzed with higher temporal resolution than obtained by fMRI. "
fMRI Based Localization of EEG ,"This work introduces a novel EEG/fMRI integration approach that attempts to improve the spatial resolution of EEG using fMRI data, based on concurrent EEG/fMRI recordings. Advanced signal processing and machine learning, are used to improve EEG spatial resolution for specific regions based on simultaneous fMRI activity measurements of these areas.  We concentrate on demonstrating improved EEG localization in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model which is based on time/frequency representation of EEG data can predict the amygdala activity significantly better than traditional theta/alpha activity.  The important outcome is that with the proposed framework, the activity of sub-cortical regions can be analyzed with higher temporal resolution than obtained by fMRI. "
fMRI Based Localization of EEG ,"This work introduces a novel EEG/fMRI integration approach that attempts to improve the spatial resolution of EEG using fMRI data, based on concurrent EEG/fMRI recordings. Advanced signal processing and machine learning, are used to improve EEG spatial resolution for specific regions based on simultaneous fMRI activity measurements of these areas.  We concentrate on demonstrating improved EEG localization in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model which is based on time/frequency representation of EEG data can predict the amygdala activity significantly better than traditional theta/alpha activity.  The important outcome is that with the proposed framework, the activity of sub-cortical regions can be analyzed with higher temporal resolution than obtained by fMRI. "
fMRI Based Localization of EEG ,"This work introduces a novel EEG/fMRI integration approach that attempts to improve the spatial resolution of EEG using fMRI data, based on concurrent EEG/fMRI recordings. Advanced signal processing and machine learning, are used to improve EEG spatial resolution for specific regions based on simultaneous fMRI activity measurements of these areas.  We concentrate on demonstrating improved EEG localization in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model which is based on time/frequency representation of EEG data can predict the amygdala activity significantly better than traditional theta/alpha activity.  The important outcome is that with the proposed framework, the activity of sub-cortical regions can be analyzed with higher temporal resolution than obtained by fMRI. "
Bayesian Natural Actor Critic via Automatic Relevance Determination,"We present an algorithm which computes the posterior distribution of the natural policy gradient. The posterior mean of the gradient is used to update the parameters of the policy. The covariance matrix of the gradient can be used to evaluate the accuracy. Our algorithm can automatically prune out the irrelevant features in the value function approximation, resulting in a sparse model. This mechanism is achieved through automatic relevance determination which is similar with that in the relevance vector machine.  Finally, we demonstrate the performance of the algorithm experimentally and show that our work naturally combines the value basis function selection and the Bayesian natural policy gradient together."
Multi-Task Multi-Sample Learning,"In the ensemble of exemplar SVMs (E-SVMs) approach (Malisiewicz et al., ICCV 2011), each SVM is trained independently with only a single positive sample and all negative samples for the class . In this paper we borrow techniques from multi-task learning to develop a multi-sample learning (MSL) model which learns E-SVMs in a joint framework without any additional cost over the ensemble learning.In MSL, the degree of sharing between positive samples can be controlled, such that the classification performance of either an ensemble of E-SVMs (sample independence) or a standard SVM (all positive samples used) is reproduced. However, between the two limits the model can exceed the performance of either. We also introduce a further multi-task extension to MSL to multi-task multi-sample learning (MTMSL). This model encourages the sharing between both classes and sample specific classifiers within each class. Both MSL and MTMSL have convex objective functions.The MSL and MTMSL models are evaluated on standard benchmarks including the MNIST and Animals with attributes datasets. They achieve a significant performance improvement over both SVM and ensemble of E-SVMs."
Multi-Task Multi-Sample Learning,"In the ensemble of exemplar SVMs (E-SVMs) approach (Malisiewicz et al., ICCV 2011), each SVM is trained independently with only a single positive sample and all negative samples for the class . In this paper we borrow techniques from multi-task learning to develop a multi-sample learning (MSL) model which learns E-SVMs in a joint framework without any additional cost over the ensemble learning.In MSL, the degree of sharing between positive samples can be controlled, such that the classification performance of either an ensemble of E-SVMs (sample independence) or a standard SVM (all positive samples used) is reproduced. However, between the two limits the model can exceed the performance of either. We also introduce a further multi-task extension to MSL to multi-task multi-sample learning (MTMSL). This model encourages the sharing between both classes and sample specific classifiers within each class. Both MSL and MTMSL have convex objective functions.The MSL and MTMSL models are evaluated on standard benchmarks including the MNIST and Animals with attributes datasets. They achieve a significant performance improvement over both SVM and ensemble of E-SVMs."
Mixed Norm Regularization for Multi Class Learning,"Multiclass problems are everywhere, given an input the goal is topredict one of few possible classes. Most previous work reducedlearning to minimization the empirical loss over some training set,and an additional regulariztion term, prompting simple models, or someother prior knowledge. A common learning regulariztion promotessparsity, that is, small models or small number of features, asperformed in group LASSO. Yet, these assumption do not always hold, insome problems, for each class, there is a small set of features thatrepresents it well, yet the union of these sets is not small. Wepropose to use other regularizations that promptes this type ofsparsity, analyze the generalization property of such formulations,and show empirically that indeed, that not only perform well, but alsopromote such sparsity structure."
Mixed Norm Regularization for Multi Class Learning,"Multiclass problems are everywhere, given an input the goal is topredict one of few possible classes. Most previous work reducedlearning to minimization the empirical loss over some training set,and an additional regulariztion term, prompting simple models, or someother prior knowledge. A common learning regulariztion promotessparsity, that is, small models or small number of features, asperformed in group LASSO. Yet, these assumption do not always hold, insome problems, for each class, there is a small set of features thatrepresents it well, yet the union of these sets is not small. Wepropose to use other regularizations that promptes this type ofsparsity, analyze the generalization property of such formulations,and show empirically that indeed, that not only perform well, but alsopromote such sparsity structure."
Privacy-preserving spectral analysis of large incoherent matrices,"In this paper we provide a highly efficient method capable of performingaccurate---yet privacy-preserving---spectral analyses of large matrices.Previously this was feasible only for small or moderately sized matrices. Incontrast, our algorithm benefits from the ``blessing of dimensionality''.Indeed, we prove that the error of our algorithm is bounded in terms of thecoherence of the input matrix. High-dimensional data tends to have lowcoherence resulting in greater accuracy of our algorithm. We evaluate ouralgorithm on both real and synthetic data sets where we obtain encouragingresults."
Non-accidental configuration representation in the human ventral visual pathway,"Non-accidental configurations have been proposed to help resolve ambiguities in the inputs to the visual system. Moreover, recent theoretical accounts propose that such configurations could be formally ordered by the degree of their 'non-accidentalness'. In this study, we wanted to explore the possibility that human visual system indeed assigns a special status to non-accidental configurations. In a fast blocked fMRI design, participants were presented with slowly moving stimuli composed of two lines in one of twelve configurations while performing a perceptual similarity task. Using multi-voxel pattern analysis we were able to reliably discriminate between most of the configurations. Moreover, our fMRI data were well modeled by a simple V1 filter bank model. Further analyses failed to reveal an special status of the configurations used. Our experiment sets a baseline for future experiments on non-accidental configuration representation in the visual system."
Non-accidental configuration representation in the human ventral visual pathway,"Non-accidental configurations have been proposed to help resolve ambiguities in the inputs to the visual system. Moreover, recent theoretical accounts propose that such configurations could be formally ordered by the degree of their 'non-accidentalness'. In this study, we wanted to explore the possibility that human visual system indeed assigns a special status to non-accidental configurations. In a fast blocked fMRI design, participants were presented with slowly moving stimuli composed of two lines in one of twelve configurations while performing a perceptual similarity task. Using multi-voxel pattern analysis we were able to reliably discriminate between most of the configurations. Moreover, our fMRI data were well modeled by a simple V1 filter bank model. Further analyses failed to reveal an special status of the configurations used. Our experiment sets a baseline for future experiments on non-accidental configuration representation in the visual system."
Non-accidental configuration representation in the human ventral visual pathway,"Non-accidental configurations have been proposed to help resolve ambiguities in the inputs to the visual system. Moreover, recent theoretical accounts propose that such configurations could be formally ordered by the degree of their 'non-accidentalness'. In this study, we wanted to explore the possibility that human visual system indeed assigns a special status to non-accidental configurations. In a fast blocked fMRI design, participants were presented with slowly moving stimuli composed of two lines in one of twelve configurations while performing a perceptual similarity task. Using multi-voxel pattern analysis we were able to reliably discriminate between most of the configurations. Moreover, our fMRI data were well modeled by a simple V1 filter bank model. Further analyses failed to reveal an special status of the configurations used. Our experiment sets a baseline for future experiments on non-accidental configuration representation in the visual system."
Boosting with Side Information,"In many problems in machine learning and computer vision, there exists side information, i.e., information contained in the training data and not available in the testing phase. This motivates the recent development of a new learning approach known as learning with side information that is aimed to incorporate side information for improved training of learning algorithms. In this work, we describe a new training method of boosting classifiers that uses side information.  In particular, we propose a novel feature space imputation method to construct extra weak classifiers from the available information that simulate the performance of better weak classifiers obtained from features in side information. We apply our method to two problems, namely handwritten digit recognition and facial expression recognition from low resolution images, where it demonstrates its effectiveness in classification performance."
Boosting with Side Information,"In many problems in machine learning and computer vision, there exists side information, i.e., information contained in the training data and not available in the testing phase. This motivates the recent development of a new learning approach known as learning with side information that is aimed to incorporate side information for improved training of learning algorithms. In this work, we describe a new training method of boosting classifiers that uses side information.  In particular, we propose a novel feature space imputation method to construct extra weak classifiers from the available information that simulate the performance of better weak classifiers obtained from features in side information. We apply our method to two problems, namely handwritten digit recognition and facial expression recognition from low resolution images, where it demonstrates its effectiveness in classification performance."
Boosting with Side Information,"In many problems in machine learning and computer vision, there exists side information, i.e., information contained in the training data and not available in the testing phase. This motivates the recent development of a new learning approach known as learning with side information that is aimed to incorporate side information for improved training of learning algorithms. In this work, we describe a new training method of boosting classifiers that uses side information.  In particular, we propose a novel feature space imputation method to construct extra weak classifiers from the available information that simulate the performance of better weak classifiers obtained from features in side information. We apply our method to two problems, namely handwritten digit recognition and facial expression recognition from low resolution images, where it demonstrates its effectiveness in classification performance."
Learning curves for multi-task Gaussian process regression,"  We study the average case performance of multi-task Gaussian process (GP)  regression as captured in the learning curve, i.e.\ the average Bayes error  for a chosen task versus the total number of examples $n$ for all  tasks. For GP covariances that are the product of an  input-dependent covariance function and a free-form inter-task  covariance matrix, we  show that accurate approximations for the learning curve can be  obtainedfor an arbitrary number of tasks $T$.  We use  these to study the asymptotic learning behaviour for large  $n$. Surprisingly, multi-task learning can be asymptotically essentially  useless: examples from other tasks only help when the  degree of inter-task correlation, $\rho$, is near its maximal value  $\rho=1$. This effect is most extreme for learning of smooth target  functions as described by e.g.\ squared exponential kernels. We also  demonstrate that when learning {\em many} tasks, the learning curves  separate into an initial phase, where the Bayes error on each task  is reduced down to a plateau value by ``collective learning''   even though most tasks have not seen examples,  and a final decay that occurs only once the number of examples is  proportional to the number of tasks."
Nonsingleton Fuzzy Classifier for Attribute Noise Data,"Uncertain in data is an intrinsic problem to supervised machine learning.  Several can be the sources of problems and it is usually very hard to know, or assure, that a dataset is uncertainty free for real world data. One source of uncertainty is attribute noise that can be added by an instrument or by preprocessing data transformations that involve randomness or numerical imprecision. This work proposes to use a supervised fuzzy classifier based on a nonsingleton logic system to deal with attribute noise data. The method, firstly, models the data as fuzzy numbers and, secondly, the fuzzy data is mapped to a high dimensional space using a positive definite kernel whose domain is the fuzzy number space. Finally, the parameters are learnt by a fuzzy classifier using a support vector machine. To test the approach, experiments are done for fifteen attribute noise datasets. The results show that the nonsingleton fuzzy classifier performs well in this kind of noise datasets with no necessity of preprocessing data with noise filtering algorithms. "
Nonsingleton Fuzzy Classifier for Attribute Noise Data,"Uncertain in data is an intrinsic problem to supervised machine learning.  Several can be the sources of problems and it is usually very hard to know, or assure, that a dataset is uncertainty free for real world data. One source of uncertainty is attribute noise that can be added by an instrument or by preprocessing data transformations that involve randomness or numerical imprecision. This work proposes to use a supervised fuzzy classifier based on a nonsingleton logic system to deal with attribute noise data. The method, firstly, models the data as fuzzy numbers and, secondly, the fuzzy data is mapped to a high dimensional space using a positive definite kernel whose domain is the fuzzy number space. Finally, the parameters are learnt by a fuzzy classifier using a support vector machine. To test the approach, experiments are done for fifteen attribute noise datasets. The results show that the nonsingleton fuzzy classifier performs well in this kind of noise datasets with no necessity of preprocessing data with noise filtering algorithms. "
New Relaxations of Graph Cuts for Clustering,"In recent clustering research, the graph cut methods, such as normalized cut and ratio cut, have been well studied and applied to solve many unsupervised learning applications. The original graph cut is an NP-hard problem. Traditional approaches used spectral relaxation to solve the graph cut problem. The main disadvantage of this approach is that the obtained spectral solutions %are not the final clustering results and the post-processing step has to be applied. Thus, the final results could severely deviate from the true solution.To solve this problem, in this paper, we propose a new relaxation mechanism for graph cut methods. Instead of minimizing the squared distances of clustering results, we use the $\ell_1$-norm distance. Meanwhile, considering the normalized consistency, we also use the $\ell_1$-norm for the normalized terms in the new graph cut relaxations. Due to the sparse result from the $\ell_1$-norm minimization, the solution of our new relaxed graph cut methods get discrete values with many zeros, which is close to the ideal solution. However, the new objectives are difficult to be optimized, because the minimization problem involves the ratio of non-smooth terms. The existing sparse learning optimization algorithms cannot be applied to solve this problem. In this paper, we propose a new optimization algorithm to solve this difficult non-smooth ratio minimization problem. The extensive experiments have been performed on three two-way clustering and eight multi-way clustering data sets. All empirical results show that our new relaxation methods consistently enhance the normalized cut and ratio cut clustering results."
New Relaxations of Graph Cuts for Clustering,"In recent clustering research, the graph cut methods, such as normalized cut and ratio cut, have been well studied and applied to solve many unsupervised learning applications. The original graph cut is an NP-hard problem. Traditional approaches used spectral relaxation to solve the graph cut problem. The main disadvantage of this approach is that the obtained spectral solutions %are not the final clustering results and the post-processing step has to be applied. Thus, the final results could severely deviate from the true solution.To solve this problem, in this paper, we propose a new relaxation mechanism for graph cut methods. Instead of minimizing the squared distances of clustering results, we use the $\ell_1$-norm distance. Meanwhile, considering the normalized consistency, we also use the $\ell_1$-norm for the normalized terms in the new graph cut relaxations. Due to the sparse result from the $\ell_1$-norm minimization, the solution of our new relaxed graph cut methods get discrete values with many zeros, which is close to the ideal solution. However, the new objectives are difficult to be optimized, because the minimization problem involves the ratio of non-smooth terms. The existing sparse learning optimization algorithms cannot be applied to solve this problem. In this paper, we propose a new optimization algorithm to solve this difficult non-smooth ratio minimization problem. The extensive experiments have been performed on three two-way clustering and eight multi-way clustering data sets. All empirical results show that our new relaxation methods consistently enhance the normalized cut and ratio cut clustering results."
Dynamic Bayesian Combination of Multiple Imperfect Classifiers,"Classifier combination methods aggregate the decisions of multiple imperfect classifiers to produce higher accuracy decisions. The imperfect classifiers can vary enormously in reliability and change over time. We present a Bayesian approach to classifier combination that allows us to infer and track the changing performance of individuals, using computationally efficient variational Bayesian inference. We apply the approach to real data from a large citizen science project, Galaxy Zoo Supernovae, showing superior performance compared to established classifier combination approaches. Using this dataset we demonstrate the ability to follow changes in performance of citizen scientists over time."
Dynamic Bayesian Combination of Multiple Imperfect Classifiers,"Classifier combination methods aggregate the decisions of multiple imperfect classifiers to produce higher accuracy decisions. The imperfect classifiers can vary enormously in reliability and change over time. We present a Bayesian approach to classifier combination that allows us to infer and track the changing performance of individuals, using computationally efficient variational Bayesian inference. We apply the approach to real data from a large citizen science project, Galaxy Zoo Supernovae, showing superior performance compared to established classifier combination approaches. Using this dataset we demonstrate the ability to follow changes in performance of citizen scientists over time."
Dynamic Bayesian Combination of Multiple Imperfect Classifiers,"Classifier combination methods aggregate the decisions of multiple imperfect classifiers to produce higher accuracy decisions. The imperfect classifiers can vary enormously in reliability and change over time. We present a Bayesian approach to classifier combination that allows us to infer and track the changing performance of individuals, using computationally efficient variational Bayesian inference. We apply the approach to real data from a large citizen science project, Galaxy Zoo Supernovae, showing superior performance compared to established classifier combination approaches. Using this dataset we demonstrate the ability to follow changes in performance of citizen scientists over time."
Transformed Poisson-Dirichlet Processes for Differential Topic Modeling,"We want to compare topics from a number of different document collections:some of these topics capture shared content, others capture the different andunique aspects that the collections may contain. We propose the transformedPoisson-Dirichlet process (TPDP), which is defined to be a class of hierarchicalPoisson-Dirichlet processes (HPDP) with transformed base measures, to build differential topic model among different groups of data. The main challenge of using the TPDP is the non-conjugacy between the prior and likelihood. We propose an efficient sampling algorithm by introducing auxiliary variables, which effectively resolve this problem. Experiment results show a dramatic reduced test perplexity compared to existing approximating methods on a variety of text and image collections. The model also gives an insightful analysis of the Democrat versus Republican blogs leading up to the 2008 USA election."
Transformed Poisson-Dirichlet Processes for Differential Topic Modeling,"We want to compare topics from a number of different document collections:some of these topics capture shared content, others capture the different andunique aspects that the collections may contain. We propose the transformedPoisson-Dirichlet process (TPDP), which is defined to be a class of hierarchicalPoisson-Dirichlet processes (HPDP) with transformed base measures, to build differential topic model among different groups of data. The main challenge of using the TPDP is the non-conjugacy between the prior and likelihood. We propose an efficient sampling algorithm by introducing auxiliary variables, which effectively resolve this problem. Experiment results show a dramatic reduced test perplexity compared to existing approximating methods on a variety of text and image collections. The model also gives an insightful analysis of the Democrat versus Republican blogs leading up to the 2008 USA election."
Transformed Poisson-Dirichlet Processes for Differential Topic Modeling,"We want to compare topics from a number of different document collections:some of these topics capture shared content, others capture the different andunique aspects that the collections may contain. We propose the transformedPoisson-Dirichlet process (TPDP), which is defined to be a class of hierarchicalPoisson-Dirichlet processes (HPDP) with transformed base measures, to build differential topic model among different groups of data. The main challenge of using the TPDP is the non-conjugacy between the prior and likelihood. We propose an efficient sampling algorithm by introducing auxiliary variables, which effectively resolve this problem. Experiment results show a dramatic reduced test perplexity compared to existing approximating methods on a variety of text and image collections. The model also gives an insightful analysis of the Democrat versus Republican blogs leading up to the 2008 USA election."
Contextual Object Learning with Labeled Multi-LDA,"For many applications it is beneficial to model objects in terms of how they can be employed in human activity. In this paper we propose a Labeled Multi-Latent Dirichlet Allocation (LM-LDA) model that encodes both object appearance and how the object interacts with a human. As reported in related work, an appearance cue together with a contextual human action cue significantly improves recognition performance, compared to recognition based solely on object appearance. In addition to this, we show that our LM-LDA model encodes correlations between appearance and associated human actions, which improves classification of a previously unseen object instance, even if only a single image of the instance is available. Furthermore, given only a single view of a previously unseen instance of an object category represented in the model, the LM-LDA model can be used to infer actions that are commonly performed on objects of this category."
Contextual Object Learning with Labeled Multi-LDA,"For many applications it is beneficial to model objects in terms of how they can be employed in human activity. In this paper we propose a Labeled Multi-Latent Dirichlet Allocation (LM-LDA) model that encodes both object appearance and how the object interacts with a human. As reported in related work, an appearance cue together with a contextual human action cue significantly improves recognition performance, compared to recognition based solely on object appearance. In addition to this, we show that our LM-LDA model encodes correlations between appearance and associated human actions, which improves classification of a previously unseen object instance, even if only a single image of the instance is available. Furthermore, given only a single view of a previously unseen instance of an object category represented in the model, the LM-LDA model can be used to infer actions that are commonly performed on objects of this category."
Contextual Object Learning with Labeled Multi-LDA,"For many applications it is beneficial to model objects in terms of how they can be employed in human activity. In this paper we propose a Labeled Multi-Latent Dirichlet Allocation (LM-LDA) model that encodes both object appearance and how the object interacts with a human. As reported in related work, an appearance cue together with a contextual human action cue significantly improves recognition performance, compared to recognition based solely on object appearance. In addition to this, we show that our LM-LDA model encodes correlations between appearance and associated human actions, which improves classification of a previously unseen object instance, even if only a single image of the instance is available. Furthermore, given only a single view of a previously unseen instance of an object category represented in the model, the LM-LDA model can be used to infer actions that are commonly performed on objects of this category."
Universal restrictions on Natural Language determiners from a PAC-learnability perspective,"A classical conjecture in the generative linguistic literature is that universal restrictions on determiners in Natural Language (e.g.~{\em monotonicity}, {\em invariance}, and {\em conservativity}) serve the purpose of simplifying the language acquisition task. This paper formalizes this informal conjecture within the PAC-learnability framework.  "
Dictionary Learning from Ambiguously Labeled Data,"We propose a dictionary-based learning method for the problem of ambiguously labeled multiclass-classification, where each training sample has multiple labels and only one of the labels is correct. The dictionary learning problem is solved using an iterative alternating-algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. For each sample, a confidence/probability distribution is associated with the set of ambiguous labels. The dictionaries are updated using both soft and hard decision rules. Extensive evaluation on the Labeled Faces in the Wild, PIE, and Brodatz datasets demonstrates that the proposed method performs significantly better than state-of-the-art partially-labeled learning approaches."
Dictionary Learning from Ambiguously Labeled Data,"We propose a dictionary-based learning method for the problem of ambiguously labeled multiclass-classification, where each training sample has multiple labels and only one of the labels is correct. The dictionary learning problem is solved using an iterative alternating-algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. For each sample, a confidence/probability distribution is associated with the set of ambiguous labels. The dictionaries are updated using both soft and hard decision rules. Extensive evaluation on the Labeled Faces in the Wild, PIE, and Brodatz datasets demonstrates that the proposed method performs significantly better than state-of-the-art partially-labeled learning approaches."
Dictionary Learning from Ambiguously Labeled Data,"We propose a dictionary-based learning method for the problem of ambiguously labeled multiclass-classification, where each training sample has multiple labels and only one of the labels is correct. The dictionary learning problem is solved using an iterative alternating-algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. For each sample, a confidence/probability distribution is associated with the set of ambiguous labels. The dictionaries are updated using both soft and hard decision rules. Extensive evaluation on the Labeled Faces in the Wild, PIE, and Brodatz datasets demonstrates that the proposed method performs significantly better than state-of-the-art partially-labeled learning approaches."
Dictionary Learning from Ambiguously Labeled Data,"We propose a dictionary-based learning method for the problem of ambiguously labeled multiclass-classification, where each training sample has multiple labels and only one of the labels is correct. The dictionary learning problem is solved using an iterative alternating-algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. For each sample, a confidence/probability distribution is associated with the set of ambiguous labels. The dictionaries are updated using both soft and hard decision rules. Extensive evaluation on the Labeled Faces in the Wild, PIE, and Brodatz datasets demonstrates that the proposed method performs significantly better than state-of-the-art partially-labeled learning approaches."
Dictionary Learning from Ambiguously Labeled Data,"We propose a dictionary-based learning method for the problem of ambiguously labeled multiclass-classification, where each training sample has multiple labels and only one of the labels is correct. The dictionary learning problem is solved using an iterative alternating-algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. For each sample, a confidence/probability distribution is associated with the set of ambiguous labels. The dictionaries are updated using both soft and hard decision rules. Extensive evaluation on the Labeled Faces in the Wild, PIE, and Brodatz datasets demonstrates that the proposed method performs significantly better than state-of-the-art partially-labeled learning approaches."
Using Topic Models to Improve Object Detection in Context,"This paper describes a generative model of images, the objects they contain, their appearances, positions, sizes and locations, and the application of this model to the improvement of object detection. Our generative model has at its heart a topic model of object labels which enforces co-occurrence preferences. We demonstrate how this allows the model to take output from an object detector and enhance it with notions of context, and leverage this additional information to improve detection performance. Our proposed model exhibits key properties lacked by competing approaches: firstly, we are able to naturally model the repeat occurrence of objects in images; and secondly, our model is fully generative and thus easily interpreted and extended. We demonstrate the benefits of our model on the recently introduced SUN09 dataset, as well as the PASCAL 2007 challenge."
Using Topic Models to Improve Object Detection in Context,"This paper describes a generative model of images, the objects they contain, their appearances, positions, sizes and locations, and the application of this model to the improvement of object detection. Our generative model has at its heart a topic model of object labels which enforces co-occurrence preferences. We demonstrate how this allows the model to take output from an object detector and enhance it with notions of context, and leverage this additional information to improve detection performance. Our proposed model exhibits key properties lacked by competing approaches: firstly, we are able to naturally model the repeat occurrence of objects in images; and secondly, our model is fully generative and thus easily interpreted and extended. We demonstrate the benefits of our model on the recently introduced SUN09 dataset, as well as the PASCAL 2007 challenge."
Dictionary-based Spatially-Variant Image Deblurring with Dual-Exposure Stereo,"Camera shake induces image blurring, which was assumed to be spatially  invariant in most traditional image restoration methods. State-of-the-art image deblurring methods utilize combination of projected views to handle camera rotation or approximate motion blur. Besides rotational motion, translation also induces non-uniform blurring because of non-uniform depth of the scene. Due to the complicated camera motion blur, a pair of images acquired with different settings could provide additional information to faciliate image deblurring. However, traditional methods consider image pairs which share almost the same viewpoint and mainly focus on uniform blur. In this paper, we use a stereo pair of images which contain one noisy image and one blurred image. We use sparse representation and kernel estimation techniques to decompose non-uniform motion blur and camera rotation blur. The proposed novel framework could be efficiently solved by convex optimization or compressive sensing algorithms, and it can be easily extended by increasing the size of the dictionary for sparse representation. The proposed algorithm can be used to recover the stereo pair with spatially varying blur. Experimental results show the proposed deblurring algorithm can effectively reduce the motion blur and provide better results than traditional methods."
Dictionary-based Spatially-Variant Image Deblurring with Dual-Exposure Stereo,"Camera shake induces image blurring, which was assumed to be spatially  invariant in most traditional image restoration methods. State-of-the-art image deblurring methods utilize combination of projected views to handle camera rotation or approximate motion blur. Besides rotational motion, translation also induces non-uniform blurring because of non-uniform depth of the scene. Due to the complicated camera motion blur, a pair of images acquired with different settings could provide additional information to faciliate image deblurring. However, traditional methods consider image pairs which share almost the same viewpoint and mainly focus on uniform blur. In this paper, we use a stereo pair of images which contain one noisy image and one blurred image. We use sparse representation and kernel estimation techniques to decompose non-uniform motion blur and camera rotation blur. The proposed novel framework could be efficiently solved by convex optimization or compressive sensing algorithms, and it can be easily extended by increasing the size of the dictionary for sparse representation. The proposed algorithm can be used to recover the stereo pair with spatially varying blur. Experimental results show the proposed deblurring algorithm can effectively reduce the motion blur and provide better results than traditional methods."
Continuous Random Field Models,"  We study the problem of labeling random field models when the labels are  continuous. Many important problems fall under this category such as: stereo,  geolocation, optical flow, etc. We build on the well understood variational  convex formulation. We extend the convex variational formulation to perform  the labeling: (1) in a coarse to fine manner, (2) to handle product of labels,  (3) to apply a weighted median to obtain better results around discontinuities  and (4) to incorporate parameter learning. Our results show significant gains  over the baseline method and better scalability to larger problems."
Continuous Random Field Models,"  We study the problem of labeling random field models when the labels are  continuous. Many important problems fall under this category such as: stereo,  geolocation, optical flow, etc. We build on the well understood variational  convex formulation. We extend the convex variational formulation to perform  the labeling: (1) in a coarse to fine manner, (2) to handle product of labels,  (3) to apply a weighted median to obtain better results around discontinuities  and (4) to incorporate parameter learning. Our results show significant gains  over the baseline method and better scalability to larger problems."
Multiple Choice Learning: Learning to Produce Multiple Structured Outputs,"The paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade. Given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one. The standard approach for handling this scenario is to learn a single model and then produce M-best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we formulate this multiple {\em choice} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem. We present a max-margin formulation that minimizes an upper-bound on this loss-function. Experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this scenario and leads to substantial improvements in prediction accuracy."
Multiple Choice Learning: Learning to Produce Multiple Structured Outputs,"The paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade. Given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one. The standard approach for handling this scenario is to learn a single model and then produce M-best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we formulate this multiple {\em choice} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem. We present a max-margin formulation that minimizes an upper-bound on this loss-function. Experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this scenario and leads to substantial improvements in prediction accuracy."
Multiple Choice Learning: Learning to Produce Multiple Structured Outputs,"The paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade. Given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one. The standard approach for handling this scenario is to learn a single model and then produce M-best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we formulate this multiple {\em choice} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem. We present a max-margin formulation that minimizes an upper-bound on this loss-function. Experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this scenario and leads to substantial improvements in prediction accuracy."
Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions,"Conditional Markov Chains (also known as Linear-Chain Conditional Random Fields  in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables. Large-sample properties of Conditional Markov Chains have been first studied by Sinn and Poupart [1]. The paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given."
Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions,"Conditional Markov Chains (also known as Linear-Chain Conditional Random Fields  in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables. Large-sample properties of Conditional Markov Chains have been first studied by Sinn and Poupart [1]. The paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given."
Extracting Representative Joint Features,"In this paper we present an novel approach for automatically learning representative combinations of single features. The proposed method generates a summarized representation of the data which encodes statistical dependencies between individual features. This increases the discriminative quality while at the same time retaining low-dimensionality. Our approach is motivated by formulating a topological cover of possible joint representations. This allows us to circumvent the combinatorial complexity traditionally associated with generating joint features and learn representations of a cardinality previously deemed impossible. Our proposed method is general and can be used to learn a joint representations of any vector space. In the special case, where the cardinality of each set constituting the cover is one, the summarization corresponds to clustering and our method reduces to traditional k-means. We exemplify the benefits of the proposed method by learning a joint representation for an image classification task. We show that by increasing the size of the sets in the cover we achieve more accurate representation description. Our method significantly improves the quality of the representations in comparison with k-means at a significantly reduced cost. "
Extracting Representative Joint Features,"In this paper we present an novel approach for automatically learning representative combinations of single features. The proposed method generates a summarized representation of the data which encodes statistical dependencies between individual features. This increases the discriminative quality while at the same time retaining low-dimensionality. Our approach is motivated by formulating a topological cover of possible joint representations. This allows us to circumvent the combinatorial complexity traditionally associated with generating joint features and learn representations of a cardinality previously deemed impossible. Our proposed method is general and can be used to learn a joint representations of any vector space. In the special case, where the cardinality of each set constituting the cover is one, the summarization corresponds to clustering and our method reduces to traditional k-means. We exemplify the benefits of the proposed method by learning a joint representation for an image classification task. We show that by increasing the size of the sets in the cover we achieve more accurate representation description. Our method significantly improves the quality of the representations in comparison with k-means at a significantly reduced cost. "
Extracting Representative Joint Features,"In this paper we present an novel approach for automatically learning representative combinations of single features. The proposed method generates a summarized representation of the data which encodes statistical dependencies between individual features. This increases the discriminative quality while at the same time retaining low-dimensionality. Our approach is motivated by formulating a topological cover of possible joint representations. This allows us to circumvent the combinatorial complexity traditionally associated with generating joint features and learn representations of a cardinality previously deemed impossible. Our proposed method is general and can be used to learn a joint representations of any vector space. In the special case, where the cardinality of each set constituting the cover is one, the summarization corresponds to clustering and our method reduces to traditional k-means. We exemplify the benefits of the proposed method by learning a joint representation for an image classification task. We show that by increasing the size of the sets in the cover we achieve more accurate representation description. Our method significantly improves the quality of the representations in comparison with k-means at a significantly reduced cost. "
Persistent Homology for Learning Densities with Bounded Support,"We present a novel method for learning densities with bounded supportwhich enables us to incorporate `hard' topological constraints.In particular, we show how emerging techniques from computational algebraic topologyand the notion of Persistent Homology can be combined with kernel based methods from Machine Learning for the purpose of density estimation.The proposed formalism facilitates learning of models with bounded support in a principledway, and -- by incorporating Persistent Homology techniques in ourapproach -- we are able to encode algebraic-topological constraints which are notaddressed in current state-of the art probabilistic models. We study thebehaviour of our method on two synthetic examples for various samplesizes and exemplify the benefits of the proposed approach on areal-world data-set by learning a motion model for a racecar. Weshow how to learn a model which respects the underlying topologicalstructure of the racetrack, constraining the trajectories of thecar."
Persistent Homology for Learning Densities with Bounded Support,"We present a novel method for learning densities with bounded supportwhich enables us to incorporate `hard' topological constraints.In particular, we show how emerging techniques from computational algebraic topologyand the notion of Persistent Homology can be combined with kernel based methods from Machine Learning for the purpose of density estimation.The proposed formalism facilitates learning of models with bounded support in a principledway, and -- by incorporating Persistent Homology techniques in ourapproach -- we are able to encode algebraic-topological constraints which are notaddressed in current state-of the art probabilistic models. We study thebehaviour of our method on two synthetic examples for various samplesizes and exemplify the benefits of the proposed approach on areal-world data-set by learning a motion model for a racecar. Weshow how to learn a model which respects the underlying topologicalstructure of the racetrack, constraining the trajectories of thecar."
Persistent Homology for Learning Densities with Bounded Support,"We present a novel method for learning densities with bounded supportwhich enables us to incorporate `hard' topological constraints.In particular, we show how emerging techniques from computational algebraic topologyand the notion of Persistent Homology can be combined with kernel based methods from Machine Learning for the purpose of density estimation.The proposed formalism facilitates learning of models with bounded support in a principledway, and -- by incorporating Persistent Homology techniques in ourapproach -- we are able to encode algebraic-topological constraints which are notaddressed in current state-of the art probabilistic models. We study thebehaviour of our method on two synthetic examples for various samplesizes and exemplify the benefits of the proposed approach on areal-world data-set by learning a motion model for a racecar. Weshow how to learn a model which respects the underlying topologicalstructure of the racetrack, constraining the trajectories of thecar."
Persistent Homology for Learning Densities with Bounded Support,"We present a novel method for learning densities with bounded supportwhich enables us to incorporate `hard' topological constraints.In particular, we show how emerging techniques from computational algebraic topologyand the notion of Persistent Homology can be combined with kernel based methods from Machine Learning for the purpose of density estimation.The proposed formalism facilitates learning of models with bounded support in a principledway, and -- by incorporating Persistent Homology techniques in ourapproach -- we are able to encode algebraic-topological constraints which are notaddressed in current state-of the art probabilistic models. We study thebehaviour of our method on two synthetic examples for various samplesizes and exemplify the benefits of the proposed approach on areal-world data-set by learning a motion model for a racecar. Weshow how to learn a model which respects the underlying topologicalstructure of the racetrack, constraining the trajectories of thecar."
Planning with Representation Adaptation in Continuous POMDPs,"Partially observable Markov decision processes (POMDP) provide an elegant mathematical formalism to describe decision problems in stochastic and incompletely perceivable domains. Discrete POMDPs can be solved in reasonable time. However, most applications are continuous and then the belief space is infinite-dimensional.In this paper the general concept of Planning with RepresentationAdaptation in POMDPs (PRAin) is proposed. It comprises two novel ideas to make continuous planning feasible: First, learning a problem specific,efficient space representation during value iteration to generalize results in the continuous space. Second, asserting consistency for the generalized results byself-correction. In order to constraint the POMDP models as little as possible, the concept is implemented with a decision tree learning algorithm and based on Monte Carlo approximation. In an experimental comparison PRAin converges to higher values in significantly shorter time than previous approaches. Further, thenegative influence of higher dimensions is effectively reduced to a minimum."
Planning with Representation Adaptation in Continuous POMDPs,"Partially observable Markov decision processes (POMDP) provide an elegant mathematical formalism to describe decision problems in stochastic and incompletely perceivable domains. Discrete POMDPs can be solved in reasonable time. However, most applications are continuous and then the belief space is infinite-dimensional.In this paper the general concept of Planning with RepresentationAdaptation in POMDPs (PRAin) is proposed. It comprises two novel ideas to make continuous planning feasible: First, learning a problem specific,efficient space representation during value iteration to generalize results in the continuous space. Second, asserting consistency for the generalized results byself-correction. In order to constraint the POMDP models as little as possible, the concept is implemented with a decision tree learning algorithm and based on Monte Carlo approximation. In an experimental comparison PRAin converges to higher values in significantly shorter time than previous approaches. Further, thenegative influence of higher dimensions is effectively reduced to a minimum."
Planning with Representation Adaptation in Continuous POMDPs,"Partially observable Markov decision processes (POMDP) provide an elegant mathematical formalism to describe decision problems in stochastic and incompletely perceivable domains. Discrete POMDPs can be solved in reasonable time. However, most applications are continuous and then the belief space is infinite-dimensional.In this paper the general concept of Planning with RepresentationAdaptation in POMDPs (PRAin) is proposed. It comprises two novel ideas to make continuous planning feasible: First, learning a problem specific,efficient space representation during value iteration to generalize results in the continuous space. Second, asserting consistency for the generalized results byself-correction. In order to constraint the POMDP models as little as possible, the concept is implemented with a decision tree learning algorithm and based on Monte Carlo approximation. In an experimental comparison PRAin converges to higher values in significantly shorter time than previous approaches. Further, thenegative influence of higher dimensions is effectively reduced to a minimum."
Multi-Modal Image Annotation and Retrieval with Multi-Instance Multi-Label LDA,"In image annotation tasks, one image is usually associated with multiple labels, and different image regions may provide different hints. By representing each image region as an instance, multi-instance multi-label (MIML) learning provides a natural formulation for such problems. It is worth noting that, in many real tasks such as web image applications, in addition to the image visual information, there is extra information that can be helpful, such as the surrounding texts or user tags for images. In this paper, we study a multi-modal setting where both visual and tag information are available for each image. We propose Multi-modal Multi-instance Multi-label Latent Dirichlet Allocation (M3LDA), where the model consists of a visual-label part, a tag-label part and a label-topic part. The basic idea is that the topic decided by the visual information and the topic decided by the tag information should be consistent, leading to the correct label assignment. Experiments show that M3LDA outperforms state-of-the-art approaches. Moreover, in contrast to existing approaches that can only give annotations to whole images, M3LDA is able to give annotations to image regions, providing a promising way to understand the relation between input patterns and output semantics."
Ants Crawling to Discover the Community Structure in Networks,"We cast the problem of discovering the community structure in networks as the composition of community candidates, obtained from several community detection base algorithms, into a coherent structure. In turn, this composition can be cast into a maximum-weight clique problem, and we propose an ant colony optimization algorithm to solve it. Our results show that the proposed method is able to discover better community structures, according to several evaluation criteria, than the ones obtained with the base algorithms. It also outperforms, both in quality and in speed, the recently introduced FG-Tiling algorithm."
A Method for Feature Induction in a Class of Higher-Order Random Fields,"Recently, there has been much interest, particularly in computer vision, in developing models which capture higher-order interactions between discrete random variables. Such interactions depend on the joint configuration of at least three variables, and may be described as `higher-order features' of a model, which may be data-dependent or structural, and can involve patterns over several variables, the counts of particular output values, subsets of output values in a solution, and the partition of variables by output value.  In computer vision these features have proved useful for tasks such as scene segmentation, image denoising and visual category discovery, where they can be used naturally to represent class co-occurrences, object histograms and power-law distributions of classes and segment sizes. Typically, particular higher-order features must be built into a model in advance, with the selection of the appropriate features for a given task being made manually.  We propose a feature induction method within a general class of random fields which we call `label-type networks', in which the automatic selection of higher order features of the kinds described can occur.  Our framework makes use of variational methods for both learning and inference, allowing us to draw on recent mean-field filtering techniques to construct dense models and provide the efficiency necessary to explore large feature spaces.  We demonstrate our approach on semantic segmentation using the PASCAL dataset."
A Method for Feature Induction in a Class of Higher-Order Random Fields,"Recently, there has been much interest, particularly in computer vision, in developing models which capture higher-order interactions between discrete random variables. Such interactions depend on the joint configuration of at least three variables, and may be described as `higher-order features' of a model, which may be data-dependent or structural, and can involve patterns over several variables, the counts of particular output values, subsets of output values in a solution, and the partition of variables by output value.  In computer vision these features have proved useful for tasks such as scene segmentation, image denoising and visual category discovery, where they can be used naturally to represent class co-occurrences, object histograms and power-law distributions of classes and segment sizes. Typically, particular higher-order features must be built into a model in advance, with the selection of the appropriate features for a given task being made manually.  We propose a feature induction method within a general class of random fields which we call `label-type networks', in which the automatic selection of higher order features of the kinds described can occur.  Our framework makes use of variational methods for both learning and inference, allowing us to draw on recent mean-field filtering techniques to construct dense models and provide the efficiency necessary to explore large feature spaces.  We demonstrate our approach on semantic segmentation using the PASCAL dataset."
Learning to Classify Actions using Image-level Labels,"We consider the problem of classifying whether a given person in an image is performing an action of interest. Unlike previous methods that rely on onerous and expensive annotations of training images (a tight bounding box of the person and his/her ground truth action label), we propose a  weakly supervised formulation that only requires image level labels indicating the presence or an absence of an action in an image. Specifically, we consider a set of candidate objects obtained automatically from an object detector, and treat the true action of each object as a latent variable. This allows us to adapt the recently proposed dissimilarity coefficient learning framework for our task. In order to address the commonly encountered problem of imbalance in the dataset---the negative samples far outnumber the positive samples---we extend our learning framework by introducing a relative weight for each candidate object. These relative weights are adaptively changed depending on the current estimate of the latent variables. Using the largest publicly available dataset, namely the PASCAL VOC action classification challenge, we show that our approach greatly reduces the burden of annotation while preserving the accuracy of the model."
Robust Manifold Learning via $l_1$ Minimization,"Previous research has shown that many existing manifold learning methods are sensitive to outliers in the data.  Although some robust extensions have been proposed, the techniques used by them are specifically tailored for a particular manifold learning method but cannot be applied readily to other methods.  In this paper, we propose a unified framework for robust manifold learning based on the $l_1$ norm.  By reformulating the optimization problems of several representative manifold learning methods as Rayleigh quotient maximization problems, we find that they can all be seen as minimizing the $l_2$ distance between a normalized similarity matrix and its reconstruction based on low-dimensional embedding.  Due to the sensitivity of the $l_2$ distance measure to outliers in the data, we replace it with an $l_1$ distance measure which is more robust.  To solve a convex relaxation of the optimization problem, instead of using off-the-shelf solvers, we devise an efficient algorithm based on the augmented Lagrange multipliers method by exploiting the special structure of the problem.  For experimental validation, we use the COIL image and UCI database and compare four representative manifold learning methods with their robust variants formulated based on the unified framework proposed."
The Incremental Risk Functional: Basics of a Novel Incremental Learning Approach,"Incremental learning gets increasing attention in research and practice as it has the advantages of continuous adaptation to nonstationary conditions with a low computation and memory demand at the same time. Several approaches have been proposed recently for specific approximators, but only few work has been done to pose the general problem of incremental learning. Hence, we introduce the \emph{incremental risk functional} describing the optimization problem to be solved for incremental learning, regardless of the specific task or approximator. Exemplarily, we apply this approach to regression estimation through linear-in-parameter approximators. We show that the resulting learning algorithm converges and changes the global functional behavior only as little as necessary with every learning step, thus resulting in a stable incremental learning approach."
The Incremental Risk Functional: Basics of a Novel Incremental Learning Approach,"Incremental learning gets increasing attention in research and practice as it has the advantages of continuous adaptation to nonstationary conditions with a low computation and memory demand at the same time. Several approaches have been proposed recently for specific approximators, but only few work has been done to pose the general problem of incremental learning. Hence, we introduce the \emph{incremental risk functional} describing the optimization problem to be solved for incremental learning, regardless of the specific task or approximator. Exemplarily, we apply this approach to regression estimation through linear-in-parameter approximators. We show that the resulting learning algorithm converges and changes the global functional behavior only as little as necessary with every learning step, thus resulting in a stable incremental learning approach."
The Incremental Risk Functional: Basics of a Novel Incremental Learning Approach,"Incremental learning gets increasing attention in research and practice as it has the advantages of continuous adaptation to nonstationary conditions with a low computation and memory demand at the same time. Several approaches have been proposed recently for specific approximators, but only few work has been done to pose the general problem of incremental learning. Hence, we introduce the \emph{incremental risk functional} describing the optimization problem to be solved for incremental learning, regardless of the specific task or approximator. Exemplarily, we apply this approach to regression estimation through linear-in-parameter approximators. We show that the resulting learning algorithm converges and changes the global functional behavior only as little as necessary with every learning step, thus resulting in a stable incremental learning approach."
The Incremental Risk Functional: Basics of a Novel Incremental Learning Approach,"Incremental learning gets increasing attention in research and practice as it has the advantages of continuous adaptation to nonstationary conditions with a low computation and memory demand at the same time. Several approaches have been proposed recently for specific approximators, but only few work has been done to pose the general problem of incremental learning. Hence, we introduce the \emph{incremental risk functional} describing the optimization problem to be solved for incremental learning, regardless of the specific task or approximator. Exemplarily, we apply this approach to regression estimation through linear-in-parameter approximators. We show that the resulting learning algorithm converges and changes the global functional behavior only as little as necessary with every learning step, thus resulting in a stable incremental learning approach."
Perceptual switching for a multistable motion stimulus: modelling and bifurcation analysis ,"Perceptual switches are known to occur during prolongedpresentations of barber pole stimuli viewed through a squareaperture due to competition between 1D motion cues aligned withthe grating's motion direction and 2D motion cues aligned withaperture edges. We study the temporal dynamics of this phenomenonwith a neural fields, population-level representation of activityin MT, a cortical area dedicated to motion estimation. Numericaltools from bifurcation analysis are used to investigate themodel's behaviour in the presence of different types of input;this general approach could be applied to a range of neuralfields models that are typically studied only in terms of theirspontaneous activity. The model reproduces known multistablebehaviour in terms of the predominant interpretations (percepts)of the barber pole stimulus. We describe key differences in thecharacteristic behaviour for two contrast regimes and we predicta change in the underlying distribution of percept switchingtimes for the different regimes."
Perceptual switching for a multistable motion stimulus: modelling and bifurcation analysis ,"Perceptual switches are known to occur during prolongedpresentations of barber pole stimuli viewed through a squareaperture due to competition between 1D motion cues aligned withthe grating's motion direction and 2D motion cues aligned withaperture edges. We study the temporal dynamics of this phenomenonwith a neural fields, population-level representation of activityin MT, a cortical area dedicated to motion estimation. Numericaltools from bifurcation analysis are used to investigate themodel's behaviour in the presence of different types of input;this general approach could be applied to a range of neuralfields models that are typically studied only in terms of theirspontaneous activity. The model reproduces known multistablebehaviour in terms of the predominant interpretations (percepts)of the barber pole stimulus. We describe key differences in thecharacteristic behaviour for two contrast regimes and we predicta change in the underlying distribution of percept switchingtimes for the different regimes."
Perceptual switching for a multistable motion stimulus: modelling and bifurcation analysis ,"Perceptual switches are known to occur during prolongedpresentations of barber pole stimuli viewed through a squareaperture due to competition between 1D motion cues aligned withthe grating's motion direction and 2D motion cues aligned withaperture edges. We study the temporal dynamics of this phenomenonwith a neural fields, population-level representation of activityin MT, a cortical area dedicated to motion estimation. Numericaltools from bifurcation analysis are used to investigate themodel's behaviour in the presence of different types of input;this general approach could be applied to a range of neuralfields models that are typically studied only in terms of theirspontaneous activity. The model reproduces known multistablebehaviour in terms of the predominant interpretations (percepts)of the barber pole stimulus. We describe key differences in thecharacteristic behaviour for two contrast regimes and we predicta change in the underlying distribution of percept switchingtimes for the different regimes."
Perceptual switching for a multistable motion stimulus: modelling and bifurcation analysis ,"Perceptual switches are known to occur during prolongedpresentations of barber pole stimuli viewed through a squareaperture due to competition between 1D motion cues aligned withthe grating's motion direction and 2D motion cues aligned withaperture edges. We study the temporal dynamics of this phenomenonwith a neural fields, population-level representation of activityin MT, a cortical area dedicated to motion estimation. Numericaltools from bifurcation analysis are used to investigate themodel's behaviour in the presence of different types of input;this general approach could be applied to a range of neuralfields models that are typically studied only in terms of theirspontaneous activity. The model reproduces known multistablebehaviour in terms of the predominant interpretations (percepts)of the barber pole stimulus. We describe key differences in thecharacteristic behaviour for two contrast regimes and we predicta change in the underlying distribution of percept switchingtimes for the different regimes."
Perceptual switching for a multistable motion stimulus: modelling and bifurcation analysis ,"Perceptual switches are known to occur during prolongedpresentations of barber pole stimuli viewed through a squareaperture due to competition between 1D motion cues aligned withthe grating's motion direction and 2D motion cues aligned withaperture edges. We study the temporal dynamics of this phenomenonwith a neural fields, population-level representation of activityin MT, a cortical area dedicated to motion estimation. Numericaltools from bifurcation analysis are used to investigate themodel's behaviour in the presence of different types of input;this general approach could be applied to a range of neuralfields models that are typically studied only in terms of theirspontaneous activity. The model reproduces known multistablebehaviour in terms of the predominant interpretations (percepts)of the barber pole stimulus. We describe key differences in thecharacteristic behaviour for two contrast regimes and we predicta change in the underlying distribution of percept switchingtimes for the different regimes."
Balanced kernel k-means for comparing large graphs with landmarks,"In this article, we study the problem of comparing two large geometric graphs with landmark points to each other via kernel functions. For instance, this problem arises in computer vision, brain imaging and bioimaging. Landmark points make the comparison more efficient, as they allow to cluster the nodes in the huge networks into segments around landmarks. However, the runtime still scales quadratically in the size of the largest segment that is created by clustering.Here we propose a graph kernel on size-balanced clusters around landmarks, which minimizes the size of the largest cluster, thereby optimizing the runtime of kernel computation. The resulting kernel on huge graphs with landmarks outperforms kernels based on unbalanced clustering in terms of runtime, while retaining high levels of accuracy on classification benchmarks."
Balanced kernel k-means for comparing large graphs with landmarks,"In this article, we study the problem of comparing two large geometric graphs with landmark points to each other via kernel functions. For instance, this problem arises in computer vision, brain imaging and bioimaging. Landmark points make the comparison more efficient, as they allow to cluster the nodes in the huge networks into segments around landmarks. However, the runtime still scales quadratically in the size of the largest segment that is created by clustering.Here we propose a graph kernel on size-balanced clusters around landmarks, which minimizes the size of the largest cluster, thereby optimizing the runtime of kernel computation. The resulting kernel on huge graphs with landmarks outperforms kernels based on unbalanced clustering in terms of runtime, while retaining high levels of accuracy on classification benchmarks."
Balanced kernel k-means for comparing large graphs with landmarks,"In this article, we study the problem of comparing two large geometric graphs with landmark points to each other via kernel functions. For instance, this problem arises in computer vision, brain imaging and bioimaging. Landmark points make the comparison more efficient, as they allow to cluster the nodes in the huge networks into segments around landmarks. However, the runtime still scales quadratically in the size of the largest segment that is created by clustering.Here we propose a graph kernel on size-balanced clusters around landmarks, which minimizes the size of the largest cluster, thereby optimizing the runtime of kernel computation. The resulting kernel on huge graphs with landmarks outperforms kernels based on unbalanced clustering in terms of runtime, while retaining high levels of accuracy on classification benchmarks."
Balanced kernel k-means for comparing large graphs with landmarks,"In this article, we study the problem of comparing two large geometric graphs with landmark points to each other via kernel functions. For instance, this problem arises in computer vision, brain imaging and bioimaging. Landmark points make the comparison more efficient, as they allow to cluster the nodes in the huge networks into segments around landmarks. However, the runtime still scales quadratically in the size of the largest segment that is created by clustering.Here we propose a graph kernel on size-balanced clusters around landmarks, which minimizes the size of the largest cluster, thereby optimizing the runtime of kernel computation. The resulting kernel on huge graphs with landmarks outperforms kernels based on unbalanced clustering in terms of runtime, while retaining high levels of accuracy on classification benchmarks."
On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes," We consider infinite-horizon stationary $\gamma$-discounted Markov  Decision Processes, for which it is known that there exists a  stationary optimal policy. Using Value and Policy Iteration with  some error $\epsilon$ at each iteration, it is well-known that one  can compute stationary policies that are $\frac{2\gamma{(1-\gamma)^2}\epsilon$-optimal. After arguing that this  guarantee is tight, we develop variations of Value and Policy  Iteration for computing non-stationary policies that can be up to  $\frac{2\gamma}{1-\gamma}\epsilon$-optimal, which constitutes a significant  improvement in the usual situation when $\gamma$ is close to  $1$. Surprisingly, this shows that the problem of ``computing near-optimal non-stationary policies'' is much simpler than that  of ``computing near-optimal stationary policies''."
On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes," We consider infinite-horizon stationary $\gamma$-discounted Markov  Decision Processes, for which it is known that there exists a  stationary optimal policy. Using Value and Policy Iteration with  some error $\epsilon$ at each iteration, it is well-known that one  can compute stationary policies that are $\frac{2\gamma{(1-\gamma)^2}\epsilon$-optimal. After arguing that this  guarantee is tight, we develop variations of Value and Policy  Iteration for computing non-stationary policies that can be up to  $\frac{2\gamma}{1-\gamma}\epsilon$-optimal, which constitutes a significant  improvement in the usual situation when $\gamma$ is close to  $1$. Surprisingly, this shows that the problem of ``computing near-optimal non-stationary policies'' is much simpler than that  of ``computing near-optimal stationary policies''."
Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model,"Neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli. While adaptation is an intrinsic feature of neuronal models like the Hodgkin-Huxley model, the challenge is to integrate adaptation in models of neural computation. Recent computational models like the Adaptive Spike Response Model implement adaptation as spike-based addition of fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents. Such adaptation has been shown to accurately model neural spiking behavior over a limited dynamic range. Taking a cue from kinetic models of adaptation, we propose a multiplicative Adaptive Spike Response Model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking. We show that unlike the additive adaptation model, the firing rate in the multiplicative adaptation model saturates to a maximum spike-rate. When simulating variance switching experiments, the model also quantitatively fits the experimental data over a wide dynamic range. Furthermore, dynamic threshold models of adaptation suggest a straightforward interpretation of neural activity in terms of dynamic signal encoding with shifted and weighted exponential kernels. We show that when thus encoding rectified filtered stimulus signals, the multiplicative Adaptive Spike Response Model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude, without changing model parameters. "
Turing Test Imaging based on Non-Artificial Evidence Criterion,"We in this paper present a Turing test image (TTI) generated method. Using our produced image, we can tell computers and humans apart and protectwebsite from attack. To this end, we propose a robust TTI generated framework based on novelNon-Artificial Evidence Criterion (NAEC). Extensive user study is conduced to prove our TTI is user-friendly. Experiments demonstrate that our proposed Turing test image can resist attack from state-of-the-art object detection and imageclassification system."
Measuring Correlations Using Information Dimension,"We propose to measure the correlation between two continuous random variables by the mutual information dimension (MID), and present an efficient parameter-free estimation method of MID with linearithmical complexity with respect to the number of data points. Our method gives an effective solution, supported by sound dimension theory, to the problem of detecting interesting relationships of variables in massive data, which is now becoming a heavily studied topic not only in machine learning but also all over science. Different from the classical Pearson correlation coefficient, MID is zero if and only if two random variables are statistically independent and is translation and scaling invariant. We experimentally show superior performance of MID for various types of relationships in detection of them in the presence of noise."
Measuring Correlations Using Information Dimension,"We propose to measure the correlation between two continuous random variables by the mutual information dimension (MID), and present an efficient parameter-free estimation method of MID with linearithmical complexity with respect to the number of data points. Our method gives an effective solution, supported by sound dimension theory, to the problem of detecting interesting relationships of variables in massive data, which is now becoming a heavily studied topic not only in machine learning but also all over science. Different from the classical Pearson correlation coefficient, MID is zero if and only if two random variables are statistically independent and is translation and scaling invariant. We experimentally show superior performance of MID for various types of relationships in detection of them in the presence of noise."
MAP Inference in Chains using Column Generation,"Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning."
MAP Inference in Chains using Column Generation,"Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning."
MAP Inference in Chains using Column Generation,"Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning."
MAP Inference in Chains using Column Generation,"Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning."
"Scoring Bayesian Networks with Informative, Causal and Associative Priors","A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. Currently however, there are limited practical ways of assigning priors to each possible network. In this paper, we present a method for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between a pair of variables X and Y. This type of knowledge naturally arises from prior experimental and observational datasets, among others. We show that incorporating such prior knowledge may not only improve the learning of the direction of the causal relations in the network, but also the learning of the network skeleton. This is particularly the case when sample size is low and thus prior knowledge increases in importance. Our approach is based on converting possibly-incoherent beliefs about marginals to joint distributions of priors by use of optimization theory."
"Scoring Bayesian Networks with Informative, Causal and Associative Priors","A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. Currently however, there are limited practical ways of assigning priors to each possible network. In this paper, we present a method for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between a pair of variables X and Y. This type of knowledge naturally arises from prior experimental and observational datasets, among others. We show that incorporating such prior knowledge may not only improve the learning of the direction of the causal relations in the network, but also the learning of the network skeleton. This is particularly the case when sample size is low and thus prior knowledge increases in importance. Our approach is based on converting possibly-incoherent beliefs about marginals to joint distributions of priors by use of optimization theory."
Multi-Step Regression Learning for Compositional Distributional Semantics,"This paper presents a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it against two existing experiments, and find it to perform better than comparable methods in one, and on par with other best-performing models in the other. We argue in our analysis that the nature of this learning method renders it suitable for solving more subtle problems compositional distributional models might face, and suggest future work in this area."
Multi-Step Regression Learning for Compositional Distributional Semantics,"This paper presents a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it against two existing experiments, and find it to perform better than comparable methods in one, and on par with other best-performing models in the other. We argue in our analysis that the nature of this learning method renders it suitable for solving more subtle problems compositional distributional models might face, and suggest future work in this area."
Multi-Step Regression Learning for Compositional Distributional Semantics,"This paper presents a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it against two existing experiments, and find it to perform better than comparable methods in one, and on par with other best-performing models in the other. We argue in our analysis that the nature of this learning method renders it suitable for solving more subtle problems compositional distributional models might face, and suggest future work in this area."
PRISMA: PRoximal Iterative SMoothing Algorithm,"Motivated by learning problems including max-norm regularized matrix completion and clustering, robust PCA and basis pursuit, we propose a novel optimization algorithm for minimizing a convex objective which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz part, and a simple non-smooth non-Lipschitz part. Our algorithm combines the methodology of forward-backward splitting, smoothing, and accelerated proximal methods."
PRISMA: PRoximal Iterative SMoothing Algorithm,"Motivated by learning problems including max-norm regularized matrix completion and clustering, robust PCA and basis pursuit, we propose a novel optimization algorithm for minimizing a convex objective which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz part, and a simple non-smooth non-Lipschitz part. Our algorithm combines the methodology of forward-backward splitting, smoothing, and accelerated proximal methods."
PRISMA: PRoximal Iterative SMoothing Algorithm,"Motivated by learning problems including max-norm regularized matrix completion and clustering, robust PCA and basis pursuit, we propose a novel optimization algorithm for minimizing a convex objective which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz part, and a simple non-smooth non-Lipschitz part. Our algorithm combines the methodology of forward-backward splitting, smoothing, and accelerated proximal methods."
Bayesian Nonparametric Modeling of Suicide Attempts,"The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the U.S. population. In the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts."
Bayesian Nonparametric Modeling of Suicide Attempts,"The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the U.S. population. In the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts."
Bayesian Nonparametric Modeling of Suicide Attempts,"The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the U.S. population. In the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts."
Using GSMDPs in Multi-Robot Decision-Making Problems,"Markov Decision Processes (MDPs) provide an extensive theoretical background for problems of decision-making under uncertainty. In order to maintain computational tractability, however, problems domains are typically discretized both in states and actions as well as in time. Assuming synchronous state transitions and actions at fixed rates may result in models which are not strictly Markovian, or where agents are forced to idle, losing their ability to react to sudden changes in the environment. In this work, we explore the application of Generalized Semi-Markov Decision Processes (GSMDPs) to a realistic multiagent scenario. A case study will be presented in the domain of cooperative robotics, where real-time reactivity must be preserved, and therefore synchronous discrete-time approaches are sub-optimal. By allowing asynchronous events to be modeled over continuous time, the GSMDP approach will be shown to provide greater solution quality, and reduced communication usage, than its discrete-time counterparts, while still being approximately solvable by existing methods."
Using GSMDPs in Multi-Robot Decision-Making Problems,"Markov Decision Processes (MDPs) provide an extensive theoretical background for problems of decision-making under uncertainty. In order to maintain computational tractability, however, problems domains are typically discretized both in states and actions as well as in time. Assuming synchronous state transitions and actions at fixed rates may result in models which are not strictly Markovian, or where agents are forced to idle, losing their ability to react to sudden changes in the environment. In this work, we explore the application of Generalized Semi-Markov Decision Processes (GSMDPs) to a realistic multiagent scenario. A case study will be presented in the domain of cooperative robotics, where real-time reactivity must be preserved, and therefore synchronous discrete-time approaches are sub-optimal. By allowing asynchronous events to be modeled over continuous time, the GSMDP approach will be shown to provide greater solution quality, and reduced communication usage, than its discrete-time counterparts, while still being approximately solvable by existing methods."
Using GSMDPs in Multi-Robot Decision-Making Problems,"Markov Decision Processes (MDPs) provide an extensive theoretical background for problems of decision-making under uncertainty. In order to maintain computational tractability, however, problems domains are typically discretized both in states and actions as well as in time. Assuming synchronous state transitions and actions at fixed rates may result in models which are not strictly Markovian, or where agents are forced to idle, losing their ability to react to sudden changes in the environment. In this work, we explore the application of Generalized Semi-Markov Decision Processes (GSMDPs) to a realistic multiagent scenario. A case study will be presented in the domain of cooperative robotics, where real-time reactivity must be preserved, and therefore synchronous discrete-time approaches are sub-optimal. By allowing asynchronous events to be modeled over continuous time, the GSMDP approach will be shown to provide greater solution quality, and reduced communication usage, than its discrete-time counterparts, while still being approximately solvable by existing methods."
Time Series Analysis - An Online Learning Approach,"In this paper, we address the problem of predicting a time series, under minimal assumptions on the noise terms. The focus of our work is on the ARMA model, which is a standard model for time series prediction. Using regret minimization techniques, we develop an effective online learning algorithm for the prediction problem, \emph{without} assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we perform an empirical study to verify the effectiveness of our algorithm."
Time Series Analysis - An Online Learning Approach,"In this paper, we address the problem of predicting a time series, under minimal assumptions on the noise terms. The focus of our work is on the ARMA model, which is a standard model for time series prediction. Using regret minimization techniques, we develop an effective online learning algorithm for the prediction problem, \emph{without} assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we perform an empirical study to verify the effectiveness of our algorithm."
Time Series Analysis - An Online Learning Approach,"In this paper, we address the problem of predicting a time series, under minimal assumptions on the noise terms. The focus of our work is on the ARMA model, which is a standard model for time series prediction. Using regret minimization techniques, we develop an effective online learning algorithm for the prediction problem, \emph{without} assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we perform an empirical study to verify the effectiveness of our algorithm."
Time Series Analysis - An Online Learning Approach,"In this paper, we address the problem of predicting a time series, under minimal assumptions on the noise terms. The focus of our work is on the ARMA model, which is a standard model for time series prediction. Using regret minimization techniques, we develop an effective online learning algorithm for the prediction problem, \emph{without} assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we perform an empirical study to verify the effectiveness of our algorithm."
Gradient Matching with Gaussian Processes,"Parameter inference in mechanistic models based on systems of coupled differential equations is a topical yet computationally challenging problem, due to the need to follow each parameter adaptation with a numerical solution of the differential equations. Techniques based on gradient matching, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differential equations, offer a computationally appealing shortcut to the inference  problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes. We show how the inference approach proposed in an earlier NIPS paper can be substantially improved, and we demonstrate the efficiency of the technique on three benchmark systems."
Gradient Matching with Gaussian Processes,"Parameter inference in mechanistic models based on systems of coupled differential equations is a topical yet computationally challenging problem, due to the need to follow each parameter adaptation with a numerical solution of the differential equations. Techniques based on gradient matching, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differential equations, offer a computationally appealing shortcut to the inference  problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes. We show how the inference approach proposed in an earlier NIPS paper can be substantially improved, and we demonstrate the efficiency of the technique on three benchmark systems."
Gradient Matching with Gaussian Processes,"Parameter inference in mechanistic models based on systems of coupled differential equations is a topical yet computationally challenging problem, due to the need to follow each parameter adaptation with a numerical solution of the differential equations. Techniques based on gradient matching, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differential equations, offer a computationally appealing shortcut to the inference  problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes. We show how the inference approach proposed in an earlier NIPS paper can be substantially improved, and we demonstrate the efficiency of the technique on three benchmark systems."
Gradient Matching with Gaussian Processes,"Parameter inference in mechanistic models based on systems of coupled differential equations is a topical yet computationally challenging problem, due to the need to follow each parameter adaptation with a numerical solution of the differential equations. Techniques based on gradient matching, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differential equations, offer a computationally appealing shortcut to the inference  problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes. We show how the inference approach proposed in an earlier NIPS paper can be substantially improved, and we demonstrate the efficiency of the technique on three benchmark systems."
Maximally informative models and diffeomorphic modes,"Motivated by recent experiments in transcriptional regulation and sensory neuroscience, we consider the following general problem in statistical inference. A system of interest, when exposed to a stimulus $S$, adopts a deterministic response $R = \theta(S)$, of which a noisy measurement $M \sim \pi(M|R)$ is made. Given a large number of measurements and corresponding stimuli, we wish to identify the correct response function $\theta$. However, we do not know the conditional distribution $\pi(M|R)$ a priori. The standard regression approach is to maximize the likelihood of a model response function $\theta$ assuming a specific noise function $\pi$. However, an incorrect $\pi$ will typically lead to systematic error in the inferred $\theta$.  In the large data limit, we argue that one should instead maximize likelihood over both $\theta$ and $\pi$, and we show that this is equivalent to maximizing the mutual information $I[R;M]$ over $\theta$ alone. Moreover, when any candidate response model fully explains the data, maximizing mutual information becomes equivalent to simultaneously maximizing all dependence measures which obey the Data Processing Inequality. Even so, one typically finds not a single optimal $\theta$, but rather an extended subspace of optimal $\theta$. This is because experiments of the type considered cannot distinguish between different $\theta$ within certain equivalence classes. We present a formula for ``diffeomorphic modes'', directions in parameter space which lie within these equivalence classes and cannot, as a result, be constrained by data. We then derive the diffeomorphic modes of response models having either a general linear form or a specific linear-nonlinear form, and find that the number of diffeomorphic modes is often far less then the number of response model parameters. In such cases, maximizing mutual information will, in the large data limit, determine nearly all model parameters without any systematic error."
Maximally informative models and diffeomorphic modes,"Motivated by recent experiments in transcriptional regulation and sensory neuroscience, we consider the following general problem in statistical inference. A system of interest, when exposed to a stimulus $S$, adopts a deterministic response $R = \theta(S)$, of which a noisy measurement $M \sim \pi(M|R)$ is made. Given a large number of measurements and corresponding stimuli, we wish to identify the correct response function $\theta$. However, we do not know the conditional distribution $\pi(M|R)$ a priori. The standard regression approach is to maximize the likelihood of a model response function $\theta$ assuming a specific noise function $\pi$. However, an incorrect $\pi$ will typically lead to systematic error in the inferred $\theta$.  In the large data limit, we argue that one should instead maximize likelihood over both $\theta$ and $\pi$, and we show that this is equivalent to maximizing the mutual information $I[R;M]$ over $\theta$ alone. Moreover, when any candidate response model fully explains the data, maximizing mutual information becomes equivalent to simultaneously maximizing all dependence measures which obey the Data Processing Inequality. Even so, one typically finds not a single optimal $\theta$, but rather an extended subspace of optimal $\theta$. This is because experiments of the type considered cannot distinguish between different $\theta$ within certain equivalence classes. We present a formula for ``diffeomorphic modes'', directions in parameter space which lie within these equivalence classes and cannot, as a result, be constrained by data. We then derive the diffeomorphic modes of response models having either a general linear form or a specific linear-nonlinear form, and find that the number of diffeomorphic modes is often far less then the number of response model parameters. In such cases, maximizing mutual information will, in the large data limit, determine nearly all model parameters without any systematic error."
Learning to detect objects with high intra-class variation,"Objects such as humans exhibit large intra-class variations, posing significant challenges for visual object detection. State-of-the-art part-based models explicitly model object deformations, but are limited in their ability to handle image variations incurred by other geometric and photometric changes, such as object pose, lighting, occlusions, and large appearance variations. In this paper, we propose a novel approach which uses a spatially-biased hierarchical feature mapping scheme to map features into a high-dimensional space that better represents the rich set of object appearance and local deformation variations. We propose a new algorithm to jointly learn object detection and performfeature pooling in this high dimensional space, in a structured prediction setting. Our approach achieves the best detection performance on the INRIA pedestrian dataset."
Learning to detect objects with high intra-class variation,"Objects such as humans exhibit large intra-class variations, posing significant challenges for visual object detection. State-of-the-art part-based models explicitly model object deformations, but are limited in their ability to handle image variations incurred by other geometric and photometric changes, such as object pose, lighting, occlusions, and large appearance variations. In this paper, we propose a novel approach which uses a spatially-biased hierarchical feature mapping scheme to map features into a high-dimensional space that better represents the rich set of object appearance and local deformation variations. We propose a new algorithm to jointly learn object detection and performfeature pooling in this high dimensional space, in a structured prediction setting. Our approach achieves the best detection performance on the INRIA pedestrian dataset."
Learning to detect objects with high intra-class variation,"Objects such as humans exhibit large intra-class variations, posing significant challenges for visual object detection. State-of-the-art part-based models explicitly model object deformations, but are limited in their ability to handle image variations incurred by other geometric and photometric changes, such as object pose, lighting, occlusions, and large appearance variations. In this paper, we propose a novel approach which uses a spatially-biased hierarchical feature mapping scheme to map features into a high-dimensional space that better represents the rich set of object appearance and local deformation variations. We propose a new algorithm to jointly learn object detection and performfeature pooling in this high dimensional space, in a structured prediction setting. Our approach achieves the best detection performance on the INRIA pedestrian dataset."
Learning to detect objects with high intra-class variation,"Objects such as humans exhibit large intra-class variations, posing significant challenges for visual object detection. State-of-the-art part-based models explicitly model object deformations, but are limited in their ability to handle image variations incurred by other geometric and photometric changes, such as object pose, lighting, occlusions, and large appearance variations. In this paper, we propose a novel approach which uses a spatially-biased hierarchical feature mapping scheme to map features into a high-dimensional space that better represents the rich set of object appearance and local deformation variations. We propose a new algorithm to jointly learn object detection and performfeature pooling in this high dimensional space, in a structured prediction setting. Our approach achieves the best detection performance on the INRIA pedestrian dataset."
Learning to detect objects with high intra-class variation,"Objects such as humans exhibit large intra-class variations, posing significant challenges for visual object detection. State-of-the-art part-based models explicitly model object deformations, but are limited in their ability to handle image variations incurred by other geometric and photometric changes, such as object pose, lighting, occlusions, and large appearance variations. In this paper, we propose a novel approach which uses a spatially-biased hierarchical feature mapping scheme to map features into a high-dimensional space that better represents the rich set of object appearance and local deformation variations. We propose a new algorithm to jointly learn object detection and performfeature pooling in this high dimensional space, in a structured prediction setting. Our approach achieves the best detection performance on the INRIA pedestrian dataset."
Analysis on Co-Training with Insufficient Views,"Co-training is a famous semi-supervised learning paradigm which exploits unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sufficient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sufficient to correctly predict the label. First, we show that co-training might suffer from two limitations, i.e., label noise and sampling bias, when neither view is sufficient. We then define the diversity between the two views with respect to the confidence of prediction and prove that if the two views have large diversity, co-training suffers little from above two limitations and could succeed in improving the learning performance by exploiting unlabeled data even with insufficient views."
Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation,"This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture.Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP.Extensive experiments on different data sets demonstrate that EC-PBP achieves a higher topic modeling accuracyand reduces more than $80\%$ communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm."
Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation,"This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture.Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP.Extensive experiments on different data sets demonstrate that EC-PBP achieves a higher topic modeling accuracyand reduces more than $80\%$ communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm."
Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation,"This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture.Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP.Extensive experiments on different data sets demonstrate that EC-PBP achieves a higher topic modeling accuracyand reduces more than $80\%$ communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm."
Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling,"Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches."
Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling,"Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches."
Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling,"Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches."
Object Recognition with Boosted Collections of Parts,"We describe an approach for incrementally learning parts for object recognition.  We use category-independent object region proposals to pool learned parts, which removes the need for a structurally constrained spatial model and enables more flexible and extensible learning. Our base model uses an incremental learning approach to refine statistical templates of parts. Boosting is used to learn a strong classifier that scores collections of part responses into an object category. Our system achieves competitive accuracy with the best-existing systems, outperforming on more deformable categories, and is easy to extend to new features or categories."
Object Recognition with Boosted Collections of Parts,"We describe an approach for incrementally learning parts for object recognition.  We use category-independent object region proposals to pool learned parts, which removes the need for a structurally constrained spatial model and enables more flexible and extensible learning. Our base model uses an incremental learning approach to refine statistical templates of parts. Boosting is used to learn a strong classifier that scores collections of part responses into an object category. Our system achieves competitive accuracy with the best-existing systems, outperforming on more deformable categories, and is easy to extend to new features or categories."
Object Recognition with Boosted Collections of Parts,"We describe an approach for incrementally learning parts for object recognition.  We use category-independent object region proposals to pool learned parts, which removes the need for a structurally constrained spatial model and enables more flexible and extensible learning. Our base model uses an incremental learning approach to refine statistical templates of parts. Boosting is used to learn a strong classifier that scores collections of part responses into an object category. Our system achieves competitive accuracy with the best-existing systems, outperforming on more deformable categories, and is easy to extend to new features or categories."
Object Recognition with Boosted Collections of Parts,"We describe an approach for incrementally learning parts for object recognition.  We use category-independent object region proposals to pool learned parts, which removes the need for a structurally constrained spatial model and enables more flexible and extensible learning. Our base model uses an incremental learning approach to refine statistical templates of parts. Boosting is used to learn a strong classifier that scores collections of part responses into an object category. Our system achieves competitive accuracy with the best-existing systems, outperforming on more deformable categories, and is easy to extend to new features or categories."
Neurally Plausible Reinforcement Learning of Working Memory Tasks,"A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity. It is however not well known how these neurons acquire their task-relevant tuning. Here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error. We propose that the action selection stage feeds back attentional signals to earlier processing levels. These feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. A globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to (1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and (2) learn to optimally integrate probabilistic evidence for perceptual decision making."
Neurally Plausible Reinforcement Learning of Working Memory Tasks,"A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity. It is however not well known how these neurons acquire their task-relevant tuning. Here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error. We propose that the action selection stage feeds back attentional signals to earlier processing levels. These feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. A globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to (1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and (2) learn to optimally integrate probabilistic evidence for perceptual decision making."
Neurally Plausible Reinforcement Learning of Working Memory Tasks,"A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity. It is however not well known how these neurons acquire their task-relevant tuning. Here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error. We propose that the action selection stage feeds back attentional signals to earlier processing levels. These feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. A globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to (1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and (2) learn to optimally integrate probabilistic evidence for perceptual decision making."
Robust PCA from incomplete observations using l-0-surrogates,"Many applications in data analysis rely on the decomposition of a data matrix into a low-rank and a sparse component.Existing methods that tackle this task use the nuclear norm and l-1-cost functions as convex relaxations of the rank constraint and the sparsity measure, respectively, or employ thresholding techniques.We propose a method that allows for reconstructing a matrix of upper-bounded rank from incomplete and corrupted observations. It does not require any a priori information about the number of outliers. The core of our algorithm is an intrinsic Conjugate Gradient method on the set of orthogonal projection matrices, the so-called Grassmannian. Non-convex sparsity measures are used for outlier detection, which leads to improved performance in terms of robustly recovering the low-rank matrix.In particular, our approach can cope with more outliers and with an underlying matrix of higher rank than other state-of-the-art methods."
Robust PCA from incomplete observations using l-0-surrogates,"Many applications in data analysis rely on the decomposition of a data matrix into a low-rank and a sparse component.Existing methods that tackle this task use the nuclear norm and l-1-cost functions as convex relaxations of the rank constraint and the sparsity measure, respectively, or employ thresholding techniques.We propose a method that allows for reconstructing a matrix of upper-bounded rank from incomplete and corrupted observations. It does not require any a priori information about the number of outliers. The core of our algorithm is an intrinsic Conjugate Gradient method on the set of orthogonal projection matrices, the so-called Grassmannian. Non-convex sparsity measures are used for outlier detection, which leads to improved performance in terms of robustly recovering the low-rank matrix.In particular, our approach can cope with more outliers and with an underlying matrix of higher rank than other state-of-the-art methods."
The flip-the-state transition operator for Restricted Boltzmann Machines,"Most learning algorithms for restricted Boltzmann machines (RMBs) rely onMarkov chain Monte Carlo (MCMC) methods using Gibbs sampling. The mostprominent examples are Contrastive Divergence learning (CD) and its variantsas well as Parallel Tempering (PT). The performance of these methods dependsseverely on the mixing properties of the Gibbs chain. We propose a Metropolis-type MCMC algorithm relying on a transition operator maximizing the probabilityof state changes. It is shown that the operator induces an irreducible and aperi-odic and, thus, a properly converging Markov chain. The transition operator canreplace Gibbs sampling in RBM learning algorithms without producing computa-tional overhead. It is shown empirically that this leads to faster mixing and in turnto more accurate learning."
The flip-the-state transition operator for Restricted Boltzmann Machines,"Most learning algorithms for restricted Boltzmann machines (RMBs) rely onMarkov chain Monte Carlo (MCMC) methods using Gibbs sampling. The mostprominent examples are Contrastive Divergence learning (CD) and its variantsas well as Parallel Tempering (PT). The performance of these methods dependsseverely on the mixing properties of the Gibbs chain. We propose a Metropolis-type MCMC algorithm relying on a transition operator maximizing the probabilityof state changes. It is shown that the operator induces an irreducible and aperi-odic and, thus, a properly converging Markov chain. The transition operator canreplace Gibbs sampling in RBM learning algorithms without producing computa-tional overhead. It is shown empirically that this leads to faster mixing and in turnto more accurate learning."
The flip-the-state transition operator for Restricted Boltzmann Machines,"Most learning algorithms for restricted Boltzmann machines (RMBs) rely onMarkov chain Monte Carlo (MCMC) methods using Gibbs sampling. The mostprominent examples are Contrastive Divergence learning (CD) and its variantsas well as Parallel Tempering (PT). The performance of these methods dependsseverely on the mixing properties of the Gibbs chain. We propose a Metropolis-type MCMC algorithm relying on a transition operator maximizing the probabilityof state changes. It is shown that the operator induces an irreducible and aperi-odic and, thus, a properly converging Markov chain. The transition operator canreplace Gibbs sampling in RBM learning algorithms without producing computa-tional overhead. It is shown empirically that this leads to faster mixing and in turnto more accurate learning."
Repulsive Mixtures,"Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.  Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set."
Non-Linear Dimensionality Reduction by Isometric Patch Alignment,"We propose a novel dimensionality reduction method which has low computational cost. This method is inspired by two key observations: (i) the structure of reasonably large patches of high-dimensional data can be preserved as a whole, rather than divided into small neighborhoods; and (ii) attaching two neighboring patches will align them such that the overall rank does not increase. In the proposed approach, first the data is clustered, so that it is conceptually reduced to a set of overlapping low-rank clusters. Each cluster is embedded into a low-dimensional patch and then all of the patches are rearranged such that their border points are matched. We show that the rearrangement can be computed by solving a relatively small semi-definite program. The embedding computed by this optimization is provably low-rank. The proposed method is stable, fast, and scalable; experimental results demonstrate its capability for manifold learning, data visualization, and even complex tasks such as protein structure determination."
Non-Linear Dimensionality Reduction by Isometric Patch Alignment,"We propose a novel dimensionality reduction method which has low computational cost. This method is inspired by two key observations: (i) the structure of reasonably large patches of high-dimensional data can be preserved as a whole, rather than divided into small neighborhoods; and (ii) attaching two neighboring patches will align them such that the overall rank does not increase. In the proposed approach, first the data is clustered, so that it is conceptually reduced to a set of overlapping low-rank clusters. Each cluster is embedded into a low-dimensional patch and then all of the patches are rearranged such that their border points are matched. We show that the rearrangement can be computed by solving a relatively small semi-definite program. The embedding computed by this optimization is provably low-rank. The proposed method is stable, fast, and scalable; experimental results demonstrate its capability for manifold learning, data visualization, and even complex tasks such as protein structure determination."
Transductive Kernel Map Learning,"Transductive inference techniques are nowadays becoming standard  in machine learning due to their relative success in solving many real-world applications. Among them, kernel-based methods are particularly interesting but their success remains highly dependent on the choice of kernels. The latter are usually handcrafted or designed  in order to capture better similarity in training data. In this paper, we  introduce a novel transductive learning algorithmfor kernel design and classification. Our approach is basedon the minimization of an energy function mixing  i) a reconstructionterm that factorizes a matrix of  input data as a product of a learned dictionary  and a learned kernel map ii) a fidelity term thatensures consistent label predictions with those provided in a ground-truth and iii) a smoothness term which  guaranteessimilar labels for neighboring data and allows us to iteratively diffuse  kernel maps and labels from  labeled to unlabeled data. Solving this minimization problem makes it possible to learn both a decision criterion and a kernel map  that guarantee linearseparability in a high dimensional space and good generalizationperformance. Experiments conducted on object class segmentation, show improvements with respect to baseline as well as  related work on the challenging VOC database. "
Transductive Kernel Map Learning,"Transductive inference techniques are nowadays becoming standard  in machine learning due to their relative success in solving many real-world applications. Among them, kernel-based methods are particularly interesting but their success remains highly dependent on the choice of kernels. The latter are usually handcrafted or designed  in order to capture better similarity in training data. In this paper, we  introduce a novel transductive learning algorithmfor kernel design and classification. Our approach is basedon the minimization of an energy function mixing  i) a reconstructionterm that factorizes a matrix of  input data as a product of a learned dictionary  and a learned kernel map ii) a fidelity term thatensures consistent label predictions with those provided in a ground-truth and iii) a smoothness term which  guaranteessimilar labels for neighboring data and allows us to iteratively diffuse  kernel maps and labels from  labeled to unlabeled data. Solving this minimization problem makes it possible to learn both a decision criterion and a kernel map  that guarantee linearseparability in a high dimensional space and good generalizationperformance. Experiments conducted on object class segmentation, show improvements with respect to baseline as well as  related work on the challenging VOC database. "
Probabilistic Active Learning,"In the literature of active learning, most existing studies assume that the labels in the training dataset are deterministic, which is not quite realistic in many real-world applications. In many applications, however, the labels usually come in a probabilistic manner. Motivated by this observation, we propose a new frameworkwhere each label is enriched with a probability. Based on this framework, we propose an active learning algorithm. In addition, we show a theoretical bound on the label complexity of the proposed algorithm. Finally, we conducted comprehensive experiments in order to verify the effectiveness of our proposed algorithm."
Probabilistic Active Learning,"In the literature of active learning, most existing studies assume that the labels in the training dataset are deterministic, which is not quite realistic in many real-world applications. In many applications, however, the labels usually come in a probabilistic manner. Motivated by this observation, we propose a new frameworkwhere each label is enriched with a probability. Based on this framework, we propose an active learning algorithm. In addition, we show a theoretical bound on the label complexity of the proposed algorithm. Finally, we conducted comprehensive experiments in order to verify the effectiveness of our proposed algorithm."
Transductive Learning for Multi-Task Copula Processes,"We tackle the problem of multi-task learning with copula process. Multivariable prediction in spatial and spatial-temporal processes such as  natural resource estimation and pollution monitoring have been typically addressed using techniques based on Gaussian processes and co-Kriging. While the Gaussian prior assumption is convenient from analytical and computational perspectives, nature is dominated by non-Gaussian likelihoods. Copula processes are an elegant and flexible solution to handle various non-Gaussian likelihoods by capturing the dependence structure of random variables with cumulative distribution functions rather than their marginals.  We show how multi-task learning for copula processes can be used to improve multivariable prediction for problems where the simple Gaussianity prior assumption does not hold. Then, we present a transductive approximation for multi-task learning and derive analytical expressions for the copula process model. The approach is evaluated and compared to other techniques in one artificial dataset and two publicly available datasets for natural resource estimation and concrete slump prediction. "
Transductive Learning for Multi-Task Copula Processes,"We tackle the problem of multi-task learning with copula process. Multivariable prediction in spatial and spatial-temporal processes such as  natural resource estimation and pollution monitoring have been typically addressed using techniques based on Gaussian processes and co-Kriging. While the Gaussian prior assumption is convenient from analytical and computational perspectives, nature is dominated by non-Gaussian likelihoods. Copula processes are an elegant and flexible solution to handle various non-Gaussian likelihoods by capturing the dependence structure of random variables with cumulative distribution functions rather than their marginals.  We show how multi-task learning for copula processes can be used to improve multivariable prediction for problems where the simple Gaussianity prior assumption does not hold. Then, we present a transductive approximation for multi-task learning and derive analytical expressions for the copula process model. The approach is evaluated and compared to other techniques in one artificial dataset and two publicly available datasets for natural resource estimation and concrete slump prediction. "
EigenGP: KL-expansion based Gaussian process learning,"Gaussian processes (GPs) provide a nonparametric representation of functions. Given N training points, the exact GP inference incurs high computational cost. In this paper, we propose a sparse Gaussian process model, EigenGP, based on Karhunen-Lo`eve (KL) expansions of a GP prior. We use the Nystr?om approximation to obtain eigenfunctions of the covariance function and use an empirical Bayesian approach to select these eigenfunctions. To handle nonlinear likelihoods, we develop an efficient expectation propagation inference algorithm, and couple it with expectation maximization for evidence maximization. By selecting eigenfunctions of Gaussian kernels that are associated with data clusters, EigenGP is also suitable for semi-supervised learning. Our experimental results demonstrate improved predictive performance of EigenGP over alternative state-of-the-art sparse GP and semisupervised learning methods for regression, classification, and semisupervised classification."
Meta-Gaussian Information Bottleneck,"We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions witha Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuousdata and provides a solution more robust to outliers."
Meta-Gaussian Information Bottleneck,"We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions witha Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuousdata and provides a solution more robust to outliers."
A Scalable Approach to Column-Based Low-Rank Matrix Approximation,"In this paper we address the column-based low-rank matrix approximation problem in a novel parallel approach. Our approach is based on the divide-and-combine idea that it performs column selection on each submatrices in parallel, and then combines results from them. Like many existing methods, our method enjoys a relative-error bound by the theoretical analysis. Importantly, our method is scalable on large-scale matrix compared with the traditional methods. In addition, our method is deterministic in data partition and free from any matrix coherence assumption. Finally, the experiments on both simulated and real data shows that our approach is both efficient and effective. "
A Scalable Approach to Column-Based Low-Rank Matrix Approximation,"In this paper we address the column-based low-rank matrix approximation problem in a novel parallel approach. Our approach is based on the divide-and-combine idea that it performs column selection on each submatrices in parallel, and then combines results from them. Like many existing methods, our method enjoys a relative-error bound by the theoretical analysis. Importantly, our method is scalable on large-scale matrix compared with the traditional methods. In addition, our method is deterministic in data partition and free from any matrix coherence assumption. Finally, the experiments on both simulated and real data shows that our approach is both efficient and effective. "
A Scalable Approach to Column-Based Low-Rank Matrix Approximation,"In this paper we address the column-based low-rank matrix approximation problem in a novel parallel approach. Our approach is based on the divide-and-combine idea that it performs column selection on each submatrices in parallel, and then combines results from them. Like many existing methods, our method enjoys a relative-error bound by the theoretical analysis. Importantly, our method is scalable on large-scale matrix compared with the traditional methods. In addition, our method is deterministic in data partition and free from any matrix coherence assumption. Finally, the experiments on both simulated and real data shows that our approach is both efficient and effective. "
A Scalable Approach to Column-Based Low-Rank Matrix Approximation,"In this paper we address the column-based low-rank matrix approximation problem in a novel parallel approach. Our approach is based on the divide-and-combine idea that it performs column selection on each submatrices in parallel, and then combines results from them. Like many existing methods, our method enjoys a relative-error bound by the theoretical analysis. Importantly, our method is scalable on large-scale matrix compared with the traditional methods. In addition, our method is deterministic in data partition and free from any matrix coherence assumption. Finally, the experiments on both simulated and real data shows that our approach is both efficient and effective. "
A Scalable Approach to Column-Based Low-Rank Matrix Approximation,"In this paper we address the column-based low-rank matrix approximation problem in a novel parallel approach. Our approach is based on the divide-and-combine idea that it performs column selection on each submatrices in parallel, and then combines results from them. Like many existing methods, our method enjoys a relative-error bound by the theoretical analysis. Importantly, our method is scalable on large-scale matrix compared with the traditional methods. In addition, our method is deterministic in data partition and free from any matrix coherence assumption. Finally, the experiments on both simulated and real data shows that our approach is both efficient and effective. "
Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem,"Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have serious limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.We propose a simple framework for adaptive quality control in crowdsourced multiple-choice tasks. In it, we identify a novel problem we call the bandit survey problem. This framework is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations."
Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem,"Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have serious limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.We propose a simple framework for adaptive quality control in crowdsourced multiple-choice tasks. In it, we identify a novel problem we call the bandit survey problem. This framework is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations."
Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem,"Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have serious limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.We propose a simple framework for adaptive quality control in crowdsourced multiple-choice tasks. In it, we identify a novel problem we call the bandit survey problem. This framework is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations."
Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem,"Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have serious limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.We propose a simple framework for adaptive quality control in crowdsourced multiple-choice tasks. In it, we identify a novel problem we call the bandit survey problem. This framework is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations."
Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification,"This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification.  We show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation.  Applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification.  Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose five different criteria for adaptively acquiring nearest neighbors.  Experiments with both synthetic and real datasets demonstrate the effectivness of our classification criteria."
The Perturbed Variation,"We introduce a new discrepancy score between two distributions that gives an indication on their \emph{similarity}. While much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions. The new score gives an intuitive interpretation of similarity;  it optimally perturbs the distributions so that they best fit each other. The score is defined between distributions, and can be efficiently estimated from samples. We provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions. The statistical power of this procedures is presented in simulations. We also compare the score's capacity to detect similarity with that of other known measures on real data.  "
The Perturbed Variation,"We introduce a new discrepancy score between two distributions that gives an indication on their \emph{similarity}. While much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions. The new score gives an intuitive interpretation of similarity;  it optimally perturbs the distributions so that they best fit each other. The score is defined between distributions, and can be efficiently estimated from samples. We provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions. The statistical power of this procedures is presented in simulations. We also compare the score's capacity to detect similarity with that of other known measures on real data.  "
Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization,"We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\cite{kRegExpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice. "
Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization,"We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\cite{kRegExpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice. "
Developmental Stage Annotation of Drosophila Embryos using Transfer-Active Learning,"Drosophila melanogaster is the major model organism for explicating the function and interconnection of animal genes and for establishing a better understanding of human diseases. Today, images capturing gene expression have unprecedented spatial resolution, resulting in high quality maps (expression images) in model organisms. Based on morphological landmarks, the continuous process of Drosophila embryo genesis is traditionally divided into a series of consecutive stages (e.g., 1-16). The manual annotation of developmental stage of an image of an embryo is done by an expert. This manual annotation process is very expensive and time consuming. It is, therefore, tempting to design computational methods for the automated annotation of gene expression patterns. Images of geneexpression patterns vary between different image databases based on their imaging techniques and resolutions, leading to distribution difference across databases.Hence, it is a challenge to directly use an available annotated image database for building a classifier for a new database of images. In this paper, we propose a transfer and active learning based computational method to develop a classification system for annotating the gene expression patterns. Transfer learning or domain adaptation is performed on the labeled database and active sampling is performed on the new set of images.The proposed framework performs domain adaptation and active sampling simultaneously by  minimizing a common objective of reducing distribution difference between the domain adapted source, the queried samples and the rest of the unlabeled target domain data. Our empirical studies on synthetic and the Drosophila image databases demonstrate the potential of the proposed approach."
The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes,"Stochastic differential equations (SDE) are a natural tool for modelling systemsthat are inherently noisy or contain uncertainties that can be modelled as stochasticprocesses. Crucial to the process of using SDE to build mathematical modelsis the ability to estimate parameters of those models from observed data. Overthe past few decades, significant progress has been made on this problem, butwe are still far from having a definitive solution. We describe a novel methodof approximating a diffusion process that we show to be useful in Markov chainMonte-Carlo (MCMC) inference algorithms. We take the ?white? noise that drivesa diffusion process and decompose it into two terms. The first is a ?colourednoise? term that can be deterministically controlled by a set of auxilliary variables.The second term is small and enables us to form a linear Gaussian ?small noise?approximation. The decomposition allows us to take a diffusion process of interestand cast it in a form that is amenable to sampling by MCMC methods. We explainwhy many state-of-the-art inference methods fail on highly nonlinear inferenceproblems. We demonstrate experimentally that our method performs well in suchsituations. Our results show that this method is a promising new tool for use ininference and parameter estimation problems."
The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes,"Stochastic differential equations (SDE) are a natural tool for modelling systemsthat are inherently noisy or contain uncertainties that can be modelled as stochasticprocesses. Crucial to the process of using SDE to build mathematical modelsis the ability to estimate parameters of those models from observed data. Overthe past few decades, significant progress has been made on this problem, butwe are still far from having a definitive solution. We describe a novel methodof approximating a diffusion process that we show to be useful in Markov chainMonte-Carlo (MCMC) inference algorithms. We take the ?white? noise that drivesa diffusion process and decompose it into two terms. The first is a ?colourednoise? term that can be deterministically controlled by a set of auxilliary variables.The second term is small and enables us to form a linear Gaussian ?small noise?approximation. The decomposition allows us to take a diffusion process of interestand cast it in a form that is amenable to sampling by MCMC methods. We explainwhy many state-of-the-art inference methods fail on highly nonlinear inferenceproblems. We demonstrate experimentally that our method performs well in suchsituations. Our results show that this method is a promising new tool for use ininference and parameter estimation problems."
eXclusive Independent Component Analysis,"Independent Component Analysis (ICA) has been widely investigated for the both of the problems of blind source separation and efficient coding. In most of the relevant studies, the independent components of a signal are concerned. However, the exclusiveness of the components between two signals are rarely discussed. It is important to study the common basis set of two data sources as their complement basis sets. Thus, we introduce a method for finding the exclusive basis sets based on ICA, the eXclusive Independent Component Analysis (XICA). It aims to exclude the common features of two or more given signals from the the basis set in order to obtain more significant features of the given signals.In this paper, we investigated two image sets: nature images and urban images. The nature image set is composed by natural scene, and the urban image set contains artificial objects, such as buildings, cars, phones, etc. By using XICA, we will be able to see what is the essential difference between the nature and urban image sets in terms of their independent components."
Elite Opposition-based Particle Swarm Optimization,"Traditional particle swarm optimization (PSO) algorithm tends to suffer from premature convergence. We present a novel PSO in an attempt to solve this demerit based on a new elite opposition-based learning (EOBL) strategy. In EOBL, the elite particle, as a new concept, is introduced for highlighting the important role of particle?s best own experience. The aim of EOBL is to discover more useful information of elite particles by searching for their opposite solutions, which provide more chances of finding the global optimum in complex landscape of functions. This approach can be considered as an effective way to enhance the exploration ability of PSO. As another contribution of this paper, a new differential evolutionary mutation is presented to enhance the exploitation ability of PSO by searching for the neighbors of the global best particle, which is helpful to avoid the entire swarm being trapped into local optima. Experiments are conducted on a comprehensive set of benchmarks, the results compared with other nine state-of-the-art PSOs show that our proposed algorithm obtains promising performance."
Lazy Robust Derivative-Free Optimization for Hyperparameter Tuning,"Stochastic response surfaces pose a challenge to derivative-free optimization algorithmsthat rely on sampled gradients, empirical simplexes, or pointwise valuecomparisons. Achieving robustness can require averaging across many samples,which may be computationally expensive. This paper addresses both robustnessand efficiency in derivative optimization. It describes how the classic Nelder-Mead algorithm can be combined with statistical hypothesis testing to efficientlydeal with noisy evaluations that are commonplace when tuning machine learningalgorithm hyper-parameters. Employing lazy our lazy evaluation strategy leads toan efficient yet statistically sound algorithm, called Lazy Robust Nelder-Mead(LRNM). Empirical evaluation demonstrates the benefits of the approach overpopular alternatives."
Lazy Robust Derivative-Free Optimization for Hyperparameter Tuning,"Stochastic response surfaces pose a challenge to derivative-free optimization algorithmsthat rely on sampled gradients, empirical simplexes, or pointwise valuecomparisons. Achieving robustness can require averaging across many samples,which may be computationally expensive. This paper addresses both robustnessand efficiency in derivative optimization. It describes how the classic Nelder-Mead algorithm can be combined with statistical hypothesis testing to efficientlydeal with noisy evaluations that are commonplace when tuning machine learningalgorithm hyper-parameters. Employing lazy our lazy evaluation strategy leads toan efficient yet statistically sound algorithm, called Lazy Robust Nelder-Mead(LRNM). Empirical evaluation demonstrates the benefits of the approach overpopular alternatives."
Learning Low-rank Nonparametric Kernel Matrices From the Point of View of Matrix Completion,"Many existing nonparametric kernel learning methods suffer from high computational cost, which limits theirapplications to large scale real-world problems. In this paper, we propose a novel nonparametric kernel learning method based on the matrix completion technique, which emphasizes the low rank property of the learned kernel matrices. Given some pairwise constraints, we formulate the nonparametric kernel learning problem into a rank minimization problem with  constraints that enforce the learned similarities between the known data pairs equal the values of the known pairwise constraints. The resulting optimization problem can be relaxed to a convex optimization problem of minimizing nuclear norm with graph Laplacian regularization. We then develop a singular value thresholding like algorithm to solve the constrained convex problem using the similar techniques as in the matrix completion problems. Preliminary experimental results show that our algorithm performs better than or comparably to the existing best method BCDNPKL of [10] in terms of clustering accuracy and scalability."
Learning Low-rank Nonparametric Kernel Matrices From the Point of View of Matrix Completion,"Many existing nonparametric kernel learning methods suffer from high computational cost, which limits theirapplications to large scale real-world problems. In this paper, we propose a novel nonparametric kernel learning method based on the matrix completion technique, which emphasizes the low rank property of the learned kernel matrices. Given some pairwise constraints, we formulate the nonparametric kernel learning problem into a rank minimization problem with  constraints that enforce the learned similarities between the known data pairs equal the values of the known pairwise constraints. The resulting optimization problem can be relaxed to a convex optimization problem of minimizing nuclear norm with graph Laplacian regularization. We then develop a singular value thresholding like algorithm to solve the constrained convex problem using the similar techniques as in the matrix completion problems. Preliminary experimental results show that our algorithm performs better than or comparably to the existing best method BCDNPKL of [10] in terms of clustering accuracy and scalability."
Model checking For Improved Adaptive Behaviour,"Closed loop systems are traditionally analysed using simulation. We show how a formal approach, namely model checking, can be used to enhance and inform this analysis. Specifically, model checking can be used to verify properties for any execution of a system, not just a single experimental path. We describe how model checking has been used to investigate a system consisting of a robot navigating around an environment, avoiding obstacles using sequence learning. We illustrate the power of this approach by showing how a previous assumption about the system, gained though vigorous simulation, was demonstrated to be incorrect, using the formal approach."
Model checking For Improved Adaptive Behaviour,"Closed loop systems are traditionally analysed using simulation. We show how a formal approach, namely model checking, can be used to enhance and inform this analysis. Specifically, model checking can be used to verify properties for any execution of a system, not just a single experimental path. We describe how model checking has been used to investigate a system consisting of a robot navigating around an environment, avoiding obstacles using sequence learning. We illustrate the power of this approach by showing how a previous assumption about the system, gained though vigorous simulation, was demonstrated to be incorrect, using the formal approach."
Model checking For Improved Adaptive Behaviour,"Closed loop systems are traditionally analysed using simulation. We show how a formal approach, namely model checking, can be used to enhance and inform this analysis. Specifically, model checking can be used to verify properties for any execution of a system, not just a single experimental path. We describe how model checking has been used to investigate a system consisting of a robot navigating around an environment, avoiding obstacles using sequence learning. We illustrate the power of this approach by showing how a previous assumption about the system, gained though vigorous simulation, was demonstrated to be incorrect, using the formal approach."
Model checking For Improved Adaptive Behaviour,"Closed loop systems are traditionally analysed using simulation. We show how a formal approach, namely model checking, can be used to enhance and inform this analysis. Specifically, model checking can be used to verify properties for any execution of a system, not just a single experimental path. We describe how model checking has been used to investigate a system consisting of a robot navigating around an environment, avoiding obstacles using sequence learning. We illustrate the power of this approach by showing how a previous assumption about the system, gained though vigorous simulation, was demonstrated to be incorrect, using the formal approach."
Learning as MAP Inference in Discrete Graphical Models,"We present a new formulation for attacking binary classification problems. Instead of relying on convex losses and regularisers such as in SVMs, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but \emph{discrete} formulation, where estimation amounts to finding a MAP configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees. Due to the discrete nature of the formulation, it also allows for \emph{direct} regularisation through cardinality-based penalties, such as the $\ell_0$ pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner. We also outline a number of open problems arising from the formulation."
Learning as MAP Inference in Discrete Graphical Models,"We present a new formulation for attacking binary classification problems. Instead of relying on convex losses and regularisers such as in SVMs, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but \emph{discrete} formulation, where estimation amounts to finding a MAP configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees. Due to the discrete nature of the formulation, it also allows for \emph{direct} regularisation through cardinality-based penalties, such as the $\ell_0$ pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner. We also outline a number of open problems arising from the formulation."
Learning as MAP Inference in Discrete Graphical Models,"We present a new formulation for attacking binary classification problems. Instead of relying on convex losses and regularisers such as in SVMs, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but \emph{discrete} formulation, where estimation amounts to finding a MAP configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees. Due to the discrete nature of the formulation, it also allows for \emph{direct} regularisation through cardinality-based penalties, such as the $\ell_0$ pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner. We also outline a number of open problems arising from the formulation."
Parallel and Asynchronous Reinforcement Learning from Customer Interaction Sequences,"In this paper, we explore applications in which a company interacts with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur asynchronously and in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for reinforcement learning in this setting, using an asynchronous variant of temporal-difference learning to learn efficiently from partial interaction sequences. We applied our asynchronous TD algorithm to two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records. Our TD algorithm achieved good performance in both scenarios. It significantly outperformed comparable approaches that learn only from complete interaction sequences, as well as bandit-based approaches that do not exploit the sequential nature of the problem."
Parallel and Asynchronous Reinforcement Learning from Customer Interaction Sequences,"In this paper, we explore applications in which a company interacts with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur asynchronously and in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for reinforcement learning in this setting, using an asynchronous variant of temporal-difference learning to learn efficiently from partial interaction sequences. We applied our asynchronous TD algorithm to two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records. Our TD algorithm achieved good performance in both scenarios. It significantly outperformed comparable approaches that learn only from complete interaction sequences, as well as bandit-based approaches that do not exploit the sequential nature of the problem."
Parallel and Asynchronous Reinforcement Learning from Customer Interaction Sequences,"In this paper, we explore applications in which a company interacts with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur asynchronously and in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for reinforcement learning in this setting, using an asynchronous variant of temporal-difference learning to learn efficiently from partial interaction sequences. We applied our asynchronous TD algorithm to two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records. Our TD algorithm achieved good performance in both scenarios. It significantly outperformed comparable approaches that learn only from complete interaction sequences, as well as bandit-based approaches that do not exploit the sequential nature of the problem."
Parallel and Asynchronous Reinforcement Learning from Customer Interaction Sequences,"In this paper, we explore applications in which a company interacts with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur asynchronously and in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for reinforcement learning in this setting, using an asynchronous variant of temporal-difference learning to learn efficiently from partial interaction sequences. We applied our asynchronous TD algorithm to two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records. Our TD algorithm achieved good performance in both scenarios. It significantly outperformed comparable approaches that learn only from complete interaction sequences, as well as bandit-based approaches that do not exploit the sequential nature of the problem."
On a novel construction of the region-graph for the Generalized Belief Propagation,"Numerous inference problems in statistical physics, computer vision or error-correcting coding theory consist in approximating the marginal probability distributions on Markov Random Fields (MRF). The Belief Propagation (BP), an iterative message-passing algorithm, is an accurate solution that is optimal if the MRF is loopfree and supoptimal otherwise. The Low-Density Parity-Check codes used to protect sequences of bits sent through noisy channels, have a graphical representation, the Tanner graph, that is a particular MRF. This graph is a media for the BP algortithm that provides marginal probability distributions of the bits. Loops and combination thereof of the Tanner graph prevent the BP from being optimal, especially harmful topological structures called the trapping-sets. To circumvent this problem, the BP has been extended to the Generalized Belief Propagation (GBP), a message-passing algorithm that runs on a non unique mapping of the Tanner graph, namely the region-graph. Its nodes are gatherings of the Tanner graph nodes, that provides the possibility to absorb loops in a regiongraph, making the GBP more accurate than the BP. Relevant performance can be obtained by constructing the region-graph according to the topology of the Tanner graph. In this article, we expose a region graph construction suited to the Tanner code, an LDPC code whose Tanner graph is entirely covered by trapping-sets.Furthermore, we make use of classical and novel estimators to investigate the behavior of the GBP considered as a dynamic system in order to understand the way it evolves in terms of the Signal-to-Noise Ratio (SNR)."
On a novel construction of the region-graph for the Generalized Belief Propagation,"Numerous inference problems in statistical physics, computer vision or error-correcting coding theory consist in approximating the marginal probability distributions on Markov Random Fields (MRF). The Belief Propagation (BP), an iterative message-passing algorithm, is an accurate solution that is optimal if the MRF is loopfree and supoptimal otherwise. The Low-Density Parity-Check codes used to protect sequences of bits sent through noisy channels, have a graphical representation, the Tanner graph, that is a particular MRF. This graph is a media for the BP algortithm that provides marginal probability distributions of the bits. Loops and combination thereof of the Tanner graph prevent the BP from being optimal, especially harmful topological structures called the trapping-sets. To circumvent this problem, the BP has been extended to the Generalized Belief Propagation (GBP), a message-passing algorithm that runs on a non unique mapping of the Tanner graph, namely the region-graph. Its nodes are gatherings of the Tanner graph nodes, that provides the possibility to absorb loops in a regiongraph, making the GBP more accurate than the BP. Relevant performance can be obtained by constructing the region-graph according to the topology of the Tanner graph. In this article, we expose a region graph construction suited to the Tanner code, an LDPC code whose Tanner graph is entirely covered by trapping-sets.Furthermore, we make use of classical and novel estimators to investigate the behavior of the GBP considered as a dynamic system in order to understand the way it evolves in terms of the Signal-to-Noise Ratio (SNR)."
Active Learning with Hinted Support Vector Machine,"The abundance of real-world data and limited labeling budget calls for active learning, which is an important learning paradigm for reducing human labeling efforts. Many recently developed active learning algorithms consider both uncertainty and representativeness when making querying decisions. However, considering representativeness with uncertainty concurrently usually requires tackling other sophisticated and challenging learning tasks, such as clustering. In this paper, we propose a new active learning framework, called hinted sampling, which takes both uncertainty and representativeness into account in a simpler way. We design a novel active learning algorithm within the hinted sampling frameworkwith an extended support vector machine. Experimental results validate that the novel active learning algorithm can result in a better and more stable performance than that achieved by state-of-the-art algorithms."
Active Learning with Hinted Support Vector Machine,"The abundance of real-world data and limited labeling budget calls for active learning, which is an important learning paradigm for reducing human labeling efforts. Many recently developed active learning algorithms consider both uncertainty and representativeness when making querying decisions. However, considering representativeness with uncertainty concurrently usually requires tackling other sophisticated and challenging learning tasks, such as clustering. In this paper, we propose a new active learning framework, called hinted sampling, which takes both uncertainty and representativeness into account in a simpler way. We design a novel active learning algorithm within the hinted sampling frameworkwith an extended support vector machine. Experimental results validate that the novel active learning algorithm can result in a better and more stable performance than that achieved by state-of-the-art algorithms."
Active Learning with Hinted Support Vector Machine,"The abundance of real-world data and limited labeling budget calls for active learning, which is an important learning paradigm for reducing human labeling efforts. Many recently developed active learning algorithms consider both uncertainty and representativeness when making querying decisions. However, considering representativeness with uncertainty concurrently usually requires tackling other sophisticated and challenging learning tasks, such as clustering. In this paper, we propose a new active learning framework, called hinted sampling, which takes both uncertainty and representativeness into account in a simpler way. We design a novel active learning algorithm within the hinted sampling frameworkwith an extended support vector machine. Experimental results validate that the novel active learning algorithm can result in a better and more stable performance than that achieved by state-of-the-art algorithms."
A mechanistic model of early sensory processing based on subtracting sparse representations,"Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics."
A mechanistic model of early sensory processing based on subtracting sparse representations,"Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics."
A mechanistic model of early sensory processing based on subtracting sparse representations,"Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics."
Learning Canonical Correlations of Paired Tensor Sets via Tensor-to-Vector Projection,"Canonical correlation analysis (CCA) is a useful technique for measuring relationship between two sets of vectorial data. We propose a multilinear CCA (MCCA) method for paired tensorial data sets. Different from existing multilinear variations of CCA, MCCA extracts uncorrelated features under two architectures while maximizing paired correlations. One architecture imposes set-wise zero-correlation constraint while the other requires cross-correlation between different pairs to be zero. This is achieved through a pair of tensor-to-vector projections, estimated via a successive and iterative approach. We evaluate MCCA on matching facial images of different poses against competing solutions to show its superiority. We also study fusion of two architectures and observe small performance improvement, implying some complementary information captured."
Model Selection by Measuring Validated Information,"Which model best generalizes the available data?This study addresses the task of model selection.We introduce a general validation principle which is applicable to non-i.i.d. scenarios.Validated Model Information aims at selecting models whose predictions are optimally informative.The selection is performed by measuring quantifying predictive information, that is structure extracted from genuine regularities.This is in contrast to descriptive information, which includes accidental fluctuations due to noise.VMI exhibits stability with respect to resampling and Turing-universality.It is non-computable, yet effectively approximated by coding.Firstly, we evaluate the principle in a controlled setting with synthetic data.Then, we demonstrate its applicability with real biological data and external verification.Particular emphasis is given to the task of cluster model selection."
Model Selection by Measuring Validated Information,"Which model best generalizes the available data?This study addresses the task of model selection.We introduce a general validation principle which is applicable to non-i.i.d. scenarios.Validated Model Information aims at selecting models whose predictions are optimally informative.The selection is performed by measuring quantifying predictive information, that is structure extracted from genuine regularities.This is in contrast to descriptive information, which includes accidental fluctuations due to noise.VMI exhibits stability with respect to resampling and Turing-universality.It is non-computable, yet effectively approximated by coding.Firstly, we evaluate the principle in a controlled setting with synthetic data.Then, we demonstrate its applicability with real biological data and external verification.Particular emphasis is given to the task of cluster model selection."
Multi-Stage Multi-Task Feature Learning,"Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex regularization problem, which is usually suboptimal, due to its looseness for approximating an $\ell_0$-type regularizer. In this paper, we propose a non-convex regularized formulation for multi-task sparse feature learning; we propose to solve the non-convex optimization problem by employing a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms."
Latent Configuration Clustering for Discovering Action Exemplars in Video,"We introduce Latent Configuration Clustering (LCC) for the efficient clustering of data elements residing in a latent space. We are given only a distance function with which to measure relative proximity between pairs of points, but do not know the global positions of the samples. Thus, spatial partitioning techniques can not be applied. LCC generates a sparse affinity matrix in O(N log N) time without having to map the points to a Euclidean configuration. Once the affinity matrix is computed, standard clustering techniques can be applied effectively. Demonstrating the value of LCC, we discover human actions in video data by clustering subspaces spanned by video segments. The proposed method identifies exemplars from the clusters that can provide insight into the data set and be used in reducing nearest-neighbor classification to nearest-exemplar. Through unsupervised exemplar selection, we demonstrate accurate action classification using only a few exemplars from each cluster. Our nearest-exemplar unsupervised action classification demonstrates superior performance to existing unsupervised methods on a popular benchmark data set."
Latent Configuration Clustering for Discovering Action Exemplars in Video,"We introduce Latent Configuration Clustering (LCC) for the efficient clustering of data elements residing in a latent space. We are given only a distance function with which to measure relative proximity between pairs of points, but do not know the global positions of the samples. Thus, spatial partitioning techniques can not be applied. LCC generates a sparse affinity matrix in O(N log N) time without having to map the points to a Euclidean configuration. Once the affinity matrix is computed, standard clustering techniques can be applied effectively. Demonstrating the value of LCC, we discover human actions in video data by clustering subspaces spanned by video segments. The proposed method identifies exemplars from the clusters that can provide insight into the data set and be used in reducing nearest-neighbor classification to nearest-exemplar. Through unsupervised exemplar selection, we demonstrate accurate action classification using only a few exemplars from each cluster. Our nearest-exemplar unsupervised action classification demonstrates superior performance to existing unsupervised methods on a popular benchmark data set."
Detection of collective temporary stationary regions using multi-resolution time analysis,"Detecting temporal stationary regions in stochastic data has great value in signal forecasting applications. It's uses include early detection of earthquakes by analyzing seismic signals, predicting epileptic seizures by processing EEG signals, and in this research forecasting short term trends in the financial market. In this work we formulate and empirically examine a detection scheme, aimed to detect collective temporary stationary regions using a multi-resolution cross-correlation approach. We demonstrate how by applying a multi-resolution approach to collective holographic techniques taken from the world of band-limited EEG signal, processing can be used to detect temporal stationary regions in non-band-limited signals, specifically in intra-day financial signals, and make short-term trend predictions. A modification of the Functional Holographic time domain analysis [2] is performed using a multi-resolution approach in order to adapt to the non-band-limited signal characteristics. Using the multi-resolution collective holographic approach, stochastic NYSE financial signals are presented as nodes in multiple reduced correlation spaces, where the number of spaces is the number of resolutions that are used in the multi-resolution analysis. To simulate and validate the proposed detection scheme, a market indicator is formulated and tested over historical intra-day NYSE data. The financial theory which is the foundation of the proposed indicator is the existence of stealth traders that use trading strategies such as small market injection that inherit a temporal collective behavior on certain high liquid financial assets. The findings in this work verify the computational applicability of the time domain multi-resolution analysis of non-band-limited signals and also statistically model the time duration where the market exhibits temporal stationary properties."
Detection of collective temporary stationary regions using multi-resolution time analysis,"Detecting temporal stationary regions in stochastic data has great value in signal forecasting applications. It's uses include early detection of earthquakes by analyzing seismic signals, predicting epileptic seizures by processing EEG signals, and in this research forecasting short term trends in the financial market. In this work we formulate and empirically examine a detection scheme, aimed to detect collective temporary stationary regions using a multi-resolution cross-correlation approach. We demonstrate how by applying a multi-resolution approach to collective holographic techniques taken from the world of band-limited EEG signal, processing can be used to detect temporal stationary regions in non-band-limited signals, specifically in intra-day financial signals, and make short-term trend predictions. A modification of the Functional Holographic time domain analysis [2] is performed using a multi-resolution approach in order to adapt to the non-band-limited signal characteristics. Using the multi-resolution collective holographic approach, stochastic NYSE financial signals are presented as nodes in multiple reduced correlation spaces, where the number of spaces is the number of resolutions that are used in the multi-resolution analysis. To simulate and validate the proposed detection scheme, a market indicator is formulated and tested over historical intra-day NYSE data. The financial theory which is the foundation of the proposed indicator is the existence of stealth traders that use trading strategies such as small market injection that inherit a temporal collective behavior on certain high liquid financial assets. The findings in this work verify the computational applicability of the time domain multi-resolution analysis of non-band-limited signals and also statistically model the time duration where the market exhibits temporal stationary properties."
Recursive Gaussian Process Regression,"For large data sets, performing Gaussian process regression is computationally demanding or even intractable. If data can be processed sequentially, the recursive regression method proposed in this paper allows incorporating new data with constant computation time. For this purpose two operations are performed alternating on a fixed set of so-called basis vectors used for estimating the latent function: First, inference of the latent function at the new inputs. Second, utilize the new data for updating the estimate of the latent function at the basis vectors. By means of numerical simulations it is shown that the proposed approach significantly reduces the computation time compared to existing on-line and/or sparse Gaussian process regression approaches."
A Coarse-to-Fine Approach to Flexible Activity Discovery and Data Segmentation,"The growing number of mobile sensors allow collection of activity data at differentlevels of complexity and at fine-grained temporal resolution. However,accurately translating unlabeled sensor streams into meaningful activity classesremains non-trivial. The primitives that comprise an activity are usually foundin multiple activity classes; activities exhibit high-order temporal dependenciesamong primitives; and these higher-order dependencies vary from activity to activityand are blended together in the data. This paper presents a novel unsupervisedcoarse-to-fine activity discovery framework that handles the temporal heterogeneitypresent in sequences of sensor data and automatically segments activitieseven when the dependency order is not known and not fixed across activities.Our framework designs a Mixed Memory Latent Dirichlet Allocation (MM-LDA)model that iteratively discovers sequential transition patterns, segments the sensordata accordingly, and groups contiguous activity samples using a locality metric.We demonstrate the effectiveness of the approach by empirical experimentationon real-world datasets."
A Coarse-to-Fine Approach to Flexible Activity Discovery and Data Segmentation,"The growing number of mobile sensors allow collection of activity data at differentlevels of complexity and at fine-grained temporal resolution. However,accurately translating unlabeled sensor streams into meaningful activity classesremains non-trivial. The primitives that comprise an activity are usually foundin multiple activity classes; activities exhibit high-order temporal dependenciesamong primitives; and these higher-order dependencies vary from activity to activityand are blended together in the data. This paper presents a novel unsupervisedcoarse-to-fine activity discovery framework that handles the temporal heterogeneitypresent in sequences of sensor data and automatically segments activitieseven when the dependency order is not known and not fixed across activities.Our framework designs a Mixed Memory Latent Dirichlet Allocation (MM-LDA)model that iteratively discovers sequential transition patterns, segments the sensordata accordingly, and groups contiguous activity samples using a locality metric.We demonstrate the effectiveness of the approach by empirical experimentationon real-world datasets."
A Coarse-to-Fine Approach to Flexible Activity Discovery and Data Segmentation,"The growing number of mobile sensors allow collection of activity data at differentlevels of complexity and at fine-grained temporal resolution. However,accurately translating unlabeled sensor streams into meaningful activity classesremains non-trivial. The primitives that comprise an activity are usually foundin multiple activity classes; activities exhibit high-order temporal dependenciesamong primitives; and these higher-order dependencies vary from activity to activityand are blended together in the data. This paper presents a novel unsupervisedcoarse-to-fine activity discovery framework that handles the temporal heterogeneitypresent in sequences of sensor data and automatically segments activitieseven when the dependency order is not known and not fixed across activities.Our framework designs a Mixed Memory Latent Dirichlet Allocation (MM-LDA)model that iteratively discovers sequential transition patterns, segments the sensordata accordingly, and groups contiguous activity samples using a locality metric.We demonstrate the effectiveness of the approach by empirical experimentationon real-world datasets."
A Coarse-to-Fine Approach to Flexible Activity Discovery and Data Segmentation,"The growing number of mobile sensors allow collection of activity data at differentlevels of complexity and at fine-grained temporal resolution. However,accurately translating unlabeled sensor streams into meaningful activity classesremains non-trivial. The primitives that comprise an activity are usually foundin multiple activity classes; activities exhibit high-order temporal dependenciesamong primitives; and these higher-order dependencies vary from activity to activityand are blended together in the data. This paper presents a novel unsupervisedcoarse-to-fine activity discovery framework that handles the temporal heterogeneitypresent in sequences of sensor data and automatically segments activitieseven when the dependency order is not known and not fixed across activities.Our framework designs a Mixed Memory Latent Dirichlet Allocation (MM-LDA)model that iteratively discovers sequential transition patterns, segments the sensordata accordingly, and groups contiguous activity samples using a locality metric.We demonstrate the effectiveness of the approach by empirical experimentationon real-world datasets."
Towards utilization of neural correlates of loss of control for enhanced human-machine interaction,"Perceived loss of control during human-machine interaction (HMI) is a well-known problem that reduces the usability of a technical device dramatically. Recently, it was proven that changes in cognitive user state can be detected by a passive Brain-Computer Interface (BCI) in a laboratory setup. Potentially, a passive BCI can significantly improve a given HMI in shared control systems. Applying it in real world applications implies its own  constraints,  e.g. reducing the number of  electrodes and limiting the time needed for calibrating the system for the sake of improved usability. Here, we investigate neural correlates of loss of control with an independent component analysis on a set of 32 channel EEG-datasets recorded from 12 subjects. Even though the number of channels is rather low for ICA, we could identify the level of mental workload on the channel level as well as in the source space to be correlated to loss of control. This outcome forms a first basis for selection of a neuropsychologically meaningful and low-dimensional feature space, which should lead to improved reliability and reduced calibration time for a passive BCI detecting loss of control in real world applications."
Towards utilization of neural correlates of loss of control for enhanced human-machine interaction,"Perceived loss of control during human-machine interaction (HMI) is a well-known problem that reduces the usability of a technical device dramatically. Recently, it was proven that changes in cognitive user state can be detected by a passive Brain-Computer Interface (BCI) in a laboratory setup. Potentially, a passive BCI can significantly improve a given HMI in shared control systems. Applying it in real world applications implies its own  constraints,  e.g. reducing the number of  electrodes and limiting the time needed for calibrating the system for the sake of improved usability. Here, we investigate neural correlates of loss of control with an independent component analysis on a set of 32 channel EEG-datasets recorded from 12 subjects. Even though the number of channels is rather low for ICA, we could identify the level of mental workload on the channel level as well as in the source space to be correlated to loss of control. This outcome forms a first basis for selection of a neuropsychologically meaningful and low-dimensional feature space, which should lead to improved reliability and reduced calibration time for a passive BCI detecting loss of control in real world applications."
Semi-supervised Domain Adaptation on Manifolds,"In real-life problems the following semi-supervised domain adaptation scenario is often encountered: We have full access to some source data which is usually very large; The target data distribution is under certain unknown transformation of the source data distribution;Meanwhile only a small fraction of the target instances come with labels.The goal is to learn a prediction model by incorporating information from the source domain that is able to generalize well on the target test instances.We consider an explicit transformation function that maps examples from the source to the target domain, andwe argue that by proper preprocessing of the data from both source and target domains, the feasible transformation functions can be characterized by a set of rotation matrices.This naturally leads to an optimization formulation under the special orthogonal group constraints.We present an iterative coordinate descent solver that is able to jointly learn the transformation as well as the model parameters, while the geodesic update ensures the manifold constraints are always satisfied. Our framework is sufficiently general to work with a variety of loss functions and prediction problems. Empirical evaluations on synthetic and real-world experiments demonstrate the competitive performance of our method with respect to the state-of-the-art."
Semi-supervised Domain Adaptation on Manifolds,"In real-life problems the following semi-supervised domain adaptation scenario is often encountered: We have full access to some source data which is usually very large; The target data distribution is under certain unknown transformation of the source data distribution;Meanwhile only a small fraction of the target instances come with labels.The goal is to learn a prediction model by incorporating information from the source domain that is able to generalize well on the target test instances.We consider an explicit transformation function that maps examples from the source to the target domain, andwe argue that by proper preprocessing of the data from both source and target domains, the feasible transformation functions can be characterized by a set of rotation matrices.This naturally leads to an optimization formulation under the special orthogonal group constraints.We present an iterative coordinate descent solver that is able to jointly learn the transformation as well as the model parameters, while the geodesic update ensures the manifold constraints are always satisfied. Our framework is sufficiently general to work with a variety of loss functions and prediction problems. Empirical evaluations on synthetic and real-world experiments demonstrate the competitive performance of our method with respect to the state-of-the-art."
Moving Object Detection and Pixel-Level Localization From Semantic Priors and Topological Constraints,"We describe an approach to incorporate scene topology and semantics into pixel-level object detection and localization. Our method requires {\em video} to determine occlusion regions, and thence local depth ordering, and any visual recognition scheme that provides a score at local image regions, for instance detection probability.  We set up a cost functional that incorporates occlusion cues induced by moving objects, label consistency and recognition priors, and solve it using modern discrete optimization schemes. We show that our approach improves localization accuracy of existing recognition approaches, or equivalently provides recognition labels to pixel-level localization and segmentation."
Moving Object Detection and Pixel-Level Localization From Semantic Priors and Topological Constraints,"We describe an approach to incorporate scene topology and semantics into pixel-level object detection and localization. Our method requires {\em video} to determine occlusion regions, and thence local depth ordering, and any visual recognition scheme that provides a score at local image regions, for instance detection probability.  We set up a cost functional that incorporates occlusion cues induced by moving objects, label consistency and recognition priors, and solve it using modern discrete optimization schemes. We show that our approach improves localization accuracy of existing recognition approaches, or equivalently provides recognition labels to pixel-level localization and segmentation."
Moving Object Detection and Pixel-Level Localization From Semantic Priors and Topological Constraints,"We describe an approach to incorporate scene topology and semantics into pixel-level object detection and localization. Our method requires {\em video} to determine occlusion regions, and thence local depth ordering, and any visual recognition scheme that provides a score at local image regions, for instance detection probability.  We set up a cost functional that incorporates occlusion cues induced by moving objects, label consistency and recognition priors, and solve it using modern discrete optimization schemes. We show that our approach improves localization accuracy of existing recognition approaches, or equivalently provides recognition labels to pixel-level localization and segmentation."
Moving Object Detection and Pixel-Level Localization From Semantic Priors and Topological Constraints,"We describe an approach to incorporate scene topology and semantics into pixel-level object detection and localization. Our method requires {\em video} to determine occlusion regions, and thence local depth ordering, and any visual recognition scheme that provides a score at local image regions, for instance detection probability.  We set up a cost functional that incorporates occlusion cues induced by moving objects, label consistency and recognition priors, and solve it using modern discrete optimization schemes. We show that our approach improves localization accuracy of existing recognition approaches, or equivalently provides recognition labels to pixel-level localization and segmentation."
From Deformations to Parts: Motion-based Segmentation of 3D Objects,"We develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3D) poses.  We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.  To allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.  By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty.  Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods."
From Deformations to Parts: Motion-based Segmentation of 3D Objects,"We develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3D) poses.  We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.  To allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.  By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty.  Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods."
Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,"This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets."
Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,"This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets."
Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,"This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets."
Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,"This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets."
Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,"This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets."
Operator-valued kernel-based autoregressive models with application to biological network inference,"Reverse-modeling of gene regulatory network from time-series of gene expression still remains a challenging problem in computational systems biology. Works concerning network inference from temporal data usually rely on sparse linear models or Granger causality tools. A very few address the issue in the nonlinear case. In this work, we propose a nonparametric approach to dynamical system modeling that makes no assumption about the nature of the underlying nonlinear system. We introduce a new family of vector autoregressive models based on operator-valued kernels to identify the dynamical system and retrieve the target network. As in the linear case, a key issue is to control the model's sparsity. We propose an alternate minimization procedure to learn both the kernel and the basis vectors. We show very good results both in  estimation on DREAM benchmarks as well as on the IRMA datasets."
Operator-valued kernel-based autoregressive models with application to biological network inference,"Reverse-modeling of gene regulatory network from time-series of gene expression still remains a challenging problem in computational systems biology. Works concerning network inference from temporal data usually rely on sparse linear models or Granger causality tools. A very few address the issue in the nonlinear case. In this work, we propose a nonparametric approach to dynamical system modeling that makes no assumption about the nature of the underlying nonlinear system. We introduce a new family of vector autoregressive models based on operator-valued kernels to identify the dynamical system and retrieve the target network. As in the linear case, a key issue is to control the model's sparsity. We propose an alternate minimization procedure to learn both the kernel and the basis vectors. We show very good results both in  estimation on DREAM benchmarks as well as on the IRMA datasets."
What Can Pictures Tell Us About Web Pages? Improving Document Search using Images,"Traditional Web search engines do not utilize the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page. In this paper we study whether the {\em content} of the pictures appearing in a Web page can be used to enrich the semantic description of an HTML document and consequently boost the performance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using visual information extracted from the images contained in the pages. The reranker is query-independent and learned from labeled examples during an offline stage. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual information. We demonstrate our approach on the TREC 2009 Million Query Track, one of the premiere benchmarks in large-scale Web page retrieval. We show that the exploitation of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark. "
What Can Pictures Tell Us About Web Pages? Improving Document Search using Images,"Traditional Web search engines do not utilize the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page. In this paper we study whether the {\em content} of the pictures appearing in a Web page can be used to enrich the semantic description of an HTML document and consequently boost the performance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using visual information extracted from the images contained in the pages. The reranker is query-independent and learned from labeled examples during an offline stage. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual information. We demonstrate our approach on the TREC 2009 Million Query Track, one of the premiere benchmarks in large-scale Web page retrieval. We show that the exploitation of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark. "
What Can Pictures Tell Us About Web Pages? Improving Document Search using Images,"Traditional Web search engines do not utilize the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page. In this paper we study whether the {\em content} of the pictures appearing in a Web page can be used to enrich the semantic description of an HTML document and consequently boost the performance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using visual information extracted from the images contained in the pages. The reranker is query-independent and learned from labeled examples during an offline stage. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual information. We demonstrate our approach on the TREC 2009 Million Query Track, one of the premiere benchmarks in large-scale Web page retrieval. We show that the exploitation of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark. "
Structured Robust Subspace Learning: Efficient Recovery of Corrupted Low-rank Matrices,"In this paper, a Structured Robust Subspace Learning (SRSL) method is proposed for efficient low-rank recovery. Its main idea is a novel rank-minimization heuristic that imposes the group sparsity under orthonormal subspaces, which enables minimizing the rank by efficient sparse coding algorithms. Theoretical bounds of the group sparsity minimization under orthogonal subspace are given to validate this novel rank-minimization heuristic. In addition, a modified Nystrom method is introduced to further accelerate SRSL such that its sampling-based version (SRSL+) has linear complexity of the matrix size. Extensive experimental results demonstrate that SRSL and SRSL+ provide the state-of-the-art efficiency without compromising the recovery accuracy."
Structured Robust Subspace Learning: Efficient Recovery of Corrupted Low-rank Matrices,"In this paper, a Structured Robust Subspace Learning (SRSL) method is proposed for efficient low-rank recovery. Its main idea is a novel rank-minimization heuristic that imposes the group sparsity under orthonormal subspaces, which enables minimizing the rank by efficient sparse coding algorithms. Theoretical bounds of the group sparsity minimization under orthogonal subspace are given to validate this novel rank-minimization heuristic. In addition, a modified Nystrom method is introduced to further accelerate SRSL such that its sampling-based version (SRSL+) has linear complexity of the matrix size. Extensive experimental results demonstrate that SRSL and SRSL+ provide the state-of-the-art efficiency without compromising the recovery accuracy."
Structured Robust Subspace Learning: Efficient Recovery of Corrupted Low-rank Matrices,"In this paper, a Structured Robust Subspace Learning (SRSL) method is proposed for efficient low-rank recovery. Its main idea is a novel rank-minimization heuristic that imposes the group sparsity under orthonormal subspaces, which enables minimizing the rank by efficient sparse coding algorithms. Theoretical bounds of the group sparsity minimization under orthogonal subspace are given to validate this novel rank-minimization heuristic. In addition, a modified Nystrom method is introduced to further accelerate SRSL such that its sampling-based version (SRSL+) has linear complexity of the matrix size. Extensive experimental results demonstrate that SRSL and SRSL+ provide the state-of-the-art efficiency without compromising the recovery accuracy."
Convex Approximation to Mixture Models Using Step Functions,"The {\em parameter estimation} to  mixture models has been shown as a local optimal solution for decades. In this paper, we propose   a  {\em functional estimation} to  mixture models using step functions. We show that the proposed functional inference  yields  a  convex formulation and  consequently the  mixture models  are feasible for a global optimum inference with an asymptotic consistency. Furthermore, the simple gradient ascent method to optimize the convex formulation provides a theoretical explanation and clarification for  the heuristics of clustering by affinity propagation \cite{fdcpmbd07}. The proposed method opens a series of possibilities to achieve global optimal solutions to important  problems such as clustering, graph partitioning, and  finding information cascades in networks."
Convex Approximation to Mixture Models Using Step Functions,"The {\em parameter estimation} to  mixture models has been shown as a local optimal solution for decades. In this paper, we propose   a  {\em functional estimation} to  mixture models using step functions. We show that the proposed functional inference  yields  a  convex formulation and  consequently the  mixture models  are feasible for a global optimum inference with an asymptotic consistency. Furthermore, the simple gradient ascent method to optimize the convex formulation provides a theoretical explanation and clarification for  the heuristics of clustering by affinity propagation \cite{fdcpmbd07}. The proposed method opens a series of possibilities to achieve global optimal solutions to important  problems such as clustering, graph partitioning, and  finding information cascades in networks."
A Geometric take on Metric Learning,"Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way.We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structuregives us a principled way to perform dimensionality reduction and regression according to the learned metrics.Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Combined these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data."
A Geometric take on Metric Learning,"Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way.We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structuregives us a principled way to perform dimensionality reduction and regression according to the learned metrics.Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Combined these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data."
Low-rank Panoramas for Street View Videos,"In this paper, we address how to automatically generate panoramas for a street view from a long video sequence. We formulate the problem as one of robust recovery of a low-rank matrix from highly incomplete, corrupted, and deformed measurements (the video frames). We leverage powerful high-dimensional convex optimization tools from compressive sensing of sparse signals and low-rank matrices to solve this problem. In particular, we show how the new method can effectively remove severe occlusions or corruptions (caused by trees, cars, or reflections, etc.), and obtain street panoramas that have very clean global appearance and very accurate global geometry. We also show how our method can automatically and robustly establish pixel-wise accurate registration among all the video frames. We demonstrate the effectiveness of our method by conducting extensive experimental comparison with other state-of-the-art video stitching systems."
Low-rank Panoramas for Street View Videos,"In this paper, we address how to automatically generate panoramas for a street view from a long video sequence. We formulate the problem as one of robust recovery of a low-rank matrix from highly incomplete, corrupted, and deformed measurements (the video frames). We leverage powerful high-dimensional convex optimization tools from compressive sensing of sparse signals and low-rank matrices to solve this problem. In particular, we show how the new method can effectively remove severe occlusions or corruptions (caused by trees, cars, or reflections, etc.), and obtain street panoramas that have very clean global appearance and very accurate global geometry. We also show how our method can automatically and robustly establish pixel-wise accurate registration among all the video frames. We demonstrate the effectiveness of our method by conducting extensive experimental comparison with other state-of-the-art video stitching systems."
Low-rank Panoramas for Street View Videos,"In this paper, we address how to automatically generate panoramas for a street view from a long video sequence. We formulate the problem as one of robust recovery of a low-rank matrix from highly incomplete, corrupted, and deformed measurements (the video frames). We leverage powerful high-dimensional convex optimization tools from compressive sensing of sparse signals and low-rank matrices to solve this problem. In particular, we show how the new method can effectively remove severe occlusions or corruptions (caused by trees, cars, or reflections, etc.), and obtain street panoramas that have very clean global appearance and very accurate global geometry. We also show how our method can automatically and robustly establish pixel-wise accurate registration among all the video frames. We demonstrate the effectiveness of our method by conducting extensive experimental comparison with other state-of-the-art video stitching systems."
Novel Sparse Modeling by L2 + L0 Regularization,"We propose a novel sparse modeling method, the combination of L2 with L0 norms, to achieve feature selection while generating a well-predictive model at the same time.For many machine learning applications, feature selection is a crucial technique to construct the subset of features that is sufficient for prediction.We propose a novel sparse modeling framework, L0 elastic net.This framework encourages (1) reducing the redundancy of the predictive model and (2) searching for the most informative values and compact subset features.Furthermore, we propose the solver to search for the solution of L0 elastic net efficiently.As a theoretical analysis, we prove that the derived solution of L0 elastic net matches the minimal value of the L2-generalized objective function.Experimental results show that L0 elastic net is more suited to derive the compact predictive model than other previous regularization methods in a practical computational time."
Novel Sparse Modeling by L2 + L0 Regularization,"We propose a novel sparse modeling method, the combination of L2 with L0 norms, to achieve feature selection while generating a well-predictive model at the same time.For many machine learning applications, feature selection is a crucial technique to construct the subset of features that is sufficient for prediction.We propose a novel sparse modeling framework, L0 elastic net.This framework encourages (1) reducing the redundancy of the predictive model and (2) searching for the most informative values and compact subset features.Furthermore, we propose the solver to search for the solution of L0 elastic net efficiently.As a theoretical analysis, we prove that the derived solution of L0 elastic net matches the minimal value of the L2-generalized objective function.Experimental results show that L0 elastic net is more suited to derive the compact predictive model than other previous regularization methods in a practical computational time."
Novel Sparse Modeling by L2 + L0 Regularization,"We propose a novel sparse modeling method, the combination of L2 with L0 norms, to achieve feature selection while generating a well-predictive model at the same time.For many machine learning applications, feature selection is a crucial technique to construct the subset of features that is sufficient for prediction.We propose a novel sparse modeling framework, L0 elastic net.This framework encourages (1) reducing the redundancy of the predictive model and (2) searching for the most informative values and compact subset features.Furthermore, we propose the solver to search for the solution of L0 elastic net efficiently.As a theoretical analysis, we prove that the derived solution of L0 elastic net matches the minimal value of the L2-generalized objective function.Experimental results show that L0 elastic net is more suited to derive the compact predictive model than other previous regularization methods in a practical computational time."
Fast Hierarchical Topic Modeling via Nonnegative Matrix Factorization,"Hierarchical clustering is one of the cluster analysis tasks which aims at building a hierarchy of topics. Most state-of-the-art algorithms proposed in literature for hierarchical clustering are based on sampling. Their computational costs largely depend on the number of words in the given corpus, which is usually extremely. In this paper, we come up with an efficient hierarchical clustering method using non-negative matrix factorization (NMF), which is a dimension reduction technique that approximates a given matrix by a product of two low rank matrices. To maintain the hierarchy of topics, we design special constraints for low rank matrices, resulting in a novel optimization problem which we propose to solve using coordinate descent algorithms. Experiments on both artificial and real-world data sets demonstrate the effectiveness of our approach."
Low-Rank Modeling via Capped-Trace Norm,"The problem of low-rank modeling has recently received increasing attentions in machine learning. Most problems that directly tackle the rank function are known to be NP-hard and thus many heuristics such as those based on the trace norm have been proposed for low-rank modeling. The convex relaxation based on the trace norm admits a global solution and has theoretical guarantees under certain assumptions. However, the trace norm may not be a good approximation of the rank function in practice, resulting in estimation bias. In this paper, we consider low-rank modeling via the capped trace norm (CTRN) which provides a better approximation of the rank function than the trace norm. The basic idea of the CTRN is to perform thresholding on singular values and reduce the dominating impact of the top singular values. Although the capped $\ell_1$ norm has been well studied in the vector case, to our best knowledge the extension to the matrix case has not been studied in the literature. The practical challenge lies in the efficient optimization associated with the CTRN which is non-convex. We employ the difference-of-convex (DC) programming by decomposing the non-convex CTRN into the difference of two convex functions. By applying the concave-convex procedure, the problem can be iteratively computed via solving convex optimization problems. We present a block coordinate descend algorithm for general problems where the objective and/or the constraint can be a difference-of-convex function, and we present the convergence property of the algorithm. Our convergence proof is much simpler and requires weaker assumptions than existing work. We perform extensive experiments using both synthetic and real data. Our results show that the proposed algorithms outperform many popular heuristics for low-rank modeling, including the trace norm and the Schatten-$p$ norm."
Training Structured Output Predictors with Heterogeneous Data Sources,"Conventional learning formulations for structured output prediction such as SSVM assume the availability of training instances with known features and structured labels of each instance. In this paper we address the problem of training structured output predictors where in addition to normal labelled training instances, we have instances where, instead of the true label, we know a set of feasible labels where the true label comes from. We show how learning with such heterogeneous training sets can be formulated using specific loss functions in the latent SSVM formulation. To demonstrate our framework, we consider a special case of the semantic image segmentation problem where either full pixel labelling, or image level annotations (weak labelling) might be available and derive max-margin learning procedure for it. Experiments reveal that addition of weakly labelled data increases the performance of SSVM, especially when the number of fully labelled images is small."
Training Structured Output Predictors with Heterogeneous Data Sources,"Conventional learning formulations for structured output prediction such as SSVM assume the availability of training instances with known features and structured labels of each instance. In this paper we address the problem of training structured output predictors where in addition to normal labelled training instances, we have instances where, instead of the true label, we know a set of feasible labels where the true label comes from. We show how learning with such heterogeneous training sets can be formulated using specific loss functions in the latent SSVM formulation. To demonstrate our framework, we consider a special case of the semantic image segmentation problem where either full pixel labelling, or image level annotations (weak labelling) might be available and derive max-margin learning procedure for it. Experiments reveal that addition of weakly labelled data increases the performance of SSVM, especially when the number of fully labelled images is small."
Training Structured Output Predictors with Heterogeneous Data Sources,"Conventional learning formulations for structured output prediction such as SSVM assume the availability of training instances with known features and structured labels of each instance. In this paper we address the problem of training structured output predictors where in addition to normal labelled training instances, we have instances where, instead of the true label, we know a set of feasible labels where the true label comes from. We show how learning with such heterogeneous training sets can be formulated using specific loss functions in the latent SSVM formulation. To demonstrate our framework, we consider a special case of the semantic image segmentation problem where either full pixel labelling, or image level annotations (weak labelling) might be available and derive max-margin learning procedure for it. Experiments reveal that addition of weakly labelled data increases the performance of SSVM, especially when the number of fully labelled images is small."
Training Structured Output Predictors with Heterogeneous Data Sources,"Conventional learning formulations for structured output prediction such as SSVM assume the availability of training instances with known features and structured labels of each instance. In this paper we address the problem of training structured output predictors where in addition to normal labelled training instances, we have instances where, instead of the true label, we know a set of feasible labels where the true label comes from. We show how learning with such heterogeneous training sets can be formulated using specific loss functions in the latent SSVM formulation. To demonstrate our framework, we consider a special case of the semantic image segmentation problem where either full pixel labelling, or image level annotations (weak labelling) might be available and derive max-margin learning procedure for it. Experiments reveal that addition of weakly labelled data increases the performance of SSVM, especially when the number of fully labelled images is small."
Large-Scale Bandit Problems and KWIK Learning,"We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model  known as ``Knows What It Knows'' or KWIK learning. We give matching impossibility results showing that the KWIK-learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time."
Wavelet Best-Basis Representation and Reconstruction for Earthquakes Prediction,"Recently, the destructive potential of earthquakes near nuclear power plants has been observed. Early prediction of such earthquake could have reduced the disaster size significantly. This paper introduces a novel machine learning algorithm for early prediction of earthquakes from seismic data. Our proposed method detects changes in the stationarity of the seismic signal by wavelet packet decompositions and reconstructions combined with advanced machine learning techniques. The algorithm is demonstrated on data from the July 10th, 2011 earthquake in Japan. "
Wavelet Best-Basis Representation and Reconstruction for Earthquakes Prediction,"Recently, the destructive potential of earthquakes near nuclear power plants has been observed. Early prediction of such earthquake could have reduced the disaster size significantly. This paper introduces a novel machine learning algorithm for early prediction of earthquakes from seismic data. Our proposed method detects changes in the stationarity of the seismic signal by wavelet packet decompositions and reconstructions combined with advanced machine learning techniques. The algorithm is demonstrated on data from the July 10th, 2011 earthquake in Japan. "
Spike triggered covariance for strongly correlated Gaussian stimuli,"Characterizing feature selectivity is an important problem because it can shed light on how neurons process their inputs. The spike triggered covariance method (STCM) is a very commonly used method to extract the relevant set of stimulus features to which a neuron responds. One of the main advantages of STCM is that it can determine the dimensionality of the cell's relevant subspace. The method has been previously thought to be applicable when stimuli are drawn from a Gaussian ensemble, with or without stimulus correlations.  Here we use random matrix theory to show that when STCM is used with strongly correlated Gaussian stimuli, the null distribution of eigenvalues has a large outstanding mode. As a result, STCM can either yield an extra feature, which often corresponds to the strongest eigenvalue, or fail to yield any significant dimensions. We present a simple correction scheme that removes this artifact and illustrate its effectiveness by analyzing model neurons and recordings from retinal ganglion cells probed with correlated Gaussian stimuli whose second-order statistics was matched to natural stimuli. Our results can serve as guidelines for design of reverse correlation experiments that can help illuminate how neurons are optimized to code natural stimuli."
A Sparsity Nonnegative Matrix Factorization Technique for Correspondences Problems,"Graph matching is an essential problem in computer vision and pattern recognition. In this paper, we present a robust graph matching method based on nonnegative matrix factorization with sparsity constraints. We show that our sparsity NMF based solution is sparse and thus naturally imposes the discrete mapping constraints strongly in the optimization process. Promising experimental results on both synthetic point matching and real world image feature matching tasks show the effectiveness of our graph matching method."
A Sparsity Nonnegative Matrix Factorization Technique for Correspondences Problems,"Graph matching is an essential problem in computer vision and pattern recognition. In this paper, we present a robust graph matching method based on nonnegative matrix factorization with sparsity constraints. We show that our sparsity NMF based solution is sparse and thus naturally imposes the discrete mapping constraints strongly in the optimization process. Promising experimental results on both synthetic point matching and real world image feature matching tasks show the effectiveness of our graph matching method."
Big-Five Personality Prediction from User Behaviors at Social Network Sites,"A central problem in psychology is the study of personality, which uniquely characterizes a human being through a set of psychological traits. Traditionally, personality assessment is performed by means of manually filling up a self-report inventory. However, its reliability could be influenced by subjective bias including e.g. social desirability bias, and subjective perception may also be unreliable, due to participants limited cognitive ability.In this paper, we propose a computational and objective approach to predict the so called Big-Five personality of an individual from his/her Social Network Site(SNS) behaviors. This is the first such attempt ever to objectively measure the Big-Five Personality from SNS. Inspired by the Multi-Task Learning (MTL) methods from machine learning community, we develop a dedicated learning method to address this problem. Empirical results on SNS behavior dataset demonstrate its superior performance comparing to the state-of-the-art MTL methods. Interestingly, the results suggest that features involving recent behaviors such as recent blog publishing frequency and frequency of making comments are more likely to be related to personality."
Big-Five Personality Prediction from User Behaviors at Social Network Sites,"A central problem in psychology is the study of personality, which uniquely characterizes a human being through a set of psychological traits. Traditionally, personality assessment is performed by means of manually filling up a self-report inventory. However, its reliability could be influenced by subjective bias including e.g. social desirability bias, and subjective perception may also be unreliable, due to participants limited cognitive ability.In this paper, we propose a computational and objective approach to predict the so called Big-Five personality of an individual from his/her Social Network Site(SNS) behaviors. This is the first such attempt ever to objectively measure the Big-Five Personality from SNS. Inspired by the Multi-Task Learning (MTL) methods from machine learning community, we develop a dedicated learning method to address this problem. Empirical results on SNS behavior dataset demonstrate its superior performance comparing to the state-of-the-art MTL methods. Interestingly, the results suggest that features involving recent behaviors such as recent blog publishing frequency and frequency of making comments are more likely to be related to personality."
Big-Five Personality Prediction from User Behaviors at Social Network Sites,"A central problem in psychology is the study of personality, which uniquely characterizes a human being through a set of psychological traits. Traditionally, personality assessment is performed by means of manually filling up a self-report inventory. However, its reliability could be influenced by subjective bias including e.g. social desirability bias, and subjective perception may also be unreliable, due to participants limited cognitive ability.In this paper, we propose a computational and objective approach to predict the so called Big-Five personality of an individual from his/her Social Network Site(SNS) behaviors. This is the first such attempt ever to objectively measure the Big-Five Personality from SNS. Inspired by the Multi-Task Learning (MTL) methods from machine learning community, we develop a dedicated learning method to address this problem. Empirical results on SNS behavior dataset demonstrate its superior performance comparing to the state-of-the-art MTL methods. Interestingly, the results suggest that features involving recent behaviors such as recent blog publishing frequency and frequency of making comments are more likely to be related to personality."
Cooperating with a Markovian Ad Hoc Teammate,"This paper focuses on learning in the presence of a Markovian teammate in Ad hoc teams. A Markovian teammate's policy is a function of a set of discrete feature values derived from the joint history of interaction, where the feature values transition in a Markovian fashion on each time step. We introduce a novel algorithm ``Learning to Cooperate with a Markovian teammate'', or LCM, that converges to optimal cooperation with any Markovian teammate that satisfies certain assumptions, and achieves safety with any arbitrary teammate, in tractable sample complexity. The novel aspect of LCM is the manner in which it satisfies the above two goals via efficient exploration and exploitation. The main contribution of this paper is a full specification and a detailed analysis of LCM's theoretical properties. "
Pointwise Tracking the Optimal Regression Function,"This paper examines the possibility of a `reject option' in the context of least squares regression. It is shown that using rejection it is theoretically possible to learn `selective' regressors that can $\epsilon$-pointwise trackthe best regressor in hindsight from the same hypothesis class, while rejecting only a bounded portion of the domain.Moreover, the rejected volume vanishes with the training set size, under certain conditions. We then develop efficient and exact implementation of these selective regressors for the case of linear regression. Empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error."
Pointwise Tracking the Optimal Regression Function,"This paper examines the possibility of a `reject option' in the context of least squares regression. It is shown that using rejection it is theoretically possible to learn `selective' regressors that can $\epsilon$-pointwise trackthe best regressor in hindsight from the same hypothesis class, while rejecting only a bounded portion of the domain.Moreover, the rejected volume vanishes with the training set size, under certain conditions. We then develop efficient and exact implementation of these selective regressors for the case of linear regression. Empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error."
"Phase-Invariance, Sparsity and Image Category Discrimination","Approaches to image object recognition and categorization have tended to favour Gaussian scale space over Gabor-like representations. This is largely due to the success of descriptors based on Histograms of Gradients (HoG) to create efficient ndexing schemes. In this paper, we look at the alternative epresentation of complex-valued directional filters, widely used in face and texture recognition, and as models for peak rate of firing for phase-invariant, complex cells in primary visual area V1. We ask the question: can such Gabor-like models  be equipped with focus-of-attention operators and effective region descriptors, as for Gaussian scale-space ? We present some conclusions to this research: first, optimizing the tiling of Fourier space to encourage sparse coding turns out to be important to achieving good performance in interest-point localization and scale estimation. We suggest a simple spatial pooler, and find that a Winner-Take-All (WTA) approach to encourage sparse descriptors, gives better performance than without this step. We use   Area-Under-Curve (AUC) measures applied to Receiver Operator   Characteristic (ROC) curves of descriptor distances within and  between image classes. Results are assessed on standard image atabases used for categorization performance in computer vision. The significance of this work is that systems based on complex direction-selective filters can, with suitable adjustments, achieve the scalability of keypoint-based approaches, and potentially yield performance that is state-of-the-art.  This would potentially remove the need for parallel feature stacks in general-purpose vision systems designed to handle a wide variety of object classes and vision tasks."
"Phase-Invariance, Sparsity and Image Category Discrimination","Approaches to image object recognition and categorization have tended to favour Gaussian scale space over Gabor-like representations. This is largely due to the success of descriptors based on Histograms of Gradients (HoG) to create efficient ndexing schemes. In this paper, we look at the alternative epresentation of complex-valued directional filters, widely used in face and texture recognition, and as models for peak rate of firing for phase-invariant, complex cells in primary visual area V1. We ask the question: can such Gabor-like models  be equipped with focus-of-attention operators and effective region descriptors, as for Gaussian scale-space ? We present some conclusions to this research: first, optimizing the tiling of Fourier space to encourage sparse coding turns out to be important to achieving good performance in interest-point localization and scale estimation. We suggest a simple spatial pooler, and find that a Winner-Take-All (WTA) approach to encourage sparse descriptors, gives better performance than without this step. We use   Area-Under-Curve (AUC) measures applied to Receiver Operator   Characteristic (ROC) curves of descriptor distances within and  between image classes. Results are assessed on standard image atabases used for categorization performance in computer vision. The significance of this work is that systems based on complex direction-selective filters can, with suitable adjustments, achieve the scalability of keypoint-based approaches, and potentially yield performance that is state-of-the-art.  This would potentially remove the need for parallel feature stacks in general-purpose vision systems designed to handle a wide variety of object classes and vision tasks."
Reconstructing ecological networks with hierarchical Bayesian regression and Mondrian processes,"Ecological systems consist of complex sets of interactions among species and their environment, the understanding of which has implications for predicting environmental response to perturbations such as invading species and climate change.  However, the revelation of these interactions is not straightforward, nor are the interactions necessarily stable across space. Machine learning can enable the recovery of such complex, spatially varying interactions from relatively easily obtained species abundance data. Here, we describe a novel Bayesian regression and Mondrian process model (BRAMP) for reconstructing species interaction networks from observed field data. BRAMP enables robust inference of species interactions considering autocorrelation in species abundances and allowing for variation in the interactions across space. We evaluate the model on spatially explicit simulated data, produced using a trophic niche model combined with stochastic population dynamics. We compare the model's performance against L1-penalized sparse regression (LASSO) and non-linear Bayesian networks with the BDe scoring scheme. Finally, we apply BRAMP to real ecological data."
Reconstructing ecological networks with hierarchical Bayesian regression and Mondrian processes,"Ecological systems consist of complex sets of interactions among species and their environment, the understanding of which has implications for predicting environmental response to perturbations such as invading species and climate change.  However, the revelation of these interactions is not straightforward, nor are the interactions necessarily stable across space. Machine learning can enable the recovery of such complex, spatially varying interactions from relatively easily obtained species abundance data. Here, we describe a novel Bayesian regression and Mondrian process model (BRAMP) for reconstructing species interaction networks from observed field data. BRAMP enables robust inference of species interactions considering autocorrelation in species abundances and allowing for variation in the interactions across space. We evaluate the model on spatially explicit simulated data, produced using a trophic niche model combined with stochastic population dynamics. We compare the model's performance against L1-penalized sparse regression (LASSO) and non-linear Bayesian networks with the BDe scoring scheme. Finally, we apply BRAMP to real ecological data."
Reconstructing ecological networks with hierarchical Bayesian regression and Mondrian processes,"Ecological systems consist of complex sets of interactions among species and their environment, the understanding of which has implications for predicting environmental response to perturbations such as invading species and climate change.  However, the revelation of these interactions is not straightforward, nor are the interactions necessarily stable across space. Machine learning can enable the recovery of such complex, spatially varying interactions from relatively easily obtained species abundance data. Here, we describe a novel Bayesian regression and Mondrian process model (BRAMP) for reconstructing species interaction networks from observed field data. BRAMP enables robust inference of species interactions considering autocorrelation in species abundances and allowing for variation in the interactions across space. We evaluate the model on spatially explicit simulated data, produced using a trophic niche model combined with stochastic population dynamics. We compare the model's performance against L1-penalized sparse regression (LASSO) and non-linear Bayesian networks with the BDe scoring scheme. Finally, we apply BRAMP to real ecological data."
Bayesian nonparametric models for bipartite graphs,"We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, an Indian Buffet-like generative process for network growth, and a simple and efficient Gibbs sampler for posterior simulation. Our model is shown to be well fitted to several real-world social networks."
Reducing statistical time-series problems to  binary classification,"We  show how binary classification methods developed to work on i.i.d.\ data can be used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solvging these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data."
Reducing statistical time-series problems to  binary classification,"We  show how binary classification methods developed to work on i.i.d.\ data can be used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solvging these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data."
Learning a manipulation task of cascaded blocks with passive joints from inexpert demonstrations using sample based stochastic policy,"Reinforcement learning (RL) methods have been successfully applied tothe learning of the robot controland becomes to allow a robot to learn various kinds of realistic tasksby designing the policy functionwith a prior knowledge about the control task.We present a novel representation of a stochastic policybased on a non-parametric model to obtain the knowledge directly from the demonstrations by human teacher.In this method, the action is selected from stored samples,and the importance of each sample is trainedby an RL method as a policy parameter.Since samples are generated from demonstrations,our method is expected to allow the robot to extractuseful knowledge included in human instructions.We applied our method to the manipulation task of a cascaded rigid linksand experimental results show that a good controller can be obtainedby our method."
Learning a manipulation task of cascaded blocks with passive joints from inexpert demonstrations using sample based stochastic policy,"Reinforcement learning (RL) methods have been successfully applied tothe learning of the robot controland becomes to allow a robot to learn various kinds of realistic tasksby designing the policy functionwith a prior knowledge about the control task.We present a novel representation of a stochastic policybased on a non-parametric model to obtain the knowledge directly from the demonstrations by human teacher.In this method, the action is selected from stored samples,and the importance of each sample is trainedby an RL method as a policy parameter.Since samples are generated from demonstrations,our method is expected to allow the robot to extractuseful knowledge included in human instructions.We applied our method to the manipulation task of a cascaded rigid linksand experimental results show that a good controller can be obtainedby our method."
Learning a manipulation task of cascaded blocks with passive joints from inexpert demonstrations using sample based stochastic policy,"Reinforcement learning (RL) methods have been successfully applied tothe learning of the robot controland becomes to allow a robot to learn various kinds of realistic tasksby designing the policy functionwith a prior knowledge about the control task.We present a novel representation of a stochastic policybased on a non-parametric model to obtain the knowledge directly from the demonstrations by human teacher.In this method, the action is selected from stored samples,and the importance of each sample is trainedby an RL method as a policy parameter.Since samples are generated from demonstrations,our method is expected to allow the robot to extractuseful knowledge included in human instructions.We applied our method to the manipulation task of a cascaded rigid linksand experimental results show that a good controller can be obtainedby our method."
Learning a manipulation task of cascaded blocks with passive joints from inexpert demonstrations using sample based stochastic policy,"Reinforcement learning (RL) methods have been successfully applied tothe learning of the robot controland becomes to allow a robot to learn various kinds of realistic tasksby designing the policy functionwith a prior knowledge about the control task.We present a novel representation of a stochastic policybased on a non-parametric model to obtain the knowledge directly from the demonstrations by human teacher.In this method, the action is selected from stored samples,and the importance of each sample is trainedby an RL method as a policy parameter.Since samples are generated from demonstrations,our method is expected to allow the robot to extractuseful knowledge included in human instructions.We applied our method to the manipulation task of a cascaded rigid linksand experimental results show that a good controller can be obtainedby our method."
Learning a manipulation task of cascaded blocks with passive joints from inexpert demonstrations using sample based stochastic policy,"Reinforcement learning (RL) methods have been successfully applied tothe learning of the robot controland becomes to allow a robot to learn various kinds of realistic tasksby designing the policy functionwith a prior knowledge about the control task.We present a novel representation of a stochastic policybased on a non-parametric model to obtain the knowledge directly from the demonstrations by human teacher.In this method, the action is selected from stored samples,and the importance of each sample is trainedby an RL method as a policy parameter.Since samples are generated from demonstrations,our method is expected to allow the robot to extractuseful knowledge included in human instructions.We applied our method to the manipulation task of a cascaded rigid linksand experimental results show that a good controller can be obtainedby our method."
Estimating noise correlations under signal drift,"Large data in biological and information sciences often show nonstationary trends andare beyond the scope of the conventional methods under assumption of stationary.Especially, if infinitely many possible trends can occur unpredictably, it is difficult to tackle them with a single algorithm without previous knowledge.However, it is possible to estimate interesting statistical parameters from the data with unpredictable drifts for some semiparametric statistical models.In this paper, we consider a semiparametric, mixture of Gaussian models where the trend distribution is not restricted at all.We propose estimators of noise correlations for multivariate time series with brain signals in mind and demonstrate thatit works robustly against any unpredictable drift in signals (means) while the conventional correlogram shows spurious correlations contaminated by the temporal drift."
Classification Calibration Dimension for General Multiclass Losses,"We study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of \emph{classification calibration dimension} of a multiclass loss matrix, which measures the smallest `size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al.\ (2010) for analyzing the difficulty of designing `low-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems."
Classification Calibration Dimension for General Multiclass Losses,"We study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of \emph{classification calibration dimension} of a multiclass loss matrix, which measures the smallest `size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al.\ (2010) for analyzing the difficulty of designing `low-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems."
Online Information-Geometric Change Detection with Exponential Families,This paper studies online change detection with exponential families. We formulate a generic statistical framework for sequential abrupt change detection and introduce generalized likelihood ratio test statistics with arbitrary estimators. We show intrinsic links of these statistics with maximum likelihood estimates and interpret this within the context of information geometry. It provides a unifying view of change detection for many common statistical models and corresponding distances. We also derive a computationally efficient scheme for change detection based on exact generalized likelihood ratios with maximum likelihood estimators. This scheme is applied to synthetic and real-world datasets of various natures.
Online Information-Geometric Change Detection with Exponential Families,This paper studies online change detection with exponential families. We formulate a generic statistical framework for sequential abrupt change detection and introduce generalized likelihood ratio test statistics with arbitrary estimators. We show intrinsic links of these statistics with maximum likelihood estimates and interpret this within the context of information geometry. It provides a unifying view of change detection for many common statistical models and corresponding distances. We also derive a computationally efficient scheme for change detection based on exact generalized likelihood ratios with maximum likelihood estimators. This scheme is applied to synthetic and real-world datasets of various natures.
Collaborative Gaussian Processes for Preference Learning,"We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms."
Efficient 3D Kernel Estimation for Non-uniform Camera Shake Removal,"Non-uniform camera shake removal is a knotty problem which plagues the researchers due to the huge computational cost of high-dimensional blur kernel estimation. To address this problem, we propose to estimate a 3D blur kernel from its 2D projections, which are computed from image patches, by solving a linear equation system. Under this scheme, we propose a perpendicular acquisition system to increase the projection variance of 3D kernel and thus decrease ill-posedness. Correspondingly, the 3D kernel estimator is obtained by efficient intersection operation. Finally, a RANSAC-based framework is developed to raise the robustness to estimation error of 2D local blur kernels. In experiments, we test our algorithm on both synthetic and real captured data, and both results show that results validate the effectiveness and efficiency of our approach."
Efficient 3D Kernel Estimation for Non-uniform Camera Shake Removal,"Non-uniform camera shake removal is a knotty problem which plagues the researchers due to the huge computational cost of high-dimensional blur kernel estimation. To address this problem, we propose to estimate a 3D blur kernel from its 2D projections, which are computed from image patches, by solving a linear equation system. Under this scheme, we propose a perpendicular acquisition system to increase the projection variance of 3D kernel and thus decrease ill-posedness. Correspondingly, the 3D kernel estimator is obtained by efficient intersection operation. Finally, a RANSAC-based framework is developed to raise the robustness to estimation error of 2D local blur kernels. In experiments, we test our algorithm on both synthetic and real captured data, and both results show that results validate the effectiveness and efficiency of our approach."
Conditional gradient algorithms for regularized learning,"We consider the problem of optimizing learning objectives with a regularization penalty in high-dimensional settings. For several important learning problems, state-of-the-art optimization approaches such as proximal gradient algorithms are difficult to apply and do not scale up to large datasets. We propose new conditional-type algorithms, with theoretical guarantees, resp. for norm-minimization and penalized learning problems. Promising experimental results are presented on two large-scale real-world datasets. "
Conditional gradient algorithms for regularized learning,"We consider the problem of optimizing learning objectives with a regularization penalty in high-dimensional settings. For several important learning problems, state-of-the-art optimization approaches such as proximal gradient algorithms are difficult to apply and do not scale up to large datasets. We propose new conditional-type algorithms, with theoretical guarantees, resp. for norm-minimization and penalized learning problems. Promising experimental results are presented on two large-scale real-world datasets. "
Approximating Concavely Parameterized Optimization Problems,"We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\varepsilon >0$ by a set of size $O(1/\sqrt{\varepsilon})$. A lower bound of size $\Omega (1/\sqrt{\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\sqrt{\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion."
Approximating Concavely Parameterized Optimization Problems,"We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\varepsilon >0$ by a set of size $O(1/\sqrt{\varepsilon})$. A lower bound of size $\Omega (1/\sqrt{\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\sqrt{\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion."
Approximating Concavely Parameterized Optimization Problems,"We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\varepsilon >0$ by a set of size $O(1/\sqrt{\varepsilon})$. A lower bound of size $\Omega (1/\sqrt{\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\sqrt{\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion."
Approximating Concavely Parameterized Optimization Problems,"We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\varepsilon >0$ by a set of size $O(1/\sqrt{\varepsilon})$. A lower bound of size $\Omega (1/\sqrt{\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\sqrt{\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion."
Lasso Screening Rules via Dual Polytope Projection,"Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. Bytransforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose a fast and efficient screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. We have evaluated our screening rule using both synthetic and real data sets. Results show that our rule is more effective to identifyinactive predictors than existing state-of-art screening rules. We also extend our screening rule to identify inactive groups in group Lasso."
Gradient-based kernel method for feature extraction and variable selection,"We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter finds a subset of predictors. First, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method.  In comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets.  Second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by Chen et al. (2010).  Experimental results show that the proposed methods successfully find effective features and variables without parametric models."
MEAN-FIELD ANNEALING BASED COMMITTEE MACHINES  Estimation of Weather-Related Outages in Power Distribution Systems,"The reliability of electrical delivery is an important concern for utility companies. Weather related outages have a significant impact on it. There are many regression based models to estimate outages from weather factors in overhead distribution system. This paper proposes the use of committee machines composed of multiple neural networks to estimate outages. A major challenge for using a committee machine is to properly combine predictions from multiple networks, since the performance of individual networks is input dependent due to mapping misrepresentation. This paper presents a new method in which the individual network predictions are combined dynamically. The error minimization is performed using the mean field annealing theory. Results obtained for the study area in Kansas are compared with observed outages to evaluate the performance of the model for estimating these outages. The results are also compared with previously studied regression and neural network models to determine an appropriate model to represent effects of wind and lightning on outages."
MEAN-FIELD ANNEALING BASED COMMITTEE MACHINES  Estimation of Weather-Related Outages in Power Distribution Systems,"The reliability of electrical delivery is an important concern for utility companies. Weather related outages have a significant impact on it. There are many regression based models to estimate outages from weather factors in overhead distribution system. This paper proposes the use of committee machines composed of multiple neural networks to estimate outages. A major challenge for using a committee machine is to properly combine predictions from multiple networks, since the performance of individual networks is input dependent due to mapping misrepresentation. This paper presents a new method in which the individual network predictions are combined dynamically. The error minimization is performed using the mean field annealing theory. Results obtained for the study area in Kansas are compared with observed outages to evaluate the performance of the model for estimating these outages. The results are also compared with previously studied regression and neural network models to determine an appropriate model to represent effects of wind and lightning on outages."
MEAN-FIELD ANNEALING BASED COMMITTEE MACHINES  Estimation of Weather-Related Outages in Power Distribution Systems,"The reliability of electrical delivery is an important concern for utility companies. Weather related outages have a significant impact on it. There are many regression based models to estimate outages from weather factors in overhead distribution system. This paper proposes the use of committee machines composed of multiple neural networks to estimate outages. A major challenge for using a committee machine is to properly combine predictions from multiple networks, since the performance of individual networks is input dependent due to mapping misrepresentation. This paper presents a new method in which the individual network predictions are combined dynamically. The error minimization is performed using the mean field annealing theory. Results obtained for the study area in Kansas are compared with observed outages to evaluate the performance of the model for estimating these outages. The results are also compared with previously studied regression and neural network models to determine an appropriate model to represent effects of wind and lightning on outages."
Structuring Relevant Feature Sets with Multiple Model Learning,"Feature selection is one of the most prominent learning tasks, especially in high-dimensional datasets in which the goal is to understand the mechanisms that underly the learning dataset. However most of them typically deliver just a flat set of relevant features and provide no further information on what kind of structures, e.g.feature groupings, might underly the set of relevant features. In this paper we propose a new learning paradigmin which our goal is to uncover the structures that underly the set of relevant features for a given learning problem.We uncover two types of features sets, non-replaceable features that contain important information about the targetvariable and cannot be replaced by other features, and functionally similar features sets that can be used interchangeably in learned models, given the presence of the non-replaceable features, with no change in the predictive performance. To do so we propose a new learning algorithm that learns a number of disjoint models using a model disjointness regularizationconstraint together with a constraint on the predictive agreement of the disjoint models. We explore the behavior of ourapproach on a number of high-dimensional datasets, and show that, as expected by their construction, these satisfy a number of properties. Namely, model disjointness, a high predictive agreement, and a similar predictive performance to models learned on the full set of relevant features. The ability to structure the set of relevant features in such a manner can become a valuable tool in different applications of scientific knowledge discovery."
Structuring Relevant Feature Sets with Multiple Model Learning,"Feature selection is one of the most prominent learning tasks, especially in high-dimensional datasets in which the goal is to understand the mechanisms that underly the learning dataset. However most of them typically deliver just a flat set of relevant features and provide no further information on what kind of structures, e.g.feature groupings, might underly the set of relevant features. In this paper we propose a new learning paradigmin which our goal is to uncover the structures that underly the set of relevant features for a given learning problem.We uncover two types of features sets, non-replaceable features that contain important information about the targetvariable and cannot be replaced by other features, and functionally similar features sets that can be used interchangeably in learned models, given the presence of the non-replaceable features, with no change in the predictive performance. To do so we propose a new learning algorithm that learns a number of disjoint models using a model disjointness regularizationconstraint together with a constraint on the predictive agreement of the disjoint models. We explore the behavior of ourapproach on a number of high-dimensional datasets, and show that, as expected by their construction, these satisfy a number of properties. Namely, model disjointness, a high predictive agreement, and a similar predictive performance to models learned on the full set of relevant features. The ability to structure the set of relevant features in such a manner can become a valuable tool in different applications of scientific knowledge discovery."
Topographic Analysis of Correlated Components,"Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants area assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the data where the components can have linear or higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations that are sensitive to linear or higher order correlations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data."
Topographic Analysis of Correlated Components,"Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants area assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the data where the components can have linear or higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations that are sensitive to linear or higher order correlations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data."
Topographic Analysis of Correlated Components,"Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants area assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the data where the components can have linear or higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations that are sensitive to linear or higher order correlations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data."
Topographic Analysis of Correlated Components,"Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants area assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the data where the components can have linear or higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations that are sensitive to linear or higher order correlations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data."
Exact inference in flipper graphs and their use for approximate inference using dual decomposition,"We show exact inference is possible for a subclass of binary labeled graphs that we call flipper graphs. These are graphs where the submodular and non-submodular edges follow a specific pattern-- essentially that an isomorphism exists to a fully submodular graph. Examples of flipper graphs include tree-structured graphs, submodular graphs, and bipartite graphs with all non-submodular edges. We investigate the use of flipper graphs in dual decomposition, based on two different decomposition strategies. Experimentally, we find that decomposition using flipper graphs outperform traditional tree-based dual decomposition."
Toward automated semantic leaf categorization by image analysis,"Understanding the diversity of the plant community is a crucial issue for the development of many botanical industries as well as for the conservation of the ecosystem biodiversity. Traditionally, botanists have proposed detailed key descriptions (generally qualitative) or concepts about the morphology of plants and particularly of leaves that allow them to construct relationships between different plants and between them and their species. However, extracting these concepts is complicated, painstaking and can only be carried out by experts. One way to accelerate and broaden the use of these key descriptions is to automatically extract them directly from images. In this paper, we focus mainly on one of the most commonly used key leaf descriptions which is the foliage arrangement (simple, compound, pinnate, palmate). To do so, we analyse the spatial distribution of the leaf contour points and mainly its maxima (concave and convex) and inflexion points. For each category, we define a particular geometric feature that describes its point distribution. We test the proposed method on real world leaf images (Pl@ntLeaves scans). The experiments have demonstrated the robustness of our algorithm for a high number of leaf species (70 species) and even in the presence of some distortions (such as rotation, some leaf damage, partial leaf overlapping). In addition to its accuracy, the proposed approach satisfies real-time requirements with a low computational cost."
Toward automated semantic leaf categorization by image analysis,"Understanding the diversity of the plant community is a crucial issue for the development of many botanical industries as well as for the conservation of the ecosystem biodiversity. Traditionally, botanists have proposed detailed key descriptions (generally qualitative) or concepts about the morphology of plants and particularly of leaves that allow them to construct relationships between different plants and between them and their species. However, extracting these concepts is complicated, painstaking and can only be carried out by experts. One way to accelerate and broaden the use of these key descriptions is to automatically extract them directly from images. In this paper, we focus mainly on one of the most commonly used key leaf descriptions which is the foliage arrangement (simple, compound, pinnate, palmate). To do so, we analyse the spatial distribution of the leaf contour points and mainly its maxima (concave and convex) and inflexion points. For each category, we define a particular geometric feature that describes its point distribution. We test the proposed method on real world leaf images (Pl@ntLeaves scans). The experiments have demonstrated the robustness of our algorithm for a high number of leaf species (70 species) and even in the presence of some distortions (such as rotation, some leaf damage, partial leaf overlapping). In addition to its accuracy, the proposed approach satisfies real-time requirements with a low computational cost."
Toward automated semantic leaf categorization by image analysis,"Understanding the diversity of the plant community is a crucial issue for the development of many botanical industries as well as for the conservation of the ecosystem biodiversity. Traditionally, botanists have proposed detailed key descriptions (generally qualitative) or concepts about the morphology of plants and particularly of leaves that allow them to construct relationships between different plants and between them and their species. However, extracting these concepts is complicated, painstaking and can only be carried out by experts. One way to accelerate and broaden the use of these key descriptions is to automatically extract them directly from images. In this paper, we focus mainly on one of the most commonly used key leaf descriptions which is the foliage arrangement (simple, compound, pinnate, palmate). To do so, we analyse the spatial distribution of the leaf contour points and mainly its maxima (concave and convex) and inflexion points. For each category, we define a particular geometric feature that describes its point distribution. We test the proposed method on real world leaf images (Pl@ntLeaves scans). The experiments have demonstrated the robustness of our algorithm for a high number of leaf species (70 species) and even in the presence of some distortions (such as rotation, some leaf damage, partial leaf overlapping). In addition to its accuracy, the proposed approach satisfies real-time requirements with a low computational cost."
Toward automated semantic leaf categorization by image analysis,"Understanding the diversity of the plant community is a crucial issue for the development of many botanical industries as well as for the conservation of the ecosystem biodiversity. Traditionally, botanists have proposed detailed key descriptions (generally qualitative) or concepts about the morphology of plants and particularly of leaves that allow them to construct relationships between different plants and between them and their species. However, extracting these concepts is complicated, painstaking and can only be carried out by experts. One way to accelerate and broaden the use of these key descriptions is to automatically extract them directly from images. In this paper, we focus mainly on one of the most commonly used key leaf descriptions which is the foliage arrangement (simple, compound, pinnate, palmate). To do so, we analyse the spatial distribution of the leaf contour points and mainly its maxima (concave and convex) and inflexion points. For each category, we define a particular geometric feature that describes its point distribution. We test the proposed method on real world leaf images (Pl@ntLeaves scans). The experiments have demonstrated the robustness of our algorithm for a high number of leaf species (70 species) and even in the presence of some distortions (such as rotation, some leaf damage, partial leaf overlapping). In addition to its accuracy, the proposed approach satisfies real-time requirements with a low computational cost."
"Feature Selection using L_{p,\infty } norm","In this paper, we present a feature selection method using  L_{p,\infty} norm as a regularization term. Compared with standard  L_{p,\infty} feature selection,  L_{p,\infty } feature selection gives more flexible to approach the number of non-zero features/variables, which is the desired goal in feature/variable selection tasks.   We use proximal gradient method to solve  L_{p,\infty } norm problem. An efficient algorithm is proposed to solve the associated proximal operator with rigorous analysis.    Extensive experiments on both multi-class and multi-label feature selection tasks demonstrate the effectiveness of our methods. The experiment results also suggest smaller $p$ values(say p=0.25) give relatively better results. "
Local Features for Robust Manifold Modeling,"Despite the promise of low-dimensional manifold models for various vision and machine learning tasks, their utility has been hamstrung in practice by two fundamental challenges: practical image manifolds are plagued by a large number of nuisance variables and are non-isometric to the parameter space. In this paper, we develop a new manifold modeling, learning, and processing framework that directly addresses these challenges. At the heart of the framework are two key ideas: the use of the Earth Mover's Distance (EMD) to enable isometry of image manifolds to the underlying parameter space, and the use of local image features (such as SIFT features) to enable robust learning even to nuisance articulations.We analytically describe the performance of our approach and propose a fast kernel-based method for approximate EMD calculation to ensure computational efficiency. A powerful application of our approach is the automatic organization of large, unstructured collections of photographs gathered from the internet. "
A non-parametric Bayesian prior for causal inference of auditory streaming,"Perceptual grouping of sequential auditory cues has traditionally been modeled using a mechanistic approach. The problem however is essentially one of source inference ? a problem that has recently been tackled using statistical Bayesian models in visual and auditory-visual modalities. Usually the models are restricted to performing inference over just one or two possible sources, but human perceptual systems have to deal with much more complex scenarios. We have developed a Bayesian inference model that allows an unlimited number of signal sources to be considered: it is general enough to allow any discrete sequential cues, from any modality. The model uses a non-parametric prior, so increased complexity of the signal does not necessitate more parameters. The model not only determines the most likely number of sources, but also specifies the source that each signal is associated with. The model gives an excellent fit to data from an auditory stream segregation experiment in which the pitch and presentation rate of pure tones determined the perceived number of sources."
A non-parametric Bayesian prior for causal inference of auditory streaming,"Perceptual grouping of sequential auditory cues has traditionally been modeled using a mechanistic approach. The problem however is essentially one of source inference ? a problem that has recently been tackled using statistical Bayesian models in visual and auditory-visual modalities. Usually the models are restricted to performing inference over just one or two possible sources, but human perceptual systems have to deal with much more complex scenarios. We have developed a Bayesian inference model that allows an unlimited number of signal sources to be considered: it is general enough to allow any discrete sequential cues, from any modality. The model uses a non-parametric prior, so increased complexity of the signal does not necessitate more parameters. The model not only determines the most likely number of sources, but also specifies the source that each signal is associated with. The model gives an excellent fit to data from an auditory stream segregation experiment in which the pitch and presentation rate of pure tones determined the perceived number of sources."
Low Rank Data Recovery Using Schatten p-Norm,"In this paper, we present Schatten $p$-Norm model for low-rank data recovery. Besides playing the role of data recovery, Schatten p-Norm model is more attractive due to its suppression on the shrinkage of singular values at smaller p.The proposed Schatten p-Norm model can be transformed into solving the proximal operator, and an efficient algorithm based on ALM method is proposed.  Another iterative algorithm is also presented to solve this problem with rigorous convergence analysis.Extensive experiment results on 6 occluded datasets indicate the relatively better data recovery results at smaller p values."
Modeling 4D Human-Object Interactions for Object Detection,"In recent years, human-object context has been utilized for improving both object detection and action recognition, but most work has been primarily focused on modelling human-object relations in 2D static images and such contextual cues are often compromised due to their sensitivity to viewpoint changes. In this paper, we propose to build human-object interaction models in 3D space plus the time axis, using videos captured by Kineck cameras which provide rather accurate 3D human poses in time and the point clouds for the contextual objects.  Our contextual model are learned form such data and includes three components: i) co-occurrence of human action labels and object labels; ii) geometric compatibility between human body parts and object bounding boxes in 3D space; and iii) the sequence of actions/poses in time are grouped into events to provide temporal context. Such model provides much more reliable and accurate mutual context information for  object detection and action recognition through joint inference which resolve the ambiguities through a top-down and bottom-up computing process. In experiment, we show that our method achieve satisfactory results on an indoor dataset that we collect and will be released to the public. "
Modeling 4D Human-Object Interactions for Object Detection,"In recent years, human-object context has been utilized for improving both object detection and action recognition, but most work has been primarily focused on modelling human-object relations in 2D static images and such contextual cues are often compromised due to their sensitivity to viewpoint changes. In this paper, we propose to build human-object interaction models in 3D space plus the time axis, using videos captured by Kineck cameras which provide rather accurate 3D human poses in time and the point clouds for the contextual objects.  Our contextual model are learned form such data and includes three components: i) co-occurrence of human action labels and object labels; ii) geometric compatibility between human body parts and object bounding boxes in 3D space; and iii) the sequence of actions/poses in time are grouped into events to provide temporal context. Such model provides much more reliable and accurate mutual context information for  object detection and action recognition through joint inference which resolve the ambiguities through a top-down and bottom-up computing process. In experiment, we show that our method achieve satisfactory results on an indoor dataset that we collect and will be released to the public. "
Modeling 4D Human-Object Interactions for Object Detection,"In recent years, human-object context has been utilized for improving both object detection and action recognition, but most work has been primarily focused on modelling human-object relations in 2D static images and such contextual cues are often compromised due to their sensitivity to viewpoint changes. In this paper, we propose to build human-object interaction models in 3D space plus the time axis, using videos captured by Kineck cameras which provide rather accurate 3D human poses in time and the point clouds for the contextual objects.  Our contextual model are learned form such data and includes three components: i) co-occurrence of human action labels and object labels; ii) geometric compatibility between human body parts and object bounding boxes in 3D space; and iii) the sequence of actions/poses in time are grouped into events to provide temporal context. Such model provides much more reliable and accurate mutual context information for  object detection and action recognition through joint inference which resolve the ambiguities through a top-down and bottom-up computing process. In experiment, we show that our method achieve satisfactory results on an indoor dataset that we collect and will be released to the public. "
Privacy preserving data publishing for Bayesian network structure learning,"In order to prevent the leak of personal electronic health record, the federal Health Insurance Portability and Accountability Act (HIPAA) has set a national standard to protect privacy of this kind of information. In this paper, we propose a privacy preserving data publishing method for Bayesian network structure learning. We come up with a new method that using simulated annealing to find an optimal noise adding to the original dataset so that the resulting dataset will be different from the original one. By publishing this noised dataset, other research institutes are able to use their own Bayesian network structure learning algorithms to build a network whose accuracy is comparable to the accuracy of network built with the original dataset."
Privacy preserving data publishing for Bayesian network structure learning,"In order to prevent the leak of personal electronic health record, the federal Health Insurance Portability and Accountability Act (HIPAA) has set a national standard to protect privacy of this kind of information. In this paper, we propose a privacy preserving data publishing method for Bayesian network structure learning. We come up with a new method that using simulated annealing to find an optimal noise adding to the original dataset so that the resulting dataset will be different from the original one. By publishing this noised dataset, other research institutes are able to use their own Bayesian network structure learning algorithms to build a network whose accuracy is comparable to the accuracy of network built with the original dataset."
Privacy preserving data publishing for Bayesian network structure learning,"In order to prevent the leak of personal electronic health record, the federal Health Insurance Portability and Accountability Act (HIPAA) has set a national standard to protect privacy of this kind of information. In this paper, we propose a privacy preserving data publishing method for Bayesian network structure learning. We come up with a new method that using simulated annealing to find an optimal noise adding to the original dataset so that the resulting dataset will be different from the original one. By publishing this noised dataset, other research institutes are able to use their own Bayesian network structure learning algorithms to build a network whose accuracy is comparable to the accuracy of network built with the original dataset."
Parametric Learning of Generalized Decision Trees,"Decision trees are efficient models for representing piecewise-defined functions. We present gradient based learning algorithms to estimate the parameters for two general classes of decision trees. Decision trees can be distinguished based on their type of split functions. The first class divides the input space into disjunct regions by making hard splits. The second class, which we call generalized decision trees, relaxes the constraints of the split functions to enable a soft partitioning. This richer class is well suited to represent smooth functions, but has an increased computational complexity. The first contribution is a gradient based learning algo- rithm for generalized decision trees, that can learn in on-line settings with limited resources. The second contribution is a learning algorithm for decision trees with hard splits. We derive this learning scheme by combining the learning capabilities of the first algorithm with a continuation method. This is especially useful for post optimizing trees that are constructed with heuristic methods."
Parametric Learning of Generalized Decision Trees,"Decision trees are efficient models for representing piecewise-defined functions. We present gradient based learning algorithms to estimate the parameters for two general classes of decision trees. Decision trees can be distinguished based on their type of split functions. The first class divides the input space into disjunct regions by making hard splits. The second class, which we call generalized decision trees, relaxes the constraints of the split functions to enable a soft partitioning. This richer class is well suited to represent smooth functions, but has an increased computational complexity. The first contribution is a gradient based learning algo- rithm for generalized decision trees, that can learn in on-line settings with limited resources. The second contribution is a learning algorithm for decision trees with hard splits. We derive this learning scheme by combining the learning capabilities of the first algorithm with a continuation method. This is especially useful for post optimizing trees that are constructed with heuristic methods."
Parametric Learning of Generalized Decision Trees,"Decision trees are efficient models for representing piecewise-defined functions. We present gradient based learning algorithms to estimate the parameters for two general classes of decision trees. Decision trees can be distinguished based on their type of split functions. The first class divides the input space into disjunct regions by making hard splits. The second class, which we call generalized decision trees, relaxes the constraints of the split functions to enable a soft partitioning. This richer class is well suited to represent smooth functions, but has an increased computational complexity. The first contribution is a gradient based learning algo- rithm for generalized decision trees, that can learn in on-line settings with limited resources. The second contribution is a learning algorithm for decision trees with hard splits. We derive this learning scheme by combining the learning capabilities of the first algorithm with a continuation method. This is especially useful for post optimizing trees that are constructed with heuristic methods."
Complex Activity Recognition using Granger Constrained DBN (GCDBN) in Sports and Surveillance Video,"Common approaches for modeling interactions of multiple interacting and co-occurring objects in complex activities can theoretically model any number of co-occurring agents or events. However, these can be intractable for complex representations, require manual structure definitions, and/or generatively learn their structures. Our approach involves automatically constraining the nodes and links of a Dynamic Bayesian Network (DBN) in an informative and discriminative manner, resulting in sparse models that are both tractable and improve classification. This is accomplished by explicitly constraining the temporal links based on a direct measure of temporal dependence using Granger Cause statistics, resulting in the Granger Constrained DBN (GCDBN). The experiments show how the GCDBN outperforms other state-of-the-art models in complex activity classification on both handball and surveillance video data."
Complex Activity Recognition using Granger Constrained DBN (GCDBN) in Sports and Surveillance Video,"Common approaches for modeling interactions of multiple interacting and co-occurring objects in complex activities can theoretically model any number of co-occurring agents or events. However, these can be intractable for complex representations, require manual structure definitions, and/or generatively learn their structures. Our approach involves automatically constraining the nodes and links of a Dynamic Bayesian Network (DBN) in an informative and discriminative manner, resulting in sparse models that are both tractable and improve classification. This is accomplished by explicitly constraining the temporal links based on a direct measure of temporal dependence using Granger Cause statistics, resulting in the Granger Constrained DBN (GCDBN). The experiments show how the GCDBN outperforms other state-of-the-art models in complex activity classification on both handball and surveillance video data."
Robust exponential binary pattern storage in Little-Hopfield networks,"The Little-Hopfield network is an auto-associative computational model of neural memory storage and retrieval.  This model is known to robustly store collections of randomly generated binary patterns as stable-points of the network dynamics.  However, the number of binary memories so storable scales linearly in the number of neurons, and it has been a long-standing open problem whether robust exponential storage of binary patterns was possible in such a network memory model.  In this note, we design elementary families of Little-Hopfield networks that solve this problem affirmitavely."
Linear Time Solver for Primal SVM,"Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, and designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with proved linear convergence for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easy parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state of the art solvers such as SVMperf, Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms."
Linear Time Solver for Primal SVM,"Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, and designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with proved linear convergence for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easy parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state of the art solvers such as SVMperf, Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms."
Parallel Execution of Self-Organizing Maps,"Self-organizing maps have been noted as useful tools for augmenting scientificdata visualizations, particularly in the case where visualization of multidimensionaldata is required. However, a chief disadvantage associated with the selforganizingmap in this capacity is its large runtime complexity, which may resultin impractical execution times in real-world use cases. Because of the propertiesassociated with neural networks, we tested the feasibility of applying parallel executionto the self-organizing map in an attempt to reduce execution time. Thoughwe had predicted that the parallelization of self-organizing maps would result innear-linear speedup, we have discovered evidence against this hypothesis."
Parallel Execution of Self-Organizing Maps,"Self-organizing maps have been noted as useful tools for augmenting scientificdata visualizations, particularly in the case where visualization of multidimensionaldata is required. However, a chief disadvantage associated with the selforganizingmap in this capacity is its large runtime complexity, which may resultin impractical execution times in real-world use cases. Because of the propertiesassociated with neural networks, we tested the feasibility of applying parallel executionto the self-organizing map in an attempt to reduce execution time. Thoughwe had predicted that the parallelization of self-organizing maps would result innear-linear speedup, we have discovered evidence against this hypothesis."
Parallel Execution of Self-Organizing Maps,"Self-organizing maps have been noted as useful tools for augmenting scientificdata visualizations, particularly in the case where visualization of multidimensionaldata is required. However, a chief disadvantage associated with the selforganizingmap in this capacity is its large runtime complexity, which may resultin impractical execution times in real-world use cases. Because of the propertiesassociated with neural networks, we tested the feasibility of applying parallel executionto the self-organizing map in an attempt to reduce execution time. Thoughwe had predicted that the parallelization of self-organizing maps would result innear-linear speedup, we have discovered evidence against this hypothesis."
Gaussian Process Model Predictive Control: A Comparison with Conventional Techniques,"Gaussian processes are gaining increased popularity in the area of system identification for control. In this paper a new Model Predictive Control (MPC) algorithm based on a Gaussian process internal model is defined. The framework can be used for modelling and control of arbitrary nonlinear state-space systems, an extension of previous work which only considered modelling of ARMAX models. The performance of the algorithm is then compared with standard MPC and an adaptive Linear Quadratic Regulator (LQR), which use linearised models of the real system, on a benchmark industrial process control problem."
Generic Active Appearance Models Revisited: The Active Orientation Models Paradigm ,"The proposed Active Orientation Models (AOMs) are generative models of facial shape and appearance. Their main differences with the well-known paradigm of Active Appearance Models (AAMs) are (i) they use a different statistical model of appearance (ii) they are accompanied by a robust algorithm for model fitting and parameter estimation and (iii) and, most importantly, they generalize well to unseen faces and variations. Their main similarity is computational complexity. The project-out version of AOMs is as computationally efficient as the standard project-out inverse compositional algorithm which is admittedly the fastest algorithm for fitting AAMs. We show that not only does the AOM generalize well to unseen identities, but also it outperforms state-of-the-art algorithms for the same task by a large margin. Finally, we prove our claims by providing Matlab code for reproducing our experiments."
Generic Active Appearance Models Revisited: The Active Orientation Models Paradigm ,"The proposed Active Orientation Models (AOMs) are generative models of facial shape and appearance. Their main differences with the well-known paradigm of Active Appearance Models (AAMs) are (i) they use a different statistical model of appearance (ii) they are accompanied by a robust algorithm for model fitting and parameter estimation and (iii) and, most importantly, they generalize well to unseen faces and variations. Their main similarity is computational complexity. The project-out version of AOMs is as computationally efficient as the standard project-out inverse compositional algorithm which is admittedly the fastest algorithm for fitting AAMs. We show that not only does the AOM generalize well to unseen identities, but also it outperforms state-of-the-art algorithms for the same task by a large margin. Finally, we prove our claims by providing Matlab code for reproducing our experiments."
Generic Active Appearance Models Revisited: The Active Orientation Models Paradigm ,"The proposed Active Orientation Models (AOMs) are generative models of facial shape and appearance. Their main differences with the well-known paradigm of Active Appearance Models (AAMs) are (i) they use a different statistical model of appearance (ii) they are accompanied by a robust algorithm for model fitting and parameter estimation and (iii) and, most importantly, they generalize well to unseen faces and variations. Their main similarity is computational complexity. The project-out version of AOMs is as computationally efficient as the standard project-out inverse compositional algorithm which is admittedly the fastest algorithm for fitting AAMs. We show that not only does the AOM generalize well to unseen identities, but also it outperforms state-of-the-art algorithms for the same task by a large margin. Finally, we prove our claims by providing Matlab code for reproducing our experiments."
Compressible Motion Fields,"Traditional video compression methods obtain a compactrepresentation for image frames by computing coarse motion fieldsdefined on patches of pixels called blocks. This piecewiseconstant flow approximation reduces the size of the motion fieldbut introduces block artifacts in the decoded (warped) imageframe. In this paper, we address the problem of estimating densemotion fields that, while accurately predicting one frame from agiven reference frame by warping it with the field, are also\emph{compressible}. We introduce a representation for motionfields based on wavelet bases, and approximate thecompressibility of their coefficients with a piecewise smoothsurrogate function that is relatively easy to optimize. We thenshow how to quantize and encode such coefficients with adaptiveprecision. We demonstrate the effectiveness of our approach bycomparing its performance with a state-of-the-art videocompression algorithm. Experimental results reveal that ourmethod significantly outperforms classical block-based motioncompensation adopted in most modern video compression algorithms."
Efficient and optimal Little-Hopfield auto-associative memory storage using minimum probability flow,"We present an algorithm to store binary memories in a Hopfield neural network using minimum probability flow, a recent technique to fit parameters in energy-based probabilistic models.  In the case of memories without noise, our algorithm provably achieves optimal pattern storage (which we show is at least one pattern per neuron) and outperforms classical methods both in speed and memory recovery.  Moreover, when trained on noisy or corrupted versions of a fixed set of binary patterns, our algorithm finds networks which correctly store the originals.  We also demonstrate this finding visually with the unsupervised storage and clean-up of large binary fingerprint images from significantly corrupted samples."
Smooth Convolutional Stacked Autoencoder for Feature Learning in ECoG Based Brain Computer Interface,"Recent years have seen great interest in ECoG based Brain Computer Interface (BCI) systems. The performance of the BCI systems largely depends on the low-level features used in the decoding algorithm. The currently widely used features are based on frequency decompositions and designed manually, which are only justified empirically. In this work, we describe an automatic feature learning framework for ECoG based BCI decoding algorithms, using a multi-layer deep learning structure we termed as {\em smooth convolutional stacked auto-encoders (SCSA)}. One advantage of SCSA is that it is an open architecture that facilities incorporating various domain-specific constraints, e.g., smoothness of the extracted features over time.  Based on SCSA and data sets of ECoG signals, we demonstrate significant improvement in performance over the current state of the art methods based on manual features. Furthermore, the automatically extracted features from SCSA also shed light on the optimality of those features obtained empirically."
Smooth Convolutional Stacked Autoencoder for Feature Learning in ECoG Based Brain Computer Interface,"Recent years have seen great interest in ECoG based Brain Computer Interface (BCI) systems. The performance of the BCI systems largely depends on the low-level features used in the decoding algorithm. The currently widely used features are based on frequency decompositions and designed manually, which are only justified empirically. In this work, we describe an automatic feature learning framework for ECoG based BCI decoding algorithms, using a multi-layer deep learning structure we termed as {\em smooth convolutional stacked auto-encoders (SCSA)}. One advantage of SCSA is that it is an open architecture that facilities incorporating various domain-specific constraints, e.g., smoothness of the extracted features over time.  Based on SCSA and data sets of ECoG signals, we demonstrate significant improvement in performance over the current state of the art methods based on manual features. Furthermore, the automatically extracted features from SCSA also shed light on the optimality of those features obtained empirically."
Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL,"Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.  We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging."
How safe is your room? 3D scene parsing using intuitive mechanics,"This paper proposes a new perspective for 3D scene parsing through reasoningabout object stability and safety using intuitive mechanics. Our approach utilizesa simple observation that, by human design, objects in static scenes should bestable and safe with respect to various disturbances, such as gravity, earthquakeand daily human activities. Given a 2.5D depth map captured by Kinect cameraor a 3D point cloud for the full scene by SLAM, firstly, our method performsvolumetric reasoning to recover the solid 3D volumes from defective point cloud.Secondly we introduce two new representations for intuitive mechanical reason-ing: i) A disconnectivity graph (DG) to represent the potential energy landscapeswith local minima (stable points), local maxima (unstable equilibrium) and energybarriers between them; and ii) A disturbance field (DF) to represent the possiblephysical work applied to each position in the scene. These allow us to evaluate theunsafeness as the expected risk, such as falling by gravity, knocked off by passing-by human etc. If a 3D entity (voxel in point cloud, shape primitive, or object) is atrisk, our algorithm applies two possible operators: a) attaching or connecting it toother objects to form a larger stable object; b) supporting it by hidden parts in theinvisible volume. By minimizing an energy function defined on both unsafenessand scene prior, our algorithm achieves the following objectives. i) Grouping theunstable primitives hierarchically to form stable objects in a parse graph; ii) Infer-ring hidden parts in the occluded areas; and iii) computing some cognitive maps,such as what area can a cup move freely on the scene, what objects are risky inthe room."
How safe is your room? 3D scene parsing using intuitive mechanics,"This paper proposes a new perspective for 3D scene parsing through reasoningabout object stability and safety using intuitive mechanics. Our approach utilizesa simple observation that, by human design, objects in static scenes should bestable and safe with respect to various disturbances, such as gravity, earthquakeand daily human activities. Given a 2.5D depth map captured by Kinect cameraor a 3D point cloud for the full scene by SLAM, firstly, our method performsvolumetric reasoning to recover the solid 3D volumes from defective point cloud.Secondly we introduce two new representations for intuitive mechanical reason-ing: i) A disconnectivity graph (DG) to represent the potential energy landscapeswith local minima (stable points), local maxima (unstable equilibrium) and energybarriers between them; and ii) A disturbance field (DF) to represent the possiblephysical work applied to each position in the scene. These allow us to evaluate theunsafeness as the expected risk, such as falling by gravity, knocked off by passing-by human etc. If a 3D entity (voxel in point cloud, shape primitive, or object) is atrisk, our algorithm applies two possible operators: a) attaching or connecting it toother objects to form a larger stable object; b) supporting it by hidden parts in theinvisible volume. By minimizing an energy function defined on both unsafenessand scene prior, our algorithm achieves the following objectives. i) Grouping theunstable primitives hierarchically to form stable objects in a parse graph; ii) Infer-ring hidden parts in the occluded areas; and iii) computing some cognitive maps,such as what area can a cup move freely on the scene, what objects are risky inthe room."
How safe is your room? 3D scene parsing using intuitive mechanics,"This paper proposes a new perspective for 3D scene parsing through reasoningabout object stability and safety using intuitive mechanics. Our approach utilizesa simple observation that, by human design, objects in static scenes should bestable and safe with respect to various disturbances, such as gravity, earthquakeand daily human activities. Given a 2.5D depth map captured by Kinect cameraor a 3D point cloud for the full scene by SLAM, firstly, our method performsvolumetric reasoning to recover the solid 3D volumes from defective point cloud.Secondly we introduce two new representations for intuitive mechanical reason-ing: i) A disconnectivity graph (DG) to represent the potential energy landscapeswith local minima (stable points), local maxima (unstable equilibrium) and energybarriers between them; and ii) A disturbance field (DF) to represent the possiblephysical work applied to each position in the scene. These allow us to evaluate theunsafeness as the expected risk, such as falling by gravity, knocked off by passing-by human etc. If a 3D entity (voxel in point cloud, shape primitive, or object) is atrisk, our algorithm applies two possible operators: a) attaching or connecting it toother objects to form a larger stable object; b) supporting it by hidden parts in theinvisible volume. By minimizing an energy function defined on both unsafenessand scene prior, our algorithm achieves the following objectives. i) Grouping theunstable primitives hierarchically to form stable objects in a parse graph; ii) Infer-ring hidden parts in the occluded areas; and iii) computing some cognitive maps,such as what area can a cup move freely on the scene, what objects are risky inthe room."
How safe is your room? 3D scene parsing using intuitive mechanics,"This paper proposes a new perspective for 3D scene parsing through reasoningabout object stability and safety using intuitive mechanics. Our approach utilizesa simple observation that, by human design, objects in static scenes should bestable and safe with respect to various disturbances, such as gravity, earthquakeand daily human activities. Given a 2.5D depth map captured by Kinect cameraor a 3D point cloud for the full scene by SLAM, firstly, our method performsvolumetric reasoning to recover the solid 3D volumes from defective point cloud.Secondly we introduce two new representations for intuitive mechanical reason-ing: i) A disconnectivity graph (DG) to represent the potential energy landscapeswith local minima (stable points), local maxima (unstable equilibrium) and energybarriers between them; and ii) A disturbance field (DF) to represent the possiblephysical work applied to each position in the scene. These allow us to evaluate theunsafeness as the expected risk, such as falling by gravity, knocked off by passing-by human etc. If a 3D entity (voxel in point cloud, shape primitive, or object) is atrisk, our algorithm applies two possible operators: a) attaching or connecting it toother objects to form a larger stable object; b) supporting it by hidden parts in theinvisible volume. By minimizing an energy function defined on both unsafenessand scene prior, our algorithm achieves the following objectives. i) Grouping theunstable primitives hierarchically to form stable objects in a parse graph; ii) Infer-ring hidden parts in the occluded areas; and iii) computing some cognitive maps,such as what area can a cup move freely on the scene, what objects are risky inthe room."
Gradient-based Laplacian Feature Selection,"In many research fields, one is often confronted with very high dimensional noisy  data. Feature selection techniques are designed to find the relevant feature subset that can facilitate classification or pattern detection. Traditional feature selection methods utilize label information to guide the identification of relevant feature subsets. In this paper, however, we consider the unsupervised feature selection problem. Without the label information, it is particularly difficult to identify a small set of relevant features due to the noisy nature of real-world data which corrupts the intrinsic structure of the data. A Gradient-based Laplacian Feature Selection (GLFS) is proposed to select important features by minimizing the variance of the Laplacian regularized least squares regression model. With $\ell_1$ relaxation, GLFS can find a sparse subset of features that is relevant to the Laplacian manifolds. Through extensive experimental evidences using simulated, three real-world object recognition and two computational biology datasets, we illustrate the power and superior performances of our approach over multiple state-of-the-art unsupervised feature selection methods. Additionally, we show that GLFS selects a sparser set of more relevant features in a supervised setting outperforming the popular elastic net methodology."
Gradient-based Laplacian Feature Selection,"In many research fields, one is often confronted with very high dimensional noisy  data. Feature selection techniques are designed to find the relevant feature subset that can facilitate classification or pattern detection. Traditional feature selection methods utilize label information to guide the identification of relevant feature subsets. In this paper, however, we consider the unsupervised feature selection problem. Without the label information, it is particularly difficult to identify a small set of relevant features due to the noisy nature of real-world data which corrupts the intrinsic structure of the data. A Gradient-based Laplacian Feature Selection (GLFS) is proposed to select important features by minimizing the variance of the Laplacian regularized least squares regression model. With $\ell_1$ relaxation, GLFS can find a sparse subset of features that is relevant to the Laplacian manifolds. Through extensive experimental evidences using simulated, three real-world object recognition and two computational biology datasets, we illustrate the power and superior performances of our approach over multiple state-of-the-art unsupervised feature selection methods. Additionally, we show that GLFS selects a sparser set of more relevant features in a supervised setting outperforming the popular elastic net methodology."
"Texture, Structure and Visual Matching","We propose a formal definition of ``visual texture'' and characterize it by the approximate sufficient statistics of the underlying process. These are inferred from data and used for compression, extrapolation, inpainting and segmentation. The formalization highlights relations between textures and other early vision operations, such as co-variant feature selection and correspondence. We show that co-variant frames (``structures'') are the complement of textures in an image. Unlike prior work on texture/structure partitioning, however, we show that such a decomposition requires multiple images: to attribute structures in the image to properties of the scene, a proper sampling condition has to be satisfied, which requires multiple realizations."
"Texture, Structure and Visual Matching","We propose a formal definition of ``visual texture'' and characterize it by the approximate sufficient statistics of the underlying process. These are inferred from data and used for compression, extrapolation, inpainting and segmentation. The formalization highlights relations between textures and other early vision operations, such as co-variant feature selection and correspondence. We show that co-variant frames (``structures'') are the complement of textures in an image. Unlike prior work on texture/structure partitioning, however, we show that such a decomposition requires multiple images: to attribute structures in the image to properties of the scene, a proper sampling condition has to be satisfied, which requires multiple realizations."
"Texture, Structure and Visual Matching","We propose a formal definition of ``visual texture'' and characterize it by the approximate sufficient statistics of the underlying process. These are inferred from data and used for compression, extrapolation, inpainting and segmentation. The formalization highlights relations between textures and other early vision operations, such as co-variant feature selection and correspondence. We show that co-variant frames (``structures'') are the complement of textures in an image. Unlike prior work on texture/structure partitioning, however, we show that such a decomposition requires multiple images: to attribute structures in the image to properties of the scene, a proper sampling condition has to be satisfied, which requires multiple realizations."
Spectral Learning of General Weighted Automata via Constrained Matrix Completion,"Many tasks in text and speech processing and computational biology involve functions from variable-length strings to real numbers. A wide class of such functions can be computed by weighted automata. Spectral methods based on singular value decompositions of Hankel matrices have been recently proposed for learning probability distributions over strings that can be computed by weighted automata. In this paper we show how this method can be applied to the problem of learning a general weighted automata from a sample of string-label pairs generated by an arbitrary distribution. The main obstruction to this approach is that in general some entries of the Hankel matrix that needs to be decomposed may be missing. We propose a solution based on solving a constrained matrix completion problem. Combining these two ingredients, a whole new family of algorithms for learning general weighted automata is obtained. Generalization bounds for a particular algorithm in this class are given. The proofs rely on a stability analysis of matrix completion and spectrallearning."
HGLMMF: Generalizing Matrix Factorization with Hierarchical Generalized Linear Model,"Matrix factorization (MF) has become the dominant method of collaborative filtering. Recently, various MF methods have been proposed and tried to jointly model multiple relations. However, such methods are vulnerable to the changes of data or sub-models. Moreover, data often follows the Pareto rule, which may lead to a poor result due to the global bias caused by such imbalanced data. To overcome these defects, we designed a generalized MF method based on hierarchical generalized linear models (HGLMMF) that augments knowledge with learned extra information from other related models. More specifically, HGLMMF uses one portion of the augmented knowledge to construct augmented covariates to better capture fixed effects while the other portion is used to model the cluster-specific random effects to adjust the global bias problem. We also demonstrate that a number of state-of-the-art MF models can be viewed as special cases of HGLMMF."
HGLMMF: Generalizing Matrix Factorization with Hierarchical Generalized Linear Model,"Matrix factorization (MF) has become the dominant method of collaborative filtering. Recently, various MF methods have been proposed and tried to jointly model multiple relations. However, such methods are vulnerable to the changes of data or sub-models. Moreover, data often follows the Pareto rule, which may lead to a poor result due to the global bias caused by such imbalanced data. To overcome these defects, we designed a generalized MF method based on hierarchical generalized linear models (HGLMMF) that augments knowledge with learned extra information from other related models. More specifically, HGLMMF uses one portion of the augmented knowledge to construct augmented covariates to better capture fixed effects while the other portion is used to model the cluster-specific random effects to adjust the global bias problem. We also demonstrate that a number of state-of-the-art MF models can be viewed as special cases of HGLMMF."
Inferring Hidden Fluents Using Action and Causality,"In object and event detection, the identification of fluents (object statuses) that are not directly observable has been overlooked.  Inferring the values of such hidden fluents is important to fully understanding which actions are available to agents.  These values can be filled in by applying causal reasoning to video.  This paper presents a reasoning framework for the changes in, or maintenance of, the values of fluents over time by extending the Causal And-Or Graph, a stochastic grammar model for causality that integrates with current grammar models for event and object detection. The model incorporates causal reasoning with spatio-temporal detection to generate multiple interpretations of what is transpiring in a scene and to rank those interpretations according to probability.  We show that such interpretations can be used to correct (mis)detections of fluents and events in video; results are comparable to humans' performance in reasoning values of hidden fluents."
Inferring Hidden Fluents Using Action and Causality,"In object and event detection, the identification of fluents (object statuses) that are not directly observable has been overlooked.  Inferring the values of such hidden fluents is important to fully understanding which actions are available to agents.  These values can be filled in by applying causal reasoning to video.  This paper presents a reasoning framework for the changes in, or maintenance of, the values of fluents over time by extending the Causal And-Or Graph, a stochastic grammar model for causality that integrates with current grammar models for event and object detection. The model incorporates causal reasoning with spatio-temporal detection to generate multiple interpretations of what is transpiring in a scene and to rank those interpretations according to probability.  We show that such interpretations can be used to correct (mis)detections of fluents and events in video; results are comparable to humans' performance in reasoning values of hidden fluents."
Inferring Hidden Fluents Using Action and Causality,"In object and event detection, the identification of fluents (object statuses) that are not directly observable has been overlooked.  Inferring the values of such hidden fluents is important to fully understanding which actions are available to agents.  These values can be filled in by applying causal reasoning to video.  This paper presents a reasoning framework for the changes in, or maintenance of, the values of fluents over time by extending the Causal And-Or Graph, a stochastic grammar model for causality that integrates with current grammar models for event and object detection. The model incorporates causal reasoning with spatio-temporal detection to generate multiple interpretations of what is transpiring in a scene and to rank those interpretations according to probability.  We show that such interpretations can be used to correct (mis)detections of fluents and events in video; results are comparable to humans' performance in reasoning values of hidden fluents."
Inverse Reinforcement Learning for Dynamic Features with Applications to Socially Normative Robot Behavior Learning,"Inverse reinforcement learning (IRL) is a general framework for learning reward functions for Markov Decision Processes from demonstrated samples of its optimal policy. Prior work has assumed the reward features to be static or semi-static which prevented IRL to be effectively applied for agents in dynamic environments with features that continuously change over action executions. In this paper, we develop an approach to compute and match the estimated and predicted expectations of dynamic feature counts by taking a Monte Carlo-based path sampling approach. The method learns feature-based cost weights, induced by the modeled maximum entropy distribution, from many examples of locally optimal behavior. By joining local gradients and using alternative gradient descent methods, we obtain a particularly compact and efficient learning algorithm. Driven by the scenario of robots that learn socially normative behavior from demonstration, we apply the method to several examples of human-like navigation maneuvers in simulation and with a real robot and show how the proposed method outperforms alternative approaches."
Inverse Reinforcement Learning for Dynamic Features with Applications to Socially Normative Robot Behavior Learning,"Inverse reinforcement learning (IRL) is a general framework for learning reward functions for Markov Decision Processes from demonstrated samples of its optimal policy. Prior work has assumed the reward features to be static or semi-static which prevented IRL to be effectively applied for agents in dynamic environments with features that continuously change over action executions. In this paper, we develop an approach to compute and match the estimated and predicted expectations of dynamic feature counts by taking a Monte Carlo-based path sampling approach. The method learns feature-based cost weights, induced by the modeled maximum entropy distribution, from many examples of locally optimal behavior. By joining local gradients and using alternative gradient descent methods, we obtain a particularly compact and efficient learning algorithm. Driven by the scenario of robots that learn socially normative behavior from demonstration, we apply the method to several examples of human-like navigation maneuvers in simulation and with a real robot and show how the proposed method outperforms alternative approaches."
Inverse Reinforcement Learning for Dynamic Features with Applications to Socially Normative Robot Behavior Learning,"Inverse reinforcement learning (IRL) is a general framework for learning reward functions for Markov Decision Processes from demonstrated samples of its optimal policy. Prior work has assumed the reward features to be static or semi-static which prevented IRL to be effectively applied for agents in dynamic environments with features that continuously change over action executions. In this paper, we develop an approach to compute and match the estimated and predicted expectations of dynamic feature counts by taking a Monte Carlo-based path sampling approach. The method learns feature-based cost weights, induced by the modeled maximum entropy distribution, from many examples of locally optimal behavior. By joining local gradients and using alternative gradient descent methods, we obtain a particularly compact and efficient learning algorithm. Driven by the scenario of robots that learn socially normative behavior from demonstration, we apply the method to several examples of human-like navigation maneuvers in simulation and with a real robot and show how the proposed method outperforms alternative approaches."
Inverse Reinforcement Learning for Dynamic Features with Applications to Socially Normative Robot Behavior Learning,"Inverse reinforcement learning (IRL) is a general framework for learning reward functions for Markov Decision Processes from demonstrated samples of its optimal policy. Prior work has assumed the reward features to be static or semi-static which prevented IRL to be effectively applied for agents in dynamic environments with features that continuously change over action executions. In this paper, we develop an approach to compute and match the estimated and predicted expectations of dynamic feature counts by taking a Monte Carlo-based path sampling approach. The method learns feature-based cost weights, induced by the modeled maximum entropy distribution, from many examples of locally optimal behavior. By joining local gradients and using alternative gradient descent methods, we obtain a particularly compact and efficient learning algorithm. Driven by the scenario of robots that learn socially normative behavior from demonstration, we apply the method to several examples of human-like navigation maneuvers in simulation and with a real robot and show how the proposed method outperforms alternative approaches."
Monitoring Cardiac Stress Using Acoustic Heart Signals,"Cardiovascular complications arising from non-cardiac surgery exceed 1 million patients worldwide each year. Due to the increasing size of the elderly population it is predicted that during the coming decades, perioperative complications will increase by 100%. Current non-invasive cardiac monitoring techniques are based mostly on ECG signals. This kind of monitoring reflects cardiac electrical activity, but not its mechanical activity.It is known that acoustic heart sounds carry significant information about the cardiac state. In this work we present a novel monitoring that is based on the mechanical activity of the heart and is manifested by the sounds emitted from the heart. Two physiological features are extracted, which reflect cardiac morphology change from its baseline behavior. We use laparoscopic surgery as a model for a procedure which involves externally induced cardiac stress. The framework is applied to heart sounds recorded during laparoscopic surgeries of 25 patients. We demonstrate that suggested features change during cardiac stress, and are more significant for patients with cardiac problems. Furthermore, we show that other ECG morphology features are less sensitive during cardiac stress."
Active Learning for Multiclass Cost-sensitive Classification Using Probabilistic Models,"Multiclass cost-sensitive active learning is a relatively new problem. In this paper, we derive the maximum expected cost and cost-weighted minimum margin strategy for multiclass cost-sensitive active learning. These two strategies can be seem as the extended version of classical cost-insensitive active learning strategies. The experimental results demonstrate that the derived strategies are promising for costsensitive active learning. In particular, the cost-sensitive strategies outperform cost-insensitive ones. The experimental results reveal how the hardness of data affects the performance of active learning strategies. Thus, in practical active learning applications, data analysis before strategy selection can be important."
Active Learning for Multiclass Cost-sensitive Classification Using Probabilistic Models,"Multiclass cost-sensitive active learning is a relatively new problem. In this paper, we derive the maximum expected cost and cost-weighted minimum margin strategy for multiclass cost-sensitive active learning. These two strategies can be seem as the extended version of classical cost-insensitive active learning strategies. The experimental results demonstrate that the derived strategies are promising for costsensitive active learning. In particular, the cost-sensitive strategies outperform cost-insensitive ones. The experimental results reveal how the hardness of data affects the performance of active learning strategies. Thus, in practical active learning applications, data analysis before strategy selection can be important."
Bayesian Adaptive Mean Shift (BAMS),"The Adaptive Mean Shift (AMS) algorithm is a popular and simple non-parametric clustering approach based on Kernel Density Estimation. In this paper AMS is reformulated in a Bayesian framework, which permits a natural generalization in several directions that are shown to improve performance. The Bayesian framework considers the AMS to be a method to obtain a  posterior mode. This allows the algorithm to be generalized with three components which are not considered in the conventional approach: node weights, a prior for a particular location and a posterior distribution for the bandwidth. Practical methods to build the three different components are considered. The Bayesian Adaptive Mean Shift (BAMS) algorithm is evaluated with synthetic datasets and several real datasets."
Bayesian Adaptive Mean Shift (BAMS),"The Adaptive Mean Shift (AMS) algorithm is a popular and simple non-parametric clustering approach based on Kernel Density Estimation. In this paper AMS is reformulated in a Bayesian framework, which permits a natural generalization in several directions that are shown to improve performance. The Bayesian framework considers the AMS to be a method to obtain a  posterior mode. This allows the algorithm to be generalized with three components which are not considered in the conventional approach: node weights, a prior for a particular location and a posterior distribution for the bandwidth. Practical methods to build the three different components are considered. The Bayesian Adaptive Mean Shift (BAMS) algorithm is evaluated with synthetic datasets and several real datasets."
Bayesian Adaptive Mean Shift (BAMS),"The Adaptive Mean Shift (AMS) algorithm is a popular and simple non-parametric clustering approach based on Kernel Density Estimation. In this paper AMS is reformulated in a Bayesian framework, which permits a natural generalization in several directions that are shown to improve performance. The Bayesian framework considers the AMS to be a method to obtain a  posterior mode. This allows the algorithm to be generalized with three components which are not considered in the conventional approach: node weights, a prior for a particular location and a posterior distribution for the bandwidth. Practical methods to build the three different components are considered. The Bayesian Adaptive Mean Shift (BAMS) algorithm is evaluated with synthetic datasets and several real datasets."
Algorithms for Learning Markov Field Policies,"We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications.The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects."
Algorithms for Learning Markov Field Policies,"We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications.The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects."
Algorithms for Learning Markov Field Policies,"We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications.The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects."
Schizophrenia Detection and Classification by Advanced Analysis of EEG Recordings using a Single Electrode Approach ,"Electroencephalographic (EEG) analysis has emerged as a powerful tool for brain state interpretation and diagnosis. However, it has not emerged as a powerful tool in diagnosis of mental disorders. This may be explained by the low spatial resolution or depth sensitivity of EEG.  This paper concerns diagnosis of Schizophrenia using EEG, which currently suffers from few cardinal problems: it heavily depends on assumptions, conditions and prior knowledge of the patient. Additionally, the diagnostic experiments take hours, and accuracy of the analysis is low or unreliable.This article presents a novel approach for Schizophrenia detection showing great success in classification accuracy. The methodology is built for a single electrode recording attempting to make the data acquisition process feasible and quick for most patients."
Schizophrenia Detection and Classification by Advanced Analysis of EEG Recordings using a Single Electrode Approach ,"Electroencephalographic (EEG) analysis has emerged as a powerful tool for brain state interpretation and diagnosis. However, it has not emerged as a powerful tool in diagnosis of mental disorders. This may be explained by the low spatial resolution or depth sensitivity of EEG.  This paper concerns diagnosis of Schizophrenia using EEG, which currently suffers from few cardinal problems: it heavily depends on assumptions, conditions and prior knowledge of the patient. Additionally, the diagnostic experiments take hours, and accuracy of the analysis is low or unreliable.This article presents a novel approach for Schizophrenia detection showing great success in classification accuracy. The methodology is built for a single electrode recording attempting to make the data acquisition process feasible and quick for most patients."
Affine Independent Variational Inference,"We present a method for approximate inference for a broad class of non-conjugate probabilistic models. In particular, for the family of generalized linear model target densities we describe a rich class of variational approximating densities which can be best fit to the target by minimizing the Kullback-Leibler divergence.  Our approach is based on using the Fourier representation which we show results in efficient and scalable inference."
Simulation of Database-Valued Markov Chains Using SimSQL,"This paper describes the SimSQL system, which allows for SQL-based specification,simulation, and querying of database-valued Markov chains, i.e., chainswhose value at any time step comprises the contents of an entire database. This isof particular interest in statistical machine learning, because SimSQL can easilybe used to declaratively specify Markov Chain Monte Carlo simulations that areautomatically parallelized to run on a large compute cluster."
Simulation of Database-Valued Markov Chains Using SimSQL,"This paper describes the SimSQL system, which allows for SQL-based specification,simulation, and querying of database-valued Markov chains, i.e., chainswhose value at any time step comprises the contents of an entire database. This isof particular interest in statistical machine learning, because SimSQL can easilybe used to declaratively specify Markov Chain Monte Carlo simulations that areautomatically parallelized to run on a large compute cluster."
Multi-Task Averaging,"We present a multi-task learning approach to jointly estimate the means of multipleindependent data sets. We prove that the proposed multi-task averaging (MTA) algorithmresults in a convex combination of the single-task maximum likelihood estimates.We derive the optimal amount of regularization, and show that it can be effectivelyestimated. Simulations and real data experiments demonstrate that MTAoutperforms both maximum likelihood and James-Stein estimators, and that ourapproach to estimating the amount of regularization rivals cross-validation in performancebut is more computationally efficient."
Multi-Task Averaging,"We present a multi-task learning approach to jointly estimate the means of multipleindependent data sets. We prove that the proposed multi-task averaging (MTA) algorithmresults in a convex combination of the single-task maximum likelihood estimates.We derive the optimal amount of regularization, and show that it can be effectivelyestimated. Simulations and real data experiments demonstrate that MTAoutperforms both maximum likelihood and James-Stein estimators, and that ourapproach to estimating the amount of regularization rivals cross-validation in performancebut is more computationally efficient."
Functional Mesh Learning for Pattern Analysis of Cognitive Processes,"Here we propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning machine, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using functional neighborhood concept. In order to define functional neighborhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighboring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Local Relational Features (LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely K-Nearest Neighbor and Support Vector Machine, are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The proposed model exhibits classification performance superior to the state of the art methods available in the literature."
Functional Mesh Learning for Pattern Analysis of Cognitive Processes,"Here we propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning machine, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using functional neighborhood concept. In order to define functional neighborhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighboring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Local Relational Features (LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely K-Nearest Neighbor and Support Vector Machine, are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The proposed model exhibits classification performance superior to the state of the art methods available in the literature."
Functional Mesh Learning for Pattern Analysis of Cognitive Processes,"Here we propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning machine, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using functional neighborhood concept. In order to define functional neighborhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighboring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Local Relational Features (LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely K-Nearest Neighbor and Support Vector Machine, are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The proposed model exhibits classification performance superior to the state of the art methods available in the literature."
Functional Mesh Learning for Pattern Analysis of Cognitive Processes,"Here we propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning machine, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using functional neighborhood concept. In order to define functional neighborhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighboring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Local Relational Features (LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely K-Nearest Neighbor and Support Vector Machine, are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The proposed model exhibits classification performance superior to the state of the art methods available in the literature."
Functional Mesh Learning for Pattern Analysis of Cognitive Processes,"Here we propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning machine, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using functional neighborhood concept. In order to define functional neighborhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighboring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Local Relational Features (LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely K-Nearest Neighbor and Support Vector Machine, are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The proposed model exhibits classification performance superior to the state of the art methods available in the literature."
Block Coordinate Descent for Sparse NMF ," Nonnegative matrix factorization (NMF) is now an ubiquitous tool for  data analysis.  An  important variant is the Sparse NMF  problem which arises when we explicitly require the learnt features to be sparse. A natural  measure of sparsity  is the L$_0$-norm, however its  optimization is NP-hard.  Instead,  we consider the surrogate sparsitymeasure  linear in  the  ratio of  the  L$_1$ and  L$_2$ norms,  and  propose an efficient algorithm  to handle these norm constraints while  minimizing the reconstruction error.The  key  novelty  of  our algorithm is in its use of sequential updates  instead of the usual  batch  approach. Existing  algorithms for  solving this  problem are  typically inefficient.  We present experimental evidence  that our new algorithm performs an order of magnitude faster compared to the state-of-the-art solvers."
Block Coordinate Descent for Sparse NMF ," Nonnegative matrix factorization (NMF) is now an ubiquitous tool for  data analysis.  An  important variant is the Sparse NMF  problem which arises when we explicitly require the learnt features to be sparse. A natural  measure of sparsity  is the L$_0$-norm, however its  optimization is NP-hard.  Instead,  we consider the surrogate sparsitymeasure  linear in  the  ratio of  the  L$_1$ and  L$_2$ norms,  and  propose an efficient algorithm  to handle these norm constraints while  minimizing the reconstruction error.The  key  novelty  of  our algorithm is in its use of sequential updates  instead of the usual  batch  approach. Existing  algorithms for  solving this  problem are  typically inefficient.  We present experimental evidence  that our new algorithm performs an order of magnitude faster compared to the state-of-the-art solvers."
Block Coordinate Descent for Sparse NMF ," Nonnegative matrix factorization (NMF) is now an ubiquitous tool for  data analysis.  An  important variant is the Sparse NMF  problem which arises when we explicitly require the learnt features to be sparse. A natural  measure of sparsity  is the L$_0$-norm, however its  optimization is NP-hard.  Instead,  we consider the surrogate sparsitymeasure  linear in  the  ratio of  the  L$_1$ and  L$_2$ norms,  and  propose an efficient algorithm  to handle these norm constraints while  minimizing the reconstruction error.The  key  novelty  of  our algorithm is in its use of sequential updates  instead of the usual  batch  approach. Existing  algorithms for  solving this  problem are  typically inefficient.  We present experimental evidence  that our new algorithm performs an order of magnitude faster compared to the state-of-the-art solvers."
Block Coordinate Descent for Sparse NMF ," Nonnegative matrix factorization (NMF) is now an ubiquitous tool for  data analysis.  An  important variant is the Sparse NMF  problem which arises when we explicitly require the learnt features to be sparse. A natural  measure of sparsity  is the L$_0$-norm, however its  optimization is NP-hard.  Instead,  we consider the surrogate sparsitymeasure  linear in  the  ratio of  the  L$_1$ and  L$_2$ norms,  and  propose an efficient algorithm  to handle these norm constraints while  minimizing the reconstruction error.The  key  novelty  of  our algorithm is in its use of sequential updates  instead of the usual  batch  approach. Existing  algorithms for  solving this  problem are  typically inefficient.  We present experimental evidence  that our new algorithm performs an order of magnitude faster compared to the state-of-the-art solvers."
Block Coordinate Descent for Sparse NMF ," Nonnegative matrix factorization (NMF) is now an ubiquitous tool for  data analysis.  An  important variant is the Sparse NMF  problem which arises when we explicitly require the learnt features to be sparse. A natural  measure of sparsity  is the L$_0$-norm, however its  optimization is NP-hard.  Instead,  we consider the surrogate sparsitymeasure  linear in  the  ratio of  the  L$_1$ and  L$_2$ norms,  and  propose an efficient algorithm  to handle these norm constraints while  minimizing the reconstruction error.The  key  novelty  of  our algorithm is in its use of sequential updates  instead of the usual  batch  approach. Existing  algorithms for  solving this  problem are  typically inefficient.  We present experimental evidence  that our new algorithm performs an order of magnitude faster compared to the state-of-the-art solvers."
Robust Dictionary Learning by Source Decomposition,"It is now well established that sparse coding is well suited to many applications such as image restoration, denoising and classification. Especially, adaptive sparsemodels learned from data, or ?dictionary?, outperformfixed basis such as Discrete Cosine Basis and Fourier Basis. This paper extends this line of research to consider outlier elimination, proposing two methods to decompose the reconstructive residual into two components: a non-sparse component for small universal noises as well as a sparse one for the outliers respectively. In addition, further analysis reveals the connection between our model and the ?partial? dictionary learning approach, updating only part of the codewords (informative codewords DInfo)with remaining (noisy one DNoisy) fixed. We validate and evaluate our new approach on synthetic data as well as real applications and achieved satisfactory performance."
Robust Dictionary Learning by Source Decomposition,"It is now well established that sparse coding is well suited to many applications such as image restoration, denoising and classification. Especially, adaptive sparsemodels learned from data, or ?dictionary?, outperformfixed basis such as Discrete Cosine Basis and Fourier Basis. This paper extends this line of research to consider outlier elimination, proposing two methods to decompose the reconstructive residual into two components: a non-sparse component for small universal noises as well as a sparse one for the outliers respectively. In addition, further analysis reveals the connection between our model and the ?partial? dictionary learning approach, updating only part of the codewords (informative codewords DInfo)with remaining (noisy one DNoisy) fixed. We validate and evaluate our new approach on synthetic data as well as real applications and achieved satisfactory performance."
Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders,"Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density.  This paper contributes to the mathematical understanding of this phenomenon and helps define better justified sampling algorithms for deep learning based on auto-encoder variants. We consider an MCMC where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density. First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures estimators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed sampling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder."
Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders,"Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density.  This paper contributes to the mathematical understanding of this phenomenon and helps define better justified sampling algorithms for deep learning based on auto-encoder variants. We consider an MCMC where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density. First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures estimators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed sampling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder."
Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders,"Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density.  This paper contributes to the mathematical understanding of this phenomenon and helps define better justified sampling algorithms for deep learning based on auto-encoder variants. We consider an MCMC where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density. First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures estimators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed sampling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder."
Geodesic Distance Function Learning: Theory and an Algorithm,"Learning a distance function is of great importance in machine learning and pattern recognition. Geodesic distance, which has been widely used, is one of the most important intrinsic distances on the manifold. In this paper, we study the geodesic distance function $d(p, x)$ for a fixed point $p$. We provide two theorems to exactly characterize such a distance function. Our theoretical analysis shows if a function $r_p(x)$ is a Euclidean distance function in a neighborhood of $p$ in exponential coordinates and the gradient field of $r_p(x)$ has unit norm almost everywhere, then $r_p(x)$ must be the unique geodesic distance function $d(p,x)$. Based on our theoretical analysis, a novel approach from vector field perspective is proposed to learn the geodesic distance function. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm."
Geodesic Distance Function Learning: Theory and an Algorithm,"Learning a distance function is of great importance in machine learning and pattern recognition. Geodesic distance, which has been widely used, is one of the most important intrinsic distances on the manifold. In this paper, we study the geodesic distance function $d(p, x)$ for a fixed point $p$. We provide two theorems to exactly characterize such a distance function. Our theoretical analysis shows if a function $r_p(x)$ is a Euclidean distance function in a neighborhood of $p$ in exponential coordinates and the gradient field of $r_p(x)$ has unit norm almost everywhere, then $r_p(x)$ must be the unique geodesic distance function $d(p,x)$. Based on our theoretical analysis, a novel approach from vector field perspective is proposed to learn the geodesic distance function. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm."
Geodesic Distance Function Learning: Theory and an Algorithm,"Learning a distance function is of great importance in machine learning and pattern recognition. Geodesic distance, which has been widely used, is one of the most important intrinsic distances on the manifold. In this paper, we study the geodesic distance function $d(p, x)$ for a fixed point $p$. We provide two theorems to exactly characterize such a distance function. Our theoretical analysis shows if a function $r_p(x)$ is a Euclidean distance function in a neighborhood of $p$ in exponential coordinates and the gradient field of $r_p(x)$ has unit norm almost everywhere, then $r_p(x)$ must be the unique geodesic distance function $d(p,x)$. Based on our theoretical analysis, a novel approach from vector field perspective is proposed to learn the geodesic distance function. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm."
Measuring Reproducibility of High-throughput Deep-sequencing Experiments based on Self-adaptive Mixture Copula,"Measurement of the statistical reproducibility between biological experiment replicates is vital first step of the entire series of bioinformatics analyses for meaningful biology finding in mega-data. To distinguish the real biological relevant signals from artificial signals, irreproducible discovery rate (IDR) employing Copula, which can separate dependence structure and marginal distribution from data, has been put forth. However, the main disadvantage of IDR is that it assumes the data subject to normal distribution, which does not match the real data's feature. To address the issue, we propose a Self-adaptive Mixture Copula (SaMiC) to measure the reproducibility of experiment replicates from high-throughput deep-sequencing data. Simple and easy to implement, the proposed SaMiC method can self-adaptively tune its coefficients so that the measurement of reproducibility is more effective for general distributions.  Experiments in simulated and real data indicate that compared with IDR, the SaMiC method can better estimate  reproducibility between replicate samples."
Measuring Reproducibility of High-throughput Deep-sequencing Experiments based on Self-adaptive Mixture Copula,"Measurement of the statistical reproducibility between biological experiment replicates is vital first step of the entire series of bioinformatics analyses for meaningful biology finding in mega-data. To distinguish the real biological relevant signals from artificial signals, irreproducible discovery rate (IDR) employing Copula, which can separate dependence structure and marginal distribution from data, has been put forth. However, the main disadvantage of IDR is that it assumes the data subject to normal distribution, which does not match the real data's feature. To address the issue, we propose a Self-adaptive Mixture Copula (SaMiC) to measure the reproducibility of experiment replicates from high-throughput deep-sequencing data. Simple and easy to implement, the proposed SaMiC method can self-adaptively tune its coefficients so that the measurement of reproducibility is more effective for general distributions.  Experiments in simulated and real data indicate that compared with IDR, the SaMiC method can better estimate  reproducibility between replicate samples."
Matrix Completion with Ordering Relation Constraints,"We relax the equality constraints in the very general and well-known affine Schatten p-norm minimization problem into one-side inequality constraints. Owing to the imposed equality con-straints, existing methods can only achieve some degree of denoising, via the optimization of the objective energy function. By our proposed re-laxation, the decision variables in the objective function possess flexible nonlinearity while maintaining their ordering relation constraints. We show that, our new objective function is convex, and its global minimum can be obtained by a more general form of the Fixed-Point Con-tinuation framework with almost the same com-putational cost. Experiments show that, our algo-rithm has good performance over various widely used datasets."
On the Equivalence of the Lasso and the SVM,"We investigate the relation of two fundamental tools in machine learning, that is the support vector machine (SVM) for classification, and the Lasso technique used in regression. We show that the resulting optimization problems are equivalent, in the following sense: Given any instance of one of the two problems, we construct an instance of the other, having the same optimal solution. In consequence, the two large classes of existing optimization algorithms for both SVMs and Lasso can also be applied to the respective other problem instances.Also, the equivalence allows for many known theoretical insights for SVM and Lasso to be translated between the two settings. One such implication gives a simple kernelized version of the Lasso, analogous to the kernels used in the SVM setting. Another consequence is that the sparsity of a Lasso solution is equal to the number of support vectors for the corresponding SVM instance.Furthermore, we can directly relate sublinear time algorithms for the two problems, and give a new such algorithm variant for the Lasso."
Fused Multiple Graphical Lasso,"In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which  encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a blockwise coordinate descent method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."
A polygon-based interpolation operator for super-resolution imaging ,"We outline the super-resolution reconstruction problem posed as a maximization of probability. We then introduce an interpolation method based on polygonal pixel overlap, express it as a linear operator, and use it to improve reconstruction.Polygon interpolation outperforms the simpler bilinear interpolation operator and, unlike Gaussian modeling of pixels, requires no parameter estimation. A free software implementation that reproduces the results shown is provided."
A polygon-based interpolation operator for super-resolution imaging ,"We outline the super-resolution reconstruction problem posed as a maximization of probability. We then introduce an interpolation method based on polygonal pixel overlap, express it as a linear operator, and use it to improve reconstruction.Polygon interpolation outperforms the simpler bilinear interpolation operator and, unlike Gaussian modeling of pixels, requires no parameter estimation. A free software implementation that reproduces the results shown is provided."
Discovering Common Functional Connectomics Signatures,"Based on the structural connectomes constructed from diffusion tensor imaging (DTI) data, we present a novel framework to discover functional connectomics signatures from resting-state fMRI (R-fMRI) data for the characterization of brain conditions. First, by applying a sliding time window approach, the brain states represented by functional connectomes were automatically divided into temporal quasi-stable segments. These quasi-stable functional connectome segments were then integrated and pooled from populations as input to an effective dictionary learning and sparse coding algorithm, in order to identify common functional connectomes (CFC) and signature patterns, as well as their dynamic transition patterns. The computational framework was validated by benchmark stimulation data, and highly accurate results were obtained. By applying the framework on the datasets of 44 post-traumatic stress disorder (PTSD) patients and 51 healthy controls, it was found that there are 16 CFC patterns reproducible across healthy controls/PTSD patients, and two additional CFCs with altered connectivity patterns exist solely in PTSD subjects. These two signature CFCs can successfully differentiate 85% of PTSD patients, suggesting their potential use as biomarkers."
Discovering Common Functional Connectomics Signatures,"Based on the structural connectomes constructed from diffusion tensor imaging (DTI) data, we present a novel framework to discover functional connectomics signatures from resting-state fMRI (R-fMRI) data for the characterization of brain conditions. First, by applying a sliding time window approach, the brain states represented by functional connectomes were automatically divided into temporal quasi-stable segments. These quasi-stable functional connectome segments were then integrated and pooled from populations as input to an effective dictionary learning and sparse coding algorithm, in order to identify common functional connectomes (CFC) and signature patterns, as well as their dynamic transition patterns. The computational framework was validated by benchmark stimulation data, and highly accurate results were obtained. By applying the framework on the datasets of 44 post-traumatic stress disorder (PTSD) patients and 51 healthy controls, it was found that there are 16 CFC patterns reproducible across healthy controls/PTSD patients, and two additional CFCs with altered connectivity patterns exist solely in PTSD subjects. These two signature CFCs can successfully differentiate 85% of PTSD patients, suggesting their potential use as biomarkers."
Discovering Common Functional Connectomics Signatures,"Based on the structural connectomes constructed from diffusion tensor imaging (DTI) data, we present a novel framework to discover functional connectomics signatures from resting-state fMRI (R-fMRI) data for the characterization of brain conditions. First, by applying a sliding time window approach, the brain states represented by functional connectomes were automatically divided into temporal quasi-stable segments. These quasi-stable functional connectome segments were then integrated and pooled from populations as input to an effective dictionary learning and sparse coding algorithm, in order to identify common functional connectomes (CFC) and signature patterns, as well as their dynamic transition patterns. The computational framework was validated by benchmark stimulation data, and highly accurate results were obtained. By applying the framework on the datasets of 44 post-traumatic stress disorder (PTSD) patients and 51 healthy controls, it was found that there are 16 CFC patterns reproducible across healthy controls/PTSD patients, and two additional CFCs with altered connectivity patterns exist solely in PTSD subjects. These two signature CFCs can successfully differentiate 85% of PTSD patients, suggesting their potential use as biomarkers."
Discovering Common Functional Connectomics Signatures,"Based on the structural connectomes constructed from diffusion tensor imaging (DTI) data, we present a novel framework to discover functional connectomics signatures from resting-state fMRI (R-fMRI) data for the characterization of brain conditions. First, by applying a sliding time window approach, the brain states represented by functional connectomes were automatically divided into temporal quasi-stable segments. These quasi-stable functional connectome segments were then integrated and pooled from populations as input to an effective dictionary learning and sparse coding algorithm, in order to identify common functional connectomes (CFC) and signature patterns, as well as their dynamic transition patterns. The computational framework was validated by benchmark stimulation data, and highly accurate results were obtained. By applying the framework on the datasets of 44 post-traumatic stress disorder (PTSD) patients and 51 healthy controls, it was found that there are 16 CFC patterns reproducible across healthy controls/PTSD patients, and two additional CFCs with altered connectivity patterns exist solely in PTSD subjects. These two signature CFCs can successfully differentiate 85% of PTSD patients, suggesting their potential use as biomarkers."
Blind Image Deblurring by Spectral Properties of Convolution Operators,"In this paper, we study the problem of recovering a sharp version of a given blurry image when the blur kernel is unknown. Previous methods often introduce an image-independent regularizer (such as Gaussian or sparse priors) on the desired blur kernel. For the first time, this paper shows that the blurry image itself encodes rich information about the blur kernel. Such information can be found through analyzing and comparing how the spectrum of an image as a convolution operator changes before and after blurring. Our analysis leads to an effective convex regularizer on the blur kernel which depends only on the given blurry image. We show that the minimizer of this regularizer guarantees to give good approximation to the blur kernel if the original image is sharp enough. By combining this powerful regularizer with conventional image deblurring techniques, we show how we could significantly improve the deblurring results through simulations and experiments on real images, especially when the blur is large. In addition, our analysis and experiments help explaining why edges are good features for image deblurring."
Blind Image Deblurring by Spectral Properties of Convolution Operators,"In this paper, we study the problem of recovering a sharp version of a given blurry image when the blur kernel is unknown. Previous methods often introduce an image-independent regularizer (such as Gaussian or sparse priors) on the desired blur kernel. For the first time, this paper shows that the blurry image itself encodes rich information about the blur kernel. Such information can be found through analyzing and comparing how the spectrum of an image as a convolution operator changes before and after blurring. Our analysis leads to an effective convex regularizer on the blur kernel which depends only on the given blurry image. We show that the minimizer of this regularizer guarantees to give good approximation to the blur kernel if the original image is sharp enough. By combining this powerful regularizer with conventional image deblurring techniques, we show how we could significantly improve the deblurring results through simulations and experiments on real images, especially when the blur is large. In addition, our analysis and experiments help explaining why edges are good features for image deblurring."
Approximate l-fold cross-validation with Least Sqaures SVM and Kernel Ridge Regression,"Kernel methods have difficulties scaling to large modern data sets. The scalability issues are based on computational and memory requirements for working with a large matrix. These requirements have been addressed over the years by using low-rank kernel approximations or by improving the solvers' scalability. However, Least Squares Support Vector Machines (LS-SVM), a popular SVM variant, and Kernel Ridge Regression still have several scalability issues. In particular, the  $O(n^3)$ computational complexity for solving a single model, and the overall computational complexity associated with tuning hyperparameters are still major problems. We address these problems by introducing an $O(n\log n)$ approximate $l$-fold cross-validation method that uses a multi-level circulant matrix to approximate the kernel. In addition, we prove our algorithm's computational complexity and present empirical runtimes on data sets with approximately 1 million data points. We also validate our approximate method's effectiveness at selecting hyperparameters on real world and standard benchmark data sets. Lastly, we provide experimental results on using a multi-level circulant kernel approximation to solve LS-SVM problems with hyperparameters selected using our method."
Approximate l-fold cross-validation with Least Sqaures SVM and Kernel Ridge Regression,"Kernel methods have difficulties scaling to large modern data sets. The scalability issues are based on computational and memory requirements for working with a large matrix. These requirements have been addressed over the years by using low-rank kernel approximations or by improving the solvers' scalability. However, Least Squares Support Vector Machines (LS-SVM), a popular SVM variant, and Kernel Ridge Regression still have several scalability issues. In particular, the  $O(n^3)$ computational complexity for solving a single model, and the overall computational complexity associated with tuning hyperparameters are still major problems. We address these problems by introducing an $O(n\log n)$ approximate $l$-fold cross-validation method that uses a multi-level circulant matrix to approximate the kernel. In addition, we prove our algorithm's computational complexity and present empirical runtimes on data sets with approximately 1 million data points. We also validate our approximate method's effectiveness at selecting hyperparameters on real world and standard benchmark data sets. Lastly, we provide experimental results on using a multi-level circulant kernel approximation to solve LS-SVM problems with hyperparameters selected using our method."
Approximate l-fold cross-validation with Least Sqaures SVM and Kernel Ridge Regression,"Kernel methods have difficulties scaling to large modern data sets. The scalability issues are based on computational and memory requirements for working with a large matrix. These requirements have been addressed over the years by using low-rank kernel approximations or by improving the solvers' scalability. However, Least Squares Support Vector Machines (LS-SVM), a popular SVM variant, and Kernel Ridge Regression still have several scalability issues. In particular, the  $O(n^3)$ computational complexity for solving a single model, and the overall computational complexity associated with tuning hyperparameters are still major problems. We address these problems by introducing an $O(n\log n)$ approximate $l$-fold cross-validation method that uses a multi-level circulant matrix to approximate the kernel. In addition, we prove our algorithm's computational complexity and present empirical runtimes on data sets with approximately 1 million data points. We also validate our approximate method's effectiveness at selecting hyperparameters on real world and standard benchmark data sets. Lastly, we provide experimental results on using a multi-level circulant kernel approximation to solve LS-SVM problems with hyperparameters selected using our method."
Approximate l-fold cross-validation with Least Sqaures SVM and Kernel Ridge Regression,"Kernel methods have difficulties scaling to large modern data sets. The scalability issues are based on computational and memory requirements for working with a large matrix. These requirements have been addressed over the years by using low-rank kernel approximations or by improving the solvers' scalability. However, Least Squares Support Vector Machines (LS-SVM), a popular SVM variant, and Kernel Ridge Regression still have several scalability issues. In particular, the  $O(n^3)$ computational complexity for solving a single model, and the overall computational complexity associated with tuning hyperparameters are still major problems. We address these problems by introducing an $O(n\log n)$ approximate $l$-fold cross-validation method that uses a multi-level circulant matrix to approximate the kernel. In addition, we prove our algorithm's computational complexity and present empirical runtimes on data sets with approximately 1 million data points. We also validate our approximate method's effectiveness at selecting hyperparameters on real world and standard benchmark data sets. Lastly, we provide experimental results on using a multi-level circulant kernel approximation to solve LS-SVM problems with hyperparameters selected using our method."
A Marginalized Particle Gaussian Process Regression,"We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods."
A Marginalized Particle Gaussian Process Regression,"We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods."
Spherical Quantization based Binary Embedding,"This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods."
Spherical Quantization based Binary Embedding,"This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods."
Spherical Quantization based Binary Embedding,"This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods."
Spherical Quantization based Binary Embedding,"This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods."
Max-Margin Min-Entropy Hidden Conditional Random Fields,"We introduce the novel max-margin min-entropy hidden conditional random field (M$^3$E-HCRF), which encodes the conditional distribution over the latent variables and the single output variable given the input variables. The proposed M$^3$E-HCRF model provides a sparse and factorized representation of the conditional distribution. Given an observation, the M$^3$E-HCRF model infers the output by selecting the class label that minimizes the Renyi entropy of the unnormalized measure of the conditional distribution, which is equivalent to simultaneously (1) maximizing the conditional log-likelihood of the output given the inputs, and (2) minimizing the entropy of the conditional distribution of the hidden variables given the inputs and the output. The parameters of the proposed M$^3$E-HCRF model are learned by minimizing an $l_2$-regularized loss function, resulting in a non-convex optimization problem that can be solved by the non-convex bundle cutting plane algorithm. We evaluate our model's effectiveness on sequence labeling and structured learning using two public datasets, and demonstrate that our model achieves results comparable to the state of the art."
Max-Margin Min-Entropy Hidden Conditional Random Fields,"We introduce the novel max-margin min-entropy hidden conditional random field (M$^3$E-HCRF), which encodes the conditional distribution over the latent variables and the single output variable given the input variables. The proposed M$^3$E-HCRF model provides a sparse and factorized representation of the conditional distribution. Given an observation, the M$^3$E-HCRF model infers the output by selecting the class label that minimizes the Renyi entropy of the unnormalized measure of the conditional distribution, which is equivalent to simultaneously (1) maximizing the conditional log-likelihood of the output given the inputs, and (2) minimizing the entropy of the conditional distribution of the hidden variables given the inputs and the output. The parameters of the proposed M$^3$E-HCRF model are learned by minimizing an $l_2$-regularized loss function, resulting in a non-convex optimization problem that can be solved by the non-convex bundle cutting plane algorithm. We evaluate our model's effectiveness on sequence labeling and structured learning using two public datasets, and demonstrate that our model achieves results comparable to the state of the art."
Max-Margin Min-Entropy Hidden Conditional Random Fields,"We introduce the novel max-margin min-entropy hidden conditional random field (M$^3$E-HCRF), which encodes the conditional distribution over the latent variables and the single output variable given the input variables. The proposed M$^3$E-HCRF model provides a sparse and factorized representation of the conditional distribution. Given an observation, the M$^3$E-HCRF model infers the output by selecting the class label that minimizes the Renyi entropy of the unnormalized measure of the conditional distribution, which is equivalent to simultaneously (1) maximizing the conditional log-likelihood of the output given the inputs, and (2) minimizing the entropy of the conditional distribution of the hidden variables given the inputs and the output. The parameters of the proposed M$^3$E-HCRF model are learned by minimizing an $l_2$-regularized loss function, resulting in a non-convex optimization problem that can be solved by the non-convex bundle cutting plane algorithm. We evaluate our model's effectiveness on sequence labeling and structured learning using two public datasets, and demonstrate that our model achieves results comparable to the state of the art."
Optimal kernel choice for large-scale two-sample tests,"Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics."
Optimal kernel choice for large-scale two-sample tests,"Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics."
Optimal kernel choice for large-scale two-sample tests,"Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics."
Optimal kernel choice for large-scale two-sample tests,"Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics."
Optimal kernel choice for large-scale two-sample tests,"Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics."
Optimal kernel choice for large-scale two-sample tests,"Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics."
Optimal kernel choice for large-scale two-sample tests,"Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics."
Probabilistic Approximate Matrix Decomposition for Non-Negative Matrix Factorization,"This paper presents a new non-negative matrix factorization (NMF) algorithm called structured random (SR)-NMF. Probabilistic matrix decomposition is combined with NMF to generate a surrogate problem with a much lower dimension. After solving the compressed NMF problem, any standard NMF routine can be used to finish the factorization. The proposed method is evaluated in combination with several NMF algorithms on a variety of data sets. The new method combined with block principal pivoting (BPP) gives improved results. On the 300,000 article NY Times data set, in less than one tenth of the time, SR-NMF (followed by BPP) converges to the same accuracy as BPP alone. In general SR-NMF is faster than and at least as accurate as the existing evaluated methods. SR-NMF makes extremely large NMF problems tractable."
Laplacian Consistency,"Computing a faithful similarity/affinity metric is essential to many graph-based learning algorithms.In this paper, we propose a graph-based affinity learning method in an unsupervised scenario and show its application to shape retrieval, face clustering and web categorization.Our method, Laplacian Consistency  (LC), performs a dynamic diffusion process by propagating the similarity mass along the intrinsic manifold of data points.Convergence analysis is given and a closed-form solution is provided, making the LC process fast to calculate and easy to understand. Theoretical analysis shows our LC process only changes the eigenvalues gradually while keeping the eigenvector in the Laplacian spectral space. We also prove the superiority of our method from different points of views. Our method has nearly no parameter tuning and leads to significantly improved affinity maps, which help to greatly enhance the quality of various graph-based learning algorithms."
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space,"This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications."
Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space,"This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications."
Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space,"This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications."
Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space,"This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications."
A Generalized Kernel Approach to Structured Output Learning,"We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. We show that the existing KDE formulations are special cases of our framework. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on two structured output problems, and compare it to the state-of-the-art kernel-based structured output regression methods."
A Generalized Kernel Approach to Structured Output Learning,"We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. We show that the existing KDE formulations are special cases of our framework. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on two structured output problems, and compare it to the state-of-the-art kernel-based structured output regression methods."
A Generalized Kernel Approach to Structured Output Learning,"We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. We show that the existing KDE formulations are special cases of our framework. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on two structured output problems, and compare it to the state-of-the-art kernel-based structured output regression methods."
Learning from many experts: sparsity and model selection,"Experts classifying data are often imprecise. Recently, several models have been proposed to train classifiers using the noisy labels generated by these experts. Such models often have a large number of parameters, which can lead to overfitting. In order to avoid this and find better classifiers, we propose a new model which searches for sparse classifiers. We also develop a general method for model selection and apply it to optimize the tuning parameter in our model."
Learning from many experts: sparsity and model selection,"Experts classifying data are often imprecise. Recently, several models have been proposed to train classifiers using the noisy labels generated by these experts. Such models often have a large number of parameters, which can lead to overfitting. In order to avoid this and find better classifiers, we propose a new model which searches for sparse classifiers. We also develop a general method for model selection and apply it to optimize the tuning parameter in our model."
Learning-based Stereo Method using MMSE Estimation,"We model the stereo problem using a product of Gaussian mixture models(PGMM). This enables efficient sampling and optimization, which makes general parameter learning possible. The learning procedure, along with the strong modeling power of PGMM,  helps us to find prior model for stereo problem using training data. Another important contribution of this work is that the proposed method computes its solution via minimum-mean-squared-error(MMSE) estimation instead of maximum-a-posteriori(MAP) estimation. The benefits of this approach is two-fold: it utilizes the learned characteristics of our model better than MAP and the result is more robust to subtle errors in stereo model itself. Experimental results show that the performance of our method based on MMSE estimation is far better than that of MAP estimation with the same model, while achieving competitive quantitative evaluation score in comparison with other learning-based stereo methods."
Teaching Classification Tasks to Humans,"Given a classification task, what is the best way to teach the resulting boundary to a human? While machine learning techniques can provide excellent techniques for finding the boundary, they tell us little about how we would teach a human the same task. We propose to investigate the problem of example selection and presentation in the context of teaching humans, and explore a variety of mechanisms in the interests of finding what may work best. In particular, we begin with the baseline of random presentation and then examine combinations of several mechanisms: the indication of an example?s relative difficulty, the use of the shaping heuristic from the psychology literature (moving from easier examples to harder ones), and a novel kernel-based ?coverage model? of the subject?s mastery of the task. From our experiments on 53 human subjects learning classification tasks via our teaching system, we found that we can achieve the greatest gains with a combination of shaping and the coverage model."
Generalized Ambiguity Decomposition for Convex Ensembles of Experts and Arbitrary Differentiable Loss Functions,"The squared error of a convex ensemble of regressors is related to the squared error of the individual regressors and the diversity of the ensemble as measured by the weighted sum of squared errors of each regressor's prediction from the ensemble's prediction. This relationship, also known as ambiguity decomposition, highlights the impact of diversity on ensemble's performance for least squares regression. In this paper, we present a generalization of ambiguity decomposition that can be applied to any convex ensemble of experts under a differentiable loss function. The proposed decomposition is applicable to both classification and regression, and provides a task-driven notion of diversity. It is shown that the diversity term in this decomposition is scaled by a factor dependent on the instance and the loss function in a classical supervised learning setting. This lends support to the intuition that not all instances are equally important from a diversity perspective. We then derive the decomposition for some common regression and classification loss functions, and demonstrate its accuracy on different UCI datasets."
Generalized Ambiguity Decomposition for Convex Ensembles of Experts and Arbitrary Differentiable Loss Functions,"The squared error of a convex ensemble of regressors is related to the squared error of the individual regressors and the diversity of the ensemble as measured by the weighted sum of squared errors of each regressor's prediction from the ensemble's prediction. This relationship, also known as ambiguity decomposition, highlights the impact of diversity on ensemble's performance for least squares regression. In this paper, we present a generalization of ambiguity decomposition that can be applied to any convex ensemble of experts under a differentiable loss function. The proposed decomposition is applicable to both classification and regression, and provides a task-driven notion of diversity. It is shown that the diversity term in this decomposition is scaled by a factor dependent on the instance and the loss function in a classical supervised learning setting. This lends support to the intuition that not all instances are equally important from a diversity perspective. We then derive the decomposition for some common regression and classification loss functions, and demonstrate its accuracy on different UCI datasets."
Better Mixing via Deep Representations,"It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation.  To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples."
Better Mixing via Deep Representations,"It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation.  To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples."
Better Mixing via Deep Representations,"It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation.  To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples."
Better Mixing via Deep Representations,"It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation.  To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples."
Predicting Functional Cortical ROIs via Joint Modeling of Anatomical and Connectional Profiles,"Localization of functional cortical ROIs (regions of interests) in structural data such as DTI and MRI images has significant importance in basic and clinical neuroscience. However, this problem is challenging due to the lack of quantitative mapping between brain structure and function, which relies on both the availability of benchmark training data such as task-based fMRI and effective machine learning algorithms. This paper presents a novel joint modeling approach that learns predictive models of functional cortical ROIs from multimodal task-based fMRI, DTI and MRI datasets. In particular, the effective generalized multi-kernel learning algorithm was tailored to infer the intrinsic relationships between anatomical/connectional MRI/DTI features and fMRI-derived functional localizations. Then, the predictive models of functional cortical ROIs were evaluated by cross-validation studies, independent datasets and reproducibility studies, and experimental results are promising. We envision that these predictive models can be widely applied in the future in scenarios that have only DTI and/or MRI data, but without task-based fMRI data. "
Predicting Functional Cortical ROIs via Joint Modeling of Anatomical and Connectional Profiles,"Localization of functional cortical ROIs (regions of interests) in structural data such as DTI and MRI images has significant importance in basic and clinical neuroscience. However, this problem is challenging due to the lack of quantitative mapping between brain structure and function, which relies on both the availability of benchmark training data such as task-based fMRI and effective machine learning algorithms. This paper presents a novel joint modeling approach that learns predictive models of functional cortical ROIs from multimodal task-based fMRI, DTI and MRI datasets. In particular, the effective generalized multi-kernel learning algorithm was tailored to infer the intrinsic relationships between anatomical/connectional MRI/DTI features and fMRI-derived functional localizations. Then, the predictive models of functional cortical ROIs were evaluated by cross-validation studies, independent datasets and reproducibility studies, and experimental results are promising. We envision that these predictive models can be widely applied in the future in scenarios that have only DTI and/or MRI data, but without task-based fMRI data. "
Predicting Functional Cortical ROIs via Joint Modeling of Anatomical and Connectional Profiles,"Localization of functional cortical ROIs (regions of interests) in structural data such as DTI and MRI images has significant importance in basic and clinical neuroscience. However, this problem is challenging due to the lack of quantitative mapping between brain structure and function, which relies on both the availability of benchmark training data such as task-based fMRI and effective machine learning algorithms. This paper presents a novel joint modeling approach that learns predictive models of functional cortical ROIs from multimodal task-based fMRI, DTI and MRI datasets. In particular, the effective generalized multi-kernel learning algorithm was tailored to infer the intrinsic relationships between anatomical/connectional MRI/DTI features and fMRI-derived functional localizations. Then, the predictive models of functional cortical ROIs were evaluated by cross-validation studies, independent datasets and reproducibility studies, and experimental results are promising. We envision that these predictive models can be widely applied in the future in scenarios that have only DTI and/or MRI data, but without task-based fMRI data. "
Predicting Functional Cortical ROIs via Joint Modeling of Anatomical and Connectional Profiles,"Localization of functional cortical ROIs (regions of interests) in structural data such as DTI and MRI images has significant importance in basic and clinical neuroscience. However, this problem is challenging due to the lack of quantitative mapping between brain structure and function, which relies on both the availability of benchmark training data such as task-based fMRI and effective machine learning algorithms. This paper presents a novel joint modeling approach that learns predictive models of functional cortical ROIs from multimodal task-based fMRI, DTI and MRI datasets. In particular, the effective generalized multi-kernel learning algorithm was tailored to infer the intrinsic relationships between anatomical/connectional MRI/DTI features and fMRI-derived functional localizations. Then, the predictive models of functional cortical ROIs were evaluated by cross-validation studies, independent datasets and reproducibility studies, and experimental results are promising. We envision that these predictive models can be widely applied in the future in scenarios that have only DTI and/or MRI data, but without task-based fMRI data. "
Predicting Functional Cortical ROIs via Joint Modeling of Anatomical and Connectional Profiles,"Localization of functional cortical ROIs (regions of interests) in structural data such as DTI and MRI images has significant importance in basic and clinical neuroscience. However, this problem is challenging due to the lack of quantitative mapping between brain structure and function, which relies on both the availability of benchmark training data such as task-based fMRI and effective machine learning algorithms. This paper presents a novel joint modeling approach that learns predictive models of functional cortical ROIs from multimodal task-based fMRI, DTI and MRI datasets. In particular, the effective generalized multi-kernel learning algorithm was tailored to infer the intrinsic relationships between anatomical/connectional MRI/DTI features and fMRI-derived functional localizations. Then, the predictive models of functional cortical ROIs were evaluated by cross-validation studies, independent datasets and reproducibility studies, and experimental results are promising. We envision that these predictive models can be widely applied in the future in scenarios that have only DTI and/or MRI data, but without task-based fMRI data. "
Maximum Weight Subgraphs with Mutex Constraints,"In this paper, we propose a novel algorithm for computing maximum weight subgraphs(MWSs) that satisfy mutex constraints on a weighted graph. As opposedto commonly used linear equality constraints, the mutex constraints expressedin a quadratic equality form allow for a greater modeling flexibility, which canbe beneficial in many applications. Although the proposed algorithm solves arelaxed formulation of MWS problem, it obtains a discrete solution in all ourexperiments on real data, which in turn guarantees that the solution satisfies themutex constraints. We evaluated our algorithm on two hard combinatorial problems:matching of salient points under perspective and nonrigid distortion andsolving image jigsaw puzzles. It significantly outperforms known state-of-the-artalgorithms, including loopy believe propagation and Integer Projected Fixed PointMethod (IPFP), even if it is restricted to using only constraints equivalent to linearequality constraints."
Pareto-Path Multi-Task Multiple Kernel Learning,"Traditional Multi-Task Multiple Kernel Learning (MT-MKL) methods routinely optimize the sum (thus, the average) of objective functions to simultaneously improve performances for all tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO) problem, which considers the concurrent optimization of all task objectives involved in the Multi-Task Learning problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel Support Vector Machine (SVM) MT-MKL framework, that considers an implicitly-defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of improving performances uniformly over tasks, when compared to the traditional MTL approach."
Pareto-Path Multi-Task Multiple Kernel Learning,"Traditional Multi-Task Multiple Kernel Learning (MT-MKL) methods routinely optimize the sum (thus, the average) of objective functions to simultaneously improve performances for all tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO) problem, which considers the concurrent optimization of all task objectives involved in the Multi-Task Learning problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel Support Vector Machine (SVM) MT-MKL framework, that considers an implicitly-defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of improving performances uniformly over tasks, when compared to the traditional MTL approach."
A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation,"A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lovasz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset."
Fused sparsity and robust estimation for linear models with unknown variance,"In this paper, we develop a novel approach to the problem of learning sparserepresentations in the context of fused sparsity and unknown noise level. We proposean algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes theaforementioned learning task by means of a second-order cone program. A special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers. We establish finite sample risk bounds and carry out an experimental evaluation on both synthetic and real data."
Fused sparsity and robust estimation for linear models with unknown variance,"In this paper, we develop a novel approach to the problem of learning sparserepresentations in the context of fused sparsity and unknown noise level. We proposean algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes theaforementioned learning task by means of a second-order cone program. A special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers. We establish finite sample risk bounds and carry out an experimental evaluation on both synthetic and real data."
Exact and Efficient Parallel Inference for Nonparametric Mixture Models,"Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to sample from the true posterior in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods."
Iterative Learning in Modular Associative Memories,"The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a  retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance  can be made linear in the size of the network. "
Iterative Learning in Modular Associative Memories,"The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a  retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance  can be made linear in the size of the network. "
Iterative Learning in Modular Associative Memories,"The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a  retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance  can be made linear in the size of the network. "
Biased perception leads to biased action: Validating a Bayesian model of interception," We tested whether and how biases in visual perception might influence motor actions. To do so, we designed an  interception task where subjects had to indicate the time when a moving object, whose trajectory was occluded from the  subjects, would reach a target-area. Subjects made their judgements based on a brief display of the objects initial  motion at a starting point. Based on the known illusion that slow contrast stimuli appear to move slower than high  contrast ones, we predict that if perception directly influences motion actions subjects would show delayed  interception times for low contrast objects. In order to provide a more quantitative prediction, we developed a Bayesian  model for the complete sensory-motor interception task. Using fit parameters for the prior and likelihood on visual  speed from a previous study we were able to predict not only the expected interception times but also the precise  characteristics of response variability. Psychophysical experiments confirm the model's predictions. Individual  differences in subjects timing response can be accounted for by individual differences in the perceptual priors on  visual speed. Taken together, our behavioral and model results show that biases in perception percolate downstream to  bias action response in a predictable manner. Furthermore, our work emphasizes that the Bayesian model of speed  perception is generalizable to new domains."
High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer Disease Progression Prediction,"Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.  "
High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer Disease Progression Prediction,"Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.  "
High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer Disease Progression Prediction,"Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.  "
High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer Disease Progression Prediction,"Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.  "
High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer Disease Progression Prediction,"Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.  "
Discriminative Learning of Infinite Latent Variable Models,"We propose probabilistic models to infer discriminative latent variables in Hamming space from observed data. Our models allow for a simultaneous inference of the dimension of the binary latent variables, and their entries values. Further, the latent variables are discriminative in the sense that objects in the same category or semantic concept have similar latent values, and objects in different categories have dis-similar latent values. The inferred latent variables can be directly used to perform a nearest neighbour search for the purpose of classification or retrieval. We formulate this discriminative infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the proposed method is able to find semantically similar neighbours due to the discriminative nature of the latent space. The coupling structure of the inferred latent space lends itself to an application of extending hash codes in a discriminative way."
Discriminative Learning of Infinite Latent Variable Models,"We propose probabilistic models to infer discriminative latent variables in Hamming space from observed data. Our models allow for a simultaneous inference of the dimension of the binary latent variables, and their entries values. Further, the latent variables are discriminative in the sense that objects in the same category or semantic concept have similar latent values, and objects in different categories have dis-similar latent values. The inferred latent variables can be directly used to perform a nearest neighbour search for the purpose of classification or retrieval. We formulate this discriminative infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the proposed method is able to find semantically similar neighbours due to the discriminative nature of the latent space. The coupling structure of the inferred latent space lends itself to an application of extending hash codes in a discriminative way."
Discriminative Learning of Infinite Latent Variable Models,"We propose probabilistic models to infer discriminative latent variables in Hamming space from observed data. Our models allow for a simultaneous inference of the dimension of the binary latent variables, and their entries values. Further, the latent variables are discriminative in the sense that objects in the same category or semantic concept have similar latent values, and objects in different categories have dis-similar latent values. The inferred latent variables can be directly used to perform a nearest neighbour search for the purpose of classification or retrieval. We formulate this discriminative infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the proposed method is able to find semantically similar neighbours due to the discriminative nature of the latent space. The coupling structure of the inferred latent space lends itself to an application of extending hash codes in a discriminative way."
Discriminative Learning of Infinite Latent Variable Models,"We propose probabilistic models to infer discriminative latent variables in Hamming space from observed data. Our models allow for a simultaneous inference of the dimension of the binary latent variables, and their entries values. Further, the latent variables are discriminative in the sense that objects in the same category or semantic concept have similar latent values, and objects in different categories have dis-similar latent values. The inferred latent variables can be directly used to perform a nearest neighbour search for the purpose of classification or retrieval. We formulate this discriminative infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the proposed method is able to find semantically similar neighbours due to the discriminative nature of the latent space. The coupling structure of the inferred latent space lends itself to an application of extending hash codes in a discriminative way."
Compressed Sparse Concept Coding for Large Scale Data Representation,"Data representation is a fundamental problem in various research fields.When representing data as vectors, the feature space is usually of very high dimensionality, which makes itdifficult for applying learning algorithms for analysis.One then hope to apply matrix factorization techniques,such as Singular Vector Decomposition (SVD) to learnthe low dimensional hidden concept space. Among various techniques, sparse coding receives considerableinterests in recent years because its sparse representation leads to an elegant interpretation.However, most of the existing sparse coding algorithms are computational expensive since theycompute the basis vectors and the representations iteratively. Moreover, all the existing methodsare linear and not be able to capture the non-linear structure of the data. To tackle these issues, wepropose a novel sparse coding method, called {\em Compressed Sparse Concept Coding} (CSCC), for largescale data representation in this paper. Our method is non-linear and scales linearly with the numberof samples. Extensive experimental results on real world applicationsdemonstrate the effectiveness and efficiency of the proposed approach."
Multi-Relational Learning via Hierarchical Nonparametric Bayesian Collective Matrix Factorization,"Relational learning addresses problems where the data come from multiple sources and are linked together throughcomplex relational networks. Two important goals are pattern discovery (e.g. by (co)-clustering) and predicting unknown values of a relation, given a set of entities and observed relations among entities. In the presence of multiple relations, combining information from different but related relations can lead to better insights and improved prediction. For this purpose we propose a nonparametric hierarchical Bayesian model that improves on existing collaborative factorization models and frames a large number of relational learning problems. The proposed model naturally incorporates (co)-clustering and prediction analysis in a single unified framework, and allows for the estimation of entire missing row or column vectors. We develop an efficient Gibbs algorithm and a hybrid Gibbs using Newton?s method to enable fast computation in high dimensions. We demonstrate the value of our framework on simulated experiments as well as two real world problems: discovering kinship systems and predicting the authors of certain articles based on article-word co-occurrence features."
Multi-Relational Learning via Hierarchical Nonparametric Bayesian Collective Matrix Factorization,"Relational learning addresses problems where the data come from multiple sources and are linked together throughcomplex relational networks. Two important goals are pattern discovery (e.g. by (co)-clustering) and predicting unknown values of a relation, given a set of entities and observed relations among entities. In the presence of multiple relations, combining information from different but related relations can lead to better insights and improved prediction. For this purpose we propose a nonparametric hierarchical Bayesian model that improves on existing collaborative factorization models and frames a large number of relational learning problems. The proposed model naturally incorporates (co)-clustering and prediction analysis in a single unified framework, and allows for the estimation of entire missing row or column vectors. We develop an efficient Gibbs algorithm and a hybrid Gibbs using Newton?s method to enable fast computation in high dimensions. We demonstrate the value of our framework on simulated experiments as well as two real world problems: discovering kinship systems and predicting the authors of certain articles based on article-word co-occurrence features."
Functional Brain Interactions during Free Viewing of Video Stream,"Natural stimulus fMRI (N-fMRI) such as free viewing of video streams provides an uncontrolled environment to study the human brain's perception and cognition engaged in natural scene comprehension. Hence, it is receiving increasing interest in neuroimaging and multimedia analysis in recent years. In these fields, researchers rely on consistent and discriminative functional interactions such as functional or effective connectivity to measure the human brain's responses. However, the computational cost increases significantly in model-driven methods such as the dynamic causal modeling (DCM) when the cortical regions of interests (ROIs) are dense (e.g., 358 in this paper). In this paper, we present a data-driven computational pipeline to explore consistent and discriminative functional interactions during free viewing of video. The underlying premise is that the functional interactions, characterizing the semantic content of video samples in multiple categories and derived from N-fMRI data of multiple subjects, are simultaneously selected by multiple feature selection methods to pose both consistency and discriminativity. Then the spatial distribution of the ROIs involved in the identified interactions and the distribution of the functional sub-networks associated with the ROIs are assessed. Meanwhile, structural connectivity derived from diffusion tensor imaging (DTI) and video classification is used to evaluate the consistency and discriminativity of the identified functional interactions, respectively. Our findings provide new insights into the functional mechanism of the human brain in perception and cognition of complex natural scenes."
Functional Brain Interactions during Free Viewing of Video Stream,"Natural stimulus fMRI (N-fMRI) such as free viewing of video streams provides an uncontrolled environment to study the human brain's perception and cognition engaged in natural scene comprehension. Hence, it is receiving increasing interest in neuroimaging and multimedia analysis in recent years. In these fields, researchers rely on consistent and discriminative functional interactions such as functional or effective connectivity to measure the human brain's responses. However, the computational cost increases significantly in model-driven methods such as the dynamic causal modeling (DCM) when the cortical regions of interests (ROIs) are dense (e.g., 358 in this paper). In this paper, we present a data-driven computational pipeline to explore consistent and discriminative functional interactions during free viewing of video. The underlying premise is that the functional interactions, characterizing the semantic content of video samples in multiple categories and derived from N-fMRI data of multiple subjects, are simultaneously selected by multiple feature selection methods to pose both consistency and discriminativity. Then the spatial distribution of the ROIs involved in the identified interactions and the distribution of the functional sub-networks associated with the ROIs are assessed. Meanwhile, structural connectivity derived from diffusion tensor imaging (DTI) and video classification is used to evaluate the consistency and discriminativity of the identified functional interactions, respectively. Our findings provide new insights into the functional mechanism of the human brain in perception and cognition of complex natural scenes."
Functional Brain Interactions during Free Viewing of Video Stream,"Natural stimulus fMRI (N-fMRI) such as free viewing of video streams provides an uncontrolled environment to study the human brain's perception and cognition engaged in natural scene comprehension. Hence, it is receiving increasing interest in neuroimaging and multimedia analysis in recent years. In these fields, researchers rely on consistent and discriminative functional interactions such as functional or effective connectivity to measure the human brain's responses. However, the computational cost increases significantly in model-driven methods such as the dynamic causal modeling (DCM) when the cortical regions of interests (ROIs) are dense (e.g., 358 in this paper). In this paper, we present a data-driven computational pipeline to explore consistent and discriminative functional interactions during free viewing of video. The underlying premise is that the functional interactions, characterizing the semantic content of video samples in multiple categories and derived from N-fMRI data of multiple subjects, are simultaneously selected by multiple feature selection methods to pose both consistency and discriminativity. Then the spatial distribution of the ROIs involved in the identified interactions and the distribution of the functional sub-networks associated with the ROIs are assessed. Meanwhile, structural connectivity derived from diffusion tensor imaging (DTI) and video classification is used to evaluate the consistency and discriminativity of the identified functional interactions, respectively. Our findings provide new insights into the functional mechanism of the human brain in perception and cognition of complex natural scenes."
Functional Brain Interactions during Free Viewing of Video Stream,"Natural stimulus fMRI (N-fMRI) such as free viewing of video streams provides an uncontrolled environment to study the human brain's perception and cognition engaged in natural scene comprehension. Hence, it is receiving increasing interest in neuroimaging and multimedia analysis in recent years. In these fields, researchers rely on consistent and discriminative functional interactions such as functional or effective connectivity to measure the human brain's responses. However, the computational cost increases significantly in model-driven methods such as the dynamic causal modeling (DCM) when the cortical regions of interests (ROIs) are dense (e.g., 358 in this paper). In this paper, we present a data-driven computational pipeline to explore consistent and discriminative functional interactions during free viewing of video. The underlying premise is that the functional interactions, characterizing the semantic content of video samples in multiple categories and derived from N-fMRI data of multiple subjects, are simultaneously selected by multiple feature selection methods to pose both consistency and discriminativity. Then the spatial distribution of the ROIs involved in the identified interactions and the distribution of the functional sub-networks associated with the ROIs are assessed. Meanwhile, structural connectivity derived from diffusion tensor imaging (DTI) and video classification is used to evaluate the consistency and discriminativity of the identified functional interactions, respectively. Our findings provide new insights into the functional mechanism of the human brain in perception and cognition of complex natural scenes."
Functional Brain Interactions during Free Viewing of Video Stream,"Natural stimulus fMRI (N-fMRI) such as free viewing of video streams provides an uncontrolled environment to study the human brain's perception and cognition engaged in natural scene comprehension. Hence, it is receiving increasing interest in neuroimaging and multimedia analysis in recent years. In these fields, researchers rely on consistent and discriminative functional interactions such as functional or effective connectivity to measure the human brain's responses. However, the computational cost increases significantly in model-driven methods such as the dynamic causal modeling (DCM) when the cortical regions of interests (ROIs) are dense (e.g., 358 in this paper). In this paper, we present a data-driven computational pipeline to explore consistent and discriminative functional interactions during free viewing of video. The underlying premise is that the functional interactions, characterizing the semantic content of video samples in multiple categories and derived from N-fMRI data of multiple subjects, are simultaneously selected by multiple feature selection methods to pose both consistency and discriminativity. Then the spatial distribution of the ROIs involved in the identified interactions and the distribution of the functional sub-networks associated with the ROIs are assessed. Meanwhile, structural connectivity derived from diffusion tensor imaging (DTI) and video classification is used to evaluate the consistency and discriminativity of the identified functional interactions, respectively. Our findings provide new insights into the functional mechanism of the human brain in perception and cognition of complex natural scenes."
Constructing Deep Neural Networks via the Extended Restricted Boltzmann Machines,"We exploit an uniform training algorithm for the extended restricted Boltzmann machines (ERBM) to initialize the parameters of a deep neural network (DNN). Due to the conservative energy-based generative model and the intractable samples from the model distribution, the restricted Boltzmann machines (RBM) is extended as the following two aspects. Firstly, using a new method that introduces the free-weighting matrices, a novel energy-based generative model is proposed to establish an excellent RBM. To adapt to different high-dimensional data distribution and to speed up the Contrastive Divergence (CD), secondly we use the normal and no-sampling methods instead of the uniform-sampling method in CD. Our experiments finally confirm that the free-weighting matrices have important positive effects on DNN in face and MNIST datasets and DNN with no-sampling method can reconstruct better image than one with normal and uniform-sampling methods in face datasets."
Constructing Deep Neural Networks via the Extended Restricted Boltzmann Machines,"We exploit an uniform training algorithm for the extended restricted Boltzmann machines (ERBM) to initialize the parameters of a deep neural network (DNN). Due to the conservative energy-based generative model and the intractable samples from the model distribution, the restricted Boltzmann machines (RBM) is extended as the following two aspects. Firstly, using a new method that introduces the free-weighting matrices, a novel energy-based generative model is proposed to establish an excellent RBM. To adapt to different high-dimensional data distribution and to speed up the Contrastive Divergence (CD), secondly we use the normal and no-sampling methods instead of the uniform-sampling method in CD. Our experiments finally confirm that the free-weighting matrices have important positive effects on DNN in face and MNIST datasets and DNN with no-sampling method can reconstruct better image than one with normal and uniform-sampling methods in face datasets."
Constructing Deep Neural Networks via the Extended Restricted Boltzmann Machines,"We exploit an uniform training algorithm for the extended restricted Boltzmann machines (ERBM) to initialize the parameters of a deep neural network (DNN). Due to the conservative energy-based generative model and the intractable samples from the model distribution, the restricted Boltzmann machines (RBM) is extended as the following two aspects. Firstly, using a new method that introduces the free-weighting matrices, a novel energy-based generative model is proposed to establish an excellent RBM. To adapt to different high-dimensional data distribution and to speed up the Contrastive Divergence (CD), secondly we use the normal and no-sampling methods instead of the uniform-sampling method in CD. Our experiments finally confirm that the free-weighting matrices have important positive effects on DNN in face and MNIST datasets and DNN with no-sampling method can reconstruct better image than one with normal and uniform-sampling methods in face datasets."
Spatial-Visual Label Propagation for Local Feature Classification,In this paper we propose a novel approach to integrate feature similarity and spatial consistency of local features to achieve coherent and accurate labeling of feature points in a simple and effective way. We introduced our Spatial-Visual Label Propagation algorithm to infer the labels of local features in a test image from known labels. This is done in a transductive manner to provide spatial and feature smoothing over the learned labels. We show the value of our novel approach by a diverse set of experiments with successful improvements over previous methods and baseline classifiers.
Spatial-Visual Label Propagation for Local Feature Classification,In this paper we propose a novel approach to integrate feature similarity and spatial consistency of local features to achieve coherent and accurate labeling of feature points in a simple and effective way. We introduced our Spatial-Visual Label Propagation algorithm to infer the labels of local features in a test image from known labels. This is done in a transductive manner to provide spatial and feature smoothing over the learned labels. We show the value of our novel approach by a diverse set of experiments with successful improvements over previous methods and baseline classifiers.
Spatial-Visual Label Propagation for Local Feature Classification,In this paper we propose a novel approach to integrate feature similarity and spatial consistency of local features to achieve coherent and accurate labeling of feature points in a simple and effective way. We introduced our Spatial-Visual Label Propagation algorithm to infer the labels of local features in a test image from known labels. This is done in a transductive manner to provide spatial and feature smoothing over the learned labels. We show the value of our novel approach by a diverse set of experiments with successful improvements over previous methods and baseline classifiers.
Fixed-Point Model For Structured Labeling,"In this paper, we propose a simple but effective method to the structured labelingproblem: a fixed-pointmodel. Recently, layered models with sequential classifiers(regressors) have gained an increasing amount of interests for structural predictiondue to their capability in capturing a large degree of contexts and correlations. Inthis paper, we design an algorithm with a new perspective on layered models andstructured (contexts-based) learning problem; we provide a fixed-point functionwith the structured labelings being both the output and the input; it alleviates theburden in learning multiple/different classifiers in different layers. We devise atraining and testing strategy for our method and provide conditions/justificationsfor the fixed-point function for being a contraction mapping for the convergence.The learned function captures rich contextual information and is easy to train andtest. On three well-known structured prediction problems, the Optical CharacterRecognition (OCR) task, the Part of Speech (POS) task, and the Hypertext Classificationtask, the proposed fixed-point model observes significant improvement intraining efficiency and, at the same time, achieves comparable (most often better)prediction results over the state-of-the-art algorithms."
Fixed-Point Model For Structured Labeling,"In this paper, we propose a simple but effective method to the structured labelingproblem: a fixed-pointmodel. Recently, layered models with sequential classifiers(regressors) have gained an increasing amount of interests for structural predictiondue to their capability in capturing a large degree of contexts and correlations. Inthis paper, we design an algorithm with a new perspective on layered models andstructured (contexts-based) learning problem; we provide a fixed-point functionwith the structured labelings being both the output and the input; it alleviates theburden in learning multiple/different classifiers in different layers. We devise atraining and testing strategy for our method and provide conditions/justificationsfor the fixed-point function for being a contraction mapping for the convergence.The learned function captures rich contextual information and is easy to train andtest. On three well-known structured prediction problems, the Optical CharacterRecognition (OCR) task, the Part of Speech (POS) task, and the Hypertext Classificationtask, the proposed fixed-point model observes significant improvement intraining efficiency and, at the same time, achieves comparable (most often better)prediction results over the state-of-the-art algorithms."
Fixed-Point Model For Structured Labeling,"In this paper, we propose a simple but effective method to the structured labelingproblem: a fixed-pointmodel. Recently, layered models with sequential classifiers(regressors) have gained an increasing amount of interests for structural predictiondue to their capability in capturing a large degree of contexts and correlations. Inthis paper, we design an algorithm with a new perspective on layered models andstructured (contexts-based) learning problem; we provide a fixed-point functionwith the structured labelings being both the output and the input; it alleviates theburden in learning multiple/different classifiers in different layers. We devise atraining and testing strategy for our method and provide conditions/justificationsfor the fixed-point function for being a contraction mapping for the convergence.The learned function captures rich contextual information and is easy to train andtest. On three well-known structured prediction problems, the Optical CharacterRecognition (OCR) task, the Part of Speech (POS) task, and the Hypertext Classificationtask, the proposed fixed-point model observes significant improvement intraining efficiency and, at the same time, achieves comparable (most often better)prediction results over the state-of-the-art algorithms."
Decision-theoretic Sparsification for Gaussian Process Preference Learning,"We propose a decision-theoretic sparsification method for Gaussian process preference learning.  This method overcomes the loss-insensitive nature of popular sparsification approaches such as the Informative Vector Machine (IVM). Instead of selecting a subset of users and items as inducing points based on uncertainty-reduction principles, our sparsification approach is underpinned by decision theory and directly incorporates the loss function inherent to the underlying preference learning problem.  We show that by selecting different specifications of the loss function, the IVM's differential entropy criterion, a value of information criterion, and an upper confidence bound (UCB) criterion used in the bandit setting can all be recovered from our decision-theoretic framework.  We refer to our method as the Valuable Vector Machine (VVM) as it selects the most useful items during sparsification to minimize the corresponding loss.  Experiments show that variants of the VVM outperform  the IVM under similar computational constraints."
Decision-theoretic Sparsification for Gaussian Process Preference Learning,"We propose a decision-theoretic sparsification method for Gaussian process preference learning.  This method overcomes the loss-insensitive nature of popular sparsification approaches such as the Informative Vector Machine (IVM). Instead of selecting a subset of users and items as inducing points based on uncertainty-reduction principles, our sparsification approach is underpinned by decision theory and directly incorporates the loss function inherent to the underlying preference learning problem.  We show that by selecting different specifications of the loss function, the IVM's differential entropy criterion, a value of information criterion, and an upper confidence bound (UCB) criterion used in the bandit setting can all be recovered from our decision-theoretic framework.  We refer to our method as the Valuable Vector Machine (VVM) as it selects the most useful items during sparsification to minimize the corresponding loss.  Experiments show that variants of the VVM outperform  the IVM under similar computational constraints."
Decision-theoretic Sparsification for Gaussian Process Preference Learning,"We propose a decision-theoretic sparsification method for Gaussian process preference learning.  This method overcomes the loss-insensitive nature of popular sparsification approaches such as the Informative Vector Machine (IVM). Instead of selecting a subset of users and items as inducing points based on uncertainty-reduction principles, our sparsification approach is underpinned by decision theory and directly incorporates the loss function inherent to the underlying preference learning problem.  We show that by selecting different specifications of the loss function, the IVM's differential entropy criterion, a value of information criterion, and an upper confidence bound (UCB) criterion used in the bandit setting can all be recovered from our decision-theoretic framework.  We refer to our method as the Valuable Vector Machine (VVM) as it selects the most useful items during sparsification to minimize the corresponding loss.  Experiments show that variants of the VVM outperform  the IVM under similar computational constraints."
Modeling Fashion,"We propose a method to try to model fashionable dresses in this paper. We first discover common visual patterns that appear in dress images using a human-in-the-loop, active clustering approach. A fashionable dress is expected to contain certain visual patterns which make it fashionable. An approach is proposed to jointly identify fashionable visual patterns and learn a discriminative fashion classifier. The results show that interesting fashionable patterns can be discovered on a newly collected dress dataset. Our model can also achieve high accuracy on distinguishing fashionable and unfashionable dresses. We test visual pattern centric dress retrieval, which is promising and interesting for visual shopping."
Modeling Fashion,"We propose a method to try to model fashionable dresses in this paper. We first discover common visual patterns that appear in dress images using a human-in-the-loop, active clustering approach. A fashionable dress is expected to contain certain visual patterns which make it fashionable. An approach is proposed to jointly identify fashionable visual patterns and learn a discriminative fashion classifier. The results show that interesting fashionable patterns can be discovered on a newly collected dress dataset. Our model can also achieve high accuracy on distinguishing fashionable and unfashionable dresses. We test visual pattern centric dress retrieval, which is promising and interesting for visual shopping."
Modeling Fashion,"We propose a method to try to model fashionable dresses in this paper. We first discover common visual patterns that appear in dress images using a human-in-the-loop, active clustering approach. A fashionable dress is expected to contain certain visual patterns which make it fashionable. An approach is proposed to jointly identify fashionable visual patterns and learn a discriminative fashion classifier. The results show that interesting fashionable patterns can be discovered on a newly collected dress dataset. Our model can also achieve high accuracy on distinguishing fashionable and unfashionable dresses. We test visual pattern centric dress retrieval, which is promising and interesting for visual shopping."
Symmetric Correspondence Topic Models for Multilingual Text Analysis,"Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models."
Symmetric Correspondence Topic Models for Multilingual Text Analysis,"Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models."
Efficient coding connects prior and likelihood function in perceptual Bayesian inference,"  A common challenge for Bayesian approaches in modeling perceptual behavior is the fact that the two fundamental  components of a Bayesian model, the prior distribution and the likelihood function, are formally unconstrained. Here  we argue that a neural system that emulates Bayesian inference naturally imposes constraints by way of how it  represents sensory information in populations of neurons. More specifically, we propose an efficient encoding  principle that constrains both the likelihood and the prior based on low-level environmental statistics. The resulting  Bayesian estimates can show biases away from the peaks of a prior distribution, a behavior seemingly at odds  with the traditional view of Bayesian estimates yet one that has indeed been reported in human perception of visual orientation. We demonstrate that our framework correctly predicts these biases, and show  that the efficient encoding characteristics of the model neural population matches the reported orientation tuning  characteristics of neurons in primary visual cortex. Our results suggest that efficient coding can be a promising  hypothesis in constraining neural implementations of Bayesian inference."
Efficient Sampling for Bipartite Matching Problems,"Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in real-world applications of these problems is intractable, making efficient approximation methods essential for learning and inference. In this paper we propose a novel {\it sequential matching} sampler based on the generalization of the Plackett-Luce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems - ranking and image correspondence - which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches."
Efficient Sampling for Bipartite Matching Problems,"Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in real-world applications of these problems is intractable, making efficient approximation methods essential for learning and inference. In this paper we propose a novel {\it sequential matching} sampler based on the generalization of the Plackett-Luce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems - ranking and image correspondence - which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches."
Learning visual motion in recurrent neural networks,"We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model."
Learning visual motion in recurrent neural networks,"We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model."
Tensor Analyzers,"Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its \emph{additive} nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact \emph{multiplicatively}. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe a fairly efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches and of images containing a variety of simple shapes that vary in size and color. Tensor Analyzers can also accurately recognize a face under significantly pose and illumination variations when given only one previous image of that face. We also show that Mixtures of Tensor Analyzers outperform Mixtures of Factor Analyzers at modeling natural image patches and artificial data produced using multiplicative interactions."
Tensor Analyzers,"Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its \emph{additive} nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact \emph{multiplicatively}. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe a fairly efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches and of images containing a variety of simple shapes that vary in size and color. Tensor Analyzers can also accurately recognize a face under significantly pose and illumination variations when given only one previous image of that face. We also show that Mixtures of Tensor Analyzers outperform Mixtures of Factor Analyzers at modeling natural image patches and artificial data produced using multiplicative interactions."
Tensor Analyzers,"Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its \emph{additive} nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact \emph{multiplicatively}. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe a fairly efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches and of images containing a variety of simple shapes that vary in size and color. Tensor Analyzers can also accurately recognize a face under significantly pose and illumination variations when given only one previous image of that face. We also show that Mixtures of Tensor Analyzers outperform Mixtures of Factor Analyzers at modeling natural image patches and artificial data produced using multiplicative interactions."
Online Learning for Auction Mechanism in Bandit Setting,"This paper is concerned with the online learning of the optimal auction mechanism for sponsored search in a bandit setting. We point out that this task corresponds to a new type of bandit problem, which we call the \textit{armed bandit problem with shared information} (AB-SI). In the AB-SI problem, the arm space (corresponding to the parameter space of the auction mechanism which can be discrete or continuous) is partitioned into a finite number of clusters (corresponding to the finite number of rankings of the ads), and the arms in the same cluster share the explored information (i.e., the click-through rates of the ads in the same ranked list) when any arm from the cluster is pulled. We propose an upper confidence bound algorithm called UCB-SI to tackle this new problem. We show that when the total number of arms is finite, the regret bound obtained by our proposed algorithm is tighter than the classical UCB algorithm. In the continuum armed bandit setting, our algorithm can handle a larger classes of reward function and achieve a reasonable regret bound of $O(T^{2/3}(d\ln T)^{1/3})$, where $d$ is the pseudo dimension for the real-valued reward function class."
Online Learning for Auction Mechanism in Bandit Setting,"This paper is concerned with the online learning of the optimal auction mechanism for sponsored search in a bandit setting. We point out that this task corresponds to a new type of bandit problem, which we call the \textit{armed bandit problem with shared information} (AB-SI). In the AB-SI problem, the arm space (corresponding to the parameter space of the auction mechanism which can be discrete or continuous) is partitioned into a finite number of clusters (corresponding to the finite number of rankings of the ads), and the arms in the same cluster share the explored information (i.e., the click-through rates of the ads in the same ranked list) when any arm from the cluster is pulled. We propose an upper confidence bound algorithm called UCB-SI to tackle this new problem. We show that when the total number of arms is finite, the regret bound obtained by our proposed algorithm is tighter than the classical UCB algorithm. In the continuum armed bandit setting, our algorithm can handle a larger classes of reward function and achieve a reasonable regret bound of $O(T^{2/3}(d\ln T)^{1/3})$, where $d$ is the pseudo dimension for the real-valued reward function class."
Online Learning for Auction Mechanism in Bandit Setting,"This paper is concerned with the online learning of the optimal auction mechanism for sponsored search in a bandit setting. We point out that this task corresponds to a new type of bandit problem, which we call the \textit{armed bandit problem with shared information} (AB-SI). In the AB-SI problem, the arm space (corresponding to the parameter space of the auction mechanism which can be discrete or continuous) is partitioned into a finite number of clusters (corresponding to the finite number of rankings of the ads), and the arms in the same cluster share the explored information (i.e., the click-through rates of the ads in the same ranked list) when any arm from the cluster is pulled. We propose an upper confidence bound algorithm called UCB-SI to tackle this new problem. We show that when the total number of arms is finite, the regret bound obtained by our proposed algorithm is tighter than the classical UCB algorithm. In the continuum armed bandit setting, our algorithm can handle a larger classes of reward function and achieve a reasonable regret bound of $O(T^{2/3}(d\ln T)^{1/3})$, where $d$ is the pseudo dimension for the real-valued reward function class."
Online Learning for Auction Mechanism in Bandit Setting,"This paper is concerned with the online learning of the optimal auction mechanism for sponsored search in a bandit setting. We point out that this task corresponds to a new type of bandit problem, which we call the \textit{armed bandit problem with shared information} (AB-SI). In the AB-SI problem, the arm space (corresponding to the parameter space of the auction mechanism which can be discrete or continuous) is partitioned into a finite number of clusters (corresponding to the finite number of rankings of the ads), and the arms in the same cluster share the explored information (i.e., the click-through rates of the ads in the same ranked list) when any arm from the cluster is pulled. We propose an upper confidence bound algorithm called UCB-SI to tackle this new problem. We show that when the total number of arms is finite, the regret bound obtained by our proposed algorithm is tighter than the classical UCB algorithm. In the continuum armed bandit setting, our algorithm can handle a larger classes of reward function and achieve a reasonable regret bound of $O(T^{2/3}(d\ln T)^{1/3})$, where $d$ is the pseudo dimension for the real-valued reward function class."
Large-Margin Tensor Decomposition for Multi-Relational Learning,"We propose a novel large-margin framework for multi-relational learning via tensor decomposition. In this setting, the training data consists of multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a transformed linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using a weighted objective function. The objective combines multiple task-specific loss functions, to accommodate different types of relations. For the typical cases of real-valued functions and binary relations, we propose a combination of quadratic and smooth hinge losses and derive the associated parameter gradients. We solve the resulting optimization problem using memory efficient quasi-Newton methods. We evaluate our method on synthetic and real data, showing that it obtains significant accuracy improvement over related techniques even when training data is limited. Further, we show that our decomposition is able to transfer information across the various relations, thus better exploiting the multi-relational structure."
Large-Margin Tensor Decomposition for Multi-Relational Learning,"We propose a novel large-margin framework for multi-relational learning via tensor decomposition. In this setting, the training data consists of multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a transformed linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using a weighted objective function. The objective combines multiple task-specific loss functions, to accommodate different types of relations. For the typical cases of real-valued functions and binary relations, we propose a combination of quadratic and smooth hinge losses and derive the associated parameter gradients. We solve the resulting optimization problem using memory efficient quasi-Newton methods. We evaluate our method on synthetic and real data, showing that it obtains significant accuracy improvement over related techniques even when training data is limited. Further, we show that our decomposition is able to transfer information across the various relations, thus better exploiting the multi-relational structure."
Large-Margin Tensor Decomposition for Multi-Relational Learning,"We propose a novel large-margin framework for multi-relational learning via tensor decomposition. In this setting, the training data consists of multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a transformed linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using a weighted objective function. The objective combines multiple task-specific loss functions, to accommodate different types of relations. For the typical cases of real-valued functions and binary relations, we propose a combination of quadratic and smooth hinge losses and derive the associated parameter gradients. We solve the resulting optimization problem using memory efficient quasi-Newton methods. We evaluate our method on synthetic and real data, showing that it obtains significant accuracy improvement over related techniques even when training data is limited. Further, we show that our decomposition is able to transfer information across the various relations, thus better exploiting the multi-relational structure."
Large-Margin Tensor Decomposition for Multi-Relational Learning,"We propose a novel large-margin framework for multi-relational learning via tensor decomposition. In this setting, the training data consists of multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a transformed linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using a weighted objective function. The objective combines multiple task-specific loss functions, to accommodate different types of relations. For the typical cases of real-valued functions and binary relations, we propose a combination of quadratic and smooth hinge losses and derive the associated parameter gradients. We solve the resulting optimization problem using memory efficient quasi-Newton methods. We evaluate our method on synthetic and real data, showing that it obtains significant accuracy improvement over related techniques even when training data is limited. Further, we show that our decomposition is able to transfer information across the various relations, thus better exploiting the multi-relational structure."
The Interplay between Stability and Regret in Online Learning,"This paper considers the stability of online learning algorithms and its implications for learnability (bounded regret).  We introduce a novel quantity called {\em forward regret} that intuitively measures how good an online learning algorithm is if it is allowed a one-step look-ahead into the future.  We show that given stability, bounded forward regret is equivalent to bounded regret. We also show that the existence of an algorithm with bounded regret implies the existence of a stable algorithm with bounded regret and bounded forward regret. The equivalence results apply to general, possibly non-convex problems. To the best of our knowledge, our analysis provides the first general connection between stability and regret in the online setting that is not restricted to a particular class of algorithms. Our stability-regret connection provides a simple recipe for analysing  regret incurred by any online learning algorithm.  We illustrate our recipe by providing a novel dimension independent regret bound for the follow-the-perturbed-leader (FTPL) algorithm for online linear programming (OLP) over a hypersphere, a non-convex set. Using our framework, we analyse several existing online learning algorithms as well as the ``approximate'' versions  of algorithms like RDA that solve an optimization problem at each iteration. Our proofs are simpler than existing analysis for the respective algorithms, show a clear trade-off between stability and forward regret, and provide tighter regret bounds in some cases."
Understanding Indoor Scenes with Latent Interaction Template Models,"Visual scene understanding is a difficult problem, interleaving object detection, geometric reasoning and scene classification. In this paper, we present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the latent Interaction Template Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings, while also improving individual object detections."
Convex Tensor Decomposition via Structured Schatten Norm Regularization,"Conventionally, tensor decomposition has beenformulated as non-convex optimization problems, which despite theirempirical success, hindered the analysis of their performance. In thispaper, we propose structured Schatten norms for tensor decomposition based onconvex optimization. The proposed norms include two recently  proposedapproaches for convex tensor decomposition, which we call overlapped approach andlatent approach. Moreover, we mathematically analyze the performance of the latentapproach, which was empirically found to perform better than theother one in some settings. We show theoretically that this is indeedthe case. In particular, when the unknown true tensor is low-rank in a specific mode, the latent approach performs as good as knowing the mode with the smallest rank. We confirm through numerical simulations that our theoretical prediction can precisely predict the scaling behaviour of the mean squared error. "
Convex Tensor Decomposition via Structured Schatten Norm Regularization,"Conventionally, tensor decomposition has beenformulated as non-convex optimization problems, which despite theirempirical success, hindered the analysis of their performance. In thispaper, we propose structured Schatten norms for tensor decomposition based onconvex optimization. The proposed norms include two recently  proposedapproaches for convex tensor decomposition, which we call overlapped approach andlatent approach. Moreover, we mathematically analyze the performance of the latentapproach, which was empirically found to perform better than theother one in some settings. We show theoretically that this is indeedthe case. In particular, when the unknown true tensor is low-rank in a specific mode, the latent approach performs as good as knowing the mode with the smallest rank. We confirm through numerical simulations that our theoretical prediction can precisely predict the scaling behaviour of the mean squared error. "
Towards Sparse Representation on Cosine Distance,"Sparse code is a regularized least squares solution by $L_1$ or $L_0$ constraint, based on Euclidean distance between original and reconstructed signals with respect to a pre-defined dictionary.  The Euclidean distance, however, is not a good metric for many visual feature descriptors especially histogram features,~\eg~SIFT, HOG, LBP and Spatial Pyramid.  Instead, a cosine distance is a semantically meaningful metric for the visual features.  To leverage the benefit of cosine distance in sparse representation, we formulate a new sparse coding objective function based on approximate cosine distance by forcing a norm of reconstructed signal to be close to the norm of original signal.  We evaluate our new formulation on two datasets: Extended YaleB and AR dataset.  Our formulation shows consistent improvement over the traditional Euclidean distance based sparse coding formulation in our evaluations and achieve the state-of-the-art performance on the datasets."
Learned Prioritization for Trading Off Accuracy and Speed,"Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines."
Learned Prioritization for Trading Off Accuracy and Speed,"Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines."
Generalized Classification-based Approximate Policy Iteration,"Classification-based approximate policy iteration allows us to benefit from the regularities of the optimal policy by explicitly controlling the complexity of the policy space. This leads to considerable improvements whenever the optimal policy is easy to represent. The conventional classification-based methods, however, do not benefit from the regularities of the value function as they often use a rollout-based estimate of the action-value function, which is rather data-inefficient and cannot generalize the estimate of the action-value function over states. In this paper, we introduce a general framework for classification-based approximate policy iteration, CAPI, that lets us benefit from the present regularities of both the policy and the value.Our theoretical analysis extends existing work by allowing the policy evaluation to be performed by any reinforcement learning algorithm, by handling nonparametric representations of policies, and by providing tighter convergence bounds on the estimation error of policy learning.A small illustration shows that this approach can be faster than purely value-based methods."
Another Nonparametric Functional Estimator that Achieves Asymptotic Optimality and Adapts to Irregular Domains,"We propose a new nonparametric functional estimation method. Existingstate-of-the-art methods are designed for regular domains (such as $\mathbb{R}^{d}$ or$[0,1]^d$) -- when the domain is irregular, they run intoimplementation difficulties; More specifically, one does not know the formulaof the corresponding reproducing kernels. Our newly designed method adapts to anyirregular domain. When some boundary conditionsare satisfied, it preserves the asymptotic optimality, which includesthe optimal convergence rate. The new methodalso achieves the asymptotic efficiency, however we did not include here. A significant advantage of the new approach is that it adapts to any domain, while traditional methodsrequire restrictive conditions on the domains."
Another Nonparametric Functional Estimator that Achieves Asymptotic Optimality and Adapts to Irregular Domains,"We propose a new nonparametric functional estimation method. Existingstate-of-the-art methods are designed for regular domains (such as $\mathbb{R}^{d}$ or$[0,1]^d$) -- when the domain is irregular, they run intoimplementation difficulties; More specifically, one does not know the formulaof the corresponding reproducing kernels. Our newly designed method adapts to anyirregular domain. When some boundary conditionsare satisfied, it preserves the asymptotic optimality, which includesthe optimal convergence rate. The new methodalso achieves the asymptotic efficiency, however we did not include here. A significant advantage of the new approach is that it adapts to any domain, while traditional methodsrequire restrictive conditions on the domains."
Q-Learning on a Multi-state Markov Decision Process,"In this paper, we consider a different setting for the agent in Markov Decision Process (MDP):The agent can occupy multiple states at the same time and take an action from a set of states.We refer to this problem as multi-state MDP.This multi-state MDP has exponentially large state and action spaces which might be computationally expensive.However, we can take the advantage of the nondeterminism of the agent in multi-state MDP and propose two nondeterministic Q-learning algorithms.We show that the convergence and optimality of the standard Q-learning algorithm still holds.Furthermore, in the experiments we also show that the nondeterministic algorithms converge faster and are more robust with different learning rates than the standard Q-learning algorithm."
Q-Learning on a Multi-state Markov Decision Process,"In this paper, we consider a different setting for the agent in Markov Decision Process (MDP):The agent can occupy multiple states at the same time and take an action from a set of states.We refer to this problem as multi-state MDP.This multi-state MDP has exponentially large state and action spaces which might be computationally expensive.However, we can take the advantage of the nondeterminism of the agent in multi-state MDP and propose two nondeterministic Q-learning algorithms.We show that the convergence and optimality of the standard Q-learning algorithm still holds.Furthermore, in the experiments we also show that the nondeterministic algorithms converge faster and are more robust with different learning rates than the standard Q-learning algorithm."
Information Theoretic Pairwise Clustering,In this paper we develop an information-theoretic approach for pairwise clustering. The Laplacian of the pairwise similarity matrix can be used to define a Markov random walk on the data points. This view forms a probabilistic interpretation of spectral clustering methods. We utilize this probabilistic model to define a novel clustering cost function that is based on maximizing the mutual information between consecutively visited clusters of states of the Markovian process defined by the graph Laplacian matrix. The algorithm complexity is linear on sparse graphs. The improved performance and the reduced computational complexity of the proposed algorithm are demonstrated on several standard datasets.
Information Theoretic Pairwise Clustering,In this paper we develop an information-theoretic approach for pairwise clustering. The Laplacian of the pairwise similarity matrix can be used to define a Markov random walk on the data points. This view forms a probabilistic interpretation of spectral clustering methods. We utilize this probabilistic model to define a novel clustering cost function that is based on maximizing the mutual information between consecutively visited clusters of states of the Markovian process defined by the graph Laplacian matrix. The algorithm complexity is linear on sparse graphs. The improved performance and the reduced computational complexity of the proposed algorithm are demonstrated on several standard datasets.
Graphical Models via Generalized Linear Models,"Undirected graphical models, or Markov networks, such as Gaussian graphical models and Ising models enjoy popularity in a variety of applications.  In many settings, however, data may not follow a Gaussian or binomial distribution assumed by these models. We introduce a new class of graphical models based on generalized linear models (GLM) by assuming that node-wise conditional distributions arise from exponential families.  Our models allow one to estimate networks for a wide class of exponential distributions, such as the Poisson, negative binomial, and exponential, by fitting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We provide examples of high-throughput genomic networks learned via our GLM graphical models for multinomial and Poisson distributed data."
Shifted Subspace Tracking on Sparse Outliers,"In low-rank \& sparse matrix decomposition, the sparse part is often assumed to be generated by a random model. Analysis to its structure, which is of central interest in various problems, is rarely considered. One such example is tracking multiple object flows in video. We introduce ``shifted subspace tracking (SST)'' to both separate the object flows and recover their trajectories by exploring their shifted subspaces on the sparse outliers. SST can be summarized in two steps, i.e., background modeling and flow tracking. In step 1, we propose ``semi-soft GoDec'' to separate all the moving objects as the sparse outlier $S$ from the data matrix $X$. Its soft-thresholding of $S$ significantly speeds up GoDec and facilitates the parameter setting. In step 2, we treat the sparse $S$ in step 1 as the new $X$, and develop ``SST algorithm'' decomposing $X$ as $X=\sum\nolimits_{i=1}^k L(i)\circ\tau(i)+S+G$, wherein $L(i)$ denotes the subspace of the $i^{th}$ flow after transformation $\tau(i)$. The decomposition solves $k$ sub-problems of alternating minimization in sequel, each of which recovers a $L(i)$ and its $\tau(i)$ with randomized acceleration. Sparsity of $L(i)$ and smoothness between adjacent frames are explored to save computations. We justify the promising performance of SST on four surveillance video sequences."
Gene Context Analysis on Large-scale Genomic Data,"In this paper, we investigate one of the largest microarray datasets aggregated from the internet. We aim at detecting association between the variables (genes or gene pathways) and certain keywords of interest (tissue types or diseases, for example). We address the challenges of utilizing the text information and variable structure information in high dimensional feature selection and classification. To utilize the text information, we build keyword network borrowing the power of natural language processing and apply Nearest Shrunken Centroids (NSC) to context analysis. This procedure can fully utilize the text information and has the potential to scale up to the dimension of 1012 in minutes. To utilize the structure information, we develop a new discriminant analysis method called the group Nearest Shrunken Centroids (gNSC). By exploiting the text and variable structure information, our result verifies several biological associations and further leads to some new discoveries."
Incremental Beam Search,"Beam search is a widely applied heuristic search method. Given a beam-width, it explores that many nodes at each level until a goal node is found. However, the quality of the solution produced by beam search does not always monotonically improve with the increase in beam-width, which makes it difficult to choose an appropriate beam-width for effective use. We address this issue by proposing a new beam search algorithm called Incremental Beam search (IncB) which guarantees monotonicity. IncB is also an anytime algorithm. Experimental results on the sliding-tile puzzle problem and the traveling salesman problem show that IncB significantly outperforms iterative beam search as well as some of the state-of-the-art anytime heuristic search algorithms."
Incremental Beam Search,"Beam search is a widely applied heuristic search method. Given a beam-width, it explores that many nodes at each level until a goal node is found. However, the quality of the solution produced by beam search does not always monotonically improve with the increase in beam-width, which makes it difficult to choose an appropriate beam-width for effective use. We address this issue by proposing a new beam search algorithm called Incremental Beam search (IncB) which guarantees monotonicity. IncB is also an anytime algorithm. Experimental results on the sliding-tile puzzle problem and the traveling salesman problem show that IncB significantly outperforms iterative beam search as well as some of the state-of-the-art anytime heuristic search algorithms."
Incremental Beam Search,"Beam search is a widely applied heuristic search method. Given a beam-width, it explores that many nodes at each level until a goal node is found. However, the quality of the solution produced by beam search does not always monotonically improve with the increase in beam-width, which makes it difficult to choose an appropriate beam-width for effective use. We address this issue by proposing a new beam search algorithm called Incremental Beam search (IncB) which guarantees monotonicity. IncB is also an anytime algorithm. Experimental results on the sliding-tile puzzle problem and the traveling salesman problem show that IncB significantly outperforms iterative beam search as well as some of the state-of-the-art anytime heuristic search algorithms."
Visually-grounded Bayesian Word Learning,"Learning the meaning of a novel noun from a few labelled objects is one of the simplest aspects of learning a language, but approximating human performance on this task is still a significant challenge for current machine learning systems. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for given visual stimulus. Recent work in cognitive science on Bayesian models of word learning partially addresses this challenge, but assumes that objects are perfectly recognized and has only been evaluated in small domains. We present a system for learning words directly from images, using probabilistic predictions generated by visual classifiers as the input to Bayesian word learning, and compare this system to human performance in a large-scale automated experiment. The system captures a significant proportion of the variance in human responses. Combining the uncertain outputs of the visual classifiers with the ability to identify an appropriate level of abstraction that comes from Bayesian word learning allows the system to outperform alternatives that assume perfect recognition or use a more conventional computer vision approach."
Towards active event recognition,"Directing robot's sensors to anticipate events  like  goal-directed actions is complicated by intrinsic time constraints and spatially distributed sources of information.  The problem thus requires an integrated solution for tracking, exploration and recognition, which  traditionally have been seen as separate problems in active-vision.We propose a probabilistic generative framework  based on a mixture of Kalman filters  to use predictions in both recognition and sensor-control. This framework can efficiently use the observations of one element in a dynamic environment to provide information on other elements, and consequently enables guided exploration of the environment.Experiments on a humanoid robot  observing a human executing goal-oriented actions demonstrated improvement on recognition time and precision over baseline approaches."
Co-Regularized Hashing for Multimodal Data,"Hashing-based methods provide a very promising approach to large-scale similarity search.  To obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically.  In this paper, we study hash function learning in the context of multimodal data.  We propose a novel multimodal hash function learning method, called Co-Regularized Hashing (CRH), based on a boosted co-regularization framework.  The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized.  We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets."
Convergence and Energy Landscape for Cheeger Cut Clustering,"Unsupervised clustering of scattered, noisy and high-dimensional data points is an important and difficult problem. Continuous relaxations of balanced cut problems  yield excellent clustering results. This paper provides rigorous convergence results for two algorithms that solve the relaxed Cheeger Cut minimization.  The first algorithm is a new steepest descent algorithm and the second one is a slight modification of the Inverse Power Method algorithm \cite{pro:HeinBuhler10OneSpec}. While the steepest descent algorithm has better theoretical convergence properties,  in practice both algorithm perform equally.  We also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem, and relate this characterization to the convergence of the algorithms."
Convergence and Energy Landscape for Cheeger Cut Clustering,"Unsupervised clustering of scattered, noisy and high-dimensional data points is an important and difficult problem. Continuous relaxations of balanced cut problems  yield excellent clustering results. This paper provides rigorous convergence results for two algorithms that solve the relaxed Cheeger Cut minimization.  The first algorithm is a new steepest descent algorithm and the second one is a slight modification of the Inverse Power Method algorithm \cite{pro:HeinBuhler10OneSpec}. While the steepest descent algorithm has better theoretical convergence properties,  in practice both algorithm perform equally.  We also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem, and relate this characterization to the convergence of the algorithms."
Convergence and Energy Landscape for Cheeger Cut Clustering,"Unsupervised clustering of scattered, noisy and high-dimensional data points is an important and difficult problem. Continuous relaxations of balanced cut problems  yield excellent clustering results. This paper provides rigorous convergence results for two algorithms that solve the relaxed Cheeger Cut minimization.  The first algorithm is a new steepest descent algorithm and the second one is a slight modification of the Inverse Power Method algorithm \cite{pro:HeinBuhler10OneSpec}. While the steepest descent algorithm has better theoretical convergence properties,  in practice both algorithm perform equally.  We also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem, and relate this characterization to the convergence of the algorithms."
Convergence and Energy Landscape for Cheeger Cut Clustering,"Unsupervised clustering of scattered, noisy and high-dimensional data points is an important and difficult problem. Continuous relaxations of balanced cut problems  yield excellent clustering results. This paper provides rigorous convergence results for two algorithms that solve the relaxed Cheeger Cut minimization.  The first algorithm is a new steepest descent algorithm and the second one is a slight modification of the Inverse Power Method algorithm \cite{pro:HeinBuhler10OneSpec}. While the steepest descent algorithm has better theoretical convergence properties,  in practice both algorithm perform equally.  We also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem, and relate this characterization to the convergence of the algorithms."
Leveraging for Fitting Linear Models in Large-scale Data,"Recent empirical and theoretical work has focused on using the empirical statistical leverage scores of data matrices in order to develop improved algorithms for common matrix problems such as least-squares approximation and low-rank matrix approximation.  Existing work focuses on algorithmic issues such as worst-case running times or on the usefulness of this approach in downstream data applications.  Here, we examine the statistical properties of this leveraging paradigm in the context of fitting a linear model to data.  We derive the mean squared errors for two related leveraging-based estimates and for uniform sampling estimates.  Depending on the the mean, variance and skewness of the leverage scores, one procedure or another is preferred.  We also describe the empirical behavior of these procedures on several synthetic and real data sets."
Symbolic Dynamic Programming for Continuous State and Observation POMDPs,"Partially-observable Markov decision processes (POMDPs) provide a powerfulmodel for real-world sequential decision-making problems. In recent years, point-based value iteration methods have proven to be extremely effective techniquesfor ?nding (approximately) optimal dynamic programming solutions to POMDPswhen an initial set of belief states is known. However, no point-based work hasprovided exact point-based backups for both continuous state and observationspaces, which we tackle in this paper. Our key insight is that while there maybe an in?nite number of possible observations, there are only a ?nite number ofobservation partitionings that are relevant for optimal decision-making when a?nite, ?xed set of reachable belief states is known. To this end, we make twoimportant contributions: (1) we show how previous exact symbolic dynamic pro-gramming solutions for continuous state MDPs can be generalized to continu-ous state POMDPs with discrete observations, and (2) we show how this solutioncan be further extended via recently developed symbolic methods to continuousstate and observations to derive the minimal relevant observation partitioning forpotentially correlated, multivariate observation spaces. We demonstrate proof-of-concept results on uni- and multi-variate state and observation steam plant control."
Symbolic Dynamic Programming for Continuous State and Observation POMDPs,"Partially-observable Markov decision processes (POMDPs) provide a powerfulmodel for real-world sequential decision-making problems. In recent years, point-based value iteration methods have proven to be extremely effective techniquesfor ?nding (approximately) optimal dynamic programming solutions to POMDPswhen an initial set of belief states is known. However, no point-based work hasprovided exact point-based backups for both continuous state and observationspaces, which we tackle in this paper. Our key insight is that while there maybe an in?nite number of possible observations, there are only a ?nite number ofobservation partitionings that are relevant for optimal decision-making when a?nite, ?xed set of reachable belief states is known. To this end, we make twoimportant contributions: (1) we show how previous exact symbolic dynamic pro-gramming solutions for continuous state MDPs can be generalized to continu-ous state POMDPs with discrete observations, and (2) we show how this solutioncan be further extended via recently developed symbolic methods to continuousstate and observations to derive the minimal relevant observation partitioning forpotentially correlated, multivariate observation spaces. We demonstrate proof-of-concept results on uni- and multi-variate state and observation steam plant control."
Bayesian Probabilistic Co-Subspace Addition,"For modeling data matrices, this paper introduces Probabilistic Co-Subspace Addition (PCSA) model by simultaneously capturing the dependent structures among both rows and columns. Briefly, PCSA assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two features, which distribute in the row-wise and column-wise latent subspaces. Consequently, it captures the dependencies among entries intricately, and is able to model the non-Gaussian and heteroscedastic density. Variational inference is proposed on PCSA for  approximate Bayesian learning, where the updating for posteriors is formulated into the problem of solving Sylvester equations. Furthermore, PCSA is extended to tackling and filling missing values, to adapting its sparseness, and to modelling tensor data. In comparison with several state-of-art approaches, experiments demonstrate the effectiveness and efficiency of Bayesian (sparse) PCSA on modeling matrix (tensor) data and filling missing values."
Incremental Learning Hierarchical Functional Categories for Interacting Objects,"We present an algorithm which has been designed as an online learning module in a cognitive system to perform two fundamental and often coupled tasks: category learning and value function approximation. There are four important features in our algorithm. First, categories are incrementally constructed without using externally provided category labels. Second, categories are organized into hierarchies, making the algorithm scalable with respect to diversity of the inputs. Third, different from unsupervised hierarchical clustering algorithms, categorization in our algorithm can be influenced by externally provided feedbacks (in form of utility values or rewards). Finally, when there are multiple interacting objects from different domains (such as prey and weapon), multiple category hierarchies are learned simultaneously. We use systematically generated synthetic data to evaluate our algorithm in a function approximation task, where our algorithm is shown to learn significantly faster than standard machine learning algorithms used in cognitive systems."
Incremental Learning Hierarchical Functional Categories for Interacting Objects,"We present an algorithm which has been designed as an online learning module in a cognitive system to perform two fundamental and often coupled tasks: category learning and value function approximation. There are four important features in our algorithm. First, categories are incrementally constructed without using externally provided category labels. Second, categories are organized into hierarchies, making the algorithm scalable with respect to diversity of the inputs. Third, different from unsupervised hierarchical clustering algorithms, categorization in our algorithm can be influenced by externally provided feedbacks (in form of utility values or rewards). Finally, when there are multiple interacting objects from different domains (such as prey and weapon), multiple category hierarchies are learned simultaneously. We use systematically generated synthetic data to evaluate our algorithm in a function approximation task, where our algorithm is shown to learn significantly faster than standard machine learning algorithms used in cognitive systems."
Identity maps and their extensions on parameter spaces: Applications to anomaly detection in video processing,"It is now commonplace for analysts to model data on non-Euclidean spaces.   The non-linear structure of these parameter spaces can accurately reflect non-linear nature of complex data, and analysis is performed using algorithms which respect the geometry of the parameter space.  Several well-known linear algorithms have been generalized to analogous algorithms on parameter spaces such as k-means and PCA. These generalized algorithms have been shown to be effective ways to cluster and classify data on parameter spaces.  It is the aim of this paper to generalize an established (Euclidean based) Identity map extension (MSET) to well-known parameter spaces.  An identity map extension (IME) is a function that acts as the identity mapping when restricted to a special subset, and this property has made the MSET algorithm useful for anomaly detection.  We define a generalization of this map to several parameter spaces, we prove it has the IME property, and we evaluate its performance as an anomaly detector on a real dataset."
Identity maps and their extensions on parameter spaces: Applications to anomaly detection in video processing,"It is now commonplace for analysts to model data on non-Euclidean spaces.   The non-linear structure of these parameter spaces can accurately reflect non-linear nature of complex data, and analysis is performed using algorithms which respect the geometry of the parameter space.  Several well-known linear algorithms have been generalized to analogous algorithms on parameter spaces such as k-means and PCA. These generalized algorithms have been shown to be effective ways to cluster and classify data on parameter spaces.  It is the aim of this paper to generalize an established (Euclidean based) Identity map extension (MSET) to well-known parameter spaces.  An identity map extension (IME) is a function that acts as the identity mapping when restricted to a special subset, and this property has made the MSET algorithm useful for anomaly detection.  We define a generalization of this map to several parameter spaces, we prove it has the IME property, and we evaluate its performance as an anomaly detector on a real dataset."
Identity maps and their extensions on parameter spaces: Applications to anomaly detection in video processing,"It is now commonplace for analysts to model data on non-Euclidean spaces.   The non-linear structure of these parameter spaces can accurately reflect non-linear nature of complex data, and analysis is performed using algorithms which respect the geometry of the parameter space.  Several well-known linear algorithms have been generalized to analogous algorithms on parameter spaces such as k-means and PCA. These generalized algorithms have been shown to be effective ways to cluster and classify data on parameter spaces.  It is the aim of this paper to generalize an established (Euclidean based) Identity map extension (MSET) to well-known parameter spaces.  An identity map extension (IME) is a function that acts as the identity mapping when restricted to a special subset, and this property has made the MSET algorithm useful for anomaly detection.  We define a generalization of this map to several parameter spaces, we prove it has the IME property, and we evaluate its performance as an anomaly detector on a real dataset."
Online Self-Supervised Segmentation of Dynamic Objects,"We address the problem of learning models to automatically segment dynamic objects in an urban environment from a moving camera without manual labelling, in an online, self-supervised manner. We use input images obtained from a single uncalibrated camera placed on top of a moving vehicle, extracting and matching pairs of sparse features that represent the optical flow information between frames. This optical flow information is initially divided into two classes, static or dynamic, where the static class represents features that comply to the constraints provided by the camera motion and the dynamic class represents the ones that do not. This initial classification is used to incrementally train a Gaussian Process (GP) classifier to segment dynamic objects in new images. The hyperparameters of the GP covariance function are optimized online during navigation, and the available self-supervised dataset is updated as new relevant data is added and redundant data is removed, resulting in a near-constant computing time even after long periods of navigation. The output is a vector containing the probability that each pixel in the image belongs to either the static or dynamic class (ranging from 0 to 1), along with the corresponding uncertainty estimate of the classification. Experiments conducted in an urban environment, with cars and pedestrians as dynamic objects and no prior knowledge or additional sensors, show promising results even when the vehicle is moving at considerable speeds (up to 50 km/h), a scenario that produces a large quantity of featureless regions and false matches that is very challenging for conventional approaches. "
Online Self-Supervised Segmentation of Dynamic Objects,"We address the problem of learning models to automatically segment dynamic objects in an urban environment from a moving camera without manual labelling, in an online, self-supervised manner. We use input images obtained from a single uncalibrated camera placed on top of a moving vehicle, extracting and matching pairs of sparse features that represent the optical flow information between frames. This optical flow information is initially divided into two classes, static or dynamic, where the static class represents features that comply to the constraints provided by the camera motion and the dynamic class represents the ones that do not. This initial classification is used to incrementally train a Gaussian Process (GP) classifier to segment dynamic objects in new images. The hyperparameters of the GP covariance function are optimized online during navigation, and the available self-supervised dataset is updated as new relevant data is added and redundant data is removed, resulting in a near-constant computing time even after long periods of navigation. The output is a vector containing the probability that each pixel in the image belongs to either the static or dynamic class (ranging from 0 to 1), along with the corresponding uncertainty estimate of the classification. Experiments conducted in an urban environment, with cars and pedestrians as dynamic objects and no prior knowledge or additional sensors, show promising results even when the vehicle is moving at considerable speeds (up to 50 km/h), a scenario that produces a large quantity of featureless regions and false matches that is very challenging for conventional approaches. "
Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging ,"Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model?s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject?s conversion to Alzheimer?s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here, our new model outperforms the state of the art (p-values << 10?3 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity)."
Stochastic blockmodeling of relational event dynamics,"For continuous-time network data, several approaches have recently been proposed for modeling dyadic event rates conditioned on the observed history of events and nodal or dyadic covariates.  In many cases, however, interaction propensities -- and even the underlying mechanisms of interaction -- vary systematically across subgroups whose identities are unobserved.  For static networks, such heterogeneity has been treated via methods such as stochastic blockmodeling, which operate by assuming latent groups of individuals with similar tendencies in their group-wise interactions.  Here, we combine these two approaches by positing a latent partition of the node set such that event dynamics within and between subsets evolve in potentially distinct ways.  We illustrate the use of our model family by application to several forms of dyadic interaction data, including email communication and Twitter direct messages.  Parameter estimates from the fitted models clearly reveal heterogeneity in the dynamics among groups of individuals.  We also find that the fitted models have better predictive accuracy than either baseline models or relational event models without latent structure.  Our approach illustrates the utility of latent structure methods based on detailed dynamics, which can succeed even in the absence of differences in marginal interaction rates across groups. "
Stochastic blockmodeling of relational event dynamics,"For continuous-time network data, several approaches have recently been proposed for modeling dyadic event rates conditioned on the observed history of events and nodal or dyadic covariates.  In many cases, however, interaction propensities -- and even the underlying mechanisms of interaction -- vary systematically across subgroups whose identities are unobserved.  For static networks, such heterogeneity has been treated via methods such as stochastic blockmodeling, which operate by assuming latent groups of individuals with similar tendencies in their group-wise interactions.  Here, we combine these two approaches by positing a latent partition of the node set such that event dynamics within and between subsets evolve in potentially distinct ways.  We illustrate the use of our model family by application to several forms of dyadic interaction data, including email communication and Twitter direct messages.  Parameter estimates from the fitted models clearly reveal heterogeneity in the dynamics among groups of individuals.  We also find that the fitted models have better predictive accuracy than either baseline models or relational event models without latent structure.  Our approach illustrates the utility of latent structure methods based on detailed dynamics, which can succeed even in the absence of differences in marginal interaction rates across groups. "
Stochastic blockmodeling of relational event dynamics,"For continuous-time network data, several approaches have recently been proposed for modeling dyadic event rates conditioned on the observed history of events and nodal or dyadic covariates.  In many cases, however, interaction propensities -- and even the underlying mechanisms of interaction -- vary systematically across subgroups whose identities are unobserved.  For static networks, such heterogeneity has been treated via methods such as stochastic blockmodeling, which operate by assuming latent groups of individuals with similar tendencies in their group-wise interactions.  Here, we combine these two approaches by positing a latent partition of the node set such that event dynamics within and between subsets evolve in potentially distinct ways.  We illustrate the use of our model family by application to several forms of dyadic interaction data, including email communication and Twitter direct messages.  Parameter estimates from the fitted models clearly reveal heterogeneity in the dynamics among groups of individuals.  We also find that the fitted models have better predictive accuracy than either baseline models or relational event models without latent structure.  Our approach illustrates the utility of latent structure methods based on detailed dynamics, which can succeed even in the absence of differences in marginal interaction rates across groups. "
Alternating Update Procedures for Unconstrained and Constrained Binary Matrix Factorization,"In general, binary matrix factorization (BMF) refers to the problem of finding a matrix product of two binary low rank matrices such that the difference between the matrix product  and a given binary matrix is minimized. In the current literature on BMF,  the matrix product is not required to be  binary. We call this  unconstrained BMF (UBMF) and similarly constrained BMF (CBMF) if the matrix product is required to be  binary. In this paper, we first introduce two specific variants of CBMF and discuss the relationship between BMF and UBMF.Then we propose alternating update procedures for both UBMF and CBMF. In every iteration of the proposed  procedure, we solve a specific binary quadratic programming (BQP) problem to update the involved matrix argument. Two different algorithms are presented to cope with the BQP subproblem in the procedure. In particular, we show that the BQP subproblem can be reformulated as a specific clustering problem. Based on the clustering reformulation, we also derive an effective 2-approximation algorithm for CBMF. By exploring the interrelation between UBMF and CBMF, we show that  we can  obtain good  approximation to rank-1  UBMF. The complexity of the proposed algorithms is  discussed. Numerical results show that the proposed algorithms for UBMF are able to find better solutions in less CPU time than several other algorithms in the literature, and the solution obtained from CBMF is very close to that of UBMF."
Alternating Update Procedures for Unconstrained and Constrained Binary Matrix Factorization,"In general, binary matrix factorization (BMF) refers to the problem of finding a matrix product of two binary low rank matrices such that the difference between the matrix product  and a given binary matrix is minimized. In the current literature on BMF,  the matrix product is not required to be  binary. We call this  unconstrained BMF (UBMF) and similarly constrained BMF (CBMF) if the matrix product is required to be  binary. In this paper, we first introduce two specific variants of CBMF and discuss the relationship between BMF and UBMF.Then we propose alternating update procedures for both UBMF and CBMF. In every iteration of the proposed  procedure, we solve a specific binary quadratic programming (BQP) problem to update the involved matrix argument. Two different algorithms are presented to cope with the BQP subproblem in the procedure. In particular, we show that the BQP subproblem can be reformulated as a specific clustering problem. Based on the clustering reformulation, we also derive an effective 2-approximation algorithm for CBMF. By exploring the interrelation between UBMF and CBMF, we show that  we can  obtain good  approximation to rank-1  UBMF. The complexity of the proposed algorithms is  discussed. Numerical results show that the proposed algorithms for UBMF are able to find better solutions in less CPU time than several other algorithms in the literature, and the solution obtained from CBMF is very close to that of UBMF."
Alternating Update Procedures for Unconstrained and Constrained Binary Matrix Factorization,"In general, binary matrix factorization (BMF) refers to the problem of finding a matrix product of two binary low rank matrices such that the difference between the matrix product  and a given binary matrix is minimized. In the current literature on BMF,  the matrix product is not required to be  binary. We call this  unconstrained BMF (UBMF) and similarly constrained BMF (CBMF) if the matrix product is required to be  binary. In this paper, we first introduce two specific variants of CBMF and discuss the relationship between BMF and UBMF.Then we propose alternating update procedures for both UBMF and CBMF. In every iteration of the proposed  procedure, we solve a specific binary quadratic programming (BQP) problem to update the involved matrix argument. Two different algorithms are presented to cope with the BQP subproblem in the procedure. In particular, we show that the BQP subproblem can be reformulated as a specific clustering problem. Based on the clustering reformulation, we also derive an effective 2-approximation algorithm for CBMF. By exploring the interrelation between UBMF and CBMF, we show that  we can  obtain good  approximation to rank-1  UBMF. The complexity of the proposed algorithms is  discussed. Numerical results show that the proposed algorithms for UBMF are able to find better solutions in less CPU time than several other algorithms in the literature, and the solution obtained from CBMF is very close to that of UBMF."
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
Multi-Armed Bandit Problem with Budget Constraint and Variable Costs,"In this paper, we study the multi-armed bandit problem with budget constraint and variable costs (MAB-BV). In this setting, pulling each arm is associated with an unknown and variable cost, and the objective of a learning algorithm is to pull a sequence of arms in order to maximize the expected total reward with the number of pulled arms complying with the budget constraint. This new setting describes many Internet applications (e.g., sponsored search and cloud computing) in a more accurate manner than previous settings that either assume the pulling of arms is costless or with a fixed cost. To tackle this new kind of multi-armed bandit problem, we extend the UCB algorithms by selecting arms according to the reward-cost ratio, and propose a new algorithm called UCB-BV. Our empirical results verify the effectiveness of this algorithm. Although the extension in UCB-BV seems natural and simple, and it is practically effective, the theoretical analysis on its regret bound turns out to be very difficult. We develop a set of new proof techniques and obtain a regret bound of $O(\ln B)$. Furthermore, we show that when applying the proposed algorithm to the setting with fixed costs (which is our special case), one can improve the corresponding regret bound obtained so far."
Multi-Armed Bandit Problem with Budget Constraint and Variable Costs,"In this paper, we study the multi-armed bandit problem with budget constraint and variable costs (MAB-BV). In this setting, pulling each arm is associated with an unknown and variable cost, and the objective of a learning algorithm is to pull a sequence of arms in order to maximize the expected total reward with the number of pulled arms complying with the budget constraint. This new setting describes many Internet applications (e.g., sponsored search and cloud computing) in a more accurate manner than previous settings that either assume the pulling of arms is costless or with a fixed cost. To tackle this new kind of multi-armed bandit problem, we extend the UCB algorithms by selecting arms according to the reward-cost ratio, and propose a new algorithm called UCB-BV. Our empirical results verify the effectiveness of this algorithm. Although the extension in UCB-BV seems natural and simple, and it is practically effective, the theoretical analysis on its regret bound turns out to be very difficult. We develop a set of new proof techniques and obtain a regret bound of $O(\ln B)$. Furthermore, we show that when applying the proposed algorithm to the setting with fixed costs (which is our special case), one can improve the corresponding regret bound obtained so far."
Multi-Armed Bandit Problem with Budget Constraint and Variable Costs,"In this paper, we study the multi-armed bandit problem with budget constraint and variable costs (MAB-BV). In this setting, pulling each arm is associated with an unknown and variable cost, and the objective of a learning algorithm is to pull a sequence of arms in order to maximize the expected total reward with the number of pulled arms complying with the budget constraint. This new setting describes many Internet applications (e.g., sponsored search and cloud computing) in a more accurate manner than previous settings that either assume the pulling of arms is costless or with a fixed cost. To tackle this new kind of multi-armed bandit problem, we extend the UCB algorithms by selecting arms according to the reward-cost ratio, and propose a new algorithm called UCB-BV. Our empirical results verify the effectiveness of this algorithm. Although the extension in UCB-BV seems natural and simple, and it is practically effective, the theoretical analysis on its regret bound turns out to be very difficult. We develop a set of new proof techniques and obtain a regret bound of $O(\ln B)$. Furthermore, we show that when applying the proposed algorithm to the setting with fixed costs (which is our special case), one can improve the corresponding regret bound obtained so far."
Optimal Computational Trade-Off of Inexact Proximal Methods,"In this paper, we investigate the trade-off between convergence rate and computational cost when minimizing a composite functional with proximal-gradient methods, which are popular optimization tools in machine learning. We consider the case when the proximity operator is approximated via an iterative procedure, which leads to an algorithm with two nested loops. We show that the computationally optimal strategy to reach a desired accuracy in finite time is to set the number of inner iterations to a constant, which differs from the strategy indicated by a convergence rate analysis. In the process, we also present a new procedure called SIP that is both computationally and practically efficient. Our numerical experiments confirm the theoretical findings and suggest that SIP can be a very competitive alternative to the standard procedure."
Optimal Computational Trade-Off of Inexact Proximal Methods,"In this paper, we investigate the trade-off between convergence rate and computational cost when minimizing a composite functional with proximal-gradient methods, which are popular optimization tools in machine learning. We consider the case when the proximity operator is approximated via an iterative procedure, which leads to an algorithm with two nested loops. We show that the computationally optimal strategy to reach a desired accuracy in finite time is to set the number of inner iterations to a constant, which differs from the strategy indicated by a convergence rate analysis. In the process, we also present a new procedure called SIP that is both computationally and practically efficient. Our numerical experiments confirm the theoretical findings and suggest that SIP can be a very competitive alternative to the standard procedure."
Optimal Computational Trade-Off of Inexact Proximal Methods,"In this paper, we investigate the trade-off between convergence rate and computational cost when minimizing a composite functional with proximal-gradient methods, which are popular optimization tools in machine learning. We consider the case when the proximity operator is approximated via an iterative procedure, which leads to an algorithm with two nested loops. We show that the computationally optimal strategy to reach a desired accuracy in finite time is to set the number of inner iterations to a constant, which differs from the strategy indicated by a convergence rate analysis. In the process, we also present a new procedure called SIP that is both computationally and practically efficient. Our numerical experiments confirm the theoretical findings and suggest that SIP can be a very competitive alternative to the standard procedure."
Discovering Voxel-Level Functional Connectivity Between Cortical Regions,"Functional connectivity patterns are known to exist in the human brain at the millimeter scale, but the standard fMRI connectivity measure only computes functional correlations at a coarse level. We present the first method which identifies fine-grained functional connectivity between any two brain regions by simultaneously learning voxel-level connectivity maps over both regions. We show how to formulate this problem as a constrained least-squared optimization, with a spatial regularization term that allows connectivity maps to be learned much more efficiently. This optimization problem can be solved using a trust region approach, and can automatically discover connectivity between multiple distinct voxel clusters in the two regions. We validate our method in two experiments, demonstrating that we can successfully learn subregion connectivity structures from a small amount of training data. Our approach is shown to be substantially better at estimating fine-grained connectivity differences than state-of-the-art subregion connectivity methods, all of which learn maps over only one region at a time."
Hard and Easy Distributions of Bayesian Networks for Junction Tree Computation ,"The effort associated with Bayesian network computation is vital in many infer-ence and machine learning settings. In this paper, we introduce a novel algorithm,GPART, that generates synthetic Bayesian networks that reflect several input pa-rameters. Using the algorithm, we investigate how various parameters of Bayesiannetworks can affect junction tree characteristics and hence computation time. Wegeneralize previous approaches to randomly generating Bayesian network by (i)introducing a novel depth parameter as well as (ii) allowing state space size andnumber of parameters for a non-root node to be probability distributions. In ex-periments, we surprisingly find that increasing our novel depth parameter dramati-cally increases clique tree size and computation time. Using parameters computedfrom application networks as parameters in GPART, and comparing the resultingjunction trees, we better understand the similarities and differences between ap-plication and synthetic Bayesian networks."
Hierarchical Optimistic Region Selection driven by Curiosity,"This paper aims to take a step forwards making the term ``intrinsic motivation'' from reinforcement learning theoretically well founded,focusing on curiosity-driven learning. To that end, we consider the setting where, a fixed partition P of a continuous space X being given,and a process \nu defined on X being unknown,we are asked to sequentially decide which cell of the partition to select as well as where to sample \nu in that cell,in order to minimize a loss function that is inspired from previous work on curiosity-driven learning.The loss on each cell consists of one term measuring a simple worst case quadratic sampling error,and a penalty term proportional to the range of the variance in that cell.The corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region,and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization canbe used in order to solve this problem. The resulting procedure,called Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C) is provided together with a finite-time regret analysis."
Sparse Prediction with the $k$-Support Norm,"  We derive a novel norm that corresponds to the tightest convex  relaxation of sparsity combined with an $\ell_2$ penalty. We show  that this new norm provides a tighter relaxation than the elastic  net, and is thus a good replacement for the Lasso or the elastic net  in sparse prediction problems.  But through studying our new norm,  we also bound the looseness of the elastic net, thus shedding new  light on it and providing justification for its use."
Sparse Prediction with the $k$-Support Norm,"  We derive a novel norm that corresponds to the tightest convex  relaxation of sparsity combined with an $\ell_2$ penalty. We show  that this new norm provides a tighter relaxation than the elastic  net, and is thus a good replacement for the Lasso or the elastic net  in sparse prediction problems.  But through studying our new norm,  we also bound the looseness of the elastic net, thus shedding new  light on it and providing justification for its use."
Sparse Prediction with the $k$-Support Norm,"  We derive a novel norm that corresponds to the tightest convex  relaxation of sparsity combined with an $\ell_2$ penalty. We show  that this new norm provides a tighter relaxation than the elastic  net, and is thus a good replacement for the Lasso or the elastic net  in sparse prediction problems.  But through studying our new norm,  we also bound the looseness of the elastic net, thus shedding new  light on it and providing justification for its use."
Robust Sparse Regression and Matching Pursuit,"In this paper we consider support recovery in sparse regression, when some number $n_1$ out of $n+n_1$ total covariate/response pairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interested in understanding how many outliers, $n_1$, we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables) provide guarantees on support recovery. Perhaps surprisingly, we also show that the natural brute force algorithm that searches over all subsets of $n$ covariate/response pairs, and all subsets of possible support coordinates in order to minimize regression error, is remarkably poor, unable to correctly identify the support with even $n_1 = O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in the basic setting we consider, where all authentic measurements and noise are independent and Gaussian. In this setting, we provide a simple algorithm that gives stronger performance guarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$ corrupted points, where $p$ is the dimension of the signal to be recovered."
Active Learning of Multi-Index Function Models,"We consider the problem of actively learning \textit{multi-index} functions of the form $f(\vecx) = g(\matA\vecx)= \sum_{i=1}^k g_i(\veca_i^T\vecx)$ from point evaluations of $f$. We assume that the function $f$ is defined on an $\ell_2$-ball in $\Real^d$, $g$ is twice continuously differentiable almost everywhere, and $\matA \in \mathbb{R}^{k \times d}$ is a rank $k$ matrix, where $k \ll d$.  We propose a randomized, active sampling scheme for estimating such functions with uniform approximation guarantees. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive an estimator of the function $f$ along with sample complexity bounds. We also characterize the noise robustness of the scheme, and provide empirical evidence that the high-dimensional scaling of our sample complexity bounds are quite accurate."
Active Learning of Multi-Index Function Models,"We consider the problem of actively learning \textit{multi-index} functions of the form $f(\vecx) = g(\matA\vecx)= \sum_{i=1}^k g_i(\veca_i^T\vecx)$ from point evaluations of $f$. We assume that the function $f$ is defined on an $\ell_2$-ball in $\Real^d$, $g$ is twice continuously differentiable almost everywhere, and $\matA \in \mathbb{R}^{k \times d}$ is a rank $k$ matrix, where $k \ll d$.  We propose a randomized, active sampling scheme for estimating such functions with uniform approximation guarantees. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive an estimator of the function $f$ along with sample complexity bounds. We also characterize the noise robustness of the scheme, and provide empirical evidence that the high-dimensional scaling of our sample complexity bounds are quite accurate."
Neurocomputational Model of Cognitive Process based on Working Memory,"Retaining information on previous stimuli and activities for short periods of time is crucial for processing complex tasks occurring in real life. In this paper, we propose to use working memory as a short-term storage for cognitive processes. We present two architectures for the working memory and implement those using spiking neurons. The results show that a temporal sequence of features are orderly captured and converted into a spatial pattern of persistent activities of neurons by the working memory."
Neurocomputational Model of Cognitive Process based on Working Memory,"Retaining information on previous stimuli and activities for short periods of time is crucial for processing complex tasks occurring in real life. In this paper, we propose to use working memory as a short-term storage for cognitive processes. We present two architectures for the working memory and implement those using spiking neurons. The results show that a temporal sequence of features are orderly captured and converted into a spatial pattern of persistent activities of neurons by the working memory."
Neurocomputational Model of Cognitive Process based on Working Memory,"Retaining information on previous stimuli and activities for short periods of time is crucial for processing complex tasks occurring in real life. In this paper, we propose to use working memory as a short-term storage for cognitive processes. We present two architectures for the working memory and implement those using spiking neurons. The results show that a temporal sequence of features are orderly captured and converted into a spatial pattern of persistent activities of neurons by the working memory."
Neurocomputational Model of Cognitive Process based on Working Memory,"Retaining information on previous stimuli and activities for short periods of time is crucial for processing complex tasks occurring in real life. In this paper, we propose to use working memory as a short-term storage for cognitive processes. We present two architectures for the working memory and implement those using spiking neurons. The results show that a temporal sequence of features are orderly captured and converted into a spatial pattern of persistent activities of neurons by the working memory."
Neurocomputational Model of Cognitive Process based on Working Memory,"Retaining information on previous stimuli and activities for short periods of time is crucial for processing complex tasks occurring in real life. In this paper, we propose to use working memory as a short-term storage for cognitive processes. We present two architectures for the working memory and implement those using spiking neurons. The results show that a temporal sequence of features are orderly captured and converted into a spatial pattern of persistent activities of neurons by the working memory."
Optimally Learning Hashing Functions Using Column Generation,"Fast nearest neighbor search is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learningdata-dependent hashing functions using machine learning techniques have been developed. In this work, we propose a column generation based method for learninghashing functions on the basis of proximity comparison information. Given a set of examples of proximity comparisons among triples of data points the methodlearns hashing functions which preserve the relative distances between them as well as possible within the large-margin learning framework. The learning procedureis implemented using column generation and hence is named CGHash. At each iteration of column generation procedure the best hashing function is selected. Unlike other recent hashing methods, our method generalizes to newpoints naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposedmethod learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on publicly available datasets."
Learning Multiple Tasks using Shared Hypotheses,"In this work we consider a setting where we have a very large number  of related tasks with few examples from each individual task. Rather  than either learning each task individually (and having a large  generalization error) or learning all the tasks together using a  single hypothesis (and suffering a potentially large inherent  error), we consider learning a small pool of {\em shared    hypotheses}. Each task is then mapped to a single hypothesis in  the pool (hard association). We derive VC dimension generalization  bounds for our model, based on the number of tasks, shared  hypothesis and the VC dimension of the hypotheses  class. We conducted experiments with both synthetic problems and  sentiment of reviews, which strongly support our approach."
Learning Multiple Tasks using Shared Hypotheses,"In this work we consider a setting where we have a very large number  of related tasks with few examples from each individual task. Rather  than either learning each task individually (and having a large  generalization error) or learning all the tasks together using a  single hypothesis (and suffering a potentially large inherent  error), we consider learning a small pool of {\em shared    hypotheses}. Each task is then mapped to a single hypothesis in  the pool (hard association). We derive VC dimension generalization  bounds for our model, based on the number of tasks, shared  hypothesis and the VC dimension of the hypotheses  class. We conducted experiments with both synthetic problems and  sentiment of reviews, which strongly support our approach."
Differentially Private Learning with Kernels,"In this paper, we consider the problem of differentially private learning using kernel empirical risk minimization (ERM) where access to the training features is through a kernel function only. Existing work for this problem is either for the linear kernel or for translation invariant kernel, where (approximate) training features are available explicitly and furthermore their generalization error guarantees are dependent on the data dimensionality. Restricting access to data through kernel functions eliminates possibility of explicitly releasing the optima w^* to the kernel ERM. To alleviate this problem, we define three different models for differential private learning using kernel ERM. Our first model is an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. In the second model, learner sends back a differentially private version of the optimal parameter vector w^* but requires to see a small subset of unlabeled test set beforehand. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of w^*. For each of the model, we derive algorithms inspired by the technique for online database release by Gupta et al. 2011 and provide privacy as well as ``goodness'' guarantees. Furthermore, we show that our method can be applied to the setting of Rubinstein et al. 2009, Chaudhuri et al. 2011 also and obtain similar generalization error bounds with two distinctions: a) our bounds are independent of the data dimensionality, b) our sample complexity bounds have worse dependence on the required generalization error."
On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization,"The ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge. This paper presents a new reinforcement-learning algorithm, called iKBSF, which extends the benefits of kernel-based learning to the on-line scenario. As a kernel-based method, the proposed algorithm is stable and has good convergence properties. However, unlike other similar algorithms,iKBSF's space complexity is independent of the number of sample transitions, and as a result it can process an arbitrary amount of data. We present theoretical results showing that iKBSF can approximate (to any level of accuracy) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator. In order to show the effectiveness of the proposed algorithm in practice, we apply iKBSF to the challenging three-pole balancing task, where the ability to process a large number of transitions is crucial for achieving a high success rate."
Semiparametric Bigraphical Models,"In multivariate analysis, a Gaussian bigraphical model is commonly used for modeling matrix-valued data. In this paper we propose a semiparametric extension of the Gaussian bigraphical model, called the nonparanormal bigraphical model. Nonparametric rank-based regularization estimators are exploited to  estimate the sparse precision matrices and graphs under a  penalized profile likelihood framework. Theoretically,  our semiparametric procedure achieves the  parametric rate of convergence for both parameter estimation and graph recovery. Empirically, our semiparametric approach significantly outperforms its parametric counterpart for non-Gaussian data and behaves competitive even for Gaussian data. "
Multiclass Clustering using a Semidefinite Relaxation,"Spectral and other cut-based relaxations have been applied to graph clustering problems. In this paper, we propose a novel semidefinite relaxation for graph clustering known as Max-cut clustering. The clustering problem is formulated in terms of a discrete optimization problem and then relaxed to a SDP. To make the optimization scalable, we represent the SDP by a low-rank factorized approximation that reduces the number of variables, and then use a simple projected gradient method to solve it. To obtain the clustering, we propose a reweighted rounding scheme to get integral solutions. We also extend this formulation to a global approach to multi-class clustering and MAP inference in graphical models. Experimental results indicate that we outperform state-of-art several clustering methods. The algorithm is extended to perform MAP inference in graphical models and outperforms competing methods."
Multiclass Clustering using a Semidefinite Relaxation,"Spectral and other cut-based relaxations have been applied to graph clustering problems. In this paper, we propose a novel semidefinite relaxation for graph clustering known as Max-cut clustering. The clustering problem is formulated in terms of a discrete optimization problem and then relaxed to a SDP. To make the optimization scalable, we represent the SDP by a low-rank factorized approximation that reduces the number of variables, and then use a simple projected gradient method to solve it. To obtain the clustering, we propose a reweighted rounding scheme to get integral solutions. We also extend this formulation to a global approach to multi-class clustering and MAP inference in graphical models. Experimental results indicate that we outperform state-of-art several clustering methods. The algorithm is extended to perform MAP inference in graphical models and outperforms competing methods."
A Performance Function for Multi-class Classification,"A performance function for multi-class classification is proposed in this paper. This performance function takes Nearest Subspace (NS) residual together with Collaborative Representation (CR) residual as variables. Strong underlying geometric explanations make those well-known residual measurements effective for multi-class classification problem. Nearest Subspace Classification (NSC) is a local measurement that considers distance between testing sample and each class, while Collaborative Representation based Classifier (CRC) is a global method measuring both intra-class and inter-class measurements. These two measurements are independent to each other. The first and the second order Taylor series of the performance function are analyzed, which characterizes this function well in some degree. A Second Order Performance Function (SOPF) is derived by involving a quadratic term of the first order terms and a product term. The SOPF contains two parameters with a positive factor constraint. A classifier based on SOPF is proposed, which improves a recent reported classifier called CROC(Collaborative Representation Optimized Classifier). The proposed algorithm is tested against human face and handwritten digits recognition and it achieves competitive classification result comparing to baseline methods. A large range of parameter configuration is acceptable once the positive factor constraint is satisfied."
A Performance Function for Multi-class Classification,"A performance function for multi-class classification is proposed in this paper. This performance function takes Nearest Subspace (NS) residual together with Collaborative Representation (CR) residual as variables. Strong underlying geometric explanations make those well-known residual measurements effective for multi-class classification problem. Nearest Subspace Classification (NSC) is a local measurement that considers distance between testing sample and each class, while Collaborative Representation based Classifier (CRC) is a global method measuring both intra-class and inter-class measurements. These two measurements are independent to each other. The first and the second order Taylor series of the performance function are analyzed, which characterizes this function well in some degree. A Second Order Performance Function (SOPF) is derived by involving a quadratic term of the first order terms and a product term. The SOPF contains two parameters with a positive factor constraint. A classifier based on SOPF is proposed, which improves a recent reported classifier called CROC(Collaborative Representation Optimized Classifier). The proposed algorithm is tested against human face and handwritten digits recognition and it achieves competitive classification result comparing to baseline methods. A large range of parameter configuration is acceptable once the positive factor constraint is satisfied."
Identifiability and Unmixing of Latent Parse Trees,"This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models."
Identifiability and Unmixing of Latent Parse Trees,"This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models."
Identifiability and Unmixing of Latent Parse Trees,"This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models."
A Novel Adaptive Geometric Mapping For Multi-Class Classification,"Solving multi-class problems is one of the challenging problems in machine learning field. Although several powerful binary classifiers have been developed, it is still an ongoing research issue to effectively extend these binary classifiers for multi-class cases.  In this paper we propose a new method for multi-class classification which is based on an adaptive mapping of data points into linearly separable classes. Our strategy, which is inspired from geometry laws, can handle multi-class cases directly. It has the capability of not increasing the dimension of data while separating them in a stable dimension. This algorithm generates the suitable mapping matrix using a vector of linear coefficients. We evaluate the performance of our classifier with different state-of-art support vector machine based methods and KNN on five data sets named as IRIS, WDBC, Glass, Wine, and Liver disorders. On most of these datasets, our implementation outperforms the best reported results and achieves comparable accuracy on the rest which demonstrates its effectiveness."
A Novel Adaptive Geometric Mapping For Multi-Class Classification,"Solving multi-class problems is one of the challenging problems in machine learning field. Although several powerful binary classifiers have been developed, it is still an ongoing research issue to effectively extend these binary classifiers for multi-class cases.  In this paper we propose a new method for multi-class classification which is based on an adaptive mapping of data points into linearly separable classes. Our strategy, which is inspired from geometry laws, can handle multi-class cases directly. It has the capability of not increasing the dimension of data while separating them in a stable dimension. This algorithm generates the suitable mapping matrix using a vector of linear coefficients. We evaluate the performance of our classifier with different state-of-art support vector machine based methods and KNN on five data sets named as IRIS, WDBC, Glass, Wine, and Liver disorders. On most of these datasets, our implementation outperforms the best reported results and achieves comparable accuracy on the rest which demonstrates its effectiveness."
Preparing Deep Belief Networks for Pratical Tasks,"Deep Belief Networks (DBNs) is a probabilistic generative models composed of multiple layers of stochastic, latent variables. The network can learn many layers of features on various type of data such as gray scaled images, color images and acoustic data. This paper further examined the ability of DBNs to interpret the binary representation of data. The performance is validated by learning given distributions such as normal distribution, Poisson distribution and random number generator. We have shown that Deep Believe Networks can successfully learn the probability distribution with binary encoded dataset. With this property, we can further extend DBNs into states or properties prediction application,we will provide a example showing that DBNs can take multiple binary encoded parameter as input vector and predict the belong category of these input. Generally, the sensory input of DBNs contains information belong to a certain timestep, that is, the prediction depends only on the input. However, in some practical tasks, prediction often depend not only on the current state but also the history of states. We propose a method combining DBNs with Echo State Networks(ESNs), using the properties of ESNs' reservoir to encoded the history of previous states which gives us an idea of artificial dreaming."
Learning Hierarchical Spatial Tiling Representation for Scene Tagging,"In order to name and localize semantic tags/attributes on natural scene images, in this paper, we first propose a structure learning method to learn a novel representation for scene modeling, namely Hierarchical Space Tiling (HST). It is able to account for the structure variations of scene configurations using different parts/words in the learned tilling dictionary. Then, the association relationship between a part and a semantic tag/attribute is discovered by exploring their mutual information on scene images. Finally, given a naked image, we first parse it into a tree structure with the learned HST model, then assign tags to the terminal nodes/parts on the hierarchy. We evaluate the advantages of the proposed method from three aspects. (i) The proposed HST is compact and less ambiguous in constructing the compositions of scenes. (ii) The semantic tags are named and localized accurately on scene images. (iii) It has scalability potential to real applications by showing that the parsing + tagging is extremely fast. "
Learning Hierarchical Spatial Tiling Representation for Scene Tagging,"In order to name and localize semantic tags/attributes on natural scene images, in this paper, we first propose a structure learning method to learn a novel representation for scene modeling, namely Hierarchical Space Tiling (HST). It is able to account for the structure variations of scene configurations using different parts/words in the learned tilling dictionary. Then, the association relationship between a part and a semantic tag/attribute is discovered by exploring their mutual information on scene images. Finally, given a naked image, we first parse it into a tree structure with the learned HST model, then assign tags to the terminal nodes/parts on the hierarchy. We evaluate the advantages of the proposed method from three aspects. (i) The proposed HST is compact and less ambiguous in constructing the compositions of scenes. (ii) The semantic tags are named and localized accurately on scene images. (iii) It has scalability potential to real applications by showing that the parsing + tagging is extremely fast. "
Encoding Natural Images with Mean and Covariance,"Here we show how the mean and covariance of the statistical structure of natural images can be learned efficiently by using maximum likelihood estimation. In particular, the parameters of mean and covariance are learned with two sets of latent variables independently. Simulation results demonstrate that the model is ableto successfully capture details of the natural image distribution not represented by either covariance or mean alone. The joint approach is thus a step towards a more realistic natural image representation."
Constructing ?2-graph for Clustering,"Constructing a sparse similarity graph is an important step in graph-oriented clustering algorithms. In a similarity graph, the vertex denotes a data point and the connection weight between two data points represents the similarity. Some recent works use ?1-minimization based sparse coefficients to construct the graph for various applications, and impressive results are achieved. This paper proposes a method to construct the similarity graph, called ?2-graph, by using ?2-minimization based representation coefficients. This graph can produce a block-sparse similarity graph via enforcing locality onto the non-sparse representation. The representation is derived via solving an optimization problem to obtain an interesting closed-form solution. Experimental results using several facial databases demonstrate that the proposed method outperforms two state-of-the-art ?1-minimization based clustering algorithms, i.e., Sparse Subspace Clustering [1, 2] and Low Rank Representation [3], in accuracy, robustness and time saving."
Bayesian nonparametric models for ranked data,"We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items.   Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process.  We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation.  We then develop a time-varying extension of our model, and apply our model to the New York Times lists of weekly bestselling books."
Bayesian nonparametric models for ranked data,"We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items.   Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process.  We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation.  We then develop a time-varying extension of our model, and apply our model to the New York Times lists of weekly bestselling books."
Object Detection from Multiple Overlapping Views,"We present a method for object detection in a multi view 3D model. We use highly overlapping views, geometric data, and semantic surface classification in order to boost existing 2D algorithms. Specifically, a 3D model is computed from the overlapping views, and the model is segmented into semantic labels using height information, color and planar qualities. 2D detector is run on all images and then detections are mapped into 3D via the model. The detections are clustered in 3D and represented by 3D boxes. Finally, the detections, visibility maps and semantic labels are combined using a Support Vector Machine to achieve a more robust object detector."
Object Detection from Multiple Overlapping Views,"We present a method for object detection in a multi view 3D model. We use highly overlapping views, geometric data, and semantic surface classification in order to boost existing 2D algorithms. Specifically, a 3D model is computed from the overlapping views, and the model is segmented into semantic labels using height information, color and planar qualities. 2D detector is run on all images and then detections are mapped into 3D via the model. The detections are clustered in 3D and represented by 3D boxes. Finally, the detections, visibility maps and semantic labels are combined using a Support Vector Machine to achieve a more robust object detector."
Feature-aware Label Space Dimension Reduction for Multi-label Classification,"Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets."
Feature-aware Label Space Dimension Reduction for Multi-label Classification,"Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets."
Sparse Matrix based Random Projection for Face Recognition,"Random projection (RP) is a powerful method in dimensionality reduction for itsdata independence and lower computation requirement. For the application ofRP, the construction of random matrix is critical due to its instability in performance.However, there is few directional work in this respect. Although a fewtheoretical work has proposed some matrices in terms of performance distortionand computation cost in the past decade, to the best of our knowledge, there is nocomprehensive theoretical or experimental work to compare their performance. Inthis paper, we attempt to evaluate current popular RP matrices by extensive experimentswith face recognition, and propose one kind of most sparsest RP matrices,which shows better performance than existing RP matrices with nearly the lowestcomputation complexity."
Recovery Guarantees of Augmented Trace Norm Models in Tensor Recovery,"This paper studies the recovery guarantees of the models of minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ where $\mathcal{X}$ is a tensor and $\|\mathcal{X}\|_*$ and $\|\mathcal{X}\|_F$ are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors.In particular, they enjoy exact guarantees similar to those known for minimizing$\|\mathcal{X}\|_*$ under the conditions on the sensing operator such as its null-space property, restrictedisometry property, or spherical section property. To recover a low-rank tensor$\mathcal{X}^0$, minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ returns the same solution as minimizing $\|\mathcal{X}\|_*$ almost whenever$\alpha\geq10\mathop {\max}\limits_{i}\|X^0_{(i)}\|_2$."
Graphical Gaussian Vector for Image Categorization,"This paper proposes a novel image representation called a Graphical Gaussian Vector, which is a counterpart of the codebook and local feature matching approaches. In our method, we model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. We consider the parameter of GMRF as a feature vector of the image. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Finally we define a new image feature by embedding the metric into the parameters, which can be directly applied to scalable linear classifiers. Our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. As the proposed method simply calculates the local auto-correlations of local features, it is able to achieve both high classification accuracy and high efficiency."
High theta and low alpha waves may be a pattern for BCI illiteracy in motor imagery ,"While brain computer interfaces (BCI) can be employed for patients and healthy humans, there are problems that need to be solved before the technique will become a useful tool. In most BCI systems, the significant number of target users are not able to use BCI systems with a control paradigm such as motor imagery (MI), P300, and steady state evoked potential (SSEP). Such target users are called the BCI illiterate users. Only a few studies, however, investigated such phenomena and they have not provided a clear understanding of the BCI illiteracy mechanism or solution to this problem. Recently, alpha power in default mode network (DMN) was proposed to predict a user?s potential performance in MI BCI, and the causal relationship of gamma band to sensory motor rhythm was reported. However, what differences exist between BCI-literate and BCI-illiterate groups are not fully understood; moreover, the theta band has not been thoroughly investigated for BCI illiteracy. In this study, we sought to demonstrate the neurophysiological differences between two groups (literate, illiterate) among 52 subjects using a default mode network during the eyes-opened state. As a result, we found that high theta and low alpha waves are a pattern for BCI illiteracy relative to BCI-literate persons. Using an un-paired student t-test, we found that the spatially significant areas between the two groups were found in the frontal and post-parietal areas for theta, roughly in the overall area for alpha, and in the post-central area for gamma. In addition, from the results of the relationship between band power and offline accuracy, we propose a simple performance predictor using four band powers. This gives a Pearson correlation coefficient of r=0.59, indicating that our proposed predictor explains 35% of the variance in subject performance."
High theta and low alpha waves may be a pattern for BCI illiteracy in motor imagery ,"While brain computer interfaces (BCI) can be employed for patients and healthy humans, there are problems that need to be solved before the technique will become a useful tool. In most BCI systems, the significant number of target users are not able to use BCI systems with a control paradigm such as motor imagery (MI), P300, and steady state evoked potential (SSEP). Such target users are called the BCI illiterate users. Only a few studies, however, investigated such phenomena and they have not provided a clear understanding of the BCI illiteracy mechanism or solution to this problem. Recently, alpha power in default mode network (DMN) was proposed to predict a user?s potential performance in MI BCI, and the causal relationship of gamma band to sensory motor rhythm was reported. However, what differences exist between BCI-literate and BCI-illiterate groups are not fully understood; moreover, the theta band has not been thoroughly investigated for BCI illiteracy. In this study, we sought to demonstrate the neurophysiological differences between two groups (literate, illiterate) among 52 subjects using a default mode network during the eyes-opened state. As a result, we found that high theta and low alpha waves are a pattern for BCI illiteracy relative to BCI-literate persons. Using an un-paired student t-test, we found that the spatially significant areas between the two groups were found in the frontal and post-parietal areas for theta, roughly in the overall area for alpha, and in the post-central area for gamma. In addition, from the results of the relationship between band power and offline accuracy, we propose a simple performance predictor using four band powers. This gives a Pearson correlation coefficient of r=0.59, indicating that our proposed predictor explains 35% of the variance in subject performance."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Reinforcement Learning and Hierarchical State Representation for Modeling Incubation and Restructuring in Primate Insightful Problem Solving,The paper proposes a reinforcement learning model with hierarchical state representation for modeling the learning behavior observed in rhesus monkeys in a reverse-reward contingency task. The hierarchical state representation provides an ability to solve insightful problems by restructuring the internal belief representation of the environment through incubation in which the evidence of constructing an appropriate representation is accumulated. The experiment and simulation results show that the proposed model accounts for the three-stage learning patterns observed in experiments on rhesus monkeys. It also applies to behavioral results observed in rhesus monkeys on the same task during a transfer test when novel quantity combinations are presented.
Reinforcement Learning and Hierarchical State Representation for Modeling Incubation and Restructuring in Primate Insightful Problem Solving,The paper proposes a reinforcement learning model with hierarchical state representation for modeling the learning behavior observed in rhesus monkeys in a reverse-reward contingency task. The hierarchical state representation provides an ability to solve insightful problems by restructuring the internal belief representation of the environment through incubation in which the evidence of constructing an appropriate representation is accumulated. The experiment and simulation results show that the proposed model accounts for the three-stage learning patterns observed in experiments on rhesus monkeys. It also applies to behavioral results observed in rhesus monkeys on the same task during a transfer test when novel quantity combinations are presented.
Reinforcement Learning and Hierarchical State Representation for Modeling Incubation and Restructuring in Primate Insightful Problem Solving,The paper proposes a reinforcement learning model with hierarchical state representation for modeling the learning behavior observed in rhesus monkeys in a reverse-reward contingency task. The hierarchical state representation provides an ability to solve insightful problems by restructuring the internal belief representation of the environment through incubation in which the evidence of constructing an appropriate representation is accumulated. The experiment and simulation results show that the proposed model accounts for the three-stage learning patterns observed in experiments on rhesus monkeys. It also applies to behavioral results observed in rhesus monkeys on the same task during a transfer test when novel quantity combinations are presented.
Proper losses for learning from partial labels,"This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, establish a necessary and sufficient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. An interesting result is that the full knowledge of this matrix is not required, and losses can be constructed that are proper in a subset of the probability simplex."
Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation,"Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented."
Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation,"Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented."
Limiting memory improves online particle-?lter learners for word segmentation,"This paper shows that limiting the memory of an on-line particle-?lter algorithm for word segmentation improves its accuracy, yielding accuracies that are competitive with state-of-the-art batch algorithms. Our algorithm is derived by replacing the Chinese Restaurant Processes in a non-parametric Bayesian word segmentation model with distance-dependent Chinese Restaurant Processes (Blei and Frazier, 2011). This is easiest to do in a particle-?lter algorithm (B?rschinger and Johnson, 2011), and leads to a bounded-memory, on-line learning algorithm whose accuracy on standard evaluation data actually exceeds that of the corresponding algorithm without limited memory. We discuss the relevance of our results for ?over-learning? (speci?cally, under-segmentation) in Bayesian modelsand the ?less-is-more? effect (Newport, 1990)."
Limiting memory improves online particle-?lter learners for word segmentation,"This paper shows that limiting the memory of an on-line particle-?lter algorithm for word segmentation improves its accuracy, yielding accuracies that are competitive with state-of-the-art batch algorithms. Our algorithm is derived by replacing the Chinese Restaurant Processes in a non-parametric Bayesian word segmentation model with distance-dependent Chinese Restaurant Processes (Blei and Frazier, 2011). This is easiest to do in a particle-?lter algorithm (B?rschinger and Johnson, 2011), and leads to a bounded-memory, on-line learning algorithm whose accuracy on standard evaluation data actually exceeds that of the corresponding algorithm without limited memory. We discuss the relevance of our results for ?over-learning? (speci?cally, under-segmentation) in Bayesian modelsand the ?less-is-more? effect (Newport, 1990)."
Selecting Diverse Features via Spectral Regularization,"We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse featuresthat can predict a given objective. Diversity is usefulfor several reasons such as interpretability, robustness to noise, etc.  We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized approximately by efficient greedy and local search algorithms, with provable guarantees.  We compare our algorithms to traditional greedy and $\ell_1$-regularizationschemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations."
Selecting Diverse Features via Spectral Regularization,"We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse featuresthat can predict a given objective. Diversity is usefulfor several reasons such as interpretability, robustness to noise, etc.  We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized approximately by efficient greedy and local search algorithms, with provable guarantees.  We compare our algorithms to traditional greedy and $\ell_1$-regularizationschemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations."
Selecting Diverse Features via Spectral Regularization,"We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse featuresthat can predict a given objective. Diversity is usefulfor several reasons such as interpretability, robustness to noise, etc.  We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized approximately by efficient greedy and local search algorithms, with provable guarantees.  We compare our algorithms to traditional greedy and $\ell_1$-regularizationschemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations."
Accelerating Graphical Models Inference via Iterative Algorithms,"In tree-structured graphical models, the max-product algorithm provides efficient and exact solutions to inference problems. However, as the number of states becomes large, the max-product algorithm becomes prohibitively slow. In this paper, we propose an iterative max-product algorithm to accelerate the inference. In addition, we apply iterative procedure to accelerate the k-best inference problem (in contrast to 1-best). We show the efficiency of these iterative based algorithms using both synthetic data and real world data."
Accelerating Graphical Models Inference via Iterative Algorithms,"In tree-structured graphical models, the max-product algorithm provides efficient and exact solutions to inference problems. However, as the number of states becomes large, the max-product algorithm becomes prohibitively slow. In this paper, we propose an iterative max-product algorithm to accelerate the inference. In addition, we apply iterative procedure to accelerate the k-best inference problem (in contrast to 1-best). We show the efficiency of these iterative based algorithms using both synthetic data and real world data."
Sample-based non-negative matrix tri-factorization,"We present a general dimensionality reduction algorithm applicable to non-negative bi-dimensional data sets composed of multiple samples. The new algorithm extends the standard non-negative matrix factorization to a constrained tri-factor decomposition. Our non-negative matrix tri-factorization has the unique feature of reducing dimensionality while identifying sample-independent factors in both the rows and columns of the input matrices, separately and simultaneously. This decomposition is motivated by neurophysiological data for which time-varying signals are typically recorded from several sources and in multiple samples. We derive the main algorithm, referred to as sNM3F, and present two possibly useful variants with orthogonality and time-shifts features. By applying the method to simulated muscle patterns we demonstrate that, compared to standard decompositions, our algorithm is effective in identifying reliably the underlying structure. Finally, we use it to decompose electromyographic signals recorded during arm reaching movements into separate spatial and temporal components."
Sample-based non-negative matrix tri-factorization,"We present a general dimensionality reduction algorithm applicable to non-negative bi-dimensional data sets composed of multiple samples. The new algorithm extends the standard non-negative matrix factorization to a constrained tri-factor decomposition. Our non-negative matrix tri-factorization has the unique feature of reducing dimensionality while identifying sample-independent factors in both the rows and columns of the input matrices, separately and simultaneously. This decomposition is motivated by neurophysiological data for which time-varying signals are typically recorded from several sources and in multiple samples. We derive the main algorithm, referred to as sNM3F, and present two possibly useful variants with orthogonality and time-shifts features. By applying the method to simulated muscle patterns we demonstrate that, compared to standard decompositions, our algorithm is effective in identifying reliably the underlying structure. Finally, we use it to decompose electromyographic signals recorded during arm reaching movements into separate spatial and temporal components."
Sample-based non-negative matrix tri-factorization,"We present a general dimensionality reduction algorithm applicable to non-negative bi-dimensional data sets composed of multiple samples. The new algorithm extends the standard non-negative matrix factorization to a constrained tri-factor decomposition. Our non-negative matrix tri-factorization has the unique feature of reducing dimensionality while identifying sample-independent factors in both the rows and columns of the input matrices, separately and simultaneously. This decomposition is motivated by neurophysiological data for which time-varying signals are typically recorded from several sources and in multiple samples. We derive the main algorithm, referred to as sNM3F, and present two possibly useful variants with orthogonality and time-shifts features. By applying the method to simulated muscle patterns we demonstrate that, compared to standard decompositions, our algorithm is effective in identifying reliably the underlying structure. Finally, we use it to decompose electromyographic signals recorded during arm reaching movements into separate spatial and temporal components."
Sample-based non-negative matrix tri-factorization,"We present a general dimensionality reduction algorithm applicable to non-negative bi-dimensional data sets composed of multiple samples. The new algorithm extends the standard non-negative matrix factorization to a constrained tri-factor decomposition. Our non-negative matrix tri-factorization has the unique feature of reducing dimensionality while identifying sample-independent factors in both the rows and columns of the input matrices, separately and simultaneously. This decomposition is motivated by neurophysiological data for which time-varying signals are typically recorded from several sources and in multiple samples. We derive the main algorithm, referred to as sNM3F, and present two possibly useful variants with orthogonality and time-shifts features. By applying the method to simulated muscle patterns we demonstrate that, compared to standard decompositions, our algorithm is effective in identifying reliably the underlying structure. Finally, we use it to decompose electromyographic signals recorded during arm reaching movements into separate spatial and temporal components."
Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
Partial Gaussian Graphical Model Estimation,"This paper studies the partial estimation of Gaussian graphical models from high-dimensional empirical observations. We derive a convex formulation for this problem using l1-regularized maximum-likelihood estimation, which can be solved via a block coordinate descent algorithm. Statistical estimation performance can be established for our method. The proposed approach has competitive empirical performance compared to existing methods, as demonstrated by various experiments on synthetic and real datasets."
Sparse Online Topic Models,"Probabilistic online topic models have been developed for discovering latent semantic representations from massive data corpora. However, due to normalization constraints, probabilistic topic models can be ineffective in controlling the sparsity of discovered representations. In this paper, we present a sparse online topic model, which directly controls the sparsity of word and document codes by imposing sparsity-inducing regularization. The topical dictionary is learned by an online algorithm, which is efficient and guaranteed to converge. We extensively evaluate the basic sparse online topic model as well as its collapsed and supervised extensions on large-scale data sets. Our results demonstrate appealing performance."
Discriminative Sub-categorization,"The objective of this work is to learn sub-categories. Rather thancasting this simply as a problem of unsupervised clustering, we investigatea weakly supervised approach using both positive and negativesamples of the category.We make the following contributions: (i) we introduce a new model fordiscriminative sub-categorization which determines clustermembership for positive samples whilst simultaneously learning amax-margin classifier to separate each cluster from thenegative samples; (ii) we show that this model does not suffer fromthe degenerate cluster problem that afflicts several competing methods(e.g., Latent SVM and Max-Margin Clustering); (iii) we show that the method is able to discoverinterpretable sub-categories in various datasets.The model is evaluated experimentally over several UCI datasets, andits performance advantages over $k$-means and Latent SVM are demonstrated. We also stresstest the model and show its resilience in discovering sub-categoriesas the parameters are varied.  "
Discriminative Sub-categorization,"The objective of this work is to learn sub-categories. Rather thancasting this simply as a problem of unsupervised clustering, we investigatea weakly supervised approach using both positive and negativesamples of the category.We make the following contributions: (i) we introduce a new model fordiscriminative sub-categorization which determines clustermembership for positive samples whilst simultaneously learning amax-margin classifier to separate each cluster from thenegative samples; (ii) we show that this model does not suffer fromthe degenerate cluster problem that afflicts several competing methods(e.g., Latent SVM and Max-Margin Clustering); (iii) we show that the method is able to discoverinterpretable sub-categories in various datasets.The model is evaluated experimentally over several UCI datasets, andits performance advantages over $k$-means and Latent SVM are demonstrated. We also stresstest the model and show its resilience in discovering sub-categoriesas the parameters are varied.  "
Solving Relational MDPs with Exogenous Events and Additive Rewards,"We formalize a simple but natural subclass of service domains for relational planning problems with object-centered independent exogenous events and additive rewards, capturing, for example, problems in inventory control and fire and rescue operations. Focusing on this subclass, we then present the first complete symbolic solution for stochastic planning problems in relational domains that is able to handle exogenous events and additive rewards, and is independent of domain size.  Our planning algorithm provides a lower bound approximation on the optimal solution given by the true value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation that can represent and manipulate real-valued functions over relational world states. A preliminary experimental evaluation demonstrates the validity and potential of our approach.  "
Monte Carlo Methods for Maximum Margin Supervised Topic Models,"An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihood-based supervised topic models, of which posterior inference can be carried out using the Bayes' rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper, we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency."
Input Variable Selection for Linear Regression Model using Nearest Correlation Spectral Clustering,"Linear regression models have been widely accepted in many scientific and engineering fields for the estimation or interpretation of phenomena.  When a linear regression model is built, appropriate input variables have to be selected to achieve high estimation performance.  This work proposes new methodologies for selecting input variables for linear regression models using nearest correlation spectral clustering (NCSC), which is a correlation-based clustering method.  In the present work, NCSC is used for variable group construction, and a few variable groups are selected by their contribution to estimates or by group Lasso; they are referred to as NCSC-based variable selection (NCSC-VS) and NCSC-group Lasso (NCSC-GL). The performances of the proposed NCSC-VS and NCSC-GL are examined through a case study of chemometrics data."
Input Variable Selection for Linear Regression Model using Nearest Correlation Spectral Clustering,"Linear regression models have been widely accepted in many scientific and engineering fields for the estimation or interpretation of phenomena.  When a linear regression model is built, appropriate input variables have to be selected to achieve high estimation performance.  This work proposes new methodologies for selecting input variables for linear regression models using nearest correlation spectral clustering (NCSC), which is a correlation-based clustering method.  In the present work, NCSC is used for variable group construction, and a few variable groups are selected by their contribution to estimates or by group Lasso; they are referred to as NCSC-based variable selection (NCSC-VS) and NCSC-group Lasso (NCSC-GL). The performances of the proposed NCSC-VS and NCSC-GL are examined through a case study of chemometrics data."
Reinforcement Learning Behavior in Sponsored Search,"This paper is concerned with the modeling of advertiser behavior in sponsored search. Modeling advertiser behavior can help search engines better serve advertisers, improve auction mechanism, and forecast future revenue. Most of previous work assume that advertisers have perfect access to necessary information for making their decisions. However, as we know, there are many factors in the system which are hardly known to advertisers and make the above assumptions unreasonable. To tackle the challenge, we propose viewing sponsored search as a reinforcement learning (RL) system for each advertiser, and employing a RL behavior model to describe how each advertiser responds to the system. In the proposed model, advertisers estimate the utility for winning each rank position based on the signals provided by the search engine, target the most preferred position based on the estimation, and then adjust their bids accordingly to achieve this target. The proposed model does not assume perfect information access, but only assumes the observation of the signals provided by the search engine. Furthermore, it does not specify how advertisers actually utilize the observed information to make decision. Instead, one single parameter, the learning rate, is used to describe advertisers' ability in collecting and analyzing information, and learning how to appropriately behave. Our experiments show that our model outperforms previous models in the task of bid prediction and rank position prediction, demonstrating its advantage and flexibility in fitting real data. In addition to the short-term prediction capability, we also study the long-term outcome of the sponsored search system, if all the advertisers behave according to the proposed RL behavior model. Our theoretical analysis shows that under certain conditions, the dynamic system of sponsored search will converge to a locally envy-free equilibrium, which verifies the soundness of our model from another angle."
Reinforcement Learning Behavior in Sponsored Search,"This paper is concerned with the modeling of advertiser behavior in sponsored search. Modeling advertiser behavior can help search engines better serve advertisers, improve auction mechanism, and forecast future revenue. Most of previous work assume that advertisers have perfect access to necessary information for making their decisions. However, as we know, there are many factors in the system which are hardly known to advertisers and make the above assumptions unreasonable. To tackle the challenge, we propose viewing sponsored search as a reinforcement learning (RL) system for each advertiser, and employing a RL behavior model to describe how each advertiser responds to the system. In the proposed model, advertisers estimate the utility for winning each rank position based on the signals provided by the search engine, target the most preferred position based on the estimation, and then adjust their bids accordingly to achieve this target. The proposed model does not assume perfect information access, but only assumes the observation of the signals provided by the search engine. Furthermore, it does not specify how advertisers actually utilize the observed information to make decision. Instead, one single parameter, the learning rate, is used to describe advertisers' ability in collecting and analyzing information, and learning how to appropriately behave. Our experiments show that our model outperforms previous models in the task of bid prediction and rank position prediction, demonstrating its advantage and flexibility in fitting real data. In addition to the short-term prediction capability, we also study the long-term outcome of the sponsored search system, if all the advertisers behave according to the proposed RL behavior model. Our theoretical analysis shows that under certain conditions, the dynamic system of sponsored search will converge to a locally envy-free equilibrium, which verifies the soundness of our model from another angle."
Reinforcement Learning Behavior in Sponsored Search,"This paper is concerned with the modeling of advertiser behavior in sponsored search. Modeling advertiser behavior can help search engines better serve advertisers, improve auction mechanism, and forecast future revenue. Most of previous work assume that advertisers have perfect access to necessary information for making their decisions. However, as we know, there are many factors in the system which are hardly known to advertisers and make the above assumptions unreasonable. To tackle the challenge, we propose viewing sponsored search as a reinforcement learning (RL) system for each advertiser, and employing a RL behavior model to describe how each advertiser responds to the system. In the proposed model, advertisers estimate the utility for winning each rank position based on the signals provided by the search engine, target the most preferred position based on the estimation, and then adjust their bids accordingly to achieve this target. The proposed model does not assume perfect information access, but only assumes the observation of the signals provided by the search engine. Furthermore, it does not specify how advertisers actually utilize the observed information to make decision. Instead, one single parameter, the learning rate, is used to describe advertisers' ability in collecting and analyzing information, and learning how to appropriately behave. Our experiments show that our model outperforms previous models in the task of bid prediction and rank position prediction, demonstrating its advantage and flexibility in fitting real data. In addition to the short-term prediction capability, we also study the long-term outcome of the sponsored search system, if all the advertisers behave according to the proposed RL behavior model. Our theoretical analysis shows that under certain conditions, the dynamic system of sponsored search will converge to a locally envy-free equilibrium, which verifies the soundness of our model from another angle."
Reinforcement Learning Behavior in Sponsored Search,"This paper is concerned with the modeling of advertiser behavior in sponsored search. Modeling advertiser behavior can help search engines better serve advertisers, improve auction mechanism, and forecast future revenue. Most of previous work assume that advertisers have perfect access to necessary information for making their decisions. However, as we know, there are many factors in the system which are hardly known to advertisers and make the above assumptions unreasonable. To tackle the challenge, we propose viewing sponsored search as a reinforcement learning (RL) system for each advertiser, and employing a RL behavior model to describe how each advertiser responds to the system. In the proposed model, advertisers estimate the utility for winning each rank position based on the signals provided by the search engine, target the most preferred position based on the estimation, and then adjust their bids accordingly to achieve this target. The proposed model does not assume perfect information access, but only assumes the observation of the signals provided by the search engine. Furthermore, it does not specify how advertisers actually utilize the observed information to make decision. Instead, one single parameter, the learning rate, is used to describe advertisers' ability in collecting and analyzing information, and learning how to appropriately behave. Our experiments show that our model outperforms previous models in the task of bid prediction and rank position prediction, demonstrating its advantage and flexibility in fitting real data. In addition to the short-term prediction capability, we also study the long-term outcome of the sponsored search system, if all the advertisers behave according to the proposed RL behavior model. Our theoretical analysis shows that under certain conditions, the dynamic system of sponsored search will converge to a locally envy-free equilibrium, which verifies the soundness of our model from another angle."
On Smoothness in Low-Rank Models,"We propose the Smooth Low-Rank Models (SLRM) to address problems in applications where the data matrix is not only low-rank, but also has a small total variation. Low-rank models are important in a number of problems such as matrix completion, denoising, and motion recovery. However, exact recovery of a low-rank matrix from a set of randomly sampled entries is not guaranteed when an entire column or row is not sampled. The problem can be alleviated if  prior information, such as smoothness in data, is available. This can be formulated as a nuclear-norm minimization problem, regularized by a Total Variation (TV) constraint.   Alternating Direction Method of Multiplier (ADMM) is used for solving the model. We studied the problems of matrix completion, denoising  and  motion capture data reconstruction.  Experiments on synthetic data, motion capture data and background modeling datasets demonstrated that SLRMs can significantly improve upon the original low-rank models and superior than other state-of-the-art models, especially when the set of sampled entries are insufficient or corruption is heavy. "
On Smoothness in Low-Rank Models,"We propose the Smooth Low-Rank Models (SLRM) to address problems in applications where the data matrix is not only low-rank, but also has a small total variation. Low-rank models are important in a number of problems such as matrix completion, denoising, and motion recovery. However, exact recovery of a low-rank matrix from a set of randomly sampled entries is not guaranteed when an entire column or row is not sampled. The problem can be alleviated if  prior information, such as smoothness in data, is available. This can be formulated as a nuclear-norm minimization problem, regularized by a Total Variation (TV) constraint.   Alternating Direction Method of Multiplier (ADMM) is used for solving the model. We studied the problems of matrix completion, denoising  and  motion capture data reconstruction.  Experiments on synthetic data, motion capture data and background modeling datasets demonstrated that SLRMs can significantly improve upon the original low-rank models and superior than other state-of-the-art models, especially when the set of sampled entries are insufficient or corruption is heavy. "
On Smoothness in Low-Rank Models,"We propose the Smooth Low-Rank Models (SLRM) to address problems in applications where the data matrix is not only low-rank, but also has a small total variation. Low-rank models are important in a number of problems such as matrix completion, denoising, and motion recovery. However, exact recovery of a low-rank matrix from a set of randomly sampled entries is not guaranteed when an entire column or row is not sampled. The problem can be alleviated if  prior information, such as smoothness in data, is available. This can be formulated as a nuclear-norm minimization problem, regularized by a Total Variation (TV) constraint.   Alternating Direction Method of Multiplier (ADMM) is used for solving the model. We studied the problems of matrix completion, denoising  and  motion capture data reconstruction.  Experiments on synthetic data, motion capture data and background modeling datasets demonstrated that SLRMs can significantly improve upon the original low-rank models and superior than other state-of-the-art models, especially when the set of sampled entries are insufficient or corruption is heavy. "
Path Integral Control by Reproducing Kernel Hilbert Space Embedding,"We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided."
A Linear Time Active Learning Algorithm for Link Classification,"We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V,E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\log|V|$."
A Linear Time Active Learning Algorithm for Link Classification,"We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V,E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\log|V|$."
A Linear Time Active Learning Algorithm for Link Classification,"We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V,E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\log|V|$."
A Linear Time Active Learning Algorithm for Link Classification,"We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V,E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\log|V|$."
How can probable mechanisms of object recognition in the visual cortex be determined by visual features?,"There are lines of evidence demonstrating that the fusiform face area (FFA) preforms processes related to specific object recognition, which requires expertise (e.g. facial recognition). It seems that our brain utilize different mechanisms for generic object recognizing (e.g. face vs. non-face) and specific recognition (e.g. two different faces). To address this issue, we first introduce a biologically inspired object recognition model and then examine it in two experiments. The model has a hierarchical structure that employs a biologically plausible approach for visual feature extraction. Our results suggest that the mechanisms underlying generic and specific object recognition are performed distinctly. The results are in agreement with evidence that indicates inferotemporal cortex (IT) is responsible for object recognition and FFA is involved in tasks that requires expertise. Furthermore, the results suggest that the important factor which makes these two mechanisms different may lie under the visual feature extraction. We attempt to propose a mechanism for these two different kinds of recognition. We also investigate the influence of extracted visual features on view-invariant object recognition and show that while the target objects and distractor are very similar to each other, a moderate degree of view invariance can be achieved without making association between views. It seems that by making use of some visual features which are common between near views of objects we can obtain moderate level view invariant object recognition."
How can probable mechanisms of object recognition in the visual cortex be determined by visual features?,"There are lines of evidence demonstrating that the fusiform face area (FFA) preforms processes related to specific object recognition, which requires expertise (e.g. facial recognition). It seems that our brain utilize different mechanisms for generic object recognizing (e.g. face vs. non-face) and specific recognition (e.g. two different faces). To address this issue, we first introduce a biologically inspired object recognition model and then examine it in two experiments. The model has a hierarchical structure that employs a biologically plausible approach for visual feature extraction. Our results suggest that the mechanisms underlying generic and specific object recognition are performed distinctly. The results are in agreement with evidence that indicates inferotemporal cortex (IT) is responsible for object recognition and FFA is involved in tasks that requires expertise. Furthermore, the results suggest that the important factor which makes these two mechanisms different may lie under the visual feature extraction. We attempt to propose a mechanism for these two different kinds of recognition. We also investigate the influence of extracted visual features on view-invariant object recognition and show that while the target objects and distractor are very similar to each other, a moderate degree of view invariance can be achieved without making association between views. It seems that by making use of some visual features which are common between near views of objects we can obtain moderate level view invariant object recognition."
How can probable mechanisms of object recognition in the visual cortex be determined by visual features?,"There are lines of evidence demonstrating that the fusiform face area (FFA) preforms processes related to specific object recognition, which requires expertise (e.g. facial recognition). It seems that our brain utilize different mechanisms for generic object recognizing (e.g. face vs. non-face) and specific recognition (e.g. two different faces). To address this issue, we first introduce a biologically inspired object recognition model and then examine it in two experiments. The model has a hierarchical structure that employs a biologically plausible approach for visual feature extraction. Our results suggest that the mechanisms underlying generic and specific object recognition are performed distinctly. The results are in agreement with evidence that indicates inferotemporal cortex (IT) is responsible for object recognition and FFA is involved in tasks that requires expertise. Furthermore, the results suggest that the important factor which makes these two mechanisms different may lie under the visual feature extraction. We attempt to propose a mechanism for these two different kinds of recognition. We also investigate the influence of extracted visual features on view-invariant object recognition and show that while the target objects and distractor are very similar to each other, a moderate degree of view invariance can be achieved without making association between views. It seems that by making use of some visual features which are common between near views of objects we can obtain moderate level view invariant object recognition."
Bayesian Meta-classifier Learning from Biased Multiple Predictions,"We propose a probabilistic generative model for combining the predictions of multiple classifiers to form a meta-classifier that provides high classification accuracy.  The key feature of the model is the introduction of a latent variable that can identify whether the classifier in the ensemble is {\it related or unrelated} to each of the classes.  Our modeling is motivated by the idea that classifiers which provide incorrect predictions but are informative in terms of discriminating one class from others can also be effectively utilized for learning a meta-classifier.  The proposed meta-classifier learningscheme is particularly useful when the performance of each classifier is biased toward some specific class.  We perform empirical evaluations using both synthetic and real data. As a real case study of a combination of biased classifiers, we show its application to the high-level recognition of actual nursing activity by using accelerometers."
Bayesian Meta-classifier Learning from Biased Multiple Predictions,"We propose a probabilistic generative model for combining the predictions of multiple classifiers to form a meta-classifier that provides high classification accuracy.  The key feature of the model is the introduction of a latent variable that can identify whether the classifier in the ensemble is {\it related or unrelated} to each of the classes.  Our modeling is motivated by the idea that classifiers which provide incorrect predictions but are informative in terms of discriminating one class from others can also be effectively utilized for learning a meta-classifier.  The proposed meta-classifier learningscheme is particularly useful when the performance of each classifier is biased toward some specific class.  We perform empirical evaluations using both synthetic and real data. As a real case study of a combination of biased classifiers, we show its application to the high-level recognition of actual nursing activity by using accelerometers."
Challenge Mode Training,"This paper proposes a new training method called Challenge Mode Training which increases the accuracy as well as the robustness against irrelevant attributes of any classifier. Challenge mode training works by replacing some values with missing values. Actually, by removing information the proposed method create a different and more difficult problem which has less degrees of freedom and therefore fewer plateaus. Experiments are conducted with the challenge mode training applied to neural networks over traditional and complex datasets as well as datasets with the addition of irrelevant variables. For all of the tests, challenge mode training surpassed or performed similarly well in relation to the usual training method, in fact, the gain in accuracy provided by the proposed method was higher for complex datasets or datasets with irrelevant variables."
Challenge Mode Training,"This paper proposes a new training method called Challenge Mode Training which increases the accuracy as well as the robustness against irrelevant attributes of any classifier. Challenge mode training works by replacing some values with missing values. Actually, by removing information the proposed method create a different and more difficult problem which has less degrees of freedom and therefore fewer plateaus. Experiments are conducted with the challenge mode training applied to neural networks over traditional and complex datasets as well as datasets with the addition of irrelevant variables. For all of the tests, challenge mode training surpassed or performed similarly well in relation to the usual training method, in fact, the gain in accuracy provided by the proposed method was higher for complex datasets or datasets with irrelevant variables."
Challenge Mode Training,"This paper proposes a new training method called Challenge Mode Training which increases the accuracy as well as the robustness against irrelevant attributes of any classifier. Challenge mode training works by replacing some values with missing values. Actually, by removing information the proposed method create a different and more difficult problem which has less degrees of freedom and therefore fewer plateaus. Experiments are conducted with the challenge mode training applied to neural networks over traditional and complex datasets as well as datasets with the addition of irrelevant variables. For all of the tests, challenge mode training surpassed or performed similarly well in relation to the usual training method, in fact, the gain in accuracy provided by the proposed method was higher for complex datasets or datasets with irrelevant variables."
Towards An Order Preserving Approximation of LASSO Programs,"Sparse representation (SR) has recently drawn extensive attention in the signal processing, machine learning and machine vision communities. General SR theory emphasizes the importance of the \emph{sparsity} of a solution, as a natural regularization term for certain optimization problems. Exploiting sparsity has been shown to dramatically improve performance in specific real-world problems, e.g. \emph{face recognition}. Despite their proven success in enhancing classification performance, SR-based techniques are not readily applicable to large-scale problems because of the high computational burden incurred by solving sparse optimization programs, namely LASSO programs. As a result, there has been a recent push towards efficient techniques that adequately approximate the LASSO solution. In this paper, we propose such an approximation that serves the purpose of ordering a set of LASSO programs according to their optimal solutions. The need to order LASSO programs (e.g. to select the one with smallest objective) arises in important retrieval applications, from which we focus on two: online face recognition and visual object tracking. For such tasks, we propose a novel sampling-based algorithm that efficiently estimates the optimal solution for each LASSO program, while sufficiently preserving the relative order of these programs with high probability. To demonstrate the effectiveness and efficiency of the proposed method, we apply it to face recognition and object tracking on benchmark datasets. Our experiments show that it not only achieves state-of-the-art performance but also allows for significant computational speedup over baseline methods. %These results suggest that the proposed method can be a key enabler for such time-critical applications. "
Towards An Order Preserving Approximation of LASSO Programs,"Sparse representation (SR) has recently drawn extensive attention in the signal processing, machine learning and machine vision communities. General SR theory emphasizes the importance of the \emph{sparsity} of a solution, as a natural regularization term for certain optimization problems. Exploiting sparsity has been shown to dramatically improve performance in specific real-world problems, e.g. \emph{face recognition}. Despite their proven success in enhancing classification performance, SR-based techniques are not readily applicable to large-scale problems because of the high computational burden incurred by solving sparse optimization programs, namely LASSO programs. As a result, there has been a recent push towards efficient techniques that adequately approximate the LASSO solution. In this paper, we propose such an approximation that serves the purpose of ordering a set of LASSO programs according to their optimal solutions. The need to order LASSO programs (e.g. to select the one with smallest objective) arises in important retrieval applications, from which we focus on two: online face recognition and visual object tracking. For such tasks, we propose a novel sampling-based algorithm that efficiently estimates the optimal solution for each LASSO program, while sufficiently preserving the relative order of these programs with high probability. To demonstrate the effectiveness and efficiency of the proposed method, we apply it to face recognition and object tracking on benchmark datasets. Our experiments show that it not only achieves state-of-the-art performance but also allows for significant computational speedup over baseline methods. %These results suggest that the proposed method can be a key enabler for such time-critical applications. "
Towards An Order Preserving Approximation of LASSO Programs,"Sparse representation (SR) has recently drawn extensive attention in the signal processing, machine learning and machine vision communities. General SR theory emphasizes the importance of the \emph{sparsity} of a solution, as a natural regularization term for certain optimization problems. Exploiting sparsity has been shown to dramatically improve performance in specific real-world problems, e.g. \emph{face recognition}. Despite their proven success in enhancing classification performance, SR-based techniques are not readily applicable to large-scale problems because of the high computational burden incurred by solving sparse optimization programs, namely LASSO programs. As a result, there has been a recent push towards efficient techniques that adequately approximate the LASSO solution. In this paper, we propose such an approximation that serves the purpose of ordering a set of LASSO programs according to their optimal solutions. The need to order LASSO programs (e.g. to select the one with smallest objective) arises in important retrieval applications, from which we focus on two: online face recognition and visual object tracking. For such tasks, we propose a novel sampling-based algorithm that efficiently estimates the optimal solution for each LASSO program, while sufficiently preserving the relative order of these programs with high probability. To demonstrate the effectiveness and efficiency of the proposed method, we apply it to face recognition and object tracking on benchmark datasets. Our experiments show that it not only achieves state-of-the-art performance but also allows for significant computational speedup over baseline methods. %These results suggest that the proposed method can be a key enabler for such time-critical applications. "
Towards An Order Preserving Approximation of LASSO Programs,"Sparse representation (SR) has recently drawn extensive attention in the signal processing, machine learning and machine vision communities. General SR theory emphasizes the importance of the \emph{sparsity} of a solution, as a natural regularization term for certain optimization problems. Exploiting sparsity has been shown to dramatically improve performance in specific real-world problems, e.g. \emph{face recognition}. Despite their proven success in enhancing classification performance, SR-based techniques are not readily applicable to large-scale problems because of the high computational burden incurred by solving sparse optimization programs, namely LASSO programs. As a result, there has been a recent push towards efficient techniques that adequately approximate the LASSO solution. In this paper, we propose such an approximation that serves the purpose of ordering a set of LASSO programs according to their optimal solutions. The need to order LASSO programs (e.g. to select the one with smallest objective) arises in important retrieval applications, from which we focus on two: online face recognition and visual object tracking. For such tasks, we propose a novel sampling-based algorithm that efficiently estimates the optimal solution for each LASSO program, while sufficiently preserving the relative order of these programs with high probability. To demonstrate the effectiveness and efficiency of the proposed method, we apply it to face recognition and object tracking on benchmark datasets. Our experiments show that it not only achieves state-of-the-art performance but also allows for significant computational speedup over baseline methods. %These results suggest that the proposed method can be a key enabler for such time-critical applications. "
Bayesian Warped Gaussian Processes,"Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classification, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets."
Optimized dictionary based sparse representation for robust speaker recognition,"The mismatch between the training and the testing environments greatly degrades the performance of speaker recognition. Although many robust techniques have been proposed, speaker recognition in mismatch condition is still a challenge. To solve this problem, we propose an optimized dictionary based sparse representation for robust speaker recognition. To this end, we first train a speech dictionary and a noise dictionary, and concatenate them for sparse representation; then design an optimization algorithm to reduce the mutual coherence between the two learned dictionaries; after that, utilize mixture k-means to model speaker corresponding to sparse feature; and finally, present a distance divergence to measure the similarity. Compared with the standard Universal Background Model and Gaussian Mixture Models based speaker recognition, our preliminary experiments show that the proposed recognition framework consistently improve the robustness in mismatched condition."
Nonparametric Reduced Rank Regression,"We propose an approach to multivariate nonparametric regression thatgeneralizes reduced rank regression for linear models.  An additivemodel is estimated for each dimension of a $q$-dimensional response,with a shared $p$-dimensional predictor variable.  To control thecomplexity of the model, we employ a functional form of the Ky-Fanor nuclear norm, resulting in a set of function estimates that havelow rank.  Backfitting algorithms are derived and justified using anonparametric form of the nuclear norm subdifferential.  Oracleinequalities on excess risk are derived that exhibit the scalingbehavior of the procedure in the high dimensional setting.  Themethods are illustrated on gene expression data."
Temporal Coding of Local Spectrogram Features for Robust Sound Recognition,"There is much evidence to suggest that the human auditory system uses localised time-frequency information for the robust recognition of sounds. Despite this, conventional systems typically rely on features extracted from short windowed frames over time, covering the whole frequency spectrum. Such approaches are not inherently robust to noise, as each frame will contain a mixture of the spectral information from noise and signal.Here, we propose a novel approach based on the temporal coding of Local Spectrogram Features (LSFs), which generate spikes that are used to train a Spiking Neural Network (SNN) with temporal learning. LSFs represent robust location information in the spectrogram surrounding keypoints, which are detected in a signal-driven manner, such that the effect of noise on the temporal coding is reduced. Our system models characteristic clusters of LSFs in an unsupervised way, using tonotopic learning based on Self Organising Maps (SOMs).Our experiments demonstrate the robust performance of our approach across a variety of noise conditions, such that it is able to outperform the conventional frame-based baseline methods."
Temporal Coding of Local Spectrogram Features for Robust Sound Recognition,"There is much evidence to suggest that the human auditory system uses localised time-frequency information for the robust recognition of sounds. Despite this, conventional systems typically rely on features extracted from short windowed frames over time, covering the whole frequency spectrum. Such approaches are not inherently robust to noise, as each frame will contain a mixture of the spectral information from noise and signal.Here, we propose a novel approach based on the temporal coding of Local Spectrogram Features (LSFs), which generate spikes that are used to train a Spiking Neural Network (SNN) with temporal learning. LSFs represent robust location information in the spectrogram surrounding keypoints, which are detected in a signal-driven manner, such that the effect of noise on the temporal coding is reduced. Our system models characteristic clusters of LSFs in an unsupervised way, using tonotopic learning based on Self Organising Maps (SOMs).Our experiments demonstrate the robust performance of our approach across a variety of noise conditions, such that it is able to outperform the conventional frame-based baseline methods."
Temporal Coding of Local Spectrogram Features for Robust Sound Recognition,"There is much evidence to suggest that the human auditory system uses localised time-frequency information for the robust recognition of sounds. Despite this, conventional systems typically rely on features extracted from short windowed frames over time, covering the whole frequency spectrum. Such approaches are not inherently robust to noise, as each frame will contain a mixture of the spectral information from noise and signal.Here, we propose a novel approach based on the temporal coding of Local Spectrogram Features (LSFs), which generate spikes that are used to train a Spiking Neural Network (SNN) with temporal learning. LSFs represent robust location information in the spectrogram surrounding keypoints, which are detected in a signal-driven manner, such that the effect of noise on the temporal coding is reduced. Our system models characteristic clusters of LSFs in an unsupervised way, using tonotopic learning based on Self Organising Maps (SOMs).Our experiments demonstrate the robust performance of our approach across a variety of noise conditions, such that it is able to outperform the conventional frame-based baseline methods."
Lifted Variable Elimination: A Novel Operator and Completeness Results,"Various methods for lifted probabilistic inference have been proposed, but our understanding of these methods and the relationships between them is still limited, compared to their propositional counterparts. The only existing theoretical characterization of lifting is for weighted first-order model counting (WFOMC), which was shown to be complete domain-lifted for the class of 2-logvar models. This paper makes two contributions to lifted variable elimination (LVE). First, we introduce a novel inference operator called group inversion. Second, we prove that LVE augmented with this operator is complete in the same sense as WFOMC."
Lifted Variable Elimination: A Novel Operator and Completeness Results,"Various methods for lifted probabilistic inference have been proposed, but our understanding of these methods and the relationships between them is still limited, compared to their propositional counterparts. The only existing theoretical characterization of lifting is for weighted first-order model counting (WFOMC), which was shown to be complete domain-lifted for the class of 2-logvar models. This paper makes two contributions to lifted variable elimination (LVE). First, we introduce a novel inference operator called group inversion. Second, we prove that LVE augmented with this operator is complete in the same sense as WFOMC."
Lifted Variable Elimination: A Novel Operator and Completeness Results,"Various methods for lifted probabilistic inference have been proposed, but our understanding of these methods and the relationships between them is still limited, compared to their propositional counterparts. The only existing theoretical characterization of lifting is for weighted first-order model counting (WFOMC), which was shown to be complete domain-lifted for the class of 2-logvar models. This paper makes two contributions to lifted variable elimination (LVE). First, we introduce a novel inference operator called group inversion. Second, we prove that LVE augmented with this operator is complete in the same sense as WFOMC."
Lifted Variable Elimination: A Novel Operator and Completeness Results,"Various methods for lifted probabilistic inference have been proposed, but our understanding of these methods and the relationships between them is still limited, compared to their propositional counterparts. The only existing theoretical characterization of lifting is for weighted first-order model counting (WFOMC), which was shown to be complete domain-lifted for the class of 2-logvar models. This paper makes two contributions to lifted variable elimination (LVE). First, we introduce a novel inference operator called group inversion. Second, we prove that LVE augmented with this operator is complete in the same sense as WFOMC."
Lifted Variable Elimination: A Novel Operator and Completeness Results,"Various methods for lifted probabilistic inference have been proposed, but our understanding of these methods and the relationships between them is still limited, compared to their propositional counterparts. The only existing theoretical characterization of lifting is for weighted first-order model counting (WFOMC), which was shown to be complete domain-lifted for the class of 2-logvar models. This paper makes two contributions to lifted variable elimination (LVE). First, we introduce a novel inference operator called group inversion. Second, we prove that LVE augmented with this operator is complete in the same sense as WFOMC."
The Gaussian Nonlinear Poisson Process,"Recently, intracellular recordings of neurons in vivo have become increasingly available. There is a pressing need to develop models which can be used to characterize the statistical properties of the temporal dynamics of single cells. We propose a doubly stochastic continuous-time model of a single neuron which is supposed can capture both the membrane potential dynamics and the process of spiking. The model consists of a gaussian process which is transformed through a nonlinearity, providing the firing rate for an inhomogeneous Poisson process. We highlight the use of the moment-generating functional in order to derive the previously known $n$-point function and devise a method for fitting this model to in vivo data."
The Gaussian Nonlinear Poisson Process,"Recently, intracellular recordings of neurons in vivo have become increasingly available. There is a pressing need to develop models which can be used to characterize the statistical properties of the temporal dynamics of single cells. We propose a doubly stochastic continuous-time model of a single neuron which is supposed can capture both the membrane potential dynamics and the process of spiking. The model consists of a gaussian process which is transformed through a nonlinearity, providing the firing rate for an inhomogeneous Poisson process. We highlight the use of the moment-generating functional in order to derive the previously known $n$-point function and devise a method for fitting this model to in vivo data."
Large Margin Metric Learning for Sparse Representation-Based Classification,"Sparse representation-based classification (SRC) has achieved great successes in many visual recognition tasks in recent years, such as face recognition and image classification. However, SRC is usually performed in the original feature space, which may not be discriminative enough for some classification problems. In this paper, we propose a large margin metric learning method to learn a discriminative distance metric to calculate the sparse reconstruction errors. The distance metric is learned by enforcing a margin between intraclass reconstruction error and interclass reconstruction error, for each training example. Experiments conducted on face recognition, gait recognition, and palmprint recognition show the efficacy of our proposed method. This approach has the potential to be used to enhance the SRC method in many applications."
Large Margin Metric Learning for Sparse Representation-Based Classification,"Sparse representation-based classification (SRC) has achieved great successes in many visual recognition tasks in recent years, such as face recognition and image classification. However, SRC is usually performed in the original feature space, which may not be discriminative enough for some classification problems. In this paper, we propose a large margin metric learning method to learn a discriminative distance metric to calculate the sparse reconstruction errors. The distance metric is learned by enforcing a margin between intraclass reconstruction error and interclass reconstruction error, for each training example. Experiments conducted on face recognition, gait recognition, and palmprint recognition show the efficacy of our proposed method. This approach has the potential to be used to enhance the SRC method in many applications."
Gaussian Message Passing for Non-Gaussian Vision: Non-Lambertian Shape from Shading,"Although Expectation Propagation has become a popular and successful inference method for continuous probabilistic models, it is rarely applied to computer vision. One disadvantage is that for grid-shaped models, Gaussian EP requires quadratic space and cubic time in the number of pixels. However, run times for EP are independent of clique size, and depend only on the rank, or intrinsic dimensionality, of potentials. This property would be highly advantageous in computer vision.Here, we propose two variations of EP suitable for visual problems. The final approach exploits commonalities in natural scene statistics to achieve run times that are linear in both number of pixels and clique size.We test these methods on shape from shading. To demonstrate that the method performs well in problems with highly non-Gaussian potentials, we show performance for surfaces with arbitrary non-Lambertian reflectance and lighting. In each case, inferred shapes closely obey constraints imposed by the image and prior."
Gaussian Message Passing for Non-Gaussian Vision: Non-Lambertian Shape from Shading,"Although Expectation Propagation has become a popular and successful inference method for continuous probabilistic models, it is rarely applied to computer vision. One disadvantage is that for grid-shaped models, Gaussian EP requires quadratic space and cubic time in the number of pixels. However, run times for EP are independent of clique size, and depend only on the rank, or intrinsic dimensionality, of potentials. This property would be highly advantageous in computer vision.Here, we propose two variations of EP suitable for visual problems. The final approach exploits commonalities in natural scene statistics to achieve run times that are linear in both number of pixels and clique size.We test these methods on shape from shading. To demonstrate that the method performs well in problems with highly non-Gaussian potentials, we show performance for surfaces with arbitrary non-Lambertian reflectance and lighting. In each case, inferred shapes closely obey constraints imposed by the image and prior."
PCA transform via Partial Rotation,"We present a relaxed version of high-dimensional rotation called \emph{partial rotation}. For two $d\times k$ matrices $S_1$ and $S_2$, each consisting of $k$ selected columns from two orthonormal bases in $\RR^d$, respectively, a partial rotation of degree $k$ is a $d\times d$ orthonormal matrix $R$ such that $R S_1=S_2$ and the null space of ${S_1}^T$ coincides with the null space of ${S_2}^T$ under $R$.  We show that such a rotation can be represented by a sequence of $k$ Givens rotations and provide an efficient algorithm to find the rotation in $O(k^2d)$ time. Since a partial rotation of degree $k$ is represented by a sequence of $k$ Givens rotation, it takes only $O(kd)$ time to rotate a $d$ dimensional vector by the rotation. This is faster than the standard rotation algorithm of time complexity $O(d^2)$ by orders of magnitude when $k$ is much smaller than $d$. Partial rotation is especially useful to principal component analysis (PCA) with $k$ principal components. By substituting the standard projection method of PCA with our partial rotation method, PCA transform can be done without any information loss. Our empirical results show that even a simple brute-force algorithm using PCA with partial rotation outperforms the state-of-art techniques in high-dimensional nearest neighbor search."
PCA transform via Partial Rotation,"We present a relaxed version of high-dimensional rotation called \emph{partial rotation}. For two $d\times k$ matrices $S_1$ and $S_2$, each consisting of $k$ selected columns from two orthonormal bases in $\RR^d$, respectively, a partial rotation of degree $k$ is a $d\times d$ orthonormal matrix $R$ such that $R S_1=S_2$ and the null space of ${S_1}^T$ coincides with the null space of ${S_2}^T$ under $R$.  We show that such a rotation can be represented by a sequence of $k$ Givens rotations and provide an efficient algorithm to find the rotation in $O(k^2d)$ time. Since a partial rotation of degree $k$ is represented by a sequence of $k$ Givens rotation, it takes only $O(kd)$ time to rotate a $d$ dimensional vector by the rotation. This is faster than the standard rotation algorithm of time complexity $O(d^2)$ by orders of magnitude when $k$ is much smaller than $d$. Partial rotation is especially useful to principal component analysis (PCA) with $k$ principal components. By substituting the standard projection method of PCA with our partial rotation method, PCA transform can be done without any information loss. Our empirical results show that even a simple brute-force algorithm using PCA with partial rotation outperforms the state-of-art techniques in high-dimensional nearest neighbor search."
Local Learning Algorithms for Multi-Task Learning,"Multi-task learning is to improve the performance of one task by utilizing information from other related tasks. Almost all existing multi-task learning methods belong to global learning approach. In this paper, different from existing methods, we propose local learning methods for multi-task classification and regression problems by extending some single-task local learning methods. For classification problems, we extend k-nearest-neighbor classifier by formulating the decision function on each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific. By defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one, a regularized objective function is proposed and we develop an efficient coordinate descent method to solve it. For regression problems, we extend kernel regression to multi-task setting in a similar way to the classification case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods."
A population search algorithm for clustered connectivity patterns,"The identification of time, frequency and spatial locations between which connectivity occurs within the brain is traditionally done via a brute force search of all available locations in a specified range. However, this is inefficient and slows down progress in the identification of connectivity patterns related to previously unexplored cognitive processes. Therefore, a novel, population based, search algorithm is proposed based upon the behaviour of foraging animals.The method is evaluated on both a simple grid search problem and on the identification of time-frequency locations of statistically significant phase synchronisation in the EEG. The method is shown to exceed brute force searches in terms of speed by several times while identifying a large proportion of available solutions."
A population search algorithm for clustered connectivity patterns,"The identification of time, frequency and spatial locations between which connectivity occurs within the brain is traditionally done via a brute force search of all available locations in a specified range. However, this is inefficient and slows down progress in the identification of connectivity patterns related to previously unexplored cognitive processes. Therefore, a novel, population based, search algorithm is proposed based upon the behaviour of foraging animals.The method is evaluated on both a simple grid search problem and on the identification of time-frequency locations of statistically significant phase synchronisation in the EEG. The method is shown to exceed brute force searches in terms of speed by several times while identifying a large proportion of available solutions."
A statistic for testing equality of distributions in metric space,"Deciding whether two sets of samples originate from the same probability law is essential in many fields of science and engineering such as economics, biology, medicine and neuroscience. Although this problem has been studied extensively when the underlying random variables are categorical or real valued, it remains largely unexplored when the random variables have more exotic domains such as time series, graphs, probability measures, and spike trains. A common aspect of these latter domains is that they can be assigned appropriate distance metrics, such as edit distance. In this paper, we exploit this natural characteristic to develop a computationally efficient, and parameter free statistic for testing equality of distributions. We compare the proposed approach with other state-of-the-art methods, and demonstrate that it performs equally well."
Perplexity on Reduced Corpora,"This paper studies a relationship between perplexity and vocabulary size on a corpus (or documents),which is reduced to improve computational performance.We prove that perplexity of k-gram models and topic modelsroughly follows a power law with respect to reduced vocabulary size,when a corpus follows Zipf's law.This gives a theoretical evidence for our intuition thatlow-frequency words may not make a large contribution to learning results.We verify the correctness of our theory on synthetic corporaand examine a gap between theory and practice on real corpora."
"Dual Decomposition from the Perspective of Relax, Compensate and then Recover","Relax, Compensate and then Recover (RCR) is a paradigm for approximateinference in probabilistic graphical models that has previouslyprovided theoretical and practical insights on iterative beliefpropagation and some of its generalizations.  In this paper, wecharacterize the technique of dual decomposition in the terms of RCR,viewing it as a specific way to compensate for relaxed equivalenceconstraints.  Among other insights gathered from this perspective, wepropose novel heuristics for recovering relaxed equivalenceconstraints with the goal of incrementally tightening dualdecomposition approximations, all the way to reaching exactsolutions. We also show empirically that recovering equivalenceconstraints can sometimes tighten the corresponding approximation (andobtaining exact results), without increasing much the complexity ofinference."
"Dual Decomposition from the Perspective of Relax, Compensate and then Recover","Relax, Compensate and then Recover (RCR) is a paradigm for approximateinference in probabilistic graphical models that has previouslyprovided theoretical and practical insights on iterative beliefpropagation and some of its generalizations.  In this paper, wecharacterize the technique of dual decomposition in the terms of RCR,viewing it as a specific way to compensate for relaxed equivalenceconstraints.  Among other insights gathered from this perspective, wepropose novel heuristics for recovering relaxed equivalenceconstraints with the goal of incrementally tightening dualdecomposition approximations, all the way to reaching exactsolutions. We also show empirically that recovering equivalenceconstraints can sometimes tighten the corresponding approximation (andobtaining exact results), without increasing much the complexity ofinference."
"A meta algorithm making centralized graph computation faster, distributed and at times better","In this paper, we present a meta algorithm that takes existing centralized algorithms for graph computation and makes them distributed and faster. In a nutshell, the meta algorithm creates a randomized partition of  the graph, with each partition being a small subgraph, and it then runs the centralized algorithm on each partition separately and stitches the resulting solutions to produce a global solution. We illustrate this meta algorithm with two popular problems: computation of Maximum A Posteriori (MAP) assignment in an arbitrary pairwise Markov Random Field (MRF), and modularity optimization for clustering and community detection.We show that the resulting distributed algorithms for these problems essentially run in linear time  and that they perform as well -- or even better -- than the original centralized algorithm as long as the graphs have geometric structure. More precisely, if the centralized algorithm is a constant factor approximation, the resulting distributed algorithm is also a constant factor approximation with constant slightly bigger; but if the centralized algorithm is a non-constant (e.g. logarithmic) factor approximation, then the resulting distributed algorithm becomes a constant factor approximation. For general graphs (not necessarily geometric), we  compute explicit bounds on the loss of  performance of the distributed algorithm with respect to the centralized algorithm."
"A meta algorithm making centralized graph computation faster, distributed and at times better","In this paper, we present a meta algorithm that takes existing centralized algorithms for graph computation and makes them distributed and faster. In a nutshell, the meta algorithm creates a randomized partition of  the graph, with each partition being a small subgraph, and it then runs the centralized algorithm on each partition separately and stitches the resulting solutions to produce a global solution. We illustrate this meta algorithm with two popular problems: computation of Maximum A Posteriori (MAP) assignment in an arbitrary pairwise Markov Random Field (MRF), and modularity optimization for clustering and community detection.We show that the resulting distributed algorithms for these problems essentially run in linear time  and that they perform as well -- or even better -- than the original centralized algorithm as long as the graphs have geometric structure. More precisely, if the centralized algorithm is a constant factor approximation, the resulting distributed algorithm is also a constant factor approximation with constant slightly bigger; but if the centralized algorithm is a non-constant (e.g. logarithmic) factor approximation, then the resulting distributed algorithm becomes a constant factor approximation. For general graphs (not necessarily geometric), we  compute explicit bounds on the loss of  performance of the distributed algorithm with respect to the centralized algorithm."
Optimizing a Multiple Linear Regression-based Approach for a Priori Decision Threshold Estimation in Biometric Recognition,"Biometric recognition is a complex classification problemwhere the goal is to classify a pattern (biometric sample) as belonging or not to a certain class (user). As in other pattern recognition problems, a correct estimation of the decision threshold is essential for optimizing the biometric system's performance. A successful new approach for this estimation (prediction) based on Multiple Linear Regression has been proposed by us in a previous work. Here, we go into this proposal in greater depth, optimizing the independent variables selection by meansof the use of their matrix correlation, and only the uncorrelated ones are incorporated to the model. A study of the threshold estimation accuracy with regard to the data set size used to train the linear model is alsoperformed. Other related works have focused on a single biometric and classifier. However, our proposal is applied to different biometrics (signature and speech) and with different classifiers (Artificial Neural Network and Dynamic Time Warping), showing a good accuracy in all the tested scenarios."
Optimizing a Multiple Linear Regression-based Approach for a Priori Decision Threshold Estimation in Biometric Recognition,"Biometric recognition is a complex classification problemwhere the goal is to classify a pattern (biometric sample) as belonging or not to a certain class (user). As in other pattern recognition problems, a correct estimation of the decision threshold is essential for optimizing the biometric system's performance. A successful new approach for this estimation (prediction) based on Multiple Linear Regression has been proposed by us in a previous work. Here, we go into this proposal in greater depth, optimizing the independent variables selection by meansof the use of their matrix correlation, and only the uncorrelated ones are incorporated to the model. A study of the threshold estimation accuracy with regard to the data set size used to train the linear model is alsoperformed. Other related works have focused on a single biometric and classifier. However, our proposal is applied to different biometrics (signature and speech) and with different classifiers (Artificial Neural Network and Dynamic Time Warping), showing a good accuracy in all the tested scenarios."
Optimizing a Multiple Linear Regression-based Approach for a Priori Decision Threshold Estimation in Biometric Recognition,"Biometric recognition is a complex classification problemwhere the goal is to classify a pattern (biometric sample) as belonging or not to a certain class (user). As in other pattern recognition problems, a correct estimation of the decision threshold is essential for optimizing the biometric system's performance. A successful new approach for this estimation (prediction) based on Multiple Linear Regression has been proposed by us in a previous work. Here, we go into this proposal in greater depth, optimizing the independent variables selection by meansof the use of their matrix correlation, and only the uncorrelated ones are incorporated to the model. A study of the threshold estimation accuracy with regard to the data set size used to train the linear model is alsoperformed. Other related works have focused on a single biometric and classifier. However, our proposal is applied to different biometrics (signature and speech) and with different classifiers (Artificial Neural Network and Dynamic Time Warping), showing a good accuracy in all the tested scenarios."
Optimizing a Multiple Linear Regression-based Approach for a Priori Decision Threshold Estimation in Biometric Recognition,"Biometric recognition is a complex classification problemwhere the goal is to classify a pattern (biometric sample) as belonging or not to a certain class (user). As in other pattern recognition problems, a correct estimation of the decision threshold is essential for optimizing the biometric system's performance. A successful new approach for this estimation (prediction) based on Multiple Linear Regression has been proposed by us in a previous work. Here, we go into this proposal in greater depth, optimizing the independent variables selection by meansof the use of their matrix correlation, and only the uncorrelated ones are incorporated to the model. A study of the threshold estimation accuracy with regard to the data set size used to train the linear model is alsoperformed. Other related works have focused on a single biometric and classifier. However, our proposal is applied to different biometrics (signature and speech) and with different classifiers (Artificial Neural Network and Dynamic Time Warping), showing a good accuracy in all the tested scenarios."
On the connections between saliency and tracking,"A model connecting visual tracking and saliency has recently been proposed. Thismodel is based on the saliency hypothesis for tracking which postulates that trackingis achieved by the top-down tuning, based on target features, of discriminantcenter-surround saliency mechanisms over time. In this work, we identify threemain predictions that must hold if the hypothesis were true: 1) tracking reliabilityshould be larger for salient than for non-salient targets, 2) tracking reliabilityshould have a dependence on the defining variables of saliency, namely featurecontrast and distractor heterogeneity, and must replicate the dependence ofsaliency on these variables, and 3) saliency and tracking can be implemented withcommon low level neural mechanisms. We confirm that the first two predictionshold by reporting results from a set of human behavior studies on the connectionbetween saliency and tracking. We also show that the third prediction holds byconstructing a common neurophysiologically plausible architecture that can computationallysolve both saliency and tracking. This architecture is fully compliantwith the standard physiological models of V1 and MT, and with what is knownabout attentional control in area LIP, while explaining the results of the humanbehavior experiments."
Infinite EFH: an Infinite Undirected Latent Variable Model,"Bayesian nonparametrics has been promising in learning Bayesian networks, but very few attempts have been made under the context of undirected Markov networks. This paper presents infinite exponential family Harmoniums (iEFH), an attempt to broaden the use of Bayesian nonparametrics to automatically resolve the unknown number of hidden units in undirected latent variable models. We further generalize iEFH to the supervised infinite max-margin Harmoniums (iMMH), which directly regularizes the latent representations via imposing max-margin constraints for discovering predictive latent representations that are good for classification. We use the sparsity-inducing Indian buffet process prior to select latent units from an infinite pool. Our extensive experiments on real text and image datasets appear to demonstrate the benefits of iEFH and iMMH inherited from both Bayesian nonparametrics and max-margin learning."
"Learning to Grasp using Vision, Haptics and Proprioception","The ability to grasp and manipulate objects is an integral part of a robot's physical interaction with the environment. Humans alike, robots are expected to grasp and manipulate objects in a goal-oriented manner. In other words, objects should be grasped so to afford subsequent actions: if I am to hammer a nail, the hammer should be grasped so to afford hammering. Most of the work on grasping, commonly addresses only the problem of finding a stable grasp without considering the task/action a robot is supposed to fulfill with an object.In this paper, we present work on modeling of goal-directed robot grasping tasks based on integration of multisensory data using probabilistic generative models.  Our probabilistic framework facilitates assessment of grasp success in a goal-oriented way, taking into account both geometric constraints imposed by the task and fulfilling grasp stability requirements.  The conditional relations between tasks and the sensory data (vision, haptics and proprioception) are modeled using graphical models. We integrate high-level task information introduced by a teacher in a supervised setting with low-level stability requirements acquired through a robot's self-exploration.  The generative modeling approach enables inference of appropriate grasping configurations, as well as prediction of grasp success. The framework provides insights into dependencies between variables and features relevant for object grasping in general."
"Learning to Grasp using Vision, Haptics and Proprioception","The ability to grasp and manipulate objects is an integral part of a robot's physical interaction with the environment. Humans alike, robots are expected to grasp and manipulate objects in a goal-oriented manner. In other words, objects should be grasped so to afford subsequent actions: if I am to hammer a nail, the hammer should be grasped so to afford hammering. Most of the work on grasping, commonly addresses only the problem of finding a stable grasp without considering the task/action a robot is supposed to fulfill with an object.In this paper, we present work on modeling of goal-directed robot grasping tasks based on integration of multisensory data using probabilistic generative models.  Our probabilistic framework facilitates assessment of grasp success in a goal-oriented way, taking into account both geometric constraints imposed by the task and fulfilling grasp stability requirements.  The conditional relations between tasks and the sensory data (vision, haptics and proprioception) are modeled using graphical models. We integrate high-level task information introduced by a teacher in a supervised setting with low-level stability requirements acquired through a robot's self-exploration.  The generative modeling approach enables inference of appropriate grasping configurations, as well as prediction of grasp success. The framework provides insights into dependencies between variables and features relevant for object grasping in general."
"Learning to Grasp using Vision, Haptics and Proprioception","The ability to grasp and manipulate objects is an integral part of a robot's physical interaction with the environment. Humans alike, robots are expected to grasp and manipulate objects in a goal-oriented manner. In other words, objects should be grasped so to afford subsequent actions: if I am to hammer a nail, the hammer should be grasped so to afford hammering. Most of the work on grasping, commonly addresses only the problem of finding a stable grasp without considering the task/action a robot is supposed to fulfill with an object.In this paper, we present work on modeling of goal-directed robot grasping tasks based on integration of multisensory data using probabilistic generative models.  Our probabilistic framework facilitates assessment of grasp success in a goal-oriented way, taking into account both geometric constraints imposed by the task and fulfilling grasp stability requirements.  The conditional relations between tasks and the sensory data (vision, haptics and proprioception) are modeled using graphical models. We integrate high-level task information introduced by a teacher in a supervised setting with low-level stability requirements acquired through a robot's self-exploration.  The generative modeling approach enables inference of appropriate grasping configurations, as well as prediction of grasp success. The framework provides insights into dependencies between variables and features relevant for object grasping in general."
"Learning to Grasp using Vision, Haptics and Proprioception","The ability to grasp and manipulate objects is an integral part of a robot's physical interaction with the environment. Humans alike, robots are expected to grasp and manipulate objects in a goal-oriented manner. In other words, objects should be grasped so to afford subsequent actions: if I am to hammer a nail, the hammer should be grasped so to afford hammering. Most of the work on grasping, commonly addresses only the problem of finding a stable grasp without considering the task/action a robot is supposed to fulfill with an object.In this paper, we present work on modeling of goal-directed robot grasping tasks based on integration of multisensory data using probabilistic generative models.  Our probabilistic framework facilitates assessment of grasp success in a goal-oriented way, taking into account both geometric constraints imposed by the task and fulfilling grasp stability requirements.  The conditional relations between tasks and the sensory data (vision, haptics and proprioception) are modeled using graphical models. We integrate high-level task information introduced by a teacher in a supervised setting with low-level stability requirements acquired through a robot's self-exploration.  The generative modeling approach enables inference of appropriate grasping configurations, as well as prediction of grasp success. The framework provides insights into dependencies between variables and features relevant for object grasping in general."
Cross-Domain Information Role Classifier for Profiling Users in Online Q&A Forums,"This paper presents a novel application of text classification techniques for analyzing forum dynamics and profiling discussants in online Q&A discussions. Building on the existing Speech Act research, we identify dialogue features that capture true information seeking and providing roles of the discussants and their messages within threaded discussions. As message-level lexical information is not enough in capturing true information roles, we include additional thread-level information such as author turns and message positions in the thread. We generated and evaluated user information roles across four different Q&A forums including student group project forums and industry troubleshooting forums. The current result indicates that information role classifiers are robust across several different domains. We also found that the role-based user profiles are useful in predicting user performance or user expertise within the community."
Cross-Domain Information Role Classifier for Profiling Users in Online Q&A Forums,"This paper presents a novel application of text classification techniques for analyzing forum dynamics and profiling discussants in online Q&A discussions. Building on the existing Speech Act research, we identify dialogue features that capture true information seeking and providing roles of the discussants and their messages within threaded discussions. As message-level lexical information is not enough in capturing true information roles, we include additional thread-level information such as author turns and message positions in the thread. We generated and evaluated user information roles across four different Q&A forums including student group project forums and industry troubleshooting forums. The current result indicates that information role classifiers are robust across several different domains. We also found that the role-based user profiles are useful in predicting user performance or user expertise within the community."
On Generalization Performance of Unified Learning Model,"Recently, A.Anonymous [1] showed a unified formulation based on robust optimization thatembraces several kinds of classification methods and gave a geometric interpretation and statistical interpretation for the unified formulation. This paper extends the unified model to cover not only  binary classification but also  regression and outlier (or novelty) detection. Then we show that the unified model minimizes a well-known financial risk measure. Moreover, after deriving generalization bounds using such risk measures, we prove that the unified model gives a solution that minimizes the generalization bounds. "
Clustering Probability Densities,"We describe a clustering algorithm where each example is represented as a probability density and distortion is measured by Kullback-Leibler divergence. This setup is relevant to many applications where an example is better characterized by a probability distribution, to capture the underlying uncertainty of interest, than a finite-dimensional feature vector in Euclidean space. We first derive a k-means variant for clustering Gaussian densities which has a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalizes a single Gaussian and is typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. We report empirical results on a successfully deployed application: query clustering based on bid landscape for sponsored search auction optimization."
Clustering Probability Densities,"We describe a clustering algorithm where each example is represented as a probability density and distortion is measured by Kullback-Leibler divergence. This setup is relevant to many applications where an example is better characterized by a probability distribution, to capture the underlying uncertainty of interest, than a finite-dimensional feature vector in Euclidean space. We first derive a k-means variant for clustering Gaussian densities which has a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalizes a single Gaussian and is typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. We report empirical results on a successfully deployed application: query clustering based on bid landscape for sponsored search auction optimization."
Clustering Probability Densities,"We describe a clustering algorithm where each example is represented as a probability density and distortion is measured by Kullback-Leibler divergence. This setup is relevant to many applications where an example is better characterized by a probability distribution, to capture the underlying uncertainty of interest, than a finite-dimensional feature vector in Euclidean space. We first derive a k-means variant for clustering Gaussian densities which has a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalizes a single Gaussian and is typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. We report empirical results on a successfully deployed application: query clustering based on bid landscape for sponsored search auction optimization."
Clustering Probability Densities,"We describe a clustering algorithm where each example is represented as a probability density and distortion is measured by Kullback-Leibler divergence. This setup is relevant to many applications where an example is better characterized by a probability distribution, to capture the underlying uncertainty of interest, than a finite-dimensional feature vector in Euclidean space. We first derive a k-means variant for clustering Gaussian densities which has a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalizes a single Gaussian and is typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. We report empirical results on a successfully deployed application: query clustering based on bid landscape for sponsored search auction optimization."
Clustering Probability Densities,"We describe a clustering algorithm where each example is represented as a probability density and distortion is measured by Kullback-Leibler divergence. This setup is relevant to many applications where an example is better characterized by a probability distribution, to capture the underlying uncertainty of interest, than a finite-dimensional feature vector in Euclidean space. We first derive a k-means variant for clustering Gaussian densities which has a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalizes a single Gaussian and is typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. We report empirical results on a successfully deployed application: query clustering based on bid landscape for sponsored search auction optimization."
Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method,"We develop new stochastic optimization methods that are applicable to a wide range of {\it structured regularizations}.Basically our methods are a combination of stochastic optimization techniques and Alternating Direction Multiplier Method (ADMM).ADMM is a general framework for optimizing a composite function,and has a wide range of applications.We propose two types of online variants of ADMM corresponding to onine proximal gradient descent and regularized dual averaging respectively.The proposed algorithms are computationally efficient and easy to implement.It is shown that our methods yield $O(1/\sqrt{T})$ convergence of the expected risk.Numerical experiments show effectiveness of our methods in learning tasks with structured sparsitysuch as overlapped group lasso."
Locally Optimized Hashing,"Fast nearest neighbor search is becoming more and more important to utilize massive data. Recent work shows that hash learning is effective for nearest neighbor search in terms of computational time and space. Existing hash learning methods try to convert near samples to near binary codes, and their hash functions are globally optimized on the data manifold. However, such hash functions often have low resolution of binary codes; each bucket, a set of samples with same binary code, may contain a large number of samples in these methods, which makes it infeasible to obtian the nearest neighbors of given query with high precision. As a result, existing methods require long binary codes for precise nearest neighbor search. In this paper, we propose Locally Optimized Hashing to overcome this drawback, which explicitly partitions each bucket by solving optimization problem based on that of Spectral Hashing with stronger constraints. Our method outperforms existing methods in image and document datasets in terms of quality of both the hash table and query, especially when the code length is short."
Locally Optimized Hashing,"Fast nearest neighbor search is becoming more and more important to utilize massive data. Recent work shows that hash learning is effective for nearest neighbor search in terms of computational time and space. Existing hash learning methods try to convert near samples to near binary codes, and their hash functions are globally optimized on the data manifold. However, such hash functions often have low resolution of binary codes; each bucket, a set of samples with same binary code, may contain a large number of samples in these methods, which makes it infeasible to obtian the nearest neighbors of given query with high precision. As a result, existing methods require long binary codes for precise nearest neighbor search. In this paper, we propose Locally Optimized Hashing to overcome this drawback, which explicitly partitions each bucket by solving optimization problem based on that of Spectral Hashing with stronger constraints. Our method outperforms existing methods in image and document datasets in terms of quality of both the hash table and query, especially when the code length is short."
Locally Optimized Hashing,"Fast nearest neighbor search is becoming more and more important to utilize massive data. Recent work shows that hash learning is effective for nearest neighbor search in terms of computational time and space. Existing hash learning methods try to convert near samples to near binary codes, and their hash functions are globally optimized on the data manifold. However, such hash functions often have low resolution of binary codes; each bucket, a set of samples with same binary code, may contain a large number of samples in these methods, which makes it infeasible to obtian the nearest neighbors of given query with high precision. As a result, existing methods require long binary codes for precise nearest neighbor search. In this paper, we propose Locally Optimized Hashing to overcome this drawback, which explicitly partitions each bucket by solving optimization problem based on that of Spectral Hashing with stronger constraints. Our method outperforms existing methods in image and document datasets in terms of quality of both the hash table and query, especially when the code length is short."
Ellipsoidal Multiple Instance Learning,"We propose a large margin method for learning with ellipsoids that isparticularly suited to asymmetric detection tasks such as multipleinstance learning (MIL). In contrast to current approaches for solving MILthat involve complex computationally expensive algorithms, ourapproach is a direct geometric one.We consider the distance between ellipsoidsand the hyperplane, generalising the standard support vector machine.Negative bags in MIL contain all negativeinstances, and we treat them akin to the robust optimisationframework. However, our method allows positive bags to cross themargin, since it is not known which instances within are positive. We derive agradient descent approach to solve the resulting bilevel program, andapply it to several MIL datasets. Surprisingly, our geometric approachresults in state of the art performance."
Ellipsoidal Multiple Instance Learning,"We propose a large margin method for learning with ellipsoids that isparticularly suited to asymmetric detection tasks such as multipleinstance learning (MIL). In contrast to current approaches for solving MILthat involve complex computationally expensive algorithms, ourapproach is a direct geometric one.We consider the distance between ellipsoidsand the hyperplane, generalising the standard support vector machine.Negative bags in MIL contain all negativeinstances, and we treat them akin to the robust optimisationframework. However, our method allows positive bags to cross themargin, since it is not known which instances within are positive. We derive agradient descent approach to solve the resulting bilevel program, andapply it to several MIL datasets. Surprisingly, our geometric approachresults in state of the art performance."
A Multiscale Composite Dirichlet Process for modelling rhythm tracks,"This paper introduces a novel non-parametric Bayesian model for hierarchically structured, pseudo-repetitive data, where the distribution of data on each scale of the hierarchy is generated by a Dirichlet Process.  The generality of this model makes it easily applicable to a wide range of data types, but it is particularly successful for composing melody or drum tracks.  In music, relative positional information is important, as patterns tend to be aligned to a metrum, a regular and hierarchical division of the sequence which generates an audible impression of a regular beat.  The model described in this paper is suitable for data with such properties, and can be trained using a Gibbs sampling algorithm.  The generative process is fast enough to compose melody or rhythm tracks in real time."
Simple Models for Shunting Inhibition,"The integration of excitatory and inhibitory inputs at a neuron follows a nonlinear process, which is generally termed shunting inhibition. The experimental data has revealed that the effect of shunting inhibitionon the somatic potential can be largely expressed as a simple arithmetic rule, in which the contribution of shunting inhibition is proportional to the product between the contributions of excitatory and inhibitory inputs when they are applied individually. In this study, we develop simple neuron and network models for shunting inhibition. Our simple neuron models reproduce the experimental results qualitatively. We show that shunting inhibition can provide a mechanism to retain persistent activity in a network, andcan be well approximated as divisive normalization in describing the stationary states of continuous attractor neural networks."
Simple Models for Shunting Inhibition,"The integration of excitatory and inhibitory inputs at a neuron follows a nonlinear process, which is generally termed shunting inhibition. The experimental data has revealed that the effect of shunting inhibitionon the somatic potential can be largely expressed as a simple arithmetic rule, in which the contribution of shunting inhibition is proportional to the product between the contributions of excitatory and inhibitory inputs when they are applied individually. In this study, we develop simple neuron and network models for shunting inhibition. Our simple neuron models reproduce the experimental results qualitatively. We show that shunting inhibition can provide a mechanism to retain persistent activity in a network, andcan be well approximated as divisive normalization in describing the stationary states of continuous attractor neural networks."
Global Multi-view Subspace Learning,"Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results, particularly for high dimensional data."
Constrained fractional set programming - Tight relaxations and efficient optimization,"The problem of minimizing a ratio of set functions subject to constraints appears in several machine learning applications e.g. clustering and community detection. Convex relaxations of these NP hard problems, although having global optimality guarantees,are often too loose and do not perform well in practice. In this paper, we show that a tight continuous relaxation exists for the minimization of any ratio of non-negative set functions subject to constraints. Although global optimality cannot be guaranteed for the resulting non-convex problem, the efficient minimization scheme that we present here has a provable quality guarantee. The experiments performed on problems in local clustering and community detection clearly demonstrate the superiority of our approach. "
Constrained fractional set programming - Tight relaxations and efficient optimization,"The problem of minimizing a ratio of set functions subject to constraints appears in several machine learning applications e.g. clustering and community detection. Convex relaxations of these NP hard problems, although having global optimality guarantees,are often too loose and do not perform well in practice. In this paper, we show that a tight continuous relaxation exists for the minimization of any ratio of non-negative set functions subject to constraints. Although global optimality cannot be guaranteed for the resulting non-convex problem, the efficient minimization scheme that we present here has a provable quality guarantee. The experiments performed on problems in local clustering and community detection clearly demonstrate the superiority of our approach. "
Constrained fractional set programming - Tight relaxations and efficient optimization,"The problem of minimizing a ratio of set functions subject to constraints appears in several machine learning applications e.g. clustering and community detection. Convex relaxations of these NP hard problems, although having global optimality guarantees,are often too loose and do not perform well in practice. In this paper, we show that a tight continuous relaxation exists for the minimization of any ratio of non-negative set functions subject to constraints. Although global optimality cannot be guaranteed for the resulting non-convex problem, the efficient minimization scheme that we present here has a provable quality guarantee. The experiments performed on problems in local clustering and community detection clearly demonstrate the superiority of our approach. "
Constrained fractional set programming - Tight relaxations and efficient optimization,"The problem of minimizing a ratio of set functions subject to constraints appears in several machine learning applications e.g. clustering and community detection. Convex relaxations of these NP hard problems, although having global optimality guarantees,are often too loose and do not perform well in practice. In this paper, we show that a tight continuous relaxation exists for the minimization of any ratio of non-negative set functions subject to constraints. Although global optimality cannot be guaranteed for the resulting non-convex problem, the efficient minimization scheme that we present here has a provable quality guarantee. The experiments performed on problems in local clustering and community detection clearly demonstrate the superiority of our approach. "
Variational Bayesian Matching,"Matching of samples refers to the problem of inferring unknown co-occurrence or alignment between observations in two data sets. Given two sets of equally many samples, the task is to find for each sample a representative sample in the other set, without prior knowledge on a distance measure between the sets. Recently a few alternative solutions have been suggested, based on maximization of joint likelihood or between-data dependency. In this work we present a variational Bayesian solution for the problem, learning a   Bayesian canonical correlation analysis model with a permutation parameter for re-ordering the samples in one of the sets. We approximate the posterior over the permutations, and demonstrate that the resulting matching algorithm clearly outperforms all of the earlier solutions."
A Hierarchical Approach to Modeling Human Visual Chunk Learning,"How humans form chunks, or combinations of elementary features, is a fundamental question in visual perception and cognition. We propose a novel hierarchical chunk learning framework to detect suspicious coincidences of elements during training, and to form efficient representations of complex visual scenes. First, we utilize a hierarchical model structure to capture part-to-whole relations between visual chunks. Second, we adopt a data-driven maximum-likelihood approach to learn this model, reducing running time from several days to less than one minute. We demonstrate that our model can account for a range of experimental data concerning human chunk learning. In addition, we designed a new experiment using a paradigm that enabled human observers to form explicit representations of subparts, so that the observer could induce untrained sub-configurations in a learned hierarchical chunk. Our model based on a hierarchical approach successfully accounted for this new phenomenon, whereas previous models of chunk learning failed to predict it."
A Hierarchical Approach to Modeling Human Visual Chunk Learning,"How humans form chunks, or combinations of elementary features, is a fundamental question in visual perception and cognition. We propose a novel hierarchical chunk learning framework to detect suspicious coincidences of elements during training, and to form efficient representations of complex visual scenes. First, we utilize a hierarchical model structure to capture part-to-whole relations between visual chunks. Second, we adopt a data-driven maximum-likelihood approach to learn this model, reducing running time from several days to less than one minute. We demonstrate that our model can account for a range of experimental data concerning human chunk learning. In addition, we designed a new experiment using a paradigm that enabled human observers to form explicit representations of subparts, so that the observer could induce untrained sub-configurations in a learned hierarchical chunk. Our model based on a hierarchical approach successfully accounted for this new phenomenon, whereas previous models of chunk learning failed to predict it."
A Hierarchical Approach to Modeling Human Visual Chunk Learning,"How humans form chunks, or combinations of elementary features, is a fundamental question in visual perception and cognition. We propose a novel hierarchical chunk learning framework to detect suspicious coincidences of elements during training, and to form efficient representations of complex visual scenes. First, we utilize a hierarchical model structure to capture part-to-whole relations between visual chunks. Second, we adopt a data-driven maximum-likelihood approach to learn this model, reducing running time from several days to less than one minute. We demonstrate that our model can account for a range of experimental data concerning human chunk learning. In addition, we designed a new experiment using a paradigm that enabled human observers to form explicit representations of subparts, so that the observer could induce untrained sub-configurations in a learned hierarchical chunk. Our model based on a hierarchical approach successfully accounted for this new phenomenon, whereas previous models of chunk learning failed to predict it."
A Hierarchical Approach to Modeling Human Visual Chunk Learning,"How humans form chunks, or combinations of elementary features, is a fundamental question in visual perception and cognition. We propose a novel hierarchical chunk learning framework to detect suspicious coincidences of elements during training, and to form efficient representations of complex visual scenes. First, we utilize a hierarchical model structure to capture part-to-whole relations between visual chunks. Second, we adopt a data-driven maximum-likelihood approach to learn this model, reducing running time from several days to less than one minute. We demonstrate that our model can account for a range of experimental data concerning human chunk learning. In addition, we designed a new experiment using a paradigm that enabled human observers to form explicit representations of subparts, so that the observer could induce untrained sub-configurations in a learned hierarchical chunk. Our model based on a hierarchical approach successfully accounted for this new phenomenon, whereas previous models of chunk learning failed to predict it."
Spectral learning of linear dynamics from generalised-linear observations with application to neural population data,"Latent linear dynamical systems with generalised-linear observation models arise in a variety of applications, for example when modelling the spiking activity of populations of neurons.  Here, we show how  spectral learning methods for linear systems with Gaussian observations   (usually called subspace identification in this context) can be extended to estimate the parameters of dynamical system models observed through non-Gaussian noise models. We use this approach to obtain estimates of parameters for a dynamical model of neural population data, where the observed spike-counts are Poisson-distributed with log-rates determined by the latent dynamical process, possibly driven by external inputs. We show that the extended system identification algorithm is consistent and accurately recovers the correct parameters on large simulated data sets with much smaller computational cost than approximate expectation-maximisation (EM) due to the non-iterative nature of subspace identification. Even on smaller data sets, it provides an effective initialization for EM, leading to more robust performance and faster convergence. These benefits are shown to extend to real neural data."
Spectral learning of linear dynamics from generalised-linear observations with application to neural population data,"Latent linear dynamical systems with generalised-linear observation models arise in a variety of applications, for example when modelling the spiking activity of populations of neurons.  Here, we show how  spectral learning methods for linear systems with Gaussian observations   (usually called subspace identification in this context) can be extended to estimate the parameters of dynamical system models observed through non-Gaussian noise models. We use this approach to obtain estimates of parameters for a dynamical model of neural population data, where the observed spike-counts are Poisson-distributed with log-rates determined by the latent dynamical process, possibly driven by external inputs. We show that the extended system identification algorithm is consistent and accurately recovers the correct parameters on large simulated data sets with much smaller computational cost than approximate expectation-maximisation (EM) due to the non-iterative nature of subspace identification. Even on smaller data sets, it provides an effective initialization for EM, leading to more robust performance and faster convergence. These benefits are shown to extend to real neural data."
Spectral learning of linear dynamics from generalised-linear observations with application to neural population data,"Latent linear dynamical systems with generalised-linear observation models arise in a variety of applications, for example when modelling the spiking activity of populations of neurons.  Here, we show how  spectral learning methods for linear systems with Gaussian observations   (usually called subspace identification in this context) can be extended to estimate the parameters of dynamical system models observed through non-Gaussian noise models. We use this approach to obtain estimates of parameters for a dynamical model of neural population data, where the observed spike-counts are Poisson-distributed with log-rates determined by the latent dynamical process, possibly driven by external inputs. We show that the extended system identification algorithm is consistent and accurately recovers the correct parameters on large simulated data sets with much smaller computational cost than approximate expectation-maximisation (EM) due to the non-iterative nature of subspace identification. Even on smaller data sets, it provides an effective initialization for EM, leading to more robust performance and faster convergence. These benefits are shown to extend to real neural data."
Statistics of edge co-occurences are sufficient to categorize natural images,"The analysis and interpretation of a visual scene to extract its category, such as whether it contains an animal, is typically assumed to involve higher-level associative brain areas.  Previous proposals have been based on a series of processing steps organized in a hierarchy that would successively interpret the scene at different levels of abstraction, from contour extraction, to low-level object recognition, to object categorization. We explore here an alternate hypothesis that second-order statistics of edges are sufficient to perform a rough yet robust (translation, scale and rotation invariant) scene categorization. The method is based on a realistic model of image analysis in the primary visual cortex that extends previous work from Geisler et al. (2001). Using a scale-space analysis coupled with a sparse coding algorithm, we achieved detailed and robust extraction of edges in different sets of natural images. This edge-based representation allows for a simple characterization of the ``association field'' by computing the second-order statistics of edge co-occurences. We show that the geometry of angles is sufficient to distinguish different sets of natural images taken in a variety of environments (natural, man-made, or containing an animal). This is quantitatively illustrated by using a na?ve classifier that allows to classify images solely on the basis of this geometry which performs at similar levels to hierarchical models. Such results call for the importance of the relative geometry of local image patches and its possible applications for image analysis, for instance to improve the efficiency of visual analysis systems. Most importantly, it challenges assumptions about the flow of computations in the visual system and emphasizes on the relative importance of associative connections, and in particular of intra-areal, lateral connections, in this process."
Statistics of edge co-occurences are sufficient to categorize natural images,"The analysis and interpretation of a visual scene to extract its category, such as whether it contains an animal, is typically assumed to involve higher-level associative brain areas.  Previous proposals have been based on a series of processing steps organized in a hierarchy that would successively interpret the scene at different levels of abstraction, from contour extraction, to low-level object recognition, to object categorization. We explore here an alternate hypothesis that second-order statistics of edges are sufficient to perform a rough yet robust (translation, scale and rotation invariant) scene categorization. The method is based on a realistic model of image analysis in the primary visual cortex that extends previous work from Geisler et al. (2001). Using a scale-space analysis coupled with a sparse coding algorithm, we achieved detailed and robust extraction of edges in different sets of natural images. This edge-based representation allows for a simple characterization of the ``association field'' by computing the second-order statistics of edge co-occurences. We show that the geometry of angles is sufficient to distinguish different sets of natural images taken in a variety of environments (natural, man-made, or containing an animal). This is quantitatively illustrated by using a na?ve classifier that allows to classify images solely on the basis of this geometry which performs at similar levels to hierarchical models. Such results call for the importance of the relative geometry of local image patches and its possible applications for image analysis, for instance to improve the efficiency of visual analysis systems. Most importantly, it challenges assumptions about the flow of computations in the visual system and emphasizes on the relative importance of associative connections, and in particular of intra-areal, lateral connections, in this process."
Thompson Sampling for Complex Online Problems,"We study stochastic multi-armed bandit settings with complex actionsover the basic arms, where the decision maker has to select a subsetof the basic arms or a partition of the basic arms at every round(rather than only selecting a single basic arm). The reward of thecomplex action is some function of the basic arms' rewards, and thefeedback observed may not necessarily be the reward per-arm. Forexample, when the complex action is a subset of the arms, we may onlyobserve the total reward or the maximum reward over the chosensubset. We use Thompson sampling to decide which complex action toselect. We prove a general theorem showing that a variant of Thompsonsampling with uniform exploration obtains logarithmic regret, and weshow how the regret depends explicitly on the information gain fromthe observations. As applications, we obtain several corollaries forspecific complex bandit problem setups with improved rates. Usingparticle filters for computing posterior distributions, we devise andsimulate Thompson-sampling algorithms for subset selection andjob-scheduling problems."
Thompson Sampling for Complex Online Problems,"We study stochastic multi-armed bandit settings with complex actionsover the basic arms, where the decision maker has to select a subsetof the basic arms or a partition of the basic arms at every round(rather than only selecting a single basic arm). The reward of thecomplex action is some function of the basic arms' rewards, and thefeedback observed may not necessarily be the reward per-arm. Forexample, when the complex action is a subset of the arms, we may onlyobserve the total reward or the maximum reward over the chosensubset. We use Thompson sampling to decide which complex action toselect. We prove a general theorem showing that a variant of Thompsonsampling with uniform exploration obtains logarithmic regret, and weshow how the regret depends explicitly on the information gain fromthe observations. As applications, we obtain several corollaries forspecific complex bandit problem setups with improved rates. Usingparticle filters for computing posterior distributions, we devise andsimulate Thompson-sampling algorithms for subset selection andjob-scheduling problems."
Mixability in Statistical Learning,"Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability."
Mixability in Statistical Learning,"Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability."
Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation,"Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device."
Learning pattern-based CRF for predicting the protein local structure,"We describe a pattern-based conditional random field. Such CRFs appear naturally in sequence labeling problems of bioinformatics and can be considered as relative to Hidden Markov Models. In this model, factors that participate in conditional probability are nontrivial only if their argument belong to a certain set of sequences. We describe an inference algorithm based on dynamic programming that is optimized to the structure of the sequence set. This algorithm becomes preferable as lengths of patterns become long. Then our model is applied to predicting $\phi,\psi$ angles of all-alpha proteins. Learning of parameters for this problem was done by structural SVM technique. The accuracy in prediction of dihedral angles $\phi$ and $\psi$ we achieved was 21.1 and 48.4 degrees respectively. The MDA score, defined as the percentage of residues that are found in correctly predicted eight-residue segments, attained 57.2\%."
Learning pattern-based CRF for predicting the protein local structure,"We describe a pattern-based conditional random field. Such CRFs appear naturally in sequence labeling problems of bioinformatics and can be considered as relative to Hidden Markov Models. In this model, factors that participate in conditional probability are nontrivial only if their argument belong to a certain set of sequences. We describe an inference algorithm based on dynamic programming that is optimized to the structure of the sequence set. This algorithm becomes preferable as lengths of patterns become long. Then our model is applied to predicting $\phi,\psi$ angles of all-alpha proteins. Learning of parameters for this problem was done by structural SVM technique. The accuracy in prediction of dihedral angles $\phi$ and $\psi$ we achieved was 21.1 and 48.4 degrees respectively. The MDA score, defined as the percentage of residues that are found in correctly predicted eight-residue segments, attained 57.2\%."
Periodic Spatial Filter for Single Trial Classification of Event Related Brain Activity,"Because of the small amplitudes of event related potentials (ERPs), they are usually hidden in electroencephalogram (EEG) recordings. This is particularly a problem when analyzing single-trial data.A spatial filtering method for P300 detection in oddball paradigm is proposed in this paper which is based on the assumption that brain responses to the same stimulus look the same (or at least do not change significantly over trials). Therefore, the sequence generated by concatenating all the responses to the same type of stimulus has a hidden periodicity. Enhancing the periodic structure of this sequence, a transformation is found to project the data into a lower dimensional subspace. Experiments show that even with a small subspace of the projected data, the classification performance in single-trial P300 detection is still high."
Periodic Spatial Filter for Single Trial Classification of Event Related Brain Activity,"Because of the small amplitudes of event related potentials (ERPs), they are usually hidden in electroencephalogram (EEG) recordings. This is particularly a problem when analyzing single-trial data.A spatial filtering method for P300 detection in oddball paradigm is proposed in this paper which is based on the assumption that brain responses to the same stimulus look the same (or at least do not change significantly over trials). Therefore, the sequence generated by concatenating all the responses to the same type of stimulus has a hidden periodicity. Enhancing the periodic structure of this sequence, a transformation is found to project the data into a lower dimensional subspace. Experiments show that even with a small subspace of the projected data, the classification performance in single-trial P300 detection is still high."
Poset View and Energy Distribution Criteria for Monotonic Dual Decomposition,"Dual decomposition algorithms based on block coordinate descent are efficient techniques for approximate MAP inference in graphical models. They optimize a local dual function at each step to monotonically increase the dual function value. In this paper, we present a unified framework for constructing and optimizing the local dual function based on the partially ordered set (poset). To maximize the local dual function, we first introduce the concept of the energy distribution ratio, and then derive an explicit and globally optimal solution, which covers all the existing algorithms. We show that the differences of the monotonic algorithms can be summarized in the local dual functions and the energy distribution ratios. Furthermore, we investigate the effect of energy distribution ratios on convergence and introduce energy distribution criteria for fast convergence. New algorithms are proposed based on the criteria, and the experimental results show they outperform the existing algorithms on convergence performance."
Poset View and Energy Distribution Criteria for Monotonic Dual Decomposition,"Dual decomposition algorithms based on block coordinate descent are efficient techniques for approximate MAP inference in graphical models. They optimize a local dual function at each step to monotonically increase the dual function value. In this paper, we present a unified framework for constructing and optimizing the local dual function based on the partially ordered set (poset). To maximize the local dual function, we first introduce the concept of the energy distribution ratio, and then derive an explicit and globally optimal solution, which covers all the existing algorithms. We show that the differences of the monotonic algorithms can be summarized in the local dual functions and the energy distribution ratios. Furthermore, we investigate the effect of energy distribution ratios on convergence and introduce energy distribution criteria for fast convergence. New algorithms are proposed based on the criteria, and the experimental results show they outperform the existing algorithms on convergence performance."
A lattice filter model of the visual pathway,"Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (LGN) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons. We propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals. The stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> LGN) and its filter weights can be learned using Hebbian rules in a stage-wise sequential manner. Moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in LGN neurons and fruit fly second-order visual neurons. Therefore, the lattice filter model is a useful abstraction that may help unravel visual system function."
A lattice filter model of the visual pathway,"Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (LGN) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons. We propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals. The stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> LGN) and its filter weights can be learned using Hebbian rules in a stage-wise sequential manner. Moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in LGN neurons and fruit fly second-order visual neurons. Therefore, the lattice filter model is a useful abstraction that may help unravel visual system function."
Gossip-based On-Line Learning in Multi-Agent ?Systems with Local Decision Rules,"This paper is devoted to investigate binary classification in a distributed and on-line setting. The framework considered accounts for situations where both the training and test phases have to be performed by taking advantage of a network architecture by the means of local computations and exchange of limited information between neighbor nodes. An online learning gossip algorithm (OLGA) is introduced, together with a variant which implements a node selection procedure. Beyond a discussion of the practical advantages of the algorithm we promote, the paper proposes an analysis of the accuracy of the rules it produces, together with preliminary experimental results."
Gossip-based On-Line Learning in Multi-Agent ?Systems with Local Decision Rules,"This paper is devoted to investigate binary classification in a distributed and on-line setting. The framework considered accounts for situations where both the training and test phases have to be performed by taking advantage of a network architecture by the means of local computations and exchange of limited information between neighbor nodes. An online learning gossip algorithm (OLGA) is introduced, together with a variant which implements a node selection procedure. Beyond a discussion of the practical advantages of the algorithm we promote, the paper proposes an analysis of the accuracy of the rules it produces, together with preliminary experimental results."
Gossip-based On-Line Learning in Multi-Agent ?Systems with Local Decision Rules,"This paper is devoted to investigate binary classification in a distributed and on-line setting. The framework considered accounts for situations where both the training and test phases have to be performed by taking advantage of a network architecture by the means of local computations and exchange of limited information between neighbor nodes. An online learning gossip algorithm (OLGA) is introduced, together with a variant which implements a node selection procedure. Beyond a discussion of the practical advantages of the algorithm we promote, the paper proposes an analysis of the accuracy of the rules it produces, together with preliminary experimental results."
Gossip-based On-Line Learning in Multi-Agent ?Systems with Local Decision Rules,"This paper is devoted to investigate binary classification in a distributed and on-line setting. The framework considered accounts for situations where both the training and test phases have to be performed by taking advantage of a network architecture by the means of local computations and exchange of limited information between neighbor nodes. An online learning gossip algorithm (OLGA) is introduced, together with a variant which implements a node selection procedure. Beyond a discussion of the practical advantages of the algorithm we promote, the paper proposes an analysis of the accuracy of the rules it produces, together with preliminary experimental results."
Constrained stochastic gradient descent for large-scale least squares problem,"The least squares problem is one of the most important regression problems in statistics and machine learning. In this paper, we present a Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by using the constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound O(log{T}), and fastest convergence speed among all first order approaches. Empirical studies confirm the effectiveness of CSGD by comparing with SGD and the averaged SGD."
Constrained stochastic gradient descent for large-scale least squares problem,"The least squares problem is one of the most important regression problems in statistics and machine learning. In this paper, we present a Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by using the constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound O(log{T}), and fastest convergence speed among all first order approaches. Empirical studies confirm the effectiveness of CSGD by comparing with SGD and the averaged SGD."
Constrained stochastic gradient descent for large-scale least squares problem,"The least squares problem is one of the most important regression problems in statistics and machine learning. In this paper, we present a Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by using the constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound O(log{T}), and fastest convergence speed among all first order approaches. Empirical studies confirm the effectiveness of CSGD by comparing with SGD and the averaged SGD."
Bayesian Fusion of Image Modalities with Disjoint Attributes,"Large scale monitoring of spatial phenomenon often produces image data with multiple modalities. In the computer vision literature, fusion of images is usually formulated as a principled sensor inversion problem, with a wide variety of techniques applicable to the case where multiple observed images have the same underlying attributes. On the other hand, in many remote sensing applications the acquired modalities have no common attribute channels. For this case, we seek to use the detail of the high-resolution image modalities to enhance the low resolution modalities, considering that the data may be related through their content structure rather than the sampling process alone. This paper presents a Gaussian Process (GP) formulation to transfer spatial structure from the high-resolution image into the low-resolution modality without assuming a value mapping. Experimentation suggests the proposed approach is able to reconstruct local detail across large resolution differences, and we present fusion results from real aerial data with modalities from both a low flying unmanned robotic aircraft and a high altitude commercial hyperspectral imager."
Bayesian Fusion of Image Modalities with Disjoint Attributes,"Large scale monitoring of spatial phenomenon often produces image data with multiple modalities. In the computer vision literature, fusion of images is usually formulated as a principled sensor inversion problem, with a wide variety of techniques applicable to the case where multiple observed images have the same underlying attributes. On the other hand, in many remote sensing applications the acquired modalities have no common attribute channels. For this case, we seek to use the detail of the high-resolution image modalities to enhance the low resolution modalities, considering that the data may be related through their content structure rather than the sampling process alone. This paper presents a Gaussian Process (GP) formulation to transfer spatial structure from the high-resolution image into the low-resolution modality without assuming a value mapping. Experimentation suggests the proposed approach is able to reconstruct local detail across large resolution differences, and we present fusion results from real aerial data with modalities from both a low flying unmanned robotic aircraft and a high altitude commercial hyperspectral imager."
Bayesian Fusion of Image Modalities with Disjoint Attributes,"Large scale monitoring of spatial phenomenon often produces image data with multiple modalities. In the computer vision literature, fusion of images is usually formulated as a principled sensor inversion problem, with a wide variety of techniques applicable to the case where multiple observed images have the same underlying attributes. On the other hand, in many remote sensing applications the acquired modalities have no common attribute channels. For this case, we seek to use the detail of the high-resolution image modalities to enhance the low resolution modalities, considering that the data may be related through their content structure rather than the sampling process alone. This paper presents a Gaussian Process (GP) formulation to transfer spatial structure from the high-resolution image into the low-resolution modality without assuming a value mapping. Experimentation suggests the proposed approach is able to reconstruct local detail across large resolution differences, and we present fusion results from real aerial data with modalities from both a low flying unmanned robotic aircraft and a high altitude commercial hyperspectral imager."
Estimating Node Labels via Feature Propagation,"We propose a new method for estimating node labels from given instances (nodes) with a graph, particularly focusing on the estimation of the graph edge weights. We estimate edge weights through hyper-parameter optimization of a harmonic Gaussian field model for feature vectors, which we call feature vector propagation (FVP). FVP defines edge weights as a parameterized similarity function and optimizes edge hyper-parameters by cross-validation over feature vectors of all nodes. That is, the optimization is independent of labeled instances, leading to several important advantages, such as the robustness against sparsely labeled graphs and the applicability to multi-class problems. FVP can also capture the local structure of data by the objective function which shares the same form as the local reconstruction error in locally linear embedding. Experimental results demonstrated the effectiveness of FVP both in synthetic and real datasets."
Estimating Node Labels via Feature Propagation,"We propose a new method for estimating node labels from given instances (nodes) with a graph, particularly focusing on the estimation of the graph edge weights. We estimate edge weights through hyper-parameter optimization of a harmonic Gaussian field model for feature vectors, which we call feature vector propagation (FVP). FVP defines edge weights as a parameterized similarity function and optimizes edge hyper-parameters by cross-validation over feature vectors of all nodes. That is, the optimization is independent of labeled instances, leading to several important advantages, such as the robustness against sparsely labeled graphs and the applicability to multi-class problems. FVP can also capture the local structure of data by the objective function which shares the same form as the local reconstruction error in locally linear embedding. Experimental results demonstrated the effectiveness of FVP both in synthetic and real datasets."
How to sample if you must: On Optimal Functional Sampling,"Abstract. We examine a fundamental problem that models various active sampling setups, such as network tomography. We analyze sampling of a multivariate normal distribution with an unknown expectation that needs to be estimated: in our setup it is possible to sample the distribution from a given set of linear functionals, and the difficulty addressed is how to optimally select the combinations to achieve low estimation error. Although this problem is in the heart of the field of optimal design, no efficient solutions for the case with many functionals exist. We present some bounds and an efficient sub-optimal solution for this problem for more structured sets such as binary functionals that are induced by graph walks. "
How to sample if you must: On Optimal Functional Sampling,"Abstract. We examine a fundamental problem that models various active sampling setups, such as network tomography. We analyze sampling of a multivariate normal distribution with an unknown expectation that needs to be estimated: in our setup it is possible to sample the distribution from a given set of linear functionals, and the difficulty addressed is how to optimally select the combinations to achieve low estimation error. Although this problem is in the heart of the field of optimal design, no efficient solutions for the case with many functionals exist. We present some bounds and an efficient sub-optimal solution for this problem for more structured sets such as binary functionals that are induced by graph walks. "
Infinitesimal Annealing for Training Semi-Supervised Support Vector Machine,"The semi-supervised support vector machine(S^3VM) is a popular classification algorithm for finding the maximum-margin separating hyper-plane for both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good(local) solution of S^3VM is to gradually increase the effect of unlabeled data, `a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches. "
Infinitesimal Annealing for Training Semi-Supervised Support Vector Machine,"The semi-supervised support vector machine(S^3VM) is a popular classification algorithm for finding the maximum-margin separating hyper-plane for both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good(local) solution of S^3VM is to gradually increase the effect of unlabeled data, `a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches. "
Learning Collaborative Behaviors Through Observation: A Contextual Case-Based Planning Approach,Learning teamwork behaviors has been an increasingly important issue for different applications in multi agent systems. Observational learning of teamwork behaviors provides an intuitive and effective method to imitate the behaviors of experts. However the agents should have the ability to generalize observed behaviors for unseen situations. In this paper a new framework is employed for learning collaborative behaviors from an expert team. Our method enhances observational learning method with explicit domain knowledge. Agents observe an expert team perform some tasks to learn plans from observational data. Tasks are defined with the help of some explicit domain knowledge. A contextual case-based planning system uses learned plans to enable learners to imitate expert?s behaviors by performing tasks for different contexts. This paper describes an implemented soccer prototype built to evaluate the effectiveness of our hybrid approach by some experiments. The results of experiments show improved teamwork performance and generalization ability in comparison with other alternative methods.
Learning Collaborative Behaviors Through Observation: A Contextual Case-Based Planning Approach,Learning teamwork behaviors has been an increasingly important issue for different applications in multi agent systems. Observational learning of teamwork behaviors provides an intuitive and effective method to imitate the behaviors of experts. However the agents should have the ability to generalize observed behaviors for unseen situations. In this paper a new framework is employed for learning collaborative behaviors from an expert team. Our method enhances observational learning method with explicit domain knowledge. Agents observe an expert team perform some tasks to learn plans from observational data. Tasks are defined with the help of some explicit domain knowledge. A contextual case-based planning system uses learned plans to enable learners to imitate expert?s behaviors by performing tasks for different contexts. This paper describes an implemented soccer prototype built to evaluate the effectiveness of our hybrid approach by some experiments. The results of experiments show improved teamwork performance and generalization ability in comparison with other alternative methods.
Learning Collaborative Behaviors Through Observation: A Contextual Case-Based Planning Approach,Learning teamwork behaviors has been an increasingly important issue for different applications in multi agent systems. Observational learning of teamwork behaviors provides an intuitive and effective method to imitate the behaviors of experts. However the agents should have the ability to generalize observed behaviors for unseen situations. In this paper a new framework is employed for learning collaborative behaviors from an expert team. Our method enhances observational learning method with explicit domain knowledge. Agents observe an expert team perform some tasks to learn plans from observational data. Tasks are defined with the help of some explicit domain knowledge. A contextual case-based planning system uses learned plans to enable learners to imitate expert?s behaviors by performing tasks for different contexts. This paper describes an implemented soccer prototype built to evaluate the effectiveness of our hybrid approach by some experiments. The results of experiments show improved teamwork performance and generalization ability in comparison with other alternative methods.
Data Representation with Rank Regularized PCA,"Trace-norm is often used in low-rank data representation models. In this paper, we point out some drawbacks of the trace norm based approach and propose a rank regularized formulation which can be solved very efficiently. We did extensive experiments on six datasets. Experiments show the advantage of the proposed approach."
Data Representation with Rank Regularized PCA,"Trace-norm is often used in low-rank data representation models. In this paper, we point out some drawbacks of the trace norm based approach and propose a rank regularized formulation which can be solved very efficiently. We did extensive experiments on six datasets. Experiments show the advantage of the proposed approach."
Diversity and Capacity Control in Boosting,"Various explanations have been proposed for understanding the great success of boosting algorithms, particularly AdaBoost, mainly including the statistical view and the margin theory. However, there are still observations out of the explanations. In this paper, we investigate the learning capacity (also known as the hypothesis space complexity and structure risk) of AdaBoost, which has not been well investigated in existing explanations. Previously, the learning capacity of boosting was canonically measured by the VC-dimension of the convex hull of the linear combination of hypotheses, which shows that the capacity grows exponentially to the number of base hypotheses. This paper proves the connection between the learning capacity and the diversity among base hypotheses, which discloses that the learning capacity can be small if the diversity is large. We then reveal that AdaBoost can automatically maximize diversity while optimizing its loss function, and therefore, implicitly controls its learning capacity. The investigation on diversity of AdaBoost may provide a clue to complement the understanding of AdaBoost and boosting algorithms."
Diversity and Capacity Control in Boosting,"Various explanations have been proposed for understanding the great success of boosting algorithms, particularly AdaBoost, mainly including the statistical view and the margin theory. However, there are still observations out of the explanations. In this paper, we investigate the learning capacity (also known as the hypothesis space complexity and structure risk) of AdaBoost, which has not been well investigated in existing explanations. Previously, the learning capacity of boosting was canonically measured by the VC-dimension of the convex hull of the linear combination of hypotheses, which shows that the capacity grows exponentially to the number of base hypotheses. This paper proves the connection between the learning capacity and the diversity among base hypotheses, which discloses that the learning capacity can be small if the diversity is large. We then reveal that AdaBoost can automatically maximize diversity while optimizing its loss function, and therefore, implicitly controls its learning capacity. The investigation on diversity of AdaBoost may provide a clue to complement the understanding of AdaBoost and boosting algorithms."
Diversity and Capacity Control in Boosting,"Various explanations have been proposed for understanding the great success of boosting algorithms, particularly AdaBoost, mainly including the statistical view and the margin theory. However, there are still observations out of the explanations. In this paper, we investigate the learning capacity (also known as the hypothesis space complexity and structure risk) of AdaBoost, which has not been well investigated in existing explanations. Previously, the learning capacity of boosting was canonically measured by the VC-dimension of the convex hull of the linear combination of hypotheses, which shows that the capacity grows exponentially to the number of base hypotheses. This paper proves the connection between the learning capacity and the diversity among base hypotheses, which discloses that the learning capacity can be small if the diversity is large. We then reveal that AdaBoost can automatically maximize diversity while optimizing its loss function, and therefore, implicitly controls its learning capacity. The investigation on diversity of AdaBoost may provide a clue to complement the understanding of AdaBoost and boosting algorithms."
Gradient-Boosted Adaptive Codes for Classification and Embedding,"Discriminative classifiers pursue simultaneous embeddings of observations and classes in a shared space such that the embedding of each observation is more similar to the embedding of its associated class than to that of any other class. Boosting-based classifiers are often partitioned into two sets: those which assume a fixed embedding of the classes and those which concurrently learn to embed both the observations and classes. The classifier we introduce falls into the latter camp; it learns all dimensions of the observation and class embeddings concurrently. With L1 regularization applied to the class embedding, our method outperforms existing boosted classifiers on standard benchmarks. Using Euclidean distance to measure class-observation similarity, rather than the typical dot-product, and with additional regularization controlling the spread of each class' embedded representation, our method also produces meaningful embeddings of labeled data. We begin our presentation by recapitulating boosting as functional gradient descent and then examining a weakness in one frequently cited theorem concerning the convergence of gradient-based boosting."
Bandit Market Makers,"We propose a flexible framework for profit-seeking market-making, using a sequence of cost-function based automated market-makers with bandit learning algorithms. We do this by considering the magnitude to which a cost-function extends beyond the simplex as a bandit arm, and the minimum-expected profits consistent with a no-arbitrage condition as the rewards. This allows for the creation of market-makers that can adjust bid-asks spreads dynamically, maximising worst-case-expected profits. "
Clustered Bandits,"We consider a multi-armed bandit setting that is inspired by real-world applications in e-commerce.  In our setting, there are a few types of users, each with a specific response to the different arms. When a user enters the system, his type is unknown to the decision maker.  The decision maker can either treat each user separately ignoring the previously observed users, or can attempt to take advantage of knowing that only few types exist and cluster the users according to their response to the arms. We devise algorithms that combine the usual exploration-exploitation tradeoff with clustering of users and demonstrate the value of clustering.  In the process of developing algorithms for the clustered setting, we propose and analyze simple algorithms for the setup where a decision maker knows that a user belongs to one of few types, but does not know which one."
Clustered Bandits,"We consider a multi-armed bandit setting that is inspired by real-world applications in e-commerce.  In our setting, there are a few types of users, each with a specific response to the different arms. When a user enters the system, his type is unknown to the decision maker.  The decision maker can either treat each user separately ignoring the previously observed users, or can attempt to take advantage of knowing that only few types exist and cluster the users according to their response to the arms. We devise algorithms that combine the usual exploration-exploitation tradeoff with clustering of users and demonstrate the value of clustering.  In the process of developing algorithms for the clustered setting, we propose and analyze simple algorithms for the setup where a decision maker knows that a user belongs to one of few types, but does not know which one."
Discovering Latent Styles in Human Movements,"There are often latent styles underlying the dynamics of human movements; for instance, table tennis strokes can be executed with forehand push or backhand chop. Modeling latent styles in human movements is crucial for learning prior models in many applications. We propose a latent style dynamics model to capture the generative process of human movements from latent styles. As efficient inference is desired in practice, we introduce an approximate inference method based on proxy variables, which, despite its low complexity, also takes into account the uncertainty in latent style variables. On both synthetic data and human table tennis stroke data, our method successfully discovers interpretable latent styles and provides reliable modeling of human dynamics."
Discovering Latent Styles in Human Movements,"There are often latent styles underlying the dynamics of human movements; for instance, table tennis strokes can be executed with forehand push or backhand chop. Modeling latent styles in human movements is crucial for learning prior models in many applications. We propose a latent style dynamics model to capture the generative process of human movements from latent styles. As efficient inference is desired in practice, we introduce an approximate inference method based on proxy variables, which, despite its low complexity, also takes into account the uncertainty in latent style variables. On both synthetic data and human table tennis stroke data, our method successfully discovers interpretable latent styles and provides reliable modeling of human dynamics."
Discovering Latent Styles in Human Movements,"There are often latent styles underlying the dynamics of human movements; for instance, table tennis strokes can be executed with forehand push or backhand chop. Modeling latent styles in human movements is crucial for learning prior models in many applications. We propose a latent style dynamics model to capture the generative process of human movements from latent styles. As efficient inference is desired in practice, we introduce an approximate inference method based on proxy variables, which, despite its low complexity, also takes into account the uncertainty in latent style variables. On both synthetic data and human table tennis stroke data, our method successfully discovers interpretable latent styles and provides reliable modeling of human dynamics."
Discovering Latent Styles in Human Movements,"There are often latent styles underlying the dynamics of human movements; for instance, table tennis strokes can be executed with forehand push or backhand chop. Modeling latent styles in human movements is crucial for learning prior models in many applications. We propose a latent style dynamics model to capture the generative process of human movements from latent styles. As efficient inference is desired in practice, we introduce an approximate inference method based on proxy variables, which, despite its low complexity, also takes into account the uncertainty in latent style variables. On both synthetic data and human table tennis stroke data, our method successfully discovers interpretable latent styles and provides reliable modeling of human dynamics."
Discovering Latent Styles in Human Movements,"There are often latent styles underlying the dynamics of human movements; for instance, table tennis strokes can be executed with forehand push or backhand chop. Modeling latent styles in human movements is crucial for learning prior models in many applications. We propose a latent style dynamics model to capture the generative process of human movements from latent styles. As efficient inference is desired in practice, we introduce an approximate inference method based on proxy variables, which, despite its low complexity, also takes into account the uncertainty in latent style variables. On both synthetic data and human table tennis stroke data, our method successfully discovers interpretable latent styles and provides reliable modeling of human dynamics."
Learning from the Wisdom of Crowds by Minimax Conditional Entropy,"We consider the multiclass crowd labeling issue.  Each instance is labeled several times by different workers,  while one instance might be labeled more times than another. We propose a minimax conditional entropy principle to simultaneously estimate  worker expertise,  task ambiguities, and true labels. We also suggest an objectivity requirement for reasonably measuring worker expertise and task ambiguities, and show that the proposed method is unique in meeting  the objectivity requirement. Experimental results are presented for both synthetic and real data."
Learning from the Wisdom of Crowds by Minimax Conditional Entropy,"We consider the multiclass crowd labeling issue.  Each instance is labeled several times by different workers,  while one instance might be labeled more times than another. We propose a minimax conditional entropy principle to simultaneously estimate  worker expertise,  task ambiguities, and true labels. We also suggest an objectivity requirement for reasonably measuring worker expertise and task ambiguities, and show that the proposed method is unique in meeting  the objectivity requirement. Experimental results are presented for both synthetic and real data."
Learning from the Wisdom of Crowds by Minimax Conditional Entropy,"We consider the multiclass crowd labeling issue.  Each instance is labeled several times by different workers,  while one instance might be labeled more times than another. We propose a minimax conditional entropy principle to simultaneously estimate  worker expertise,  task ambiguities, and true labels. We also suggest an objectivity requirement for reasonably measuring worker expertise and task ambiguities, and show that the proposed method is unique in meeting  the objectivity requirement. Experimental results are presented for both synthetic and real data."
Learning from the Wisdom of Crowds by Minimax Conditional Entropy,"We consider the multiclass crowd labeling issue.  Each instance is labeled several times by different workers,  while one instance might be labeled more times than another. We propose a minimax conditional entropy principle to simultaneously estimate  worker expertise,  task ambiguities, and true labels. We also suggest an objectivity requirement for reasonably measuring worker expertise and task ambiguities, and show that the proposed method is unique in meeting  the objectivity requirement. Experimental results are presented for both synthetic and real data."
Adaptive Training for Online Transfer Learning ,"Training an effective prediction model using small amount of data is important topic in machine learning.For this purpose, one of the most widely studied frameworks is transfer learning.To achieve scalability and reduce memory, we focus on online transfer learning. Although most works on online learning intends to minimize cumulative errors throughout online training,we introduce a novel online algorithm for transfer learningto obtain the best prediction adaptivelyand improve generalization error rapidly rather than cumulative errors.We give a strong theoretical support on the predictive accuracy of our algorithm.Numerical experiments for several datasets demonstrates that our algorithm always have optimal accuracy in course of online training."
Adaptive Training for Online Transfer Learning ,"Training an effective prediction model using small amount of data is important topic in machine learning.For this purpose, one of the most widely studied frameworks is transfer learning.To achieve scalability and reduce memory, we focus on online transfer learning. Although most works on online learning intends to minimize cumulative errors throughout online training,we introduce a novel online algorithm for transfer learningto obtain the best prediction adaptivelyand improve generalization error rapidly rather than cumulative errors.We give a strong theoretical support on the predictive accuracy of our algorithm.Numerical experiments for several datasets demonstrates that our algorithm always have optimal accuracy in course of online training."
Adaptive Training for Online Transfer Learning ,"Training an effective prediction model using small amount of data is important topic in machine learning.For this purpose, one of the most widely studied frameworks is transfer learning.To achieve scalability and reduce memory, we focus on online transfer learning. Although most works on online learning intends to minimize cumulative errors throughout online training,we introduce a novel online algorithm for transfer learningto obtain the best prediction adaptivelyand improve generalization error rapidly rather than cumulative errors.We give a strong theoretical support on the predictive accuracy of our algorithm.Numerical experiments for several datasets demonstrates that our algorithm always have optimal accuracy in course of online training."
Learning Representations for Detecting and Recognizing Sequences of Animated Motion - A Neural Model,"The detection and categorization of animate motions is a crucial task underlying social interaction and perceptual decision-making. Neural representations of perceived animate objects are built in the primate cortical region STS which is a region of convergent input from intermediate level form and motion representations. Populations of STS cells exist which are selectively responsive to specific animated motion sequences, such as walkers. It is still unclear how and to which extent form and motion information contribute to the generation of such representations and what kind of mechanisms are involved in the learning processes. The paper develops a cortical model architecture for the unsupervised learning of animated motion sequence representations. We demonstrate how the model automatically selects significant motion patterns as well as meaningful static form prototypes characterized by a high degree of articulation. Such key poses are selectively reinforced during learning through a cross-talk between the motion and form processing streams. Next, we show how sequence selective representations are learned in STS by fusing static form and motion input from the segregated bottom-up driving input streams. Cells in STS, in turn, feed their activities recurrently to their input sites along top-down signal pathways. We show how such learned feedback connections enable making predictions about future input as anticipation generated by sequence-selective STS cells. Network simulations demonstrate the computational capacity of the proposed model by reproducing several experimental findings from neurosciences and by accounting for recent behavioral data."
Learning Representations for Detecting and Recognizing Sequences of Animated Motion - A Neural Model,"The detection and categorization of animate motions is a crucial task underlying social interaction and perceptual decision-making. Neural representations of perceived animate objects are built in the primate cortical region STS which is a region of convergent input from intermediate level form and motion representations. Populations of STS cells exist which are selectively responsive to specific animated motion sequences, such as walkers. It is still unclear how and to which extent form and motion information contribute to the generation of such representations and what kind of mechanisms are involved in the learning processes. The paper develops a cortical model architecture for the unsupervised learning of animated motion sequence representations. We demonstrate how the model automatically selects significant motion patterns as well as meaningful static form prototypes characterized by a high degree of articulation. Such key poses are selectively reinforced during learning through a cross-talk between the motion and form processing streams. Next, we show how sequence selective representations are learned in STS by fusing static form and motion input from the segregated bottom-up driving input streams. Cells in STS, in turn, feed their activities recurrently to their input sites along top-down signal pathways. We show how such learned feedback connections enable making predictions about future input as anticipation generated by sequence-selective STS cells. Network simulations demonstrate the computational capacity of the proposed model by reproducing several experimental findings from neurosciences and by accounting for recent behavioral data."
Learning Representations for Detecting and Recognizing Sequences of Animated Motion - A Neural Model,"The detection and categorization of animate motions is a crucial task underlying social interaction and perceptual decision-making. Neural representations of perceived animate objects are built in the primate cortical region STS which is a region of convergent input from intermediate level form and motion representations. Populations of STS cells exist which are selectively responsive to specific animated motion sequences, such as walkers. It is still unclear how and to which extent form and motion information contribute to the generation of such representations and what kind of mechanisms are involved in the learning processes. The paper develops a cortical model architecture for the unsupervised learning of animated motion sequence representations. We demonstrate how the model automatically selects significant motion patterns as well as meaningful static form prototypes characterized by a high degree of articulation. Such key poses are selectively reinforced during learning through a cross-talk between the motion and form processing streams. Next, we show how sequence selective representations are learned in STS by fusing static form and motion input from the segregated bottom-up driving input streams. Cells in STS, in turn, feed their activities recurrently to their input sites along top-down signal pathways. We show how such learned feedback connections enable making predictions about future input as anticipation generated by sequence-selective STS cells. Network simulations demonstrate the computational capacity of the proposed model by reproducing several experimental findings from neurosciences and by accounting for recent behavioral data."
The Design of a Vibro-tactile Watch for Long-term Use and Learning,"In this article, we present a mechanical tactile device that emulates a watch. This device has many practical applications but requires further study. We discuss its design, benefits, and flaws. We then discuss possible solutions to specific problems, as well as future uses of the device. We also discuss the implications this device may have on haptic research."
An Iterative Soft-Hard Thresholding Method for Low-Rank Matrix Recovery and Completion,"Low-rank matrix recovery and completion problems can be solved via their convex relaxations, which minimize the nuclear norm instead of the rank function, and have to be solved iteratively and involve singular value decomposition (SVD) at each iteration. Therefore, those algorithms suffer from high computational cost of multiple SVDs. In this paper we propose an efficient iterative soft-hard thresholding (ISHT) method to approximate the original nuclear norm minimization (NNM) problem and mitigate the computational cost of performing SVDs. The proposed ISHT method can be used to address a wide range of low-rank matrix recovery and completion problems such as low-rank representation (LRR), robust principal component analysis (RPCA) and low-rank matrix completion (MC). The ISHT method relies on first order optimization with orthogonal constraint. Furthermore, we also prove that the proposed algorithm converges to local minima. Experimental results validate the efficiency, robustness and effectiveness of our ISHT method comparing with the state-of-the-art NNM algorithm."
An Iterative Soft-Hard Thresholding Method for Low-Rank Matrix Recovery and Completion,"Low-rank matrix recovery and completion problems can be solved via their convex relaxations, which minimize the nuclear norm instead of the rank function, and have to be solved iteratively and involve singular value decomposition (SVD) at each iteration. Therefore, those algorithms suffer from high computational cost of multiple SVDs. In this paper we propose an efficient iterative soft-hard thresholding (ISHT) method to approximate the original nuclear norm minimization (NNM) problem and mitigate the computational cost of performing SVDs. The proposed ISHT method can be used to address a wide range of low-rank matrix recovery and completion problems such as low-rank representation (LRR), robust principal component analysis (RPCA) and low-rank matrix completion (MC). The ISHT method relies on first order optimization with orthogonal constraint. Furthermore, we also prove that the proposed algorithm converges to local minima. Experimental results validate the efficiency, robustness and effectiveness of our ISHT method comparing with the state-of-the-art NNM algorithm."
Spectral Differential Privacy,"Positive semidefinite matrices are important for a number of machine learning applications. We consider the problem of differentially private publication of positive semidefinite matrices computed from private information. Differential privacy is typically achieved by adding random noise.However, when the outputs form positive semidefinite matrices, element-wise additive randomization causes problems. First, when not a single element, but the entire matrix is released, the scale of noises to provide differential privacy can be too large. Second, such randomization not only destroys the positive semidefiniteness, but may be statistically denoised in some cases.For these problems, we introduce a new randomization mechanism which separately randomizes eigenvectors and eigenvalues so that the randomization does not completely destroy the spectral features. Furthermore, noting that low-rank approximation preserves useful information of matrices while discarding unnecessarily details, we incorporate low-rank approximation into randomization.We prove that the scale of perturbation required to guarantee differential privacy is inversely proportional to the rank of the output matrices in the proposed randomization mechanism. Thus, if a data analyst does not need the output matrix itself, but needs only a low-rank approximation, the scale of perturbation can be relatively smaller without sacrificing privacy. This is convenient for machine learning applications which work well even with lower-rank approximation.We experimentally demonstrate that low-rank approximation helps to implicitly control the accuracy-privacy trade-off with  a collaborative filtering example."
Spectral Differential Privacy,"Positive semidefinite matrices are important for a number of machine learning applications. We consider the problem of differentially private publication of positive semidefinite matrices computed from private information. Differential privacy is typically achieved by adding random noise.However, when the outputs form positive semidefinite matrices, element-wise additive randomization causes problems. First, when not a single element, but the entire matrix is released, the scale of noises to provide differential privacy can be too large. Second, such randomization not only destroys the positive semidefiniteness, but may be statistically denoised in some cases.For these problems, we introduce a new randomization mechanism which separately randomizes eigenvectors and eigenvalues so that the randomization does not completely destroy the spectral features. Furthermore, noting that low-rank approximation preserves useful information of matrices while discarding unnecessarily details, we incorporate low-rank approximation into randomization.We prove that the scale of perturbation required to guarantee differential privacy is inversely proportional to the rank of the output matrices in the proposed randomization mechanism. Thus, if a data analyst does not need the output matrix itself, but needs only a low-rank approximation, the scale of perturbation can be relatively smaller without sacrificing privacy. This is convenient for machine learning applications which work well even with lower-rank approximation.We experimentally demonstrate that low-rank approximation helps to implicitly control the accuracy-privacy trade-off with  a collaborative filtering example."
Nearly Optimal Algorithm for Euclidean Projection onto a Nonnegative Max-Heap,"Optimization problems with non-negative max-heap constraints arise in many regression/classification applications because the features often exhibit some hierarchical relationships. Euclidean projection is one essential step in the optimization scheme, so it is of great interest to compute the projection of a vector (of length $p$) onto a non-negative max-heap. In this paper, we develop a greedy merge-based algorithm (GMBA) for computing such projection. The algorithm benefits from powerful data structures, including Fibonacci heaps for children retrieve, heap for greedy selection, and disjoint-set for super-node finding. The proposed algorithm runs in $O(p \log p)$ for an arbitrary tree, which is much faster than the best algorithm previously known whose time complexity is $O(p^2)$. Numerical simulations show that the proposed algorithm is efficient and is significantly faster than previous one."
Nearly Optimal Algorithm for Euclidean Projection onto a Nonnegative Max-Heap,"Optimization problems with non-negative max-heap constraints arise in many regression/classification applications because the features often exhibit some hierarchical relationships. Euclidean projection is one essential step in the optimization scheme, so it is of great interest to compute the projection of a vector (of length $p$) onto a non-negative max-heap. In this paper, we develop a greedy merge-based algorithm (GMBA) for computing such projection. The algorithm benefits from powerful data structures, including Fibonacci heaps for children retrieve, heap for greedy selection, and disjoint-set for super-node finding. The proposed algorithm runs in $O(p \log p)$ for an arbitrary tree, which is much faster than the best algorithm previously known whose time complexity is $O(p^2)$. Numerical simulations show that the proposed algorithm is efficient and is significantly faster than previous one."
Nearly Optimal Algorithm for Euclidean Projection onto a Nonnegative Max-Heap,"Optimization problems with non-negative max-heap constraints arise in many regression/classification applications because the features often exhibit some hierarchical relationships. Euclidean projection is one essential step in the optimization scheme, so it is of great interest to compute the projection of a vector (of length $p$) onto a non-negative max-heap. In this paper, we develop a greedy merge-based algorithm (GMBA) for computing such projection. The algorithm benefits from powerful data structures, including Fibonacci heaps for children retrieve, heap for greedy selection, and disjoint-set for super-node finding. The proposed algorithm runs in $O(p \log p)$ for an arbitrary tree, which is much faster than the best algorithm previously known whose time complexity is $O(p^2)$. Numerical simulations show that the proposed algorithm is efficient and is significantly faster than previous one."
Multimodal Learning with Deep Boltzmann Machines,"We propose a Deep Boltzmann Machine for learning a generative model of multimodal data. We show how to use the model to extract a meaningful representation of multimodal data. We find that the learned representation is useful for classificationand information retreival tasks, and hence conforms to some notion of semantic similarity. The model defines a probability density over the space of multimodal inputs. By sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval fromboth unimodal and multimodal queries. We further demonstrate that our model can significantly outperform SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains."
Multimodal Learning with Deep Boltzmann Machines,"We propose a Deep Boltzmann Machine for learning a generative model of multimodal data. We show how to use the model to extract a meaningful representation of multimodal data. We find that the learned representation is useful for classificationand information retreival tasks, and hence conforms to some notion of semantic similarity. The model defines a probability density over the space of multimodal inputs. By sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval fromboth unimodal and multimodal queries. We further demonstrate that our model can significantly outperform SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains."
Learning with Target Prior,"In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\y$ can be modeled with a prior model $p(\y)$ and the relations between data and target variables are estimated through $p(\y)$ and a set of uncorresponded data $\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\t$ that maximizes the log likelihood of $f_\t(\x)$ on a uncorresponded training set with regards to $p(\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video."
Learning with Target Prior,"In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\y$ can be modeled with a prior model $p(\y)$ and the relations between data and target variables are estimated through $p(\y)$ and a set of uncorresponded data $\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\t$ that maximizes the log likelihood of $f_\t(\x)$ on a uncorresponded training set with regards to $p(\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video."
On Pre-training Shallow Networks with Support Vector Machine Primals,"We present a methodology to pre-train shallow neural networks with Support Vector Machine primals. We train a Support Vector Machine and extract the primal weights to embed them as pre-trained prior knowledge in a shallow neural network; we then proceed to apply backpropagation to leverage and fine tune this knowledge. This contrasts with previous work on pre-training, in which unsupervised pre-training has been used as feature extractors in deep learning. In our MNIST experimental results, we find that using even only $\frac{1}{60}$ of the original dataset for the Support Vector Machine primal pre-training yielded a consistently faster convergence in the network. We believe this paper opens up interesting opportunities for pre-training shallow networks using prior knowledge."
Slice sampling normalized kernel-weighted completely random measure mixture models,"A number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality.  However, the inference algorithms are often slow and unwieldy, and are in general highly specific to a given model formulation. In this paper, we describe a wide class of nonparametric processes, including several existingmodels, and present a slice sampler that allows efficient inference across this class of models.  "
Slice sampling normalized kernel-weighted completely random measure mixture models,"A number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality.  However, the inference algorithms are often slow and unwieldy, and are in general highly specific to a given model formulation. In this paper, we describe a wide class of nonparametric processes, including several existingmodels, and present a slice sampler that allows efficient inference across this class of models.  "
Semi-Supervised Learning with Probabilistic Smoothness on Graphs,"We study graph-based semi-supervised learning by exploiting the smoothness constraint with respect to the intrinsic structure that exists among labeled and unlabeled data. Unlike previous works that define the smoothness constraint by different cost functions, we formulate it under a probabilistic framework instead. Interestingly, our probabilistic smoothness constraint can be tied to and hence justifies a new cost function. Based on this probabilistic smoothness constraint, we further derive an algorithm PSmooth for semi-supervised learning, which can also be interpreted as a random walk on graphs. Finally, our experiments show that PSmooth consistently outperforms existing state-of-the-art algorithms on various public benchmark datasets."
Semi-Supervised Learning with Probabilistic Smoothness on Graphs,"We study graph-based semi-supervised learning by exploiting the smoothness constraint with respect to the intrinsic structure that exists among labeled and unlabeled data. Unlike previous works that define the smoothness constraint by different cost functions, we formulate it under a probabilistic framework instead. Interestingly, our probabilistic smoothness constraint can be tied to and hence justifies a new cost function. Based on this probabilistic smoothness constraint, we further derive an algorithm PSmooth for semi-supervised learning, which can also be interpreted as a random walk on graphs. Finally, our experiments show that PSmooth consistently outperforms existing state-of-the-art algorithms on various public benchmark datasets."
Deep Attribute Networks for Attribute-based Classification,"Obtaining compact and discriminative features is one of the major challenges in many of the real-world image classification tasks such as face verification and object recognition. One possible approach is to represent input image on the basis of high-level features that carry semantic meaning that humans can understand. In this paper, a model coined deep attribute network (DAN) is proposed to address this issue. For an input image, the model outputs the attributes of the input image without performing any classification. The efficacy of the proposed model is evaluated on unconstrained face verification and real-world object recognition tasks using the LFW and the PASCAL VOC datasets. We demonstrate the potential of deep learning for attribute-based classification by showing comparable results with existing state-of-the-art results. Once properly trained, DAN is fast and does away with calculating low-level features which maybe unreliable and computationally expensive."
Deep Attribute Networks for Attribute-based Classification,"Obtaining compact and discriminative features is one of the major challenges in many of the real-world image classification tasks such as face verification and object recognition. One possible approach is to represent input image on the basis of high-level features that carry semantic meaning that humans can understand. In this paper, a model coined deep attribute network (DAN) is proposed to address this issue. For an input image, the model outputs the attributes of the input image without performing any classification. The efficacy of the proposed model is evaluated on unconstrained face verification and real-world object recognition tasks using the LFW and the PASCAL VOC datasets. We demonstrate the potential of deep learning for attribute-based classification by showing comparable results with existing state-of-the-art results. Once properly trained, DAN is fast and does away with calculating low-level features which maybe unreliable and computationally expensive."
Deep Attribute Networks for Attribute-based Classification,"Obtaining compact and discriminative features is one of the major challenges in many of the real-world image classification tasks such as face verification and object recognition. One possible approach is to represent input image on the basis of high-level features that carry semantic meaning that humans can understand. In this paper, a model coined deep attribute network (DAN) is proposed to address this issue. For an input image, the model outputs the attributes of the input image without performing any classification. The efficacy of the proposed model is evaluated on unconstrained face verification and real-world object recognition tasks using the LFW and the PASCAL VOC datasets. We demonstrate the potential of deep learning for attribute-based classification by showing comparable results with existing state-of-the-art results. Once properly trained, DAN is fast and does away with calculating low-level features which maybe unreliable and computationally expensive."
Deep Attribute Networks for Attribute-based Classification,"Obtaining compact and discriminative features is one of the major challenges in many of the real-world image classification tasks such as face verification and object recognition. One possible approach is to represent input image on the basis of high-level features that carry semantic meaning that humans can understand. In this paper, a model coined deep attribute network (DAN) is proposed to address this issue. For an input image, the model outputs the attributes of the input image without performing any classification. The efficacy of the proposed model is evaluated on unconstrained face verification and real-world object recognition tasks using the LFW and the PASCAL VOC datasets. We demonstrate the potential of deep learning for attribute-based classification by showing comparable results with existing state-of-the-art results. Once properly trained, DAN is fast and does away with calculating low-level features which maybe unreliable and computationally expensive."
Learn to rank Based on Topic Relation Models,"Most of learning to rank algorithms use word based features to train ranking models. Topic features are seldom considered. In this paper we aim at building a topic based ranking model which uses the relational topic model (RTM) to learn ranking function. The original RTM is a one-class model. To make RTM learn from data with different ranking labels, we extend RTM to two-class model and regression model. We employ variational inference algorithm to compute the posterior distribution of the latent variables and use the Expectation Maximization algorithm to estimate the topic vectors and parameters of ranking function. Our experiments on OHSUMED data set show that the proposed models have better accuracy than word based ranking model(SVM) and the original RTM model."
Learn to rank Based on Topic Relation Models,"Most of learning to rank algorithms use word based features to train ranking models. Topic features are seldom considered. In this paper we aim at building a topic based ranking model which uses the relational topic model (RTM) to learn ranking function. The original RTM is a one-class model. To make RTM learn from data with different ranking labels, we extend RTM to two-class model and regression model. We employ variational inference algorithm to compute the posterior distribution of the latent variables and use the Expectation Maximization algorithm to estimate the topic vectors and parameters of ranking function. Our experiments on OHSUMED data set show that the proposed models have better accuracy than word based ranking model(SVM) and the original RTM model."
Learn to rank Based on Topic Relation Models,"Most of learning to rank algorithms use word based features to train ranking models. Topic features are seldom considered. In this paper we aim at building a topic based ranking model which uses the relational topic model (RTM) to learn ranking function. The original RTM is a one-class model. To make RTM learn from data with different ranking labels, we extend RTM to two-class model and regression model. We employ variational inference algorithm to compute the posterior distribution of the latent variables and use the Expectation Maximization algorithm to estimate the topic vectors and parameters of ranking function. Our experiments on OHSUMED data set show that the proposed models have better accuracy than word based ranking model(SVM) and the original RTM model."
Learn to rank Based on Topic Relation Models,"Most of learning to rank algorithms use word based features to train ranking models. Topic features are seldom considered. In this paper we aim at building a topic based ranking model which uses the relational topic model (RTM) to learn ranking function. The original RTM is a one-class model. To make RTM learn from data with different ranking labels, we extend RTM to two-class model and regression model. We employ variational inference algorithm to compute the posterior distribution of the latent variables and use the Expectation Maximization algorithm to estimate the topic vectors and parameters of ranking function. Our experiments on OHSUMED data set show that the proposed models have better accuracy than word based ranking model(SVM) and the original RTM model."
Maximize Short-term Memory in Direct Model of Echo State Networks,"Echo state networks (ESN) are a kind of novel recurrent neural network (RNN) which has a large number of randomly connected neurons (called ?reservoir?) and an adaptable output. The short-term memory (STM) of ESN is the ability of storing information about recent inputs in the reservoir's transient response. It is indispensable for the time varying information processing. Previous work suggested that for i.i.d. input, the upper bound of memory capacity (MC) is N, where N is the number of neurons in the reservoir. In this paper, we show that this is not always the case. We transform the iterative mathematical model of ESN to direct one. In this model, we establish a direct relationship between memory capacity of ESN and its connectivity. We find that some reservoir topologies proposed by previous papers are the special solutions of our method. Furthermore, our experimental results show that the maximum MC in ESN can exceed the upper bound N even with i.i.d. input."
Maximize Short-term Memory in Direct Model of Echo State Networks,"Echo state networks (ESN) are a kind of novel recurrent neural network (RNN) which has a large number of randomly connected neurons (called ?reservoir?) and an adaptable output. The short-term memory (STM) of ESN is the ability of storing information about recent inputs in the reservoir's transient response. It is indispensable for the time varying information processing. Previous work suggested that for i.i.d. input, the upper bound of memory capacity (MC) is N, where N is the number of neurons in the reservoir. In this paper, we show that this is not always the case. We transform the iterative mathematical model of ESN to direct one. In this model, we establish a direct relationship between memory capacity of ESN and its connectivity. We find that some reservoir topologies proposed by previous papers are the special solutions of our method. Furthermore, our experimental results show that the maximum MC in ESN can exceed the upper bound N even with i.i.d. input."
Online L1-Dictionary Learning with Application to Novel Document Detection,"Given their pervasive use, social media, such as Twitter, have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge, we introduce the problem of online L1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the L1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data, shows that this online L1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results. Our algorithm for online L1-dictionary learning could be of independent interest."
Online L1-Dictionary Learning with Application to Novel Document Detection,"Given their pervasive use, social media, such as Twitter, have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge, we introduce the problem of online L1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the L1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data, shows that this online L1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results. Our algorithm for online L1-dictionary learning could be of independent interest."
Learning from Data and Constraints: an Unified Probabilistic View of Clustering,"In this paper we introduce an unified view of clustering, i.e. learning from unlabeled data and constraints. Clustering is considered as a prediction problem for the states of latent variables, i.e. unknown labels. Unlabeled data and constraints are seen as two main sources to provide related information of latent variables. We present a probabilistic clustering model based on Hidden Markov Random Fields (HMRFs), which can embed different related information together to guide theclustering process. We also present a novel constrained clustering method, i.e. a simplified version of HMRF-based clustering model. Unlabeled data and a few pairwise constraints are combined to generate the neighborhood system between latent variables. One of the main limitations of existing constrained clusterings, which is the requirement of a large amount of constraints, can be significantly alleviated. Further, connections between HMRF-based clustering model and manyexisting clusterings are established to demonstrate the inclusiveness and flexibility of the proposed model. Experiments on synthetic and real data are also performed to demonstrate the benefits of the proposed constrained clustering method."
Learning from Data and Constraints: an Unified Probabilistic View of Clustering,"In this paper we introduce an unified view of clustering, i.e. learning from unlabeled data and constraints. Clustering is considered as a prediction problem for the states of latent variables, i.e. unknown labels. Unlabeled data and constraints are seen as two main sources to provide related information of latent variables. We present a probabilistic clustering model based on Hidden Markov Random Fields (HMRFs), which can embed different related information together to guide theclustering process. We also present a novel constrained clustering method, i.e. a simplified version of HMRF-based clustering model. Unlabeled data and a few pairwise constraints are combined to generate the neighborhood system between latent variables. One of the main limitations of existing constrained clusterings, which is the requirement of a large amount of constraints, can be significantly alleviated. Further, connections between HMRF-based clustering model and manyexisting clusterings are established to demonstrate the inclusiveness and flexibility of the proposed model. Experiments on synthetic and real data are also performed to demonstrate the benefits of the proposed constrained clustering method."
A systematic approach to extracting semantic information from functional MRI data,"This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure."
Why MCA? Nonlinear Spike-and-slab Sparse Coding for Neurally Plausible Image Encoding,"Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level components, e.g. edges. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA). The major challenge is parameter optimization because a model with either (1) or (2) results in a strongly multimodal posterior. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posterior. We design an exact piecewise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric, bimodal, and sparse activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, due to the nonlinearity, the model predicts a large number of globular receptive fields (RFs), another significant difference from standard SC. The inferred prior and the high proportion of predicted globular fields make the model more consistent with neural data than previous SC models, suggesting closer tuning of simple cells to visual stimuli than has been predicted until now. "
Why MCA? Nonlinear Spike-and-slab Sparse Coding for Neurally Plausible Image Encoding,"Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level components, e.g. edges. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA). The major challenge is parameter optimization because a model with either (1) or (2) results in a strongly multimodal posterior. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posterior. We design an exact piecewise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric, bimodal, and sparse activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, due to the nonlinearity, the model predicts a large number of globular receptive fields (RFs), another significant difference from standard SC. The inferred prior and the high proportion of predicted globular fields make the model more consistent with neural data than previous SC models, suggesting closer tuning of simple cells to visual stimuli than has been predicted until now. "
Why MCA? Nonlinear Spike-and-slab Sparse Coding for Neurally Plausible Image Encoding,"Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level components, e.g. edges. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA). The major challenge is parameter optimization because a model with either (1) or (2) results in a strongly multimodal posterior. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posterior. We design an exact piecewise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric, bimodal, and sparse activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, due to the nonlinearity, the model predicts a large number of globular receptive fields (RFs), another significant difference from standard SC. The inferred prior and the high proportion of predicted globular fields make the model more consistent with neural data than previous SC models, suggesting closer tuning of simple cells to visual stimuli than has been predicted until now. "
Why MCA? Nonlinear Spike-and-slab Sparse Coding for Neurally Plausible Image Encoding,"Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level components, e.g. edges. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA). The major challenge is parameter optimization because a model with either (1) or (2) results in a strongly multimodal posterior. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posterior. We design an exact piecewise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric, bimodal, and sparse activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, due to the nonlinearity, the model predicts a large number of globular receptive fields (RFs), another significant difference from standard SC. The inferred prior and the high proportion of predicted globular fields make the model more consistent with neural data than previous SC models, suggesting closer tuning of simple cells to visual stimuli than has been predicted until now. "
Why MCA? Nonlinear Spike-and-slab Sparse Coding for Neurally Plausible Image Encoding,"Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level components, e.g. edges. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA). The major challenge is parameter optimization because a model with either (1) or (2) results in a strongly multimodal posterior. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posterior. We design an exact piecewise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric, bimodal, and sparse activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, due to the nonlinearity, the model predicts a large number of globular receptive fields (RFs), another significant difference from standard SC. The inferred prior and the high proportion of predicted globular fields make the model more consistent with neural data than previous SC models, suggesting closer tuning of simple cells to visual stimuli than has been predicted until now. "
Learning optimal spike-based representations,"How do neural networks learn to represent information? Here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder. We define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss. The dynamical equations yield a network of integrate-and-fire neurons undergoing Hebbian plasticity. We show that, through learning, initially regular and highly correlated spike trains evolve towards Poisson-distributed and independent spike trains with much lower firing rates. The learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder. We show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs. In turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme. Our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code."
Learning optimal spike-based representations,"How do neural networks learn to represent information? Here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder. We define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss. The dynamical equations yield a network of integrate-and-fire neurons undergoing Hebbian plasticity. We show that, through learning, initially regular and highly correlated spike trains evolve towards Poisson-distributed and independent spike trains with much lower firing rates. The learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder. We show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs. In turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme. Our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code."
Learning optimal spike-based representations,"How do neural networks learn to represent information? Here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder. We define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss. The dynamical equations yield a network of integrate-and-fire neurons undergoing Hebbian plasticity. We show that, through learning, initially regular and highly correlated spike trains evolve towards Poisson-distributed and independent spike trains with much lower firing rates. The learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder. We show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs. In turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme. Our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code."
Learning optimal spike-based representations,"How do neural networks learn to represent information? Here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder. We define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss. The dynamical equations yield a network of integrate-and-fire neurons undergoing Hebbian plasticity. We show that, through learning, initially regular and highly correlated spike trains evolve towards Poisson-distributed and independent spike trains with much lower firing rates. The learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder. We show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs. In turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme. Our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code."
Collaborative Ranking With 17 Parameters,"The primary application of collaborate filtering (CF) is to recommend a small set of items to a user, which entails ranking. Most approaches, however, formulate the CF problem as rating prediction, overlooking the ranking perspective. In this work we present a method for collaborative ranking that leverages the strengths of the two main CF approaches, neighborhood- and model-based. Our novel method is highly efficient, with only seventeen parameters to optimize and a single hyperparameter to tune, and beats the state-of-the-art collaborative ranking methods. We also show that parameters learned on one dataset yield excellent results on a very different dataset, without any retraining."
Collaborative Ranking With 17 Parameters,"The primary application of collaborate filtering (CF) is to recommend a small set of items to a user, which entails ranking. Most approaches, however, formulate the CF problem as rating prediction, overlooking the ranking perspective. In this work we present a method for collaborative ranking that leverages the strengths of the two main CF approaches, neighborhood- and model-based. Our novel method is highly efficient, with only seventeen parameters to optimize and a single hyperparameter to tune, and beats the state-of-the-art collaborative ranking methods. We also show that parameters learned on one dataset yield excellent results on a very different dataset, without any retraining."
Rational inference of relative preferences,"Statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions. However, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented. In this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects. Complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function. "
Multiresolution Value Function Approximation in Reinforcement Learning using the Wavelet Basis,"We present the wavelet basis, a linear value function approximation scheme that enables multiresolution value function approximation in continuous state spaces. We apply the wavelet basis to two standard reinforcement learning domains, and show that it performs as well as or better than existing commonly used basis functions when used as a fixed basis.We also briefly demonstrate how it can be used to add representational power to better represent spatially local detail."
Multiresolution Value Function Approximation in Reinforcement Learning using the Wavelet Basis,"We present the wavelet basis, a linear value function approximation scheme that enables multiresolution value function approximation in continuous state spaces. We apply the wavelet basis to two standard reinforcement learning domains, and show that it performs as well as or better than existing commonly used basis functions when used as a fixed basis.We also briefly demonstrate how it can be used to add representational power to better represent spatially local detail."
Sparse Reward Processes,"  We introduce a learning problem where the agent is presented with a series of tasks. If they are related, information gained during execution of one has value for future tasks. Thus, the agent must be ``curious'' and explore its environment beyond the degree necessary to solve the current task.  We develop a decision theoretic setting capturing this intuition, the sparse reward process:  This is a multi-stage stochastic game between a learning agent and an opponent. The agent acts in an unknown environment, according to a utility that is arbitrarily selected by the opponent at each stage. We link our setting to other problems, examine its properties and examine the behaviour of two learning algorithms, for different opponent types."
Minimum Distortion Sketches for Learning,"The number of unique features can be prohibitively large in various classification problems, especially in document analysis. For such problems, storing all coefficients of a classifier model would require massive amount of memory on a single machine. Therefore, in this work, we propose a new sketching technique to approximately store the coefficients of a classifier model in constant memory budget. The proposed technique cuts down the space requirement by a factor of sketch depth (typically 3-7 in practice) in the best case, while matching the best known bounds of the existing sketching technique in the worst case. We also demonstrate the performance improvement on various large-scale classification problems."
Minimum Distortion Sketches for Learning,"The number of unique features can be prohibitively large in various classification problems, especially in document analysis. For such problems, storing all coefficients of a classifier model would require massive amount of memory on a single machine. Therefore, in this work, we propose a new sketching technique to approximately store the coefficients of a classifier model in constant memory budget. The proposed technique cuts down the space requirement by a factor of sketch depth (typically 3-7 in practice) in the best case, while matching the best known bounds of the existing sketching technique in the worst case. We also demonstrate the performance improvement on various large-scale classification problems."
Minimum Distortion Sketches for Learning,"The number of unique features can be prohibitively large in various classification problems, especially in document analysis. For such problems, storing all coefficients of a classifier model would require massive amount of memory on a single machine. Therefore, in this work, we propose a new sketching technique to approximately store the coefficients of a classifier model in constant memory budget. The proposed technique cuts down the space requirement by a factor of sketch depth (typically 3-7 in practice) in the best case, while matching the best known bounds of the existing sketching technique in the worst case. We also demonstrate the performance improvement on various large-scale classification problems."
Minimum Distortion Sketches for Learning,"The number of unique features can be prohibitively large in various classification problems, especially in document analysis. For such problems, storing all coefficients of a classifier model would require massive amount of memory on a single machine. Therefore, in this work, we propose a new sketching technique to approximately store the coefficients of a classifier model in constant memory budget. The proposed technique cuts down the space requirement by a factor of sketch depth (typically 3-7 in practice) in the best case, while matching the best known bounds of the existing sketching technique in the worst case. We also demonstrate the performance improvement on various large-scale classification problems."
Extrinsic Support Vector Machines for Riemannian Manifolds,"In computer vision and pattern recognition applications, the features often lie on Riemannian manifolds. In such cases, one needs good classification techniques that make use of the underlying manifold structure. Due to its superior generalization properties, in this paper, we focus on developing support vector machine (SVM) classifier for such features. For Riemannian manifolds, the popular approach for classification is to project the data onto the tangent space and learn classifiers in this space. However, the tangent space need not be optimal for classification. Furthermore, it does not even preserve the global structure of the manifold. Hence, we learn a feature space suitable for support vector classification, by minimizing the structural risk of SVM, and using the manifold structure as a regularizer. We formulate this problem of learning the optimal space as a kernel learning problem, which results in an instance of semidefinite programming, that can be solved using standard semidefinite programming solvers. We also discuss a computationally more efficient solution using the multiple kernel learning framework. Experimental evaluation on synthetic and real data sets clearly demonstrate the effectiveness of the proposed approach."
Extrinsic Support Vector Machines for Riemannian Manifolds,"In computer vision and pattern recognition applications, the features often lie on Riemannian manifolds. In such cases, one needs good classification techniques that make use of the underlying manifold structure. Due to its superior generalization properties, in this paper, we focus on developing support vector machine (SVM) classifier for such features. For Riemannian manifolds, the popular approach for classification is to project the data onto the tangent space and learn classifiers in this space. However, the tangent space need not be optimal for classification. Furthermore, it does not even preserve the global structure of the manifold. Hence, we learn a feature space suitable for support vector classification, by minimizing the structural risk of SVM, and using the manifold structure as a regularizer. We formulate this problem of learning the optimal space as a kernel learning problem, which results in an instance of semidefinite programming, that can be solved using standard semidefinite programming solvers. We also discuss a computationally more efficient solution using the multiple kernel learning framework. Experimental evaluation on synthetic and real data sets clearly demonstrate the effectiveness of the proposed approach."
Extrinsic Support Vector Machines for Riemannian Manifolds,"In computer vision and pattern recognition applications, the features often lie on Riemannian manifolds. In such cases, one needs good classification techniques that make use of the underlying manifold structure. Due to its superior generalization properties, in this paper, we focus on developing support vector machine (SVM) classifier for such features. For Riemannian manifolds, the popular approach for classification is to project the data onto the tangent space and learn classifiers in this space. However, the tangent space need not be optimal for classification. Furthermore, it does not even preserve the global structure of the manifold. Hence, we learn a feature space suitable for support vector classification, by minimizing the structural risk of SVM, and using the manifold structure as a regularizer. We formulate this problem of learning the optimal space as a kernel learning problem, which results in an instance of semidefinite programming, that can be solved using standard semidefinite programming solvers. We also discuss a computationally more efficient solution using the multiple kernel learning framework. Experimental evaluation on synthetic and real data sets clearly demonstrate the effectiveness of the proposed approach."
Recognizing Human Activities from Incompletely Observed Videos,"In this paper, we present a novel method for handling the problem of recognizing human activities from incompletely observed videos. Compared with the similar problem of human activity prediction from unfinished activities [12], in an incompletely observed video an un-observed subsequence of frames may occur any time with any duration and yield a temporal gap in the video. In practice, incompletely observed videos may occur when the video signal drops off, when camera or objects of interest are occluded, or when videos are composited from multiple sources. In this paper, we formulate the problem of human activity recognition from incompletely observed videos in a probabilistic framework. In this framework, we take a set of training video samples (completely observed) of each activity class as the basis, and then use sparse coding to derive the likelihood that an incompletely observed test video belongs to a certain activity class. Furthermore, we propose to divide each activity into multiple temporal stages, apply sparse coding to derive the activity likelihood at each stage, and finally combine the likelihoods at each stage to achieve a global posterior for the activity. We evaluate the proposed method on both the widely used UT-Interaction human activity dataset and a new human activity dataset selected from the Year-1 corpus of the DARPA Mind's Eye program [4]. For the new DARPA dataset, both the activities and the videos show very large within-class temporal, spatial, and background variation. Our results demonstrate that the proposed method performs substantially better than several competing methods on both datasets."
Online computation of sparse representations of time varying stimuli using a biologically motivated neural network,"Natural stimuli are highly redundant, possessing significant spatial and temporal correlations. While sparse coding has been proposed as an efficient strategy employed by neural systems to encode sensory stimuli, the underlying mechanisms are still not well understood. Most previous approaches model the neural dynamics by the sparse representation dictionary itself and compute the representation coefficients offline. In reality, faced with the challenge of constantly changing stimuli, neurons must compute the sparse representations dynamically in an online fashion. Here, we describe a leaky linearized Bregman iteration (LLBI) algorithm which computes the time varying sparse representations using a biologically motivated network of leaky rectifying neurons. Compared to previous attempt of dynamic sparse coding, LLBI exploits the temporal correlation of stimuli and demonstrate better performance both in representation error and the smoothness of temporal evolution of sparse coefficients."
Online computation of sparse representations of time varying stimuli using a biologically motivated neural network,"Natural stimuli are highly redundant, possessing significant spatial and temporal correlations. While sparse coding has been proposed as an efficient strategy employed by neural systems to encode sensory stimuli, the underlying mechanisms are still not well understood. Most previous approaches model the neural dynamics by the sparse representation dictionary itself and compute the representation coefficients offline. In reality, faced with the challenge of constantly changing stimuli, neurons must compute the sparse representations dynamically in an online fashion. Here, we describe a leaky linearized Bregman iteration (LLBI) algorithm which computes the time varying sparse representations using a biologically motivated network of leaky rectifying neurons. Compared to previous attempt of dynamic sparse coding, LLBI exploits the temporal correlation of stimuli and demonstrate better performance both in representation error and the smoothness of temporal evolution of sparse coefficients."
The topographic unsupervised learning of natural sounds in the auditory cortex,"The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efficient coding strategy and that the disorder in A1 reflects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reflected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner."
Safe Policy Iteration,"This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on a simple test domain."
Safe Policy Iteration,"This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on a simple test domain."
Safe Policy Iteration,"This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on a simple test domain."
Safe Policy Iteration,"This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on a simple test domain."
Fast Manifold Learning with Unsupervised Nearest Neighbors,"In this paper we introduce a simple and extremely fast dimensionality reduction method for point-wise embedding of patterns in continuous latent spaces. The approach is an iterative method, which fits nearest neighbors into the framework of unsupervised regression. We introduce unsupervised nearest neighbors for continuous latent spaces. Latent points are iteratively embedded with a stochastic approach: distances in data space are employed as standard deviation for Gaussian sampling in latent space, neighborhood relations are preserved with a nearest neighbor regression-based data space reconstruction error. We extend the approach to handle missing data, and analyze the employment of kernel functions for computation of the data space reconstruction error. Experimental studies show that kernel unsupervised nearest neighbors is an an efficient method for embedding high-dimensional patterns on artificial test data, and real-world data from astronomy."
A Simple and Practical Algorithm for Differentially Private Data Release,"We present a new algorithm for differentially private data release, based on a simple combination of the Exponential Mechanism with the Multiplicative Weights update rule.  Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques."
A Simple and Practical Algorithm for Differentially Private Data Release,"We present a new algorithm for differentially private data release, based on a simple combination of the Exponential Mechanism with the Multiplicative Weights update rule.  Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques."
Weighted Likelihood Policy Search with Model Selection,"Reinforcement learning (RL) methods based on direct policy search (DPS)have been actively discussed to achieve an efficient approach to complicatedMarkov decision processes (MDPs).Although they have brought much progress in practical applications ofRL, there still remains an unsolved problem in DPS related to model selection for the policy.In this paper, we propose a novel DPS method,{\it  weighted likelihood policy search (WLPS)}, where a policy isefficiently learned through the weighted likelihood estimation.WLPS naturally connects DPS to the statistical inference problemand thus various sophisticated techniquesin statistics can be applied to DPS problems directly.Hence, by following the idea of the {\it information criterion},we develop a new measurement for model comparison in DPSbased on the weighted log-likelihood."
Weighted Likelihood Policy Search with Model Selection,"Reinforcement learning (RL) methods based on direct policy search (DPS)have been actively discussed to achieve an efficient approach to complicatedMarkov decision processes (MDPs).Although they have brought much progress in practical applications ofRL, there still remains an unsolved problem in DPS related to model selection for the policy.In this paper, we propose a novel DPS method,{\it  weighted likelihood policy search (WLPS)}, where a policy isefficiently learned through the weighted likelihood estimation.WLPS naturally connects DPS to the statistical inference problemand thus various sophisticated techniquesin statistics can be applied to DPS problems directly.Hence, by following the idea of the {\it information criterion},we develop a new measurement for model comparison in DPSbased on the weighted log-likelihood."
Weighted Likelihood Policy Search with Model Selection,"Reinforcement learning (RL) methods based on direct policy search (DPS)have been actively discussed to achieve an efficient approach to complicatedMarkov decision processes (MDPs).Although they have brought much progress in practical applications ofRL, there still remains an unsolved problem in DPS related to model selection for the policy.In this paper, we propose a novel DPS method,{\it  weighted likelihood policy search (WLPS)}, where a policy isefficiently learned through the weighted likelihood estimation.WLPS naturally connects DPS to the statistical inference problemand thus various sophisticated techniquesin statistics can be applied to DPS problems directly.Hence, by following the idea of the {\it information criterion},we develop a new measurement for model comparison in DPSbased on the weighted log-likelihood."
Weighted Likelihood Policy Search with Model Selection,"Reinforcement learning (RL) methods based on direct policy search (DPS)have been actively discussed to achieve an efficient approach to complicatedMarkov decision processes (MDPs).Although they have brought much progress in practical applications ofRL, there still remains an unsolved problem in DPS related to model selection for the policy.In this paper, we propose a novel DPS method,{\it  weighted likelihood policy search (WLPS)}, where a policy isefficiently learned through the weighted likelihood estimation.WLPS naturally connects DPS to the statistical inference problemand thus various sophisticated techniquesin statistics can be applied to DPS problems directly.Hence, by following the idea of the {\it information criterion},we develop a new measurement for model comparison in DPSbased on the weighted log-likelihood."
Implementing Attention Focus in Model-based Reinforcement Learning,"We propose a new framework for learning the world dynamics of model-based reinforcement learning (RL) in high-dimensional, feature-rich environments. We model the world dynamics through predicting changes with the action effects, and differentiating the model features involved.  We present a factored transition function representation that supports efficient learning of the relevant features. We also introduce an online sparse coding learning technique to implement attention focus in learning the transition models. We provide theoretical analyses and empirical evaluation of our new RL algorithm, loreRL, in two benchmark domains."
Implementing Attention Focus in Model-based Reinforcement Learning,"We propose a new framework for learning the world dynamics of model-based reinforcement learning (RL) in high-dimensional, feature-rich environments. We model the world dynamics through predicting changes with the action effects, and differentiating the model features involved.  We present a factored transition function representation that supports efficient learning of the relevant features. We also introduce an online sparse coding learning technique to implement attention focus in learning the transition models. We provide theoretical analyses and empirical evaluation of our new RL algorithm, loreRL, in two benchmark domains."
Implementing Attention Focus in Model-based Reinforcement Learning,"We propose a new framework for learning the world dynamics of model-based reinforcement learning (RL) in high-dimensional, feature-rich environments. We model the world dynamics through predicting changes with the action effects, and differentiating the model features involved.  We present a factored transition function representation that supports efficient learning of the relevant features. We also introduce an online sparse coding learning technique to implement attention focus in learning the transition models. We provide theoretical analyses and empirical evaluation of our new RL algorithm, loreRL, in two benchmark domains."
Implementing Attention Focus in Model-based Reinforcement Learning,"We propose a new framework for learning the world dynamics of model-based reinforcement learning (RL) in high-dimensional, feature-rich environments. We model the world dynamics through predicting changes with the action effects, and differentiating the model features involved.  We present a factored transition function representation that supports efficient learning of the relevant features. We also introduce an online sparse coding learning technique to implement attention focus in learning the transition models. We provide theoretical analyses and empirical evaluation of our new RL algorithm, loreRL, in two benchmark domains."
Learning the Dependency Structure of Latent Factors,"In this paper, we study latent factor models with the dependency structure in the latent space.  We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance."
Learning the Dependency Structure of Latent Factors,"In this paper, we study latent factor models with the dependency structure in the latent space.  We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance."
"Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders","We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ is chosen uniformly at random from $\{+1, -1\}^n$, $\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers $A$ and $\Sigma$ up to an additive $\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \epsilon$. To accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search."
Cellular Neural Networks: A Scalable Architecture for Learning MIMO Systems,"Cellular neural networks (CNNs) are a class of sparsely connected dynamic recurrent networks (DRNs). Neural networks for implementing large complex interconnected systems consist of multiple inputs and multiple outputs. Many outputs lead to more number of parameters to be adapted. Each additional variable increases the dimensionality of the problem and hence learning becomes a challenge. By proper selection of a set of input elements that affect a particular output variable in a given application, a DRN can be modified into a CNN which significantly reduces the complexity of the neural network and allows use of simple training methods such as backpropagation for independent learning in each cell thus making it scalable. The paper demonstrates this concept of developing CNN using dimensionality reduction in a DRN for scalability and better performance. The concept has been empirically verified through applications."
Cellular Neural Networks: A Scalable Architecture for Learning MIMO Systems,"Cellular neural networks (CNNs) are a class of sparsely connected dynamic recurrent networks (DRNs). Neural networks for implementing large complex interconnected systems consist of multiple inputs and multiple outputs. Many outputs lead to more number of parameters to be adapted. Each additional variable increases the dimensionality of the problem and hence learning becomes a challenge. By proper selection of a set of input elements that affect a particular output variable in a given application, a DRN can be modified into a CNN which significantly reduces the complexity of the neural network and allows use of simple training methods such as backpropagation for independent learning in each cell thus making it scalable. The paper demonstrates this concept of developing CNN using dimensionality reduction in a DRN for scalability and better performance. The concept has been empirically verified through applications."
Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins,"While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice. However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent. In this work we propose to augment these algorithms with an $\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers."
Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins,"While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice. However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent. In this work we propose to augment these algorithms with an $\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers."
Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins,"While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice. However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent. In this work we propose to augment these algorithms with an $\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers."
Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins,"While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice. However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent. In this work we propose to augment these algorithms with an $\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers."
Approximate Factored Real-time Dynamic Programming,"Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when there is information about the initial state. Traditional RTDP approaches work best on problems with sparse transition matrices where they can often efficiently achieve e-convergence without visiting all states; however, the advantages of traditional RTDP are often lost on problems with dense transition matrices where most states are reachable in one step (which is the case of a variety of control problems with exogenous events), as we demonstrate in this paper. One approach to overcome this caveat is to exploit the regularities in the domain dynamics, reward and value function throughout factored representation and calculations. In this paper, we propose a new  factored RTDP algorithm, called FactRTDP, and its approximate version, called aFactRTDP, which is the first straight forward factored version of enumerative RTDP, i.e., without performing generalized updates. Experiments show that these new algorithms can deal with dense transition matrices and have good online behavior when compared to the best probabilistic planning systems, without engineering optimizations, but by simply exploiting factored backups."
Approximate Factored Real-time Dynamic Programming,"Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when there is information about the initial state. Traditional RTDP approaches work best on problems with sparse transition matrices where they can often efficiently achieve e-convergence without visiting all states; however, the advantages of traditional RTDP are often lost on problems with dense transition matrices where most states are reachable in one step (which is the case of a variety of control problems with exogenous events), as we demonstrate in this paper. One approach to overcome this caveat is to exploit the regularities in the domain dynamics, reward and value function throughout factored representation and calculations. In this paper, we propose a new  factored RTDP algorithm, called FactRTDP, and its approximate version, called aFactRTDP, which is the first straight forward factored version of enumerative RTDP, i.e., without performing generalized updates. Experiments show that these new algorithms can deal with dense transition matrices and have good online behavior when compared to the best probabilistic planning systems, without engineering optimizations, but by simply exploiting factored backups."
Approximate Factored Real-time Dynamic Programming,"Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when there is information about the initial state. Traditional RTDP approaches work best on problems with sparse transition matrices where they can often efficiently achieve e-convergence without visiting all states; however, the advantages of traditional RTDP are often lost on problems with dense transition matrices where most states are reachable in one step (which is the case of a variety of control problems with exogenous events), as we demonstrate in this paper. One approach to overcome this caveat is to exploit the regularities in the domain dynamics, reward and value function throughout factored representation and calculations. In this paper, we propose a new  factored RTDP algorithm, called FactRTDP, and its approximate version, called aFactRTDP, which is the first straight forward factored version of enumerative RTDP, i.e., without performing generalized updates. Experiments show that these new algorithms can deal with dense transition matrices and have good online behavior when compared to the best probabilistic planning systems, without engineering optimizations, but by simply exploiting factored backups."
Approximate Factored Real-time Dynamic Programming,"Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when there is information about the initial state. Traditional RTDP approaches work best on problems with sparse transition matrices where they can often efficiently achieve e-convergence without visiting all states; however, the advantages of traditional RTDP are often lost on problems with dense transition matrices where most states are reachable in one step (which is the case of a variety of control problems with exogenous events), as we demonstrate in this paper. One approach to overcome this caveat is to exploit the regularities in the domain dynamics, reward and value function throughout factored representation and calculations. In this paper, we propose a new  factored RTDP algorithm, called FactRTDP, and its approximate version, called aFactRTDP, which is the first straight forward factored version of enumerative RTDP, i.e., without performing generalized updates. Experiments show that these new algorithms can deal with dense transition matrices and have good online behavior when compared to the best probabilistic planning systems, without engineering optimizations, but by simply exploiting factored backups."
Recurrent Gradient Temporal Difference Networks,"Temporal-difference (TD) networks (Sutton and Tanner, 2004) are a predictive representation of state in which each node is an answer to a question about future observations or questions. Unfortunately, existing algorithms for learning TD networks are known to diverge, even in very simple problems. In this paper we present the first sound learning rule for TD networks. Our approach is to develop a true gradient descent algorithm that takes account of all three roles performed by each node in the network: as state, as an answer, and as a target for other questions. Our algorithm combines gradient temporal-difference learning (Maei et al., 2009) with real-time recurrent learning (Williams and Zipser, 1994). We provide a generalisation of the Bellman equation that corresponds to the semantics of the TD network, and prove that our algorithm converges to a fixed point of this equation. We also demonstrate empirically that our learning algorithm converges to the correct solution in a benchmark problem for which prior learning rules diverge."
Split and Approximate in Monte Carlo,"We advance a general approach to increase the efficiency of Monte Carlo algorithms for estimating multi-dimensional integrals: split the integral into two nested parts; treat the outer integral by sampling while computing the inner integral to within a guaranteed approximation ratio. We give a unifying view of some previously presented specializations of this approach, provide analytical justifications and guidelines for algorithm design, and demonstrate the power of the approach in Bayesian structure discovery in Bayesian networks."
Split and Approximate in Monte Carlo,"We advance a general approach to increase the efficiency of Monte Carlo algorithms for estimating multi-dimensional integrals: split the integral into two nested parts; treat the outer integral by sampling while computing the inner integral to within a guaranteed approximation ratio. We give a unifying view of some previously presented specializations of this approach, provide analytical justifications and guidelines for algorithm design, and demonstrate the power of the approach in Bayesian structure discovery in Bayesian networks."
Decision Tree Algorithm for Data Streams based on Alternate Formulation of Mutual Information,"Learning problems with large and streaming data sets are becoming prevalent now a days. Such large data sets require specialized algorithms that are able to learn in one pass through the data set. Standard Decision Tree algorithms are not suitable for data streams. This paper describes and evaluates IQ Tree, a novel decision tree construction algorithm that can handle data streams. We derived an alternate formulation of Mutual Information that facilitates variable levels of approximation of Information Gain. Based on this formulation, our algorithm goes through the data only once and uses pre-calculation and sampling to achieve fast decision tree construction that closely approximates standard decision tree algorithm. We demonstrate detail error analysis that shows the superiority of our algorithm. Detailed empirical evaluation also shows that IQ Tree is better than state of the art decision tree algorithms that can handle data streams."
Algorithms for finding the source of an outbreak and other epidemic inference problems,"During the course of a disease outbreaka fundamental problem in public health planning is to use surveillance data(e.g., partial subset of people who became infected) to solve variousepidemic inference problems, e.g., what is the probability that a givenindividual was the source of the outbreak, or is infected, or the expectednumber of infections (conditional on the observed infections). We developa systematic approach to formulate these problems in terms of random generationof subgraphs with specific connectivity constraints, by means of an equivalencebetween disease transmission in complex networks and the edge percolationprocess. We then use a Markov chain approach for random sampling of suchsubgraphs. We develop efficient analytical bounds on the mixing time, i.e., thetime needed for the Markov chain to reach close to its stationary distribution.We study the empirical performance of our approach on different graphs, andfind that it is able to determine the node infection probabilities quiteaccurately."
Robust Joint Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix,"We propose a robust framework to jointly perform two critical tasks of high dimensional modeling in synergy: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among the responses while adjusting for their predictors. This framework is relevant to many applications. In computational biology, for instance, it enables the integration of genomic and transcriptomic datasets. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecification. This issue is exacerbated when dealing with high dimensional noisy data. We therefore adopt an alternative approach to minimizing a regularized distance criterion, which is motivated by minimum distance estimators used in nonparametric methods. The proposed method yields an efficient algorithm that alternates between weighted versions of lasso and graphical lasso, where the sample weights intuitively explain the robustness of our method. We demonstrate the value of our framework through extensive simulation and real eQTL data analysis."
Robust Joint Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix,"We propose a robust framework to jointly perform two critical tasks of high dimensional modeling in synergy: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among the responses while adjusting for their predictors. This framework is relevant to many applications. In computational biology, for instance, it enables the integration of genomic and transcriptomic datasets. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecification. This issue is exacerbated when dealing with high dimensional noisy data. We therefore adopt an alternative approach to minimizing a regularized distance criterion, which is motivated by minimum distance estimators used in nonparametric methods. The proposed method yields an efficient algorithm that alternates between weighted versions of lasso and graphical lasso, where the sample weights intuitively explain the robustness of our method. We demonstrate the value of our framework through extensive simulation and real eQTL data analysis."
Robust Joint Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix,"We propose a robust framework to jointly perform two critical tasks of high dimensional modeling in synergy: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among the responses while adjusting for their predictors. This framework is relevant to many applications. In computational biology, for instance, it enables the integration of genomic and transcriptomic datasets. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecification. This issue is exacerbated when dealing with high dimensional noisy data. We therefore adopt an alternative approach to minimizing a regularized distance criterion, which is motivated by minimum distance estimators used in nonparametric methods. The proposed method yields an efficient algorithm that alternates between weighted versions of lasso and graphical lasso, where the sample weights intuitively explain the robustness of our method. We demonstrate the value of our framework through extensive simulation and real eQTL data analysis."
Limits of Adaptation in Crowdsourcing,"Crowdsourcing systems, where numerous tasks are electronically distributed to an unidentified pool of workers through an open call, has emerged as an effective tool for human-powered solving of data intensive tasks such as image classification, video annotation, product categorization, and transcription. Since these low-paid workers can be unreliable, all crowdsourcers need to devise a way to cope with the errors and ensure a certain reliability in their answers. A common solution is to add redundancy by asking each question to multiple workers and combining their answers using some scheme such as majority voting. A fundamental question of interest for such systems is how much redundancy is necessary to achieve a certain accuracy in our answers? In this paper, we investigate the fundamental limit on the minimum number of queries necessary to achieve the target error probability. In particular, we want to identify how much we can gain by switching to an adaptive algorithm from an existing low-complexity and non-adaptive algorithms. To establish  this result, we provid a lower bound on the probability of error achieved by the optimal adaptive algorithm.  Compared to a known upper bound for a practical and non-adaptive algorithm, this shows that there is no significant gain in using adaptive algorithms. In terms of the budget required to achieve the target error probability, the gain of using an adaptive scheme is at most a constant factor. "
No-Regret Algorithms for Unconstrained Online Convex Optimization,"Some of the most compelling applications of online convexoptimization, including online prediction and classification, areunconstrained: the natural feasible set is R^n.  Existing algorithmsfail to achieve sub-linear regret in this setting unless constraintson the comparator point x* are known in advance.  We present analgorithm that, without such prior knowledge, offers near-optimalregret bounds with respect to _any_ choice of x*.  In particular,regret with respect to x* = 0 is _constant_.  We then prove lowerbounds showing that our algorithm's guarantees are optimal in thissetting up to constant factors."
No-Regret Algorithms for Unconstrained Online Convex Optimization,"Some of the most compelling applications of online convexoptimization, including online prediction and classification, areunconstrained: the natural feasible set is R^n.  Existing algorithmsfail to achieve sub-linear regret in this setting unless constraintson the comparator point x* are known in advance.  We present analgorithm that, without such prior knowledge, offers near-optimalregret bounds with respect to _any_ choice of x*.  In particular,regret with respect to x* = 0 is _constant_.  We then prove lowerbounds showing that our algorithm's guarantees are optimal in thissetting up to constant factors."
A Gaussian Approximation of Feature Space for Fast Image Similarity,"We introduce a fast technique for the robust computation of image similarity. It builds on a re-interpretation of the recent exemplar-based SVM approach, where a linear SVM is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. Although it is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of-the-art performance on the PASCAL VOC 2007 detection task despite its simplicity. We re-interpret it by viewing the SVM between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. We show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. We then use a simple Gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. Given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. This allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. We further show that our approach is equivalent to feature-space whitening and has links to image saliency."
A Gaussian Approximation of Feature Space for Fast Image Similarity,"We introduce a fast technique for the robust computation of image similarity. It builds on a re-interpretation of the recent exemplar-based SVM approach, where a linear SVM is trained at a query point and distance is computed as the dot product with the normal to the separating hyperplane. Although it is slow because it requires a new training for each exemplar, the latter approach has shown robustness for image retrieval and object classification, yielding state-of-the-art performance on the PASCAL VOC 2007 detection task despite its simplicity. We re-interpret it by viewing the SVM between a single point and the set of negative examples as the computation of the tangent to the manifold of images at the query. We show that, in a high-dimensional space such as that of image features, all points tend to lie at the periphery and that they are usually separable from the rest of the set. We then use a simple Gaussian approximation to the set of all images in feature space, and fit it by computing the covariance matrix on a large training set. Given the covariance matrix, the computation of the tangent or normal at a point is straightforward and is a simple multiplication by the inverse covariance. This allows us to dramatically speed up image retrieval tasks, going from more than ten minutes to a single second. We further show that our approach is equivalent to feature-space whitening and has links to image saliency."
Distributed Reinforcement Learning for Policy Synchronization in Infinite-Horizon Dec-POMDPs,"In many multi-agent tasks, agents face uncertainty about the environment, the outcomes of their actions, and the behaviors of other agents. Dec-POMDPs offer a powerful modeling framework for sequential, cooperative, multiagent tasks under uncertainty. Solution techniques for infinite-horizon Dec-POMDPs have assumed prior knowledge of the model and have required centralized solvers.  We propose a method for learning infinite-horizon Dec-POMDP solutions in a distributed fashion.  We identify the issue of policy synchronization that distributed learners face and propose incorporating rewards into their learned model representations to both ameliorate this issue and to improve the quality of the agents' learned models. Most importantly, we show that even if rewards are not visible to agents during policy execution, exploiting the information contained in reward signals during learning is still beneficial."
Distributed Reinforcement Learning for Policy Synchronization in Infinite-Horizon Dec-POMDPs,"In many multi-agent tasks, agents face uncertainty about the environment, the outcomes of their actions, and the behaviors of other agents. Dec-POMDPs offer a powerful modeling framework for sequential, cooperative, multiagent tasks under uncertainty. Solution techniques for infinite-horizon Dec-POMDPs have assumed prior knowledge of the model and have required centralized solvers.  We propose a method for learning infinite-horizon Dec-POMDP solutions in a distributed fashion.  We identify the issue of policy synchronization that distributed learners face and propose incorporating rewards into their learned model representations to both ameliorate this issue and to improve the quality of the agents' learned models. Most importantly, we show that even if rewards are not visible to agents during policy execution, exploiting the information contained in reward signals during learning is still beneficial."
Video-based Object Recognition by Sets of Sets,"We address the problem of automatic object recognition in videos, where users move their mobile camera around an unknown object of interest in order to capture more information in a random manner. Using videos that capture variations in an object's appearance due to camera motions (viewpoints and scales), cluttering and lighting conditions, can accumulate evidences and improve object recognition accuracies. Most previous works have taken a single image as input, or tackled a video by a collection i.e. sum of frame-based recognition scores. In this paper, we explore two novel representations and matching methods of videos for object recognition beyond frame-based recognition: 1) Video is first represented as a set of frames, in which each frame itself is a set of detected feature points (SURF in our experiments); 2) Each feature point in the initial frame is tracked in following frames, which forms one trajectory containing a set of similar feature points, therefore a video is a set of trajectories. Each representation forms sets of sets. We combine bag-of-words (for a set of data spatially distributed) and manifolds method (for a set of data with temporal smooth changes) to depict the two set-of-set representations. Also we propose how to match such representations for object recognition. The proposed representation and matching techniques are evaluated on our video data sets, which contain 830 videos of ten objects and four environment variations. The experiments on the challenging data set show that our proposed solution significantly outperforms the traditional frame-based methods."
Video-based Object Recognition by Sets of Sets,"We address the problem of automatic object recognition in videos, where users move their mobile camera around an unknown object of interest in order to capture more information in a random manner. Using videos that capture variations in an object's appearance due to camera motions (viewpoints and scales), cluttering and lighting conditions, can accumulate evidences and improve object recognition accuracies. Most previous works have taken a single image as input, or tackled a video by a collection i.e. sum of frame-based recognition scores. In this paper, we explore two novel representations and matching methods of videos for object recognition beyond frame-based recognition: 1) Video is first represented as a set of frames, in which each frame itself is a set of detected feature points (SURF in our experiments); 2) Each feature point in the initial frame is tracked in following frames, which forms one trajectory containing a set of similar feature points, therefore a video is a set of trajectories. Each representation forms sets of sets. We combine bag-of-words (for a set of data spatially distributed) and manifolds method (for a set of data with temporal smooth changes) to depict the two set-of-set representations. Also we propose how to match such representations for object recognition. The proposed representation and matching techniques are evaluated on our video data sets, which contain 830 videos of ten objects and four environment variations. The experiments on the challenging data set show that our proposed solution significantly outperforms the traditional frame-based methods."
Video-based Object Recognition by Sets of Sets,"We address the problem of automatic object recognition in videos, where users move their mobile camera around an unknown object of interest in order to capture more information in a random manner. Using videos that capture variations in an object's appearance due to camera motions (viewpoints and scales), cluttering and lighting conditions, can accumulate evidences and improve object recognition accuracies. Most previous works have taken a single image as input, or tackled a video by a collection i.e. sum of frame-based recognition scores. In this paper, we explore two novel representations and matching methods of videos for object recognition beyond frame-based recognition: 1) Video is first represented as a set of frames, in which each frame itself is a set of detected feature points (SURF in our experiments); 2) Each feature point in the initial frame is tracked in following frames, which forms one trajectory containing a set of similar feature points, therefore a video is a set of trajectories. Each representation forms sets of sets. We combine bag-of-words (for a set of data spatially distributed) and manifolds method (for a set of data with temporal smooth changes) to depict the two set-of-set representations. Also we propose how to match such representations for object recognition. The proposed representation and matching techniques are evaluated on our video data sets, which contain 830 videos of ten objects and four environment variations. The experiments on the challenging data set show that our proposed solution significantly outperforms the traditional frame-based methods."
Video-based Object Recognition by Sets of Sets,"We address the problem of automatic object recognition in videos, where users move their mobile camera around an unknown object of interest in order to capture more information in a random manner. Using videos that capture variations in an object's appearance due to camera motions (viewpoints and scales), cluttering and lighting conditions, can accumulate evidences and improve object recognition accuracies. Most previous works have taken a single image as input, or tackled a video by a collection i.e. sum of frame-based recognition scores. In this paper, we explore two novel representations and matching methods of videos for object recognition beyond frame-based recognition: 1) Video is first represented as a set of frames, in which each frame itself is a set of detected feature points (SURF in our experiments); 2) Each feature point in the initial frame is tracked in following frames, which forms one trajectory containing a set of similar feature points, therefore a video is a set of trajectories. Each representation forms sets of sets. We combine bag-of-words (for a set of data spatially distributed) and manifolds method (for a set of data with temporal smooth changes) to depict the two set-of-set representations. Also we propose how to match such representations for object recognition. The proposed representation and matching techniques are evaluated on our video data sets, which contain 830 videos of ten objects and four environment variations. The experiments on the challenging data set show that our proposed solution significantly outperforms the traditional frame-based methods."
Bayesian models for Large-scale Hierarchical Classification ,"A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods. "
Bayesian models for Large-scale Hierarchical Classification ,"A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods. "
Rule Extraction from Neural Networks Using an Ensemble of Stacked Decision Trees,"Neural Network is a powerful pattern recognition algorithm capable of learning complex non linear patterns. However, Neural Networks have a well-known drawback of being a ?Black Box? learner that is not comprehensible or transferable thus making it unsuitable for many high risk tasks that require a rational justification for making a decision. Rule Extraction methods can resolve this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. In this paper, we present an algorithm called HERETIC that uses a symbolic learning algorithm (Decision Tree) on each unit of the Neural Network. This stacked ensemble of Decision Trees is shown to be a good approach of approximating a Neural Network. We also present HERETIC+, an extension of the basic algorithm that exploits Neural Network connection weights to attain better accuracy. Experiments and theoretical analysis show HERETIC and HERETIC+ generates highly accurate rules that closely approximates the Neural Network."
Efficient Learning in (Recurrent) Helmholtz Machines,"In this paper, we revisit the Helmholtz machine architecture as a generative model for both stationary data and sequences with long term time dependencies. We pro- pose an efficient method for calculating the gradient of the free energy by back- propagating its first and second derivatives through the model?s Gaussian genera- tive and recognition passes, as such providing an alternative to both the previous wake-sleep algorithm and REINFORCE. While not necessarily competitive with recent techniques from the deep learning community on stationary distributions, the resulting algorithm?s quick unbiased sampling procedure renders the method tractable on complicated sequential video tasks."
Efficient Learning in (Recurrent) Helmholtz Machines,"In this paper, we revisit the Helmholtz machine architecture as a generative model for both stationary data and sequences with long term time dependencies. We pro- pose an efficient method for calculating the gradient of the free energy by back- propagating its first and second derivatives through the model?s Gaussian genera- tive and recognition passes, as such providing an alternative to both the previous wake-sleep algorithm and REINFORCE. While not necessarily competitive with recent techniques from the deep learning community on stationary distributions, the resulting algorithm?s quick unbiased sampling procedure renders the method tractable on complicated sequential video tasks."
Efficient Learning in (Recurrent) Helmholtz Machines,"In this paper, we revisit the Helmholtz machine architecture as a generative model for both stationary data and sequences with long term time dependencies. We pro- pose an efficient method for calculating the gradient of the free energy by back- propagating its first and second derivatives through the model?s Gaussian genera- tive and recognition passes, as such providing an alternative to both the previous wake-sleep algorithm and REINFORCE. While not necessarily competitive with recent techniques from the deep learning community on stationary distributions, the resulting algorithm?s quick unbiased sampling procedure renders the method tractable on complicated sequential video tasks."
Decomposing information,"How can the information contained in a group of random variables be decomposed? Ideally, we would like to understand to what extent different subgroups provide the same, i.e. redundant, information, carry unique information or interact for the emergence of synergistic information.  So far, no convincing solution has been found, that captures our intuitions behind these concepts.Motivated by recent results due to Williams and Beer we discuss natural properties that such an information decomposition should have. We proof that some of these properties contradict each other, and we illustrate further puzzling aspects of shared information. We conclude that intuition and heuristic arguments might not suffice when thinking about information."
Decomposing information,"How can the information contained in a group of random variables be decomposed? Ideally, we would like to understand to what extent different subgroups provide the same, i.e. redundant, information, carry unique information or interact for the emergence of synergistic information.  So far, no convincing solution has been found, that captures our intuitions behind these concepts.Motivated by recent results due to Williams and Beer we discuss natural properties that such an information decomposition should have. We proof that some of these properties contradict each other, and we illustrate further puzzling aspects of shared information. We conclude that intuition and heuristic arguments might not suffice when thinking about information."
Decomposing information,"How can the information contained in a group of random variables be decomposed? Ideally, we would like to understand to what extent different subgroups provide the same, i.e. redundant, information, carry unique information or interact for the emergence of synergistic information.  So far, no convincing solution has been found, that captures our intuitions behind these concepts.Motivated by recent results due to Williams and Beer we discuss natural properties that such an information decomposition should have. We proof that some of these properties contradict each other, and we illustrate further puzzling aspects of shared information. We conclude that intuition and heuristic arguments might not suffice when thinking about information."
Decomposing information,"How can the information contained in a group of random variables be decomposed? Ideally, we would like to understand to what extent different subgroups provide the same, i.e. redundant, information, carry unique information or interact for the emergence of synergistic information.  So far, no convincing solution has been found, that captures our intuitions behind these concepts.Motivated by recent results due to Williams and Beer we discuss natural properties that such an information decomposition should have. We proof that some of these properties contradict each other, and we illustrate further puzzling aspects of shared information. We conclude that intuition and heuristic arguments might not suffice when thinking about information."
Multiple Operator-valued Kernel Learning,"Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces."
Multiple Operator-valued Kernel Learning,"Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces."
Multiple Operator-valued Kernel Learning,"Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces."
Multiple Operator-valued Kernel Learning,"Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces."
An efficient feature allocation for parallel stochastic optimizations with lazy updates,"  This paper proposes an efficient feature allocation algorithm to  accelerate parallelized stochastic optimization algorithms using  lazy updates for large-scale sparse data. Our key observation is  that a feature allocation governs efficiency of parallelized lazy  update algorithms. In fact, in the worst case, the total  computational cost of the parallelized algorithm is the same or even  worse than that of non-parallelized (single-core) algorithms. This  paper formulates the feature allocation problem as a specific form  of variable assignment problem whose optimal feature allocation  minimizes the computational cost for parallelized lazy  updates. Since the assignment problem itself requires large  computational cost, we propose two efficient algorithms using  randomization and a greedy search."
An efficient feature allocation for parallel stochastic optimizations with lazy updates,"  This paper proposes an efficient feature allocation algorithm to  accelerate parallelized stochastic optimization algorithms using  lazy updates for large-scale sparse data. Our key observation is  that a feature allocation governs efficiency of parallelized lazy  update algorithms. In fact, in the worst case, the total  computational cost of the parallelized algorithm is the same or even  worse than that of non-parallelized (single-core) algorithms. This  paper formulates the feature allocation problem as a specific form  of variable assignment problem whose optimal feature allocation  minimizes the computational cost for parallelized lazy  updates. Since the assignment problem itself requires large  computational cost, we propose two efficient algorithms using  randomization and a greedy search."
Explicit Embedding Learning with Kernel and Boosting Frameworks for Image Categorization,"In this article, we propose a method to learn a kernel function following a two-stage schema: one for kernel learning, and one for image categorization. We adopt a Boosting framework to design and combine weak kernel functions targeting an ideal kernel. The weak kernel selection criterion adapted to kernel combination and the weight of this combination are computed thanks to an analytic solution. We show that our method actually builds mapping functions which turn the initial input space to a new feature space where categories are better classified. We propose to learn a single kernel/mapping for all categories."
Explicit Embedding Learning with Kernel and Boosting Frameworks for Image Categorization,"In this article, we propose a method to learn a kernel function following a two-stage schema: one for kernel learning, and one for image categorization. We adopt a Boosting framework to design and combine weak kernel functions targeting an ideal kernel. The weak kernel selection criterion adapted to kernel combination and the weight of this combination are computed thanks to an analytic solution. We show that our method actually builds mapping functions which turn the initial input space to a new feature space where categories are better classified. We propose to learn a single kernel/mapping for all categories."
Explicit Embedding Learning with Kernel and Boosting Frameworks for Image Categorization,"In this article, we propose a method to learn a kernel function following a two-stage schema: one for kernel learning, and one for image categorization. We adopt a Boosting framework to design and combine weak kernel functions targeting an ideal kernel. The weak kernel selection criterion adapted to kernel combination and the weight of this combination are computed thanks to an analytic solution. We show that our method actually builds mapping functions which turn the initial input space to a new feature space where categories are better classified. We propose to learn a single kernel/mapping for all categories."
{Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning,"We consider the estimation of an i.i.d.\ vector $\xbf \in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possiblynonlinear) measurement channel. We present a method, calledadaptive generalized approximate message passing(Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\xbf$.The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriorsin the E-steps are computed via approximate message passing.The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yieldasymptotically consistent parameter estimates, which implies that the algorithmachieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic,general and computationally efficient methodapplicable to a large range of complex linear-nonlinearmodels with provable guarantees."
{Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning,"We consider the estimation of an i.i.d.\ vector $\xbf \in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possiblynonlinear) measurement channel. We present a method, calledadaptive generalized approximate message passing(Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\xbf$.The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriorsin the E-steps are computed via approximate message passing.The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yieldasymptotically consistent parameter estimates, which implies that the algorithmachieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic,general and computationally efficient methodapplicable to a large range of complex linear-nonlinearmodels with provable guarantees."
{Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning,"We consider the estimation of an i.i.d.\ vector $\xbf \in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possiblynonlinear) measurement channel. We present a method, calledadaptive generalized approximate message passing(Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\xbf$.The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriorsin the E-steps are computed via approximate message passing.The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yieldasymptotically consistent parameter estimates, which implies that the algorithmachieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic,general and computationally efficient methodapplicable to a large range of complex linear-nonlinearmodels with provable guarantees."
A Better Way to Pre-Train Deep Boltzmann Machines,"We describe how the pre-training algorithm for Deep Boltzmann Machines (DBMs) is related to the pre-training algorithm for Deep Belief Networks and we show that under certain conditions, the pre-training procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pre-training DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pre-training algorithm allows us to learn better generative models."
A Better Way to Pre-Train Deep Boltzmann Machines,"We describe how the pre-training algorithm for Deep Boltzmann Machines (DBMs) is related to the pre-training algorithm for Deep Belief Networks and we show that under certain conditions, the pre-training procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pre-training DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pre-training algorithm allows us to learn better generative models."
EEG single-trial detection in a rapid serial visual presentation paradigm task with supervised spatial filtering,"The detection of single-trial event related potentials (ERPs) in the electroencephalogram (EEG) is a difficult problem that requires efficient signal processing and machine learning techniques due to the poor spatial resolution and low signal-to-noise ratio of the EEG signal. Among the different steps that are typically used for the detection, spatial filtering is an important part. Spatial filtering allows enhancing the relevant information in the signal by combining the signal recorded across the different sensors. We propose a neural network with a convolutional layer dedicated to spatial filtering (CNN) for the detection of ERPs. The method is compared with a method based on the maximization of the signal-to-signal-plus-noise ratio (xDAWN) and common spatial pattern (CSP) that maximizes the discriminative activity to the common activity ratio for the creation of spatial filters. The two latter methods are combined with a neural network (MLP) for the classification. We have compared these methods with an MLP without spatial filtering as pre-processing. These techniques were evaluated on a rapid serial visual presentation (RSVP) task where eight participants had to detect faces from car images. The mean area under the ROC curve (AUC) is 0.843, 0.810, 0.753, and 0.820 for CNN, xDAWN+MLP, CSP+MLP, and an MLP without spatial filtering, respectively."
EEG single-trial detection in a rapid serial visual presentation paradigm task with supervised spatial filtering,"The detection of single-trial event related potentials (ERPs) in the electroencephalogram (EEG) is a difficult problem that requires efficient signal processing and machine learning techniques due to the poor spatial resolution and low signal-to-noise ratio of the EEG signal. Among the different steps that are typically used for the detection, spatial filtering is an important part. Spatial filtering allows enhancing the relevant information in the signal by combining the signal recorded across the different sensors. We propose a neural network with a convolutional layer dedicated to spatial filtering (CNN) for the detection of ERPs. The method is compared with a method based on the maximization of the signal-to-signal-plus-noise ratio (xDAWN) and common spatial pattern (CSP) that maximizes the discriminative activity to the common activity ratio for the creation of spatial filters. The two latter methods are combined with a neural network (MLP) for the classification. We have compared these methods with an MLP without spatial filtering as pre-processing. These techniques were evaluated on a rapid serial visual presentation (RSVP) task where eight participants had to detect faces from car images. The mean area under the ROC curve (AUC) is 0.843, 0.810, 0.753, and 0.820 for CNN, xDAWN+MLP, CSP+MLP, and an MLP without spatial filtering, respectively."
EEG single-trial detection in a rapid serial visual presentation paradigm task with supervised spatial filtering,"The detection of single-trial event related potentials (ERPs) in the electroencephalogram (EEG) is a difficult problem that requires efficient signal processing and machine learning techniques due to the poor spatial resolution and low signal-to-noise ratio of the EEG signal. Among the different steps that are typically used for the detection, spatial filtering is an important part. Spatial filtering allows enhancing the relevant information in the signal by combining the signal recorded across the different sensors. We propose a neural network with a convolutional layer dedicated to spatial filtering (CNN) for the detection of ERPs. The method is compared with a method based on the maximization of the signal-to-signal-plus-noise ratio (xDAWN) and common spatial pattern (CSP) that maximizes the discriminative activity to the common activity ratio for the creation of spatial filters. The two latter methods are combined with a neural network (MLP) for the classification. We have compared these methods with an MLP without spatial filtering as pre-processing. These techniques were evaluated on a rapid serial visual presentation (RSVP) task where eight participants had to detect faces from car images. The mean area under the ROC curve (AUC) is 0.843, 0.810, 0.753, and 0.820 for CNN, xDAWN+MLP, CSP+MLP, and an MLP without spatial filtering, respectively."
Modular Value Iteration Through Regional Decomposition,"To quickly solve large Reinforcement Learning problems involving complex reward functions (e.g., multiple reward sources), we decompose Markov decision processes (MDP) into regions. We introduce a novel modular version of Least Squares Policy Iteration (LSPI), called M-LSPI, which 1. breaks up the MDP states into a set of mutually exclusive regions, 2. leverages the regional decomposition to efficiently solve the MDP --- all values of each region are updated by a single matrix inversion; regional information is then propagated by value iteration. As the number of states increases, on both structured and unstructured MDPs, this yields substantial improvements over other algorithms in terms of time to convergence to the value function of the optimal policy, especially at a higher discount factor."
Modular Value Iteration Through Regional Decomposition,"To quickly solve large Reinforcement Learning problems involving complex reward functions (e.g., multiple reward sources), we decompose Markov decision processes (MDP) into regions. We introduce a novel modular version of Least Squares Policy Iteration (LSPI), called M-LSPI, which 1. breaks up the MDP states into a set of mutually exclusive regions, 2. leverages the regional decomposition to efficiently solve the MDP --- all values of each region are updated by a single matrix inversion; regional information is then propagated by value iteration. As the number of states increases, on both structured and unstructured MDPs, this yields substantial improvements over other algorithms in terms of time to convergence to the value function of the optimal policy, especially at a higher discount factor."
Modular Value Iteration Through Regional Decomposition,"To quickly solve large Reinforcement Learning problems involving complex reward functions (e.g., multiple reward sources), we decompose Markov decision processes (MDP) into regions. We introduce a novel modular version of Least Squares Policy Iteration (LSPI), called M-LSPI, which 1. breaks up the MDP states into a set of mutually exclusive regions, 2. leverages the regional decomposition to efficiently solve the MDP --- all values of each region are updated by a single matrix inversion; regional information is then propagated by value iteration. As the number of states increases, on both structured and unstructured MDPs, this yields substantial improvements over other algorithms in terms of time to convergence to the value function of the optimal policy, especially at a higher discount factor."
Modular Value Iteration Through Regional Decomposition,"To quickly solve large Reinforcement Learning problems involving complex reward functions (e.g., multiple reward sources), we decompose Markov decision processes (MDP) into regions. We introduce a novel modular version of Least Squares Policy Iteration (LSPI), called M-LSPI, which 1. breaks up the MDP states into a set of mutually exclusive regions, 2. leverages the regional decomposition to efficiently solve the MDP --- all values of each region are updated by a single matrix inversion; regional information is then propagated by value iteration. As the number of states increases, on both structured and unstructured MDPs, this yields substantial improvements over other algorithms in terms of time to convergence to the value function of the optimal policy, especially at a higher discount factor."
Towards a learning-theoretic analysis of spike-timing dependent plasticity,"This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli."
Towards a learning-theoretic analysis of spike-timing dependent plasticity,"This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli."
"On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution","We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition."
"On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution","We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition."
"On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution","We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition."
"Sliding global attractors of learning, memory and innovation by synapse and membrane plasticity","Employing conductance-based models of neural firing and synaptic plasticity, it is shown that the noninvertible, sometimes chaotic, firing process associated with neuronal learning is accompanied by the formation of a single global memory attractor in the composite space of synaptic weights, membrane activation and firing rate. The neuronal global attractors and the corresponding firing modes form six types.  Changes in membrane conductance or activation have a sliding effect on the global attractor, changing its parameter values, but not its dynamic nature. The sliding effect is eliminated by membrane conductance memory, yielding non-spurious, globally-stable retrieval. Selective membrane activation and lateral feedback from interacting neurons creates a shunting effect, yielding exponential capacity of innovation, manifested by combinatorial retrieval of neuronal firing patterns, modulated by the sliding effect. "
Taxonomic Prediction with Tree-Structured Covariances,"Taxonomies are natural structures for representing the relationships between concepts, and are useful sources of prior information to learning algorithms.  The use of taxonomies may give a statistical improvement, in that training data present in nearby classes may be leveraged to effectively increase the sample size of all classes.  Taxonomies may improve performance by serving as a modified regularizer: the taxonomic structure may guide selection from the set of possible prediction functions by indicating that risky sets of functions are those that have very different values for nearby classes.In this work, we explore taxonomic prediction in the structured output setting using joint kernel maps following Cai and Hofmann (2004).  In particular, we relate taxonomic structured prediction to two key concepts, (i) tree structured covariance matrices, and (ii) non-parametric dependence measures.  We show that the joint kernel map for taxonomic prediction is tightly coupled to the concept of a tree-structured covariance matrix, and that Tikhonov regularization results in regularization by a special case of the Hilbert-Schmidt Independence Criterion (HSIC).  Using these concepts, we derive a family of highly computationally efficient algorithms for learning with arbitrary covariance matrices over output classes and evaluate its computational and empirical performance in a number of structured prediction settings."
Taxonomic Prediction with Tree-Structured Covariances,"Taxonomies are natural structures for representing the relationships between concepts, and are useful sources of prior information to learning algorithms.  The use of taxonomies may give a statistical improvement, in that training data present in nearby classes may be leveraged to effectively increase the sample size of all classes.  Taxonomies may improve performance by serving as a modified regularizer: the taxonomic structure may guide selection from the set of possible prediction functions by indicating that risky sets of functions are those that have very different values for nearby classes.In this work, we explore taxonomic prediction in the structured output setting using joint kernel maps following Cai and Hofmann (2004).  In particular, we relate taxonomic structured prediction to two key concepts, (i) tree structured covariance matrices, and (ii) non-parametric dependence measures.  We show that the joint kernel map for taxonomic prediction is tightly coupled to the concept of a tree-structured covariance matrix, and that Tikhonov regularization results in regularization by a special case of the Hilbert-Schmidt Independence Criterion (HSIC).  Using these concepts, we derive a family of highly computationally efficient algorithms for learning with arbitrary covariance matrices over output classes and evaluate its computational and empirical performance in a number of structured prediction settings."
Detecting Local Manifold Structure for Unsupervised Feature Selection,"Unsupervised feature selection is fundamental in statistical pattern recognition, and has drawn persistent attention in the past several decades. Recently, much work have shown that feature selection can be formulated as nonlinear dimensionality reduction with discrete constraints. This line of research emphasizes the manifold learning techniques, where the Laplacian eigenmap has been extensively studied. In this paper, we propose a new feature selection perspective from locally linear embedding (LLE), which is another popular manifold learning method. Our algorithm, called locally linear selection (LLS), can select the feature subset which optimally represents the underlying data manifold. We further develop a locally linear rotation-selection (LLRS) algorithm which extends LLS to identify the optimal coordinate subset from a new space. Experimental results on five real-world datasets show that our method can be more effective than Laplacian eigenmap based feature selection methods. "
Fitting community models to large sparse networks,"Many algorithms have been proposed for fitting network models with communities but most of them do not scale well to large networks, and often fail on sparse networks.   Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees.   We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs.  We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood.   "
Learning Manifolds with K-Means and K-Flats,"We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-?ats, and analyze their performance. We extend previous results  for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-?ats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-?ats, both the results and the mathematical tools are  new."
Multivariate discrete kernels and representations for sequence data,"String kernel-based machine learning methods have yielded great success in practical tasks of structured/sequential data analysis. They often exhibit state-of-the-art performance on tasks such as document topic elucidation, music genre classification, protein superfamily and fold prediction.However, typical string kernel methods rely on analysis of discrete 1D (univariate) string data (e.g., amino acid sequences, word sequences, codeword sequences, etc).This work introduces new {\em multivariate (2D)}  representations and {\em multivariate (2D) string kernel} methods for data in the form of sequences of feature vectors(as in music MFCC sequences, biological sequence profiles, or image sequences).On three music classification tasks as well as protein sequence classification proposed multivariate (2D) representations and kernels show significant 25-40\% improvements compared to traditional codebook learning and existing state-of-the-art sequence classification methods."
Convex Collective Matrix Factorization,"In many realistic applications, multiple interlinked sources of data are available and they cannot be easily represented in the form of a single matrix. Collective matrix factorization has recently been introduced to improve generalization performances by jointly factorizing multiple relations or matrices. In this paper, we extend the trace norm for matrix factorization to the collective matrix factorization case. This norm defined on the space of relations is used to regularize the empirical loss, leading to a convex formulation of the problem. Similarly to the trace norm on matrices, we show that the collective-matrix completion problem admits afast iterative singular-value thresholding algorithm.The collective trace norm is also characterized as a decomposition norm, usefulto find an optimal solution thanks to an unconstrained minimization procedure. Empirically we show that stochastic gradient descent suits well for solving theconvex collective factorization even for large scale problems. We also show thatthe proposed algorithm directly solving the convex problem is muchfaster than unconstrained gradient minimization optimizing in the space of low-rankmatrices."
Convex Collective Matrix Factorization,"In many realistic applications, multiple interlinked sources of data are available and they cannot be easily represented in the form of a single matrix. Collective matrix factorization has recently been introduced to improve generalization performances by jointly factorizing multiple relations or matrices. In this paper, we extend the trace norm for matrix factorization to the collective matrix factorization case. This norm defined on the space of relations is used to regularize the empirical loss, leading to a convex formulation of the problem. Similarly to the trace norm on matrices, we show that the collective-matrix completion problem admits afast iterative singular-value thresholding algorithm.The collective trace norm is also characterized as a decomposition norm, usefulto find an optimal solution thanks to an unconstrained minimization procedure. Empirically we show that stochastic gradient descent suits well for solving theconvex collective factorization even for large scale problems. We also show thatthe proposed algorithm directly solving the convex problem is muchfaster than unconstrained gradient minimization optimizing in the space of low-rankmatrices."
KernelUCB for Contextual Bandits,We tackle the problem of online reward maximization over a large but finite set of actions that are described by contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume  that we have cheap access to the similarities between actions' contexts and that the expected reward is a linear function of the contexts' images in the related reproducing kernel Hilbert space. We propose a kernelised UCB algorithm based on the contextual linear bandit algorithm LinUCB and give a cumulative regret bound. We present experiments showing the benefit of a kernelised approach to contextual bandits by comparing KernelUCB with LinUCB on both synthetic and real-world data. 
KernelUCB for Contextual Bandits,We tackle the problem of online reward maximization over a large but finite set of actions that are described by contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume  that we have cheap access to the similarities between actions' contexts and that the expected reward is a linear function of the contexts' images in the related reproducing kernel Hilbert space. We propose a kernelised UCB algorithm based on the contextual linear bandit algorithm LinUCB and give a cumulative regret bound. We present experiments showing the benefit of a kernelised approach to contextual bandits by comparing KernelUCB with LinUCB on both synthetic and real-world data. 
KernelUCB for Contextual Bandits,We tackle the problem of online reward maximization over a large but finite set of actions that are described by contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume  that we have cheap access to the similarities between actions' contexts and that the expected reward is a linear function of the contexts' images in the related reproducing kernel Hilbert space. We propose a kernelised UCB algorithm based on the contextual linear bandit algorithm LinUCB and give a cumulative regret bound. We present experiments showing the benefit of a kernelised approach to contextual bandits by comparing KernelUCB with LinUCB on both synthetic and real-world data. 
KernelUCB for Contextual Bandits,We tackle the problem of online reward maximization over a large but finite set of actions that are described by contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume  that we have cheap access to the similarities between actions' contexts and that the expected reward is a linear function of the contexts' images in the related reproducing kernel Hilbert space. We propose a kernelised UCB algorithm based on the contextual linear bandit algorithm LinUCB and give a cumulative regret bound. We present experiments showing the benefit of a kernelised approach to contextual bandits by comparing KernelUCB with LinUCB on both synthetic and real-world data. 
KernelUCB for Contextual Bandits,We tackle the problem of online reward maximization over a large but finite set of actions that are described by contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume  that we have cheap access to the similarities between actions' contexts and that the expected reward is a linear function of the contexts' images in the related reproducing kernel Hilbert space. We propose a kernelised UCB algorithm based on the contextual linear bandit algorithm LinUCB and give a cumulative regret bound. We present experiments showing the benefit of a kernelised approach to contextual bandits by comparing KernelUCB with LinUCB on both synthetic and real-world data. 
Non-parametric Bayesian Clustering with Noisy Side Information,"In clustering tasks, the incorporation of side information can usually offer substantial benefits. In many practical applications the side information is extracted following empirical rules and is thus likely contaminated by errors. In this paper we propose a non-parametric Bayesian framework Two-View Clustering (TVClust) to incorporate noisy side information into clustering. We model the data instances and constraints as two independent sets of outcomes, or two views, generated from the latent cluster structure, and try to seek a consensus between the observed data and the noisy side information. Specifically, the data instances are modeled using the Mixture of Dirichlet Process and the side information is modeled as a random graph. For the estimation of model parameters and related posterior inference, we present an efficient Gibbs sampler. Experiments on six real datasets and one social media dataset demonstrate that our method achieves significant improvement over the other methods we compared to."
Non-parametric Bayesian Clustering with Noisy Side Information,"In clustering tasks, the incorporation of side information can usually offer substantial benefits. In many practical applications the side information is extracted following empirical rules and is thus likely contaminated by errors. In this paper we propose a non-parametric Bayesian framework Two-View Clustering (TVClust) to incorporate noisy side information into clustering. We model the data instances and constraints as two independent sets of outcomes, or two views, generated from the latent cluster structure, and try to seek a consensus between the observed data and the noisy side information. Specifically, the data instances are modeled using the Mixture of Dirichlet Process and the side information is modeled as a random graph. For the estimation of model parameters and related posterior inference, we present an efficient Gibbs sampler. Experiments on six real datasets and one social media dataset demonstrate that our method achieves significant improvement over the other methods we compared to."
Non-parametric Bayesian Clustering with Noisy Side Information,"In clustering tasks, the incorporation of side information can usually offer substantial benefits. In many practical applications the side information is extracted following empirical rules and is thus likely contaminated by errors. In this paper we propose a non-parametric Bayesian framework Two-View Clustering (TVClust) to incorporate noisy side information into clustering. We model the data instances and constraints as two independent sets of outcomes, or two views, generated from the latent cluster structure, and try to seek a consensus between the observed data and the noisy side information. Specifically, the data instances are modeled using the Mixture of Dirichlet Process and the side information is modeled as a random graph. For the estimation of model parameters and related posterior inference, we present an efficient Gibbs sampler. Experiments on six real datasets and one social media dataset demonstrate that our method achieves significant improvement over the other methods we compared to."
Non-parametric Bayesian Clustering with Noisy Side Information,"In clustering tasks, the incorporation of side information can usually offer substantial benefits. In many practical applications the side information is extracted following empirical rules and is thus likely contaminated by errors. In this paper we propose a non-parametric Bayesian framework Two-View Clustering (TVClust) to incorporate noisy side information into clustering. We model the data instances and constraints as two independent sets of outcomes, or two views, generated from the latent cluster structure, and try to seek a consensus between the observed data and the noisy side information. Specifically, the data instances are modeled using the Mixture of Dirichlet Process and the side information is modeled as a random graph. For the estimation of model parameters and related posterior inference, we present an efficient Gibbs sampler. Experiments on six real datasets and one social media dataset demonstrate that our method achieves significant improvement over the other methods we compared to."
Iterative ranking from pair-wise comparisons ,"The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR?s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, finding ?scores? for each object (e.g. player?s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efficacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1]."
Learning Probability Measures with respect to  Optimal Transport Metrics,"We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures."
A Flexible Integer Linear Programming Formulation of Hierarchical Clustering,"In this paper we formulate hierarchical clustering as an integer linear programming (ILP) problem.  We present algorithmic results for our objective, showing that  a special case can be solved exactly in polynomial time (using an linear programming relaxation) and provide approximationschemes for the general case.  We show the flexibility of our approach by removing the transitivity constraint typically required of hierarchies, so we can learn hierarchies that contain overlapping clusterings.  Our experiments showed that our formulation is capable of outperforming standard agglomerative clustering algorithms in a variety of settings, including traditional hierarchical clustering as well as learning overlapping clusterings."
A Flexible Integer Linear Programming Formulation of Hierarchical Clustering,"In this paper we formulate hierarchical clustering as an integer linear programming (ILP) problem.  We present algorithmic results for our objective, showing that  a special case can be solved exactly in polynomial time (using an linear programming relaxation) and provide approximationschemes for the general case.  We show the flexibility of our approach by removing the transitivity constraint typically required of hierarchies, so we can learn hierarchies that contain overlapping clusterings.  Our experiments showed that our formulation is capable of outperforming standard agglomerative clustering algorithms in a variety of settings, including traditional hierarchical clustering as well as learning overlapping clusterings."
A Flexible Integer Linear Programming Formulation of Hierarchical Clustering,"In this paper we formulate hierarchical clustering as an integer linear programming (ILP) problem.  We present algorithmic results for our objective, showing that  a special case can be solved exactly in polynomial time (using an linear programming relaxation) and provide approximationschemes for the general case.  We show the flexibility of our approach by removing the transitivity constraint typically required of hierarchies, so we can learn hierarchies that contain overlapping clusterings.  Our experiments showed that our formulation is capable of outperforming standard agglomerative clustering algorithms in a variety of settings, including traditional hierarchical clustering as well as learning overlapping clusterings."
Label Ranking with Partial Abstention based on Thresholded Probabilistic Models,"Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classification, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach."
Label Ranking with Partial Abstention based on Thresholded Probabilistic Models,"Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classification, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach."
Label Ranking with Partial Abstention based on Thresholded Probabilistic Models,"Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classification, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach."
Label Ranking with Partial Abstention based on Thresholded Probabilistic Models,"Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classification, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach."
An Online Learning Algorithm for Multi-valued Function Learning,"In this paper we introduce a learning algorithm based on an infinite mixture of linear experts (IMLE) that is able to properly learn multi-valued functions. It consists of a generative model similar to the one found in Xu et al.~\cite{xu1995ame}, together with a set of priors that improve the algorithm versatility and performance, while providing some regularization of the parameters being learned. It is trained by a generalized Expectation-Maximization algorithm in an online, incremental fashion that can automatically grow the number of active components of the mixture as needed. Contrary to most state-of-the-art function approximation algorithms, IMLE can successfully learn multi-valued functions, and it equals or even outperforms popular online learning algorithms in single-valued prediction tasks."
An Online Learning Algorithm for Multi-valued Function Learning,"In this paper we introduce a learning algorithm based on an infinite mixture of linear experts (IMLE) that is able to properly learn multi-valued functions. It consists of a generative model similar to the one found in Xu et al.~\cite{xu1995ame}, together with a set of priors that improve the algorithm versatility and performance, while providing some regularization of the parameters being learned. It is trained by a generalized Expectation-Maximization algorithm in an online, incremental fashion that can automatically grow the number of active components of the mixture as needed. Contrary to most state-of-the-art function approximation algorithms, IMLE can successfully learn multi-valued functions, and it equals or even outperforms popular online learning algorithms in single-valued prediction tasks."
Fast and Scalable Online CCA for Realtime Impact Analysis of Social Media Data,"The dynamics of temporal dependencies in time series of web graphs can be used to study the influence of single web sources on other web sources. Previous approaches to analysis of temporal dynamics in web graph data were either based on simple and manually tuned heuristics or not designed for online applications with massive amounts of data. Here we propose a simple but efficient and robust online learning approach to canonical correlation analysis. We show that the algorithm converges to the optimal canonical correlations and canonical variates. Using online CCA for canonical trend analysis we can a) assess the impact of a single node on all other nodes, b) explore the temporal dynamics of this impact and c) interpret the features (e.g. in BoW space) that gave rise to an information cascade. We provide preliminary results showing that we can efficiently estimate  canonical trends and thus assess the impact of single users in realtime on large data streams of web data obtained from the social network Twitter. Our results represent a first step beyond simple heuristics and towards an automatized content based realtime impact analysis for large scale data such as social networks activity."
Fast and Scalable Online CCA for Realtime Impact Analysis of Social Media Data,"The dynamics of temporal dependencies in time series of web graphs can be used to study the influence of single web sources on other web sources. Previous approaches to analysis of temporal dynamics in web graph data were either based on simple and manually tuned heuristics or not designed for online applications with massive amounts of data. Here we propose a simple but efficient and robust online learning approach to canonical correlation analysis. We show that the algorithm converges to the optimal canonical correlations and canonical variates. Using online CCA for canonical trend analysis we can a) assess the impact of a single node on all other nodes, b) explore the temporal dynamics of this impact and c) interpret the features (e.g. in BoW space) that gave rise to an information cascade. We provide preliminary results showing that we can efficiently estimate  canonical trends and thus assess the impact of single users in realtime on large data streams of web data obtained from the social network Twitter. Our results represent a first step beyond simple heuristics and towards an automatized content based realtime impact analysis for large scale data such as social networks activity."
Fast and Scalable Online CCA for Realtime Impact Analysis of Social Media Data,"The dynamics of temporal dependencies in time series of web graphs can be used to study the influence of single web sources on other web sources. Previous approaches to analysis of temporal dynamics in web graph data were either based on simple and manually tuned heuristics or not designed for online applications with massive amounts of data. Here we propose a simple but efficient and robust online learning approach to canonical correlation analysis. We show that the algorithm converges to the optimal canonical correlations and canonical variates. Using online CCA for canonical trend analysis we can a) assess the impact of a single node on all other nodes, b) explore the temporal dynamics of this impact and c) interpret the features (e.g. in BoW space) that gave rise to an information cascade. We provide preliminary results showing that we can efficiently estimate  canonical trends and thus assess the impact of single users in realtime on large data streams of web data obtained from the social network Twitter. Our results represent a first step beyond simple heuristics and towards an automatized content based realtime impact analysis for large scale data such as social networks activity."
Fast and Scalable Online CCA for Realtime Impact Analysis of Social Media Data,"The dynamics of temporal dependencies in time series of web graphs can be used to study the influence of single web sources on other web sources. Previous approaches to analysis of temporal dynamics in web graph data were either based on simple and manually tuned heuristics or not designed for online applications with massive amounts of data. Here we propose a simple but efficient and robust online learning approach to canonical correlation analysis. We show that the algorithm converges to the optimal canonical correlations and canonical variates. Using online CCA for canonical trend analysis we can a) assess the impact of a single node on all other nodes, b) explore the temporal dynamics of this impact and c) interpret the features (e.g. in BoW space) that gave rise to an information cascade. We provide preliminary results showing that we can efficiently estimate  canonical trends and thus assess the impact of single users in realtime on large data streams of web data obtained from the social network Twitter. Our results represent a first step beyond simple heuristics and towards an automatized content based realtime impact analysis for large scale data such as social networks activity."
Robust elastic-net nonnegative matrix factorization with box constraints,"In this paper, we propose an elastic-net nonnegative matrix factorization (NMF) with box constraints to remove grouped outliers and recover the inherent nonnegative low-rank structure of the given high dimensional noisy image data. Based on the augmented Lagrangian framework, we solve the linearly constrained minimization reformulation of the elastic-net NMF with the successive overrelaxed outer product iteration (SOOPI). We evaluate the performance of the proposed method for the background modeling of video image sequence and removal of varying illumination and grossly corrupted artifacts in face images. The numerical results show that our proposed elastic-net NMF model does better recover low-rank structure than the state-of-the-art nuclear norm based robust principal component analysis (PCA) and other robust NMF models."
Higher-order decorrelation of receptive fields using support vector machines,"Characterization of neural response properties by means of the receptive field corresponds to estimation of the linear part of a combined linear-nonlinear system. It commonly involves estimation of the stimulus auto-covariance matrix as in the reverse correlation method as a way to remove second-order stimulus correlations that occur in many stimuli of interest, in particular natural stimuli. However, non-Gaussian stimulus distributions and higher-order stimulus correlations in conjunction with nonlinear response properties result in biased estimates of the true receptive field for covariance-based approaches. We show that the problem of receptive field estimation may be reformulated in terms of a binary classification problem, an approach that alleviates the aforementioned problems.  In contrast to regression-like modification of the reverse correlation method, it works on single spikes, and unlike the spike triggered average (STA), it uses spike-eliciting and non-spike-eliciting stimulus portions for receptive field estimation. Using simulations and recordings from inferior colliculus neurons of mongolian gerbils, we show that receptive field estimates obtained using the support vector machine (SVM) classification based receptive field estimator show better decorrelation properties of higher-order stimulus correlations and are more robust against asymmetric stimulus intensity distributions than typical covariance-based approaches. The results obtained for receptive field estimation may imply relevance for general linear-nonlinear systems estimation using large-margin approaches."
Higher-order decorrelation of receptive fields using support vector machines,"Characterization of neural response properties by means of the receptive field corresponds to estimation of the linear part of a combined linear-nonlinear system. It commonly involves estimation of the stimulus auto-covariance matrix as in the reverse correlation method as a way to remove second-order stimulus correlations that occur in many stimuli of interest, in particular natural stimuli. However, non-Gaussian stimulus distributions and higher-order stimulus correlations in conjunction with nonlinear response properties result in biased estimates of the true receptive field for covariance-based approaches. We show that the problem of receptive field estimation may be reformulated in terms of a binary classification problem, an approach that alleviates the aforementioned problems.  In contrast to regression-like modification of the reverse correlation method, it works on single spikes, and unlike the spike triggered average (STA), it uses spike-eliciting and non-spike-eliciting stimulus portions for receptive field estimation. Using simulations and recordings from inferior colliculus neurons of mongolian gerbils, we show that receptive field estimates obtained using the support vector machine (SVM) classification based receptive field estimator show better decorrelation properties of higher-order stimulus correlations and are more robust against asymmetric stimulus intensity distributions than typical covariance-based approaches. The results obtained for receptive field estimation may imply relevance for general linear-nonlinear systems estimation using large-margin approaches."
Higher-order decorrelation of receptive fields using support vector machines,"Characterization of neural response properties by means of the receptive field corresponds to estimation of the linear part of a combined linear-nonlinear system. It commonly involves estimation of the stimulus auto-covariance matrix as in the reverse correlation method as a way to remove second-order stimulus correlations that occur in many stimuli of interest, in particular natural stimuli. However, non-Gaussian stimulus distributions and higher-order stimulus correlations in conjunction with nonlinear response properties result in biased estimates of the true receptive field for covariance-based approaches. We show that the problem of receptive field estimation may be reformulated in terms of a binary classification problem, an approach that alleviates the aforementioned problems.  In contrast to regression-like modification of the reverse correlation method, it works on single spikes, and unlike the spike triggered average (STA), it uses spike-eliciting and non-spike-eliciting stimulus portions for receptive field estimation. Using simulations and recordings from inferior colliculus neurons of mongolian gerbils, we show that receptive field estimates obtained using the support vector machine (SVM) classification based receptive field estimator show better decorrelation properties of higher-order stimulus correlations and are more robust against asymmetric stimulus intensity distributions than typical covariance-based approaches. The results obtained for receptive field estimation may imply relevance for general linear-nonlinear systems estimation using large-margin approaches."
Higher-order decorrelation of receptive fields using support vector machines,"Characterization of neural response properties by means of the receptive field corresponds to estimation of the linear part of a combined linear-nonlinear system. It commonly involves estimation of the stimulus auto-covariance matrix as in the reverse correlation method as a way to remove second-order stimulus correlations that occur in many stimuli of interest, in particular natural stimuli. However, non-Gaussian stimulus distributions and higher-order stimulus correlations in conjunction with nonlinear response properties result in biased estimates of the true receptive field for covariance-based approaches. We show that the problem of receptive field estimation may be reformulated in terms of a binary classification problem, an approach that alleviates the aforementioned problems.  In contrast to regression-like modification of the reverse correlation method, it works on single spikes, and unlike the spike triggered average (STA), it uses spike-eliciting and non-spike-eliciting stimulus portions for receptive field estimation. Using simulations and recordings from inferior colliculus neurons of mongolian gerbils, we show that receptive field estimates obtained using the support vector machine (SVM) classification based receptive field estimator show better decorrelation properties of higher-order stimulus correlations and are more robust against asymmetric stimulus intensity distributions than typical covariance-based approaches. The results obtained for receptive field estimation may imply relevance for general linear-nonlinear systems estimation using large-margin approaches."
Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting,"This paper proposes an efficient on-line learning algorithm to track the smoothing functions of additive models. The key idea is to combine the linear representation of additive models with a recursive least squares filter. In order to quickly track model changes and put more weight on recent data, the recursive least squares filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behavior is further enhanced by using an adaptive forgetting factor which is updated based on the gradient descent method, with the maximum admissible value of the learning rate provided by Lyapunov stability theory. The algorithm is applied to additive models tracking 6 years of electricity demand data from the French utility company EDF (Electricite de France). Compared to state-of-the-art methods, it achieves a superior performance in terms ofprediction accuracy."
Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting,"This paper proposes an efficient on-line learning algorithm to track the smoothing functions of additive models. The key idea is to combine the linear representation of additive models with a recursive least squares filter. In order to quickly track model changes and put more weight on recent data, the recursive least squares filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behavior is further enhanced by using an adaptive forgetting factor which is updated based on the gradient descent method, with the maximum admissible value of the learning rate provided by Lyapunov stability theory. The algorithm is applied to additive models tracking 6 years of electricity demand data from the French utility company EDF (Electricite de France). Compared to state-of-the-art methods, it achieves a superior performance in terms ofprediction accuracy."
Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting,"This paper proposes an efficient on-line learning algorithm to track the smoothing functions of additive models. The key idea is to combine the linear representation of additive models with a recursive least squares filter. In order to quickly track model changes and put more weight on recent data, the recursive least squares filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behavior is further enhanced by using an adaptive forgetting factor which is updated based on the gradient descent method, with the maximum admissible value of the learning rate provided by Lyapunov stability theory. The algorithm is applied to additive models tracking 6 years of electricity demand data from the French utility company EDF (Electricite de France). Compared to state-of-the-art methods, it achieves a superior performance in terms ofprediction accuracy."
Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting,"This paper proposes an efficient on-line learning algorithm to track the smoothing functions of additive models. The key idea is to combine the linear representation of additive models with a recursive least squares filter. In order to quickly track model changes and put more weight on recent data, the recursive least squares filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behavior is further enhanced by using an adaptive forgetting factor which is updated based on the gradient descent method, with the maximum admissible value of the learning rate provided by Lyapunov stability theory. The algorithm is applied to additive models tracking 6 years of electricity demand data from the French utility company EDF (Electricite de France). Compared to state-of-the-art methods, it achieves a superior performance in terms ofprediction accuracy."
Toward adaptive brain computer interfaces,"We consider the question of on-line multinomialclassification in non-stationary environments with non-reliable rewards.We present a policy gradient approach that appear to be effectivein the context of the ``oddball'' classification framework.Then, we present two series of experiments reproducing the conditions of brain computerinterfaces and compare the policy gradient to a more genuine classifier update, and showthe benefit of using the two methods simultaneously for optimal recovery."
Toward adaptive brain computer interfaces,"We consider the question of on-line multinomialclassification in non-stationary environments with non-reliable rewards.We present a policy gradient approach that appear to be effectivein the context of the ``oddball'' classification framework.Then, we present two series of experiments reproducing the conditions of brain computerinterfaces and compare the policy gradient to a more genuine classifier update, and showthe benefit of using the two methods simultaneously for optimal recovery."
Semi-supervised Eigenvectors for Locally-biased Learning,"In many applications, one has information, e.g., labels that are provided in a semi-supervised manner, about a specific target region of a large data set, and one wants to perform machine learning and data analysis tasks nearby that pre-specified target region.  Locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools.At root, the reason is that eigenvectors are inherently global quantities.In this paper, we address this issue by providing a methodology to construct semi-supervised eigenvectors of a graph Laplacian, and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning.These semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance, conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner.We also provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning."
Restricting exchangeable nonparametric distributions,"Distributions over exchangeable matrices with infinitely many columns, such as the Indian buffet process, are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution."
Restricting exchangeable nonparametric distributions,"Distributions over exchangeable matrices with infinitely many columns, such as the Indian buffet process, are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution."
Restricting exchangeable nonparametric distributions,"Distributions over exchangeable matrices with infinitely many columns, such as the Indian buffet process, are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution."
Statistical inference in compound functional models,"We consider a general nonparametric regression model called the compoundmodel. It includes, as special cases, sparse additive regression and nonparametric(or linear) regression with many covariates but possibly a small number of relevantcovariates. The compound model is characterized by three main parameters: thestructure parameter describing the macroscopic form of the compound function,the microscopic sparsity parameter indicating the maximal number of relevant covariatesin each component and the usual smoothness parameter corresponding tothe complexity of the members of the compound. We find non-asymptotic minimaxrate of convergence of estimators in such a model as a function of these threeparameters. We also show that this rate can be attained in an adaptive way."
Statistical inference in compound functional models,"We consider a general nonparametric regression model called the compoundmodel. It includes, as special cases, sparse additive regression and nonparametric(or linear) regression with many covariates but possibly a small number of relevantcovariates. The compound model is characterized by three main parameters: thestructure parameter describing the macroscopic form of the compound function,the microscopic sparsity parameter indicating the maximal number of relevant covariatesin each component and the usual smoothness parameter corresponding tothe complexity of the members of the compound. We find non-asymptotic minimaxrate of convergence of estimators in such a model as a function of these threeparameters. We also show that this rate can be attained in an adaptive way."
Statistical inference in compound functional models,"We consider a general nonparametric regression model called the compoundmodel. It includes, as special cases, sparse additive regression and nonparametric(or linear) regression with many covariates but possibly a small number of relevantcovariates. The compound model is characterized by three main parameters: thestructure parameter describing the macroscopic form of the compound function,the microscopic sparsity parameter indicating the maximal number of relevant covariatesin each component and the usual smoothness parameter corresponding tothe complexity of the members of the compound. We find non-asymptotic minimaxrate of convergence of estimators in such a model as a function of these threeparameters. We also show that this rate can be attained in an adaptive way."
Graph Denoising,"The paper is motivated by real-world applications for denoising graph data. We show that this problem amounts to solving matrix recovery for an adjacency matrix which is both sparse and low-rank under a random perturbation matrix which is also sparse. We formulate the problem as the minimization of a regularized convex objective with an $\ell_1$ loss. We present two methods: an exact method based on Douglas-Rachford splitting, and an approximate method using matrix factorization and rank-one updates which offers better scalability. Numerical experiments confirm the relevance of the approach compared to state-of-the-art methods such as robust PCA."
Graph Denoising,"The paper is motivated by real-world applications for denoising graph data. We show that this problem amounts to solving matrix recovery for an adjacency matrix which is both sparse and low-rank under a random perturbation matrix which is also sparse. We formulate the problem as the minimization of a regularized convex objective with an $\ell_1$ loss. We present two methods: an exact method based on Douglas-Rachford splitting, and an approximate method using matrix factorization and rank-one updates which offers better scalability. Numerical experiments confirm the relevance of the approach compared to state-of-the-art methods such as robust PCA."
Graph Denoising,"The paper is motivated by real-world applications for denoising graph data. We show that this problem amounts to solving matrix recovery for an adjacency matrix which is both sparse and low-rank under a random perturbation matrix which is also sparse. We formulate the problem as the minimization of a regularized convex objective with an $\ell_1$ loss. We present two methods: an exact method based on Douglas-Rachford splitting, and an approximate method using matrix factorization and rank-one updates which offers better scalability. Numerical experiments confirm the relevance of the approach compared to state-of-the-art methods such as robust PCA."
A Soft-Label Model with Impact for Active Graph Search,"We consider the problem of active search on a graph where we seek nodes belonging to a certain positive class by iteratively selecting nodes to query for their class label. The problem has similarities with active learning on a graph except that the performance is measured by number of positives identified rather than classification accuracy. Good solutions must tradeoff exploration to better fit a model against exploitation to collect likely positives and thus the problem has similarities with bandit problems as well. However, bandit algorithms are hard to adapt to the  problem since we will never choose the same node more than once.Previous work showed that the optimal active search algorithm requires a look ahead evaluation of expected utility that is exponential in the number of node selections to be made and considered heuristics that do a truncated look ahead [1]. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs.  We test the algorithm empirically on citation and wikipedia graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps."
Transferring Expectations in Model-based Reinforcement Learning,"We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without pre-defined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. "
Transferring Expectations in Model-based Reinforcement Learning,"We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without pre-defined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. "
Transferring Expectations in Model-based Reinforcement Learning,"We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without pre-defined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. "
Factorial LDA: Sparse Multi-Dimensional Text Models,"Multi-dimensional latent variable models can capture the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional latent variable model in which a document is influenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientific discipline, and focus (e.g. methods vs. applications.) Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Automating Collusion Detection in Sequential Games,"Collusion is the practice of two parties deliberately cooperating to the detriment of others.  While such behavior may be desirable in certain circumstances, in many it is considered dishonest and unfair.  If agents otherwise hold strictly to the established rules, though, collusion can be challenging to police.  In this paper, we introduce an automatic method for collusion detection in sequential games.  We achieve this through a novel object, called a collusion table, that aims to capture the effects of collusive behavior, i.e., advantage to the colluding parties, without committing to any particular pattern of behavior.  We demonstrate the effectiveness of this method in the domain of poker, a popular game where collusion is prohibited."
A Unified Framework for Probabilistic Component Analysis,"In this paper we attempt to unify many very popular and well-studied componentanalysis algorithms, such as Principal Component Analysis (PCA), LinearDiscriminant Analysis (LDA), Locality Preserving Projections (LPP) and SlowFeature Analysis (SFA) under a single, probabilistic framework. We firstly showthat the projection directions produced by all the above mentioned methods arealso produced by the Maximum Likelihood (ML) solution of a single joint probabilitydensity function (pdf), just by choosing the appropriate prior over the latentspace. Subsequently, we propose novel Expectation Maximization (EM) algorithmsutilising the proposed joint pdf. Experimental results show the usefulnessof the proposed EM framework in both simulated and real world data."
A Unified Framework for Probabilistic Component Analysis,"In this paper we attempt to unify many very popular and well-studied componentanalysis algorithms, such as Principal Component Analysis (PCA), LinearDiscriminant Analysis (LDA), Locality Preserving Projections (LPP) and SlowFeature Analysis (SFA) under a single, probabilistic framework. We firstly showthat the projection directions produced by all the above mentioned methods arealso produced by the Maximum Likelihood (ML) solution of a single joint probabilitydensity function (pdf), just by choosing the appropriate prior over the latentspace. Subsequently, we propose novel Expectation Maximization (EM) algorithmsutilising the proposed joint pdf. Experimental results show the usefulnessof the proposed EM framework in both simulated and real world data."
A Unified Framework for Probabilistic Component Analysis,"In this paper we attempt to unify many very popular and well-studied componentanalysis algorithms, such as Principal Component Analysis (PCA), LinearDiscriminant Analysis (LDA), Locality Preserving Projections (LPP) and SlowFeature Analysis (SFA) under a single, probabilistic framework. We firstly showthat the projection directions produced by all the above mentioned methods arealso produced by the Maximum Likelihood (ML) solution of a single joint probabilitydensity function (pdf), just by choosing the appropriate prior over the latentspace. Subsequently, we propose novel Expectation Maximization (EM) algorithmsutilising the proposed joint pdf. Experimental results show the usefulnessof the proposed EM framework in both simulated and real world data."
Semisupervised Classifier Evaluation and Recalibration,"How many labeled examples are needed to estimate a classifier's performance on a new dataset? We study the case where data is plentiful, but labels are expensive.  We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier's confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by re-estimating the class-conditional confidence distributions."
Modelling Reciprocating Relationships,"We present a Bayesian nonparametric model that discovers implicit socialstructure from interaction time-series data.Social groups are often formed implicitly, through actions among members ofgroups.Yet many models of social networks use explicitly declared relationships toinfer social structure.We consider a particular class of Hawkes processes, a doubly stochastic pointprocess, that is able to model reciprocity between groups of individuals.We then extend the Infinite Relational Model by using these reciprocatingHawkes processes to parameterise its edges, making events associated with edgesco-dependent through time.Our model outperforms general, unstructured Hawkes processes as well as structuredPoisson process-based models at predicting verbal and email turn-taking, andmilitary conflicts among nations."
Modelling Reciprocating Relationships,"We present a Bayesian nonparametric model that discovers implicit socialstructure from interaction time-series data.Social groups are often formed implicitly, through actions among members ofgroups.Yet many models of social networks use explicitly declared relationships toinfer social structure.We consider a particular class of Hawkes processes, a doubly stochastic pointprocess, that is able to model reciprocity between groups of individuals.We then extend the Infinite Relational Model by using these reciprocatingHawkes processes to parameterise its edges, making events associated with edgesco-dependent through time.Our model outperforms general, unstructured Hawkes processes as well as structuredPoisson process-based models at predicting verbal and email turn-taking, andmilitary conflicts among nations."
Modelling Reciprocating Relationships,"We present a Bayesian nonparametric model that discovers implicit socialstructure from interaction time-series data.Social groups are often formed implicitly, through actions among members ofgroups.Yet many models of social networks use explicitly declared relationships toinfer social structure.We consider a particular class of Hawkes processes, a doubly stochastic pointprocess, that is able to model reciprocity between groups of individuals.We then extend the Infinite Relational Model by using these reciprocatingHawkes processes to parameterise its edges, making events associated with edgesco-dependent through time.Our model outperforms general, unstructured Hawkes processes as well as structuredPoisson process-based models at predicting verbal and email turn-taking, andmilitary conflicts among nations."
Expectation Propagation in Gaussian Process Dynamical Systems,"Rich and complex time-series data, such as those generated from engineering sys-tems, financial markets, videos or neural recordings are now a common feature ofmodern data analysis. Explaining the phenomena underlying these diverse datasets requires flexible and accurate models. In this paper, we promote Gaussianprocess dynamical systems as a rich model class appropriate for such analysis. Inparticular, we present a message passing algorithm for approximate inference inGPDSs based on expectation propagation. By phrasing inference as a general mes-sage passing problem, we iterate forward-backward smoothing. We obtain moreaccurate posterior distributions over latent structures, resulting in improved pre-dictive performance compared to state-of-the-art GPDS smoothers, which are spe-cial cases of our general iterative message passing algorithm. Hence, we providea unifying approach within which to contextualize message passing in GPDSs."
A quasi-Newton proximal splitting method,"We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification."
A quasi-Newton proximal splitting method,"We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification."
Hessian-Free Optimization for Long Short-Term Memory,"The application of $2^{nd}$-order optimization techniques to overcome the inherent limitations of gradient-based learning in Recurrent Neural Networks have proven to be quite succesful. However, comparisons between Hessian-Free optimization and LSTM trained by stochastic gradient descent represent a false equivalence. The many non-linear units and gating cells in LSTM can greatly benefit from $2^{nd}$-order learning methods, without negating the powerful role played by the constant error carousel in providing . In this paper, we have presented HF-optimization for LSTM, in order to more accurately compare performance with standard RNNs which have been similarly trained. On the chosen test set, we find that LSTM outperforms RNNs by solving the task up to a full order of magnitude more quickly. Admittedly, this is a narrow comparison overall, and we intend to run experiments across a number of pathological test cases, as well as real world examples. "
Hessian-Free Optimization for Long Short-Term Memory,"The application of $2^{nd}$-order optimization techniques to overcome the inherent limitations of gradient-based learning in Recurrent Neural Networks have proven to be quite succesful. However, comparisons between Hessian-Free optimization and LSTM trained by stochastic gradient descent represent a false equivalence. The many non-linear units and gating cells in LSTM can greatly benefit from $2^{nd}$-order learning methods, without negating the powerful role played by the constant error carousel in providing . In this paper, we have presented HF-optimization for LSTM, in order to more accurately compare performance with standard RNNs which have been similarly trained. On the chosen test set, we find that LSTM outperforms RNNs by solving the task up to a full order of magnitude more quickly. Admittedly, this is a narrow comparison overall, and we intend to run experiments across a number of pathological test cases, as well as real world examples. "
Hessian-Free Optimization for Long Short-Term Memory,"The application of $2^{nd}$-order optimization techniques to overcome the inherent limitations of gradient-based learning in Recurrent Neural Networks have proven to be quite succesful. However, comparisons between Hessian-Free optimization and LSTM trained by stochastic gradient descent represent a false equivalence. The many non-linear units and gating cells in LSTM can greatly benefit from $2^{nd}$-order learning methods, without negating the powerful role played by the constant error carousel in providing . In this paper, we have presented HF-optimization for LSTM, in order to more accurately compare performance with standard RNNs which have been similarly trained. On the chosen test set, we find that LSTM outperforms RNNs by solving the task up to a full order of magnitude more quickly. Admittedly, this is a narrow comparison overall, and we intend to run experiments across a number of pathological test cases, as well as real world examples. "
Hessian-Free Optimization for Long Short-Term Memory,"The application of $2^{nd}$-order optimization techniques to overcome the inherent limitations of gradient-based learning in Recurrent Neural Networks have proven to be quite succesful. However, comparisons between Hessian-Free optimization and LSTM trained by stochastic gradient descent represent a false equivalence. The many non-linear units and gating cells in LSTM can greatly benefit from $2^{nd}$-order learning methods, without negating the powerful role played by the constant error carousel in providing . In this paper, we have presented HF-optimization for LSTM, in order to more accurately compare performance with standard RNNs which have been similarly trained. On the chosen test set, we find that LSTM outperforms RNNs by solving the task up to a full order of magnitude more quickly. Admittedly, this is a narrow comparison overall, and we intend to run experiments across a number of pathological test cases, as well as real world examples. "
Multi-Task Active Learning for Hierarchical Classification," In this paper, we present a novel combination of Active Learning and Multi-Task Learning for minimizing the training data required for effective Hierarchical Classification. For Multi-Task Learning, we describe a novel hierarchical regularization strategy that utilizes the learnt parameters of a parent category as regularizers for its children categories. For Active Learning, we leverage the multi-task relationships to selectively acquire training data that is effective for improving classification at a category as well as other categories that it influences through the regularization framework. We formulate a stochastic gradient descent solution for Multi-Task learning and an online decision criterion for Active learning to make our approach scalable for large-scale deployment of Active Hierarchical Classification. In spite of being a jointly learnt multi-task model, the approach can be easily adapted to the popular MapReduce, OpenMP and MPI frameworks for large-scale learning through differential message passing amongst categories. Through experiments on well-known hierarchical classification datasets, we demonstrate the superior performance of our approach as compared to learning the hierarchical categories in isolation (single-task setting), especially for categories with limited positive training instances. Our experiments also show significant reduction in the amount of required training data when it is selected with our novel multi-task active learning approach as compared to conventional active learning approaches that select instances for each category in isolation."
Analysis of Algorithms for the Memory Hierarchy,"Batch gradient descent looks at every data point for everystep, which is wasteful for early steps where the current position isnowhere near optimal.There has been a lot of interest in warm-start approaches to gradientdescent techniques, but little analysis. In this paper, we formallyanalyze a method of warm-starting batch gradient descent using smallbatch sizes. We argue that this approach is fundamentally differentthan mini-batch, in that after an initial shuffle, it requires only sequential passes over thedata, improving performance on datasets stored on a disk drive. We also analyze sequential gradient descent."
Supervising Unsupervised Learning: Alleviating label noise in behavioural EEG-BCI experiments,"Behavioural experiments in the neurosciences require the assessment of a given stimulus by a subject. We will study the audio signal quality judgements of subjects and their respective neural correlates as measured by an EEG-BCI.  At decision threshold the subject often guesses, thus, the psychophysical assessment of the stimulus is greatly hampered. So the labels are only partly correct and very often random, which is a problematic scenario when applying supervised learning. We contribute by devising a novel supervised-unsupervised learning scheme, that aims to diferentiate true labels from random ones. This iterated combination of unsupervised one-class outlier detection and semi-supervised one-class learning yields neuroscientifically plausible correlates to behaviour that are more pronounced and meaningful than results found by the commonly used vanilla supervised learning approach that ignores the problematic label noise. While we discuss the experimental evidence for our audio signal quality application, it should be noted that this novelsupervised-unsupervised learning proceedure is applicable also beyond the neurosciences for general psychophysical experiments or generally for high label noise."
Supervising Unsupervised Learning: Alleviating label noise in behavioural EEG-BCI experiments,"Behavioural experiments in the neurosciences require the assessment of a given stimulus by a subject. We will study the audio signal quality judgements of subjects and their respective neural correlates as measured by an EEG-BCI.  At decision threshold the subject often guesses, thus, the psychophysical assessment of the stimulus is greatly hampered. So the labels are only partly correct and very often random, which is a problematic scenario when applying supervised learning. We contribute by devising a novel supervised-unsupervised learning scheme, that aims to diferentiate true labels from random ones. This iterated combination of unsupervised one-class outlier detection and semi-supervised one-class learning yields neuroscientifically plausible correlates to behaviour that are more pronounced and meaningful than results found by the commonly used vanilla supervised learning approach that ignores the problematic label noise. While we discuss the experimental evidence for our audio signal quality application, it should be noted that this novelsupervised-unsupervised learning proceedure is applicable also beyond the neurosciences for general psychophysical experiments or generally for high label noise."
Supervising Unsupervised Learning: Alleviating label noise in behavioural EEG-BCI experiments,"Behavioural experiments in the neurosciences require the assessment of a given stimulus by a subject. We will study the audio signal quality judgements of subjects and their respective neural correlates as measured by an EEG-BCI.  At decision threshold the subject often guesses, thus, the psychophysical assessment of the stimulus is greatly hampered. So the labels are only partly correct and very often random, which is a problematic scenario when applying supervised learning. We contribute by devising a novel supervised-unsupervised learning scheme, that aims to diferentiate true labels from random ones. This iterated combination of unsupervised one-class outlier detection and semi-supervised one-class learning yields neuroscientifically plausible correlates to behaviour that are more pronounced and meaningful than results found by the commonly used vanilla supervised learning approach that ignores the problematic label noise. While we discuss the experimental evidence for our audio signal quality application, it should be noted that this novelsupervised-unsupervised learning proceedure is applicable also beyond the neurosciences for general psychophysical experiments or generally for high label noise."
Supervising Unsupervised Learning: Alleviating label noise in behavioural EEG-BCI experiments,"Behavioural experiments in the neurosciences require the assessment of a given stimulus by a subject. We will study the audio signal quality judgements of subjects and their respective neural correlates as measured by an EEG-BCI.  At decision threshold the subject often guesses, thus, the psychophysical assessment of the stimulus is greatly hampered. So the labels are only partly correct and very often random, which is a problematic scenario when applying supervised learning. We contribute by devising a novel supervised-unsupervised learning scheme, that aims to diferentiate true labels from random ones. This iterated combination of unsupervised one-class outlier detection and semi-supervised one-class learning yields neuroscientifically plausible correlates to behaviour that are more pronounced and meaningful than results found by the commonly used vanilla supervised learning approach that ignores the problematic label noise. While we discuss the experimental evidence for our audio signal quality application, it should be noted that this novelsupervised-unsupervised learning proceedure is applicable also beyond the neurosciences for general psychophysical experiments or generally for high label noise."
A Bias-Variance Analysis of Model-Based Estimation in Reinforcement Learning ,"This paper provides the first bias and variance characterization of the model-based value function estimator in finite-horizon reinforcement learning (RL) problems with discrete state spaces. The closed-formed formulas we derive to estimate the bias and variance rely on an approximation that is exact if the estimates of the transition model, reward model, and value function are normally distributed. These results can be used to characterize performance of RL systems in a wide range of application domains. We are particularly interested in applications concerning resource management domains, and therefore we include experiments demonstrating the use of our estimators to evaluate strategies for population management of animal species. We find the bias/variance estimates produced by our method to be more accurate than those produced by the well-known bootstrap or jackknife estimators. We also compare our results to the bias-variance analysis of Mannor et al. [2007], and show that even in their setting (infinite-horizon, discounted problems), our approach may be preferable."
Multilabel Classification using Bayesian Compressed Sensing,"In this paper, we present a Bayesian framework for multilabel classification using compressed sensing. The key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key benefits of the model are that a) it can naturally handle datasets that have  missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model."
Multilabel Classification using Bayesian Compressed Sensing,"In this paper, we present a Bayesian framework for multilabel classification using compressed sensing. The key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key benefits of the model are that a) it can naturally handle datasets that have  missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model."
Multilabel Classification using Bayesian Compressed Sensing,"In this paper, we present a Bayesian framework for multilabel classification using compressed sensing. The key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key benefits of the model are that a) it can naturally handle datasets that have  missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model."
On Computational Feasibility of Mapping Kernels,"The mapping kernel framework has proven useful to design kernels for discrete structures.The resulting kernelsare positive definite, and can be efficiently computed. Recently, a certain class of string kernels,called partitionable kernels, was found to havetight relation to the computational feasibility of mapping kernels.Also, it turns outonly a small portion of the entire partitionable kernelshave been actually used in the literature, and most of them remain unused.In this paper, we shed light on the unexplored area of partitionable kernels, and show interesting and useful propertiesof a certain subclass of partitionable kernels,which is important from both the theoretical and practical points of view."
Scaling Constrained Continuous Markov Random Fields with Consensus Optimization,"We study scaling a class of probabilistic graphical models well-suited to constrained, continuous domains. We show how to solve the most-probable-explanation problem for these models with a consensus-optimization framework. We derive closed-form solutions for consensus-optimization subproblems induced by several types of common dependencies. We improve the performance of consensus optimization by deriving an algorithm that can additionally find closed-form solutions to subproblems in certain cases, depending on the current optimization iterate, not just the subproblem itself. We demonstrate superior performance of our approach over commercial interior-point methods, the current state-of-the-art for the problems we study. In fact, in our evaluation our method scales linearly with the size of the problem."
Scaling Constrained Continuous Markov Random Fields with Consensus Optimization,"We study scaling a class of probabilistic graphical models well-suited to constrained, continuous domains. We show how to solve the most-probable-explanation problem for these models with a consensus-optimization framework. We derive closed-form solutions for consensus-optimization subproblems induced by several types of common dependencies. We improve the performance of consensus optimization by deriving an algorithm that can additionally find closed-form solutions to subproblems in certain cases, depending on the current optimization iterate, not just the subproblem itself. We demonstrate superior performance of our approach over commercial interior-point methods, the current state-of-the-art for the problems we study. In fact, in our evaluation our method scales linearly with the size of the problem."
Scaling Constrained Continuous Markov Random Fields with Consensus Optimization,"We study scaling a class of probabilistic graphical models well-suited to constrained, continuous domains. We show how to solve the most-probable-explanation problem for these models with a consensus-optimization framework. We derive closed-form solutions for consensus-optimization subproblems induced by several types of common dependencies. We improve the performance of consensus optimization by deriving an algorithm that can additionally find closed-form solutions to subproblems in certain cases, depending on the current optimization iterate, not just the subproblem itself. We demonstrate superior performance of our approach over commercial interior-point methods, the current state-of-the-art for the problems we study. In fact, in our evaluation our method scales linearly with the size of the problem."
A Stochastic Gradient Method with an Exponential Convergence ?Rate  with Finite Training Sets,"We propose a new stochastic gradient method for optimizing the sum of? a finite set of smooth functions, where the sum is strongly convex.? While standard stochastic gradient methods? converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence ?rate.  In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard? algorithms, both in terms of optimizing the training error and reducing the test error quickly."
A Stochastic Gradient Method with an Exponential Convergence ?Rate  with Finite Training Sets,"We propose a new stochastic gradient method for optimizing the sum of? a finite set of smooth functions, where the sum is strongly convex.? While standard stochastic gradient methods? converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence ?rate.  In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard? algorithms, both in terms of optimizing the training error and reducing the test error quickly."
A Stochastic Gradient Method with an Exponential Convergence ?Rate  with Finite Training Sets,"We propose a new stochastic gradient method for optimizing the sum of? a finite set of smooth functions, where the sum is strongly convex.? While standard stochastic gradient methods? converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence ?rate.  In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard? algorithms, both in terms of optimizing the training error and reducing the test error quickly."
"Burn-in, bias, and the rationality of anchoring","Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as burn-in. Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks. Our theoretical and empirical results suggest that the anchoring bias is consistent with approximate Bayesian inference."
Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes,"Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning.  In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics."
A Neural Autoregressive Topic Model,"We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm."
A Neural Autoregressive Topic Model,"We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm."
Discovering Naive Representations and Problem Solving on Image Manifolds,"A number of generative algorithms capture the underlying patterns of variability indata, resulting in significant reduction of dimensionality. However, the abstractedpatterns discovered remain at a subsymbolic level, where they are not compactenough to be mapped to signs and become true symbols. In this work, we attemptto construct AI-style symbol systems based on non-linear dimensionalityreduction. For a large class of perceptual input relating to object motions, the embeddingspace itself can be used as a generative model of the problem domain, andwe show that it can be used directly to discover a set of attributes and the valuesthey acquire in different situations. We consider two domains, classical mechanicsand robot motion planning. In each case, we start with sensor data (sequences ofimages), and without any priors, we learn an embedding that captures the relationshipsin the problem domain. we show how various search and planning problemscan be conducted on the embedding space and mapped back into the task space,without invoking the state parameters normally used by models in human science.If needed, such mappings are also readily available from these embeddings"
Discovering Naive Representations and Problem Solving on Image Manifolds,"A number of generative algorithms capture the underlying patterns of variability indata, resulting in significant reduction of dimensionality. However, the abstractedpatterns discovered remain at a subsymbolic level, where they are not compactenough to be mapped to signs and become true symbols. In this work, we attemptto construct AI-style symbol systems based on non-linear dimensionalityreduction. For a large class of perceptual input relating to object motions, the embeddingspace itself can be used as a generative model of the problem domain, andwe show that it can be used directly to discover a set of attributes and the valuesthey acquire in different situations. We consider two domains, classical mechanicsand robot motion planning. In each case, we start with sensor data (sequences ofimages), and without any priors, we learn an embedding that captures the relationshipsin the problem domain. we show how various search and planning problemscan be conducted on the embedding space and mapped back into the task space,without invoking the state parameters normally used by models in human science.If needed, such mappings are also readily available from these embeddings"
On the convergence and optimality of optimistic approximate policy iteration,"A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. While the former is often considered to possess practical advantages over the latter, there is, in the interactive case, currently little understanding on its behavior in the proximity of an optimum; after certain amount of guaranteed improvement, the learning process can become trapped in sustained oscillation or chattering, or it can converge to a solution with rather unknown properties. In this paper, we provide insight on the convergence behavior of the general, optimistic form of the greedy methodology by reflecting it against the policy gradient approach. First, we consider an important effect that approximations, either in state estimation or in value function representation, have on policy evaluation, and discuss how this effect defines the natural choice of methodology for policy improvement. Second, we use a recently proposed explanation to the policy oscillation phenomenon and extend it to cover also the optimistic hard-greedy case and the associated policy chattering phenomenon. Third, we show for a substantial subset of soft-greedy approaches that, while having potential for avoiding oscillation and chattering, this subset can never converge in any state to any optimal policy, except for certain pathological cases. We link this failure to an underlying incorrect interpretation of the value function and illustrate it with a minimal artificial example. Finally, we show that as softness and the step size are decreased together toward zero, the general form of Gibbs/Boltzmann soft-greedy optimistic policy iteration using an advantage function becomes, in the limit, equivalent with the natural actor-critic algorithm."
Network floods reveal regulatory control flows and minimal networks in synthetic and bacterial datasets,"Biological networks tend to have high interconnectivity, complex topologies and multiple types of interactions. This renders difficult the identification of sub-networks that are involved in condition-specific responses. In addition, we generally lack scalable methods that can reveal the information flow in gene regulatory and biochemical pathways. Doing so will help us to identify key participants and paths under specific environmental and cellular context. This paper introduces the theory of network flooding, which aims to address the problem of network minimization and regulatory information flow in biological networks. Given a regulatory biological network, and a set of source (input) and sink (output) nodes, our task is to find (a) the minimal sub-network that encodes the regulatory program involving all input and output nodes and (b) the information flow from the source to the sink nodes of the network. To this direction, we describe a novel, scalable, network traversal algorithm, and we demonstrate its ability to achieve significant network size reduction in both synthetic and E. coli networks, without disrupting the core regulatory pathways. "
Network floods reveal regulatory control flows and minimal networks in synthetic and bacterial datasets,"Biological networks tend to have high interconnectivity, complex topologies and multiple types of interactions. This renders difficult the identification of sub-networks that are involved in condition-specific responses. In addition, we generally lack scalable methods that can reveal the information flow in gene regulatory and biochemical pathways. Doing so will help us to identify key participants and paths under specific environmental and cellular context. This paper introduces the theory of network flooding, which aims to address the problem of network minimization and regulatory information flow in biological networks. Given a regulatory biological network, and a set of source (input) and sink (output) nodes, our task is to find (a) the minimal sub-network that encodes the regulatory program involving all input and output nodes and (b) the information flow from the source to the sink nodes of the network. To this direction, we describe a novel, scalable, network traversal algorithm, and we demonstrate its ability to achieve significant network size reduction in both synthetic and E. coli networks, without disrupting the core regulatory pathways. "
Network floods reveal regulatory control flows and minimal networks in synthetic and bacterial datasets,"Biological networks tend to have high interconnectivity, complex topologies and multiple types of interactions. This renders difficult the identification of sub-networks that are involved in condition-specific responses. In addition, we generally lack scalable methods that can reveal the information flow in gene regulatory and biochemical pathways. Doing so will help us to identify key participants and paths under specific environmental and cellular context. This paper introduces the theory of network flooding, which aims to address the problem of network minimization and regulatory information flow in biological networks. Given a regulatory biological network, and a set of source (input) and sink (output) nodes, our task is to find (a) the minimal sub-network that encodes the regulatory program involving all input and output nodes and (b) the information flow from the source to the sink nodes of the network. To this direction, we describe a novel, scalable, network traversal algorithm, and we demonstrate its ability to achieve significant network size reduction in both synthetic and E. coli networks, without disrupting the core regulatory pathways. "
Efficient Inference and Learning of switching Kalman filters and their Application to Gesture Recognition,"Computational models for high dimensional time series  such as video sequences, spectral trajectories of a speechsignal or the kinematic measurements of skilled human activity hold considerable interest,particularly models that capture the inherent stochastic variability in the signal.  The hidden Markov Model (HMM) is widely used  for modeling such data.  More complex models such as switching linear dynamical systems (S-LDS) account better for the continuity of the observations, which an HMM assumes to be conditionally independent, but they lack efficient learning and inference procedures.  This paper makes three advances to address these limitations\begin{enumerate}\item A previously known inference technique by Barber \cite{barber2006,mesot2007switching} is extended to S-LDS learning.  This extension provides computationally tractable EM-based estimation of S-LDS parameters.\item Under the diagonal assumption on the observation noise, a dynamic programming algorithm is proposed to speed up the per-frame inference-complexity from cubic to linear in the observation dimension.\item A system identification algorithm is provided for initializing the parameters of an S-LDS, leading to effective S-LDS learning.\end{enumerate}The effectiveness of the new algorithms is demonstrated in gesture recognition from kinematic measurements in a robot-assisted minimally invasive surgery (RMIS) task: S-LDS models show significant improvement in recognition accuracy over comparable factor analyzed HMMs.The ability to perform automatic gesture recognition in RMIS has several applications, such as assessing dexterity or manipulative skills during surgical training or providing guidance or assistance during tele-operated surgery."
Anomaly Classification with the Anti-Profile Support Vector Machine,"We introduce the anti-profile Support Vector Machine (apSVM) as a novelalgorithm to address the anomaly classification problem, an extensionof anomaly detection where the goal is to distinguish data samplesfrom a number of anomalous and heterogeneous classes based on theirpattern of deviation from a normal stable class. We showthat under heterogeneity assumptions defined here that the apSVM canbe solved as the dual of a standard SVM with an indirect kernel that measures similarityof anomalous samples through similarity to the stable normalclass. We characterize this indirect kernel as theinner product in a Reproducing Kernel Hilbert Space betweenrepresenters that are projected to the subspace spanned by therepresenters of the normal samples. We show by simulation andapplication to cancer genomics datasets that the anti-profile SVMproduces classifiers that are more accurate and stablethan the standard SVM in the anomaly classification setting."
Anomaly Classification with the Anti-Profile Support Vector Machine,"We introduce the anti-profile Support Vector Machine (apSVM) as a novelalgorithm to address the anomaly classification problem, an extensionof anomaly detection where the goal is to distinguish data samplesfrom a number of anomalous and heterogeneous classes based on theirpattern of deviation from a normal stable class. We showthat under heterogeneity assumptions defined here that the apSVM canbe solved as the dual of a standard SVM with an indirect kernel that measures similarityof anomalous samples through similarity to the stable normalclass. We characterize this indirect kernel as theinner product in a Reproducing Kernel Hilbert Space betweenrepresenters that are projected to the subspace spanned by therepresenters of the normal samples. We show by simulation andapplication to cancer genomics datasets that the anti-profile SVMproduces classifiers that are more accurate and stablethan the standard SVM in the anomaly classification setting."
A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes,"Parametric policy search algorithms are one of the methods of choice for the optimisation of Markov Decision Processes, with Expectation Maximisation and natural gradient ascent being considered the current state of the art in the field. In this article we provide a unifying perspective of these two algorithms by showing that their step-directions in the parameter space are closely related to the search direction of an approximate Newton method. This analysis leads naturally to the consideration of this approximate Newton method as an alternative gradient-based method for Markov Decision Processes. We are able show that the algorithm has numerous desirable properties, absent in the naive application of Newton's method, that make it a viable alternative to either Expectation Maximisation or natural gradient ascent. Empirical results suggest that the algorithm has excellent convergence and robustness properties, performing strongly in comparison to both Expectation Maximisation and natural gradient ascent."
A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes,"Parametric policy search algorithms are one of the methods of choice for the optimisation of Markov Decision Processes, with Expectation Maximisation and natural gradient ascent being considered the current state of the art in the field. In this article we provide a unifying perspective of these two algorithms by showing that their step-directions in the parameter space are closely related to the search direction of an approximate Newton method. This analysis leads naturally to the consideration of this approximate Newton method as an alternative gradient-based method for Markov Decision Processes. We are able show that the algorithm has numerous desirable properties, absent in the naive application of Newton's method, that make it a viable alternative to either Expectation Maximisation or natural gradient ascent. Empirical results suggest that the algorithm has excellent convergence and robustness properties, performing strongly in comparison to both Expectation Maximisation and natural gradient ascent."
Entangled Monte Carlo,"We propose a novel method for scalable parallelization of SMC algorithms,Entangled Monte Carlo simulation (EMC).  EMC avoids the transmission ofparticles between  nodes, and instead reconstructs them from the particlegenealogy. In particular, we show that we can reduce the communication tothe particle weights for each machine while efficiently maintaining implicitglobal coherence of the parallel simulation. We explain methods toefficiently maintain a genealogy of particles from which any particle can bereconstructed. We demonstrate using examples from Bayesian phylogeneticthat the computational gain from parallelization using EMCsignificantly outweighs the cost of particle reconstruction. The timingexperiments show that reconstruction of particles is indeed much more efficientas compared to transmission of particles."
On the Sample Complexity of Ranking,"Learning to rank is a core machine learning problem. When the truescoring functions are hard to learn or training data is scarce, thesample complexity for predicting a ranking with small error is ofconsiderable interest. We present a lower bound for such a samplecomplexity for any algorithm that estimates a broad class of scoringfunction based on randomly sampled binary comparisons. Additionally,we demonstrate two simple algorithms that achieve the bound inexpectation.  While one algorithm predicts rankings with roughlyuniform quality across the ranking, the other predicts more accuratelynear the top of the ranking than the bottom. Results are presented onsynthetic examples and on an application to epitope (peptide) ranking."
SARSA Training of Deep Networks in Complex Games,"We SARSA-lambda trained a program based on deep neural nets to produce a program to compete in the 2011 AI Challenge Sponsored by Google. This was an extremely complicated video game, involving controlling the motions of hundreds of ants in real time as they played in novel game boards of up to 200 by 200 for 1000 steps with imperfect information. This may be the most complex domain ever attempted by reinforcement learning. A number of engineering methods are described that allowed us to finish near the top 10% of 9000 hand-coded human submitted entrants. We are extending our methods to produce a strong player of No Limit Texas Holdem."
SARSA Training of Deep Networks in Complex Games,"We SARSA-lambda trained a program based on deep neural nets to produce a program to compete in the 2011 AI Challenge Sponsored by Google. This was an extremely complicated video game, involving controlling the motions of hundreds of ants in real time as they played in novel game boards of up to 200 by 200 for 1000 steps with imperfect information. This may be the most complex domain ever attempted by reinforcement learning. A number of engineering methods are described that allowed us to finish near the top 10% of 9000 hand-coded human submitted entrants. We are extending our methods to produce a strong player of No Limit Texas Holdem."
SARSA Training of Deep Networks in Complex Games,"We SARSA-lambda trained a program based on deep neural nets to produce a program to compete in the 2011 AI Challenge Sponsored by Google. This was an extremely complicated video game, involving controlling the motions of hundreds of ants in real time as they played in novel game boards of up to 200 by 200 for 1000 steps with imperfect information. This may be the most complex domain ever attempted by reinforcement learning. A number of engineering methods are described that allowed us to finish near the top 10% of 9000 hand-coded human submitted entrants. We are extending our methods to produce a strong player of No Limit Texas Holdem."
Probabilistic Low-Rank Subspace Clustering,"In this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we first develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in clustering and identifying outliers.  "
Probabilistic Low-Rank Subspace Clustering,"In this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we first develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in clustering and identifying outliers.  "
Probabilistic Low-Rank Subspace Clustering,"In this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we first develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in clustering and identifying outliers.  "
Density Propagation and Improved Bounds on the Partition Function,"Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds."
Learning to rank for data-driven image segmentation,"We propose to learn a similarity between images to estimate how similar are their segmentation masks. Our similarity is expressed as a dot product in a linear subspace, and we learn the subspace which minimizes the number of triplets for which there is a mismatch between the orderings imposed by our similarity and the true segmentation.By adapting the supervised semantic indexing framework \cite{Bai:09}, we derive a stochastic gradient descent (SGD) algorithm to efficiently learn the similarity. We also explore different strategies to impose large margins. Experiments on two detection scenarios demonstrate that(i) the learned similarity presents an improved ranking-by-segmentation ability; (ii) the fraction of images which are correctly detected just by direct transfer of the nearest-neighbor bounding boxes also increases. This has implications for data-driven segmentation approaches where a set of good neighbors is key to the segmentation algorithm. While existing approaches use appearance-based similarities to select the neighbors, our similarity directly optimizes the selection of good neighbors for segmentation. "
A Probabilistic Model for Joint Active Learning and Model Selection,"In active learning the goal is to train an accurate model using as few activelylabeled samples as possible. Most active learning methods do not perform modelselection because only one model is trained on the actively labeled samples. Wepresent a framework for active learning where multiple models are trained withdifferent regularization parameters. In this framework the labeled data needed formodel selection from these models is part of the total budget of labeled samples.This framework exposes a natural trade-off between the focused active samplingthat usually is most effective for training models, and the unbiased sampling that isdesirable to reliably estimate model accuracy for model selection. We present analgorithm that adds actively labeled samples to either a training set or to a set usedfor model selection, with the goal being to increase the accuracy of the best modelwith as few total samples as possible. We demonstrate the algorithm on three datasets and show that actively sampling both the train and hold out sets yields moreaccurate models with fewer labels than actively sampling the train sets alone."
A Probabilistic Model for Joint Active Learning and Model Selection,"In active learning the goal is to train an accurate model using as few activelylabeled samples as possible. Most active learning methods do not perform modelselection because only one model is trained on the actively labeled samples. Wepresent a framework for active learning where multiple models are trained withdifferent regularization parameters. In this framework the labeled data needed formodel selection from these models is part of the total budget of labeled samples.This framework exposes a natural trade-off between the focused active samplingthat usually is most effective for training models, and the unbiased sampling that isdesirable to reliably estimate model accuracy for model selection. We present analgorithm that adds actively labeled samples to either a training set or to a set usedfor model selection, with the goal being to increase the accuracy of the best modelwith as few total samples as possible. We demonstrate the algorithm on three datasets and show that actively sampling both the train and hold out sets yields moreaccurate models with fewer labels than actively sampling the train sets alone."
A Probabilistic Model for Joint Active Learning and Model Selection,"In active learning the goal is to train an accurate model using as few activelylabeled samples as possible. Most active learning methods do not perform modelselection because only one model is trained on the actively labeled samples. Wepresent a framework for active learning where multiple models are trained withdifferent regularization parameters. In this framework the labeled data needed formodel selection from these models is part of the total budget of labeled samples.This framework exposes a natural trade-off between the focused active samplingthat usually is most effective for training models, and the unbiased sampling that isdesirable to reliably estimate model accuracy for model selection. We present analgorithm that adds actively labeled samples to either a training set or to a set usedfor model selection, with the goal being to increase the accuracy of the best modelwith as few total samples as possible. We demonstrate the algorithm on three datasets and show that actively sampling both the train and hold out sets yields moreaccurate models with fewer labels than actively sampling the train sets alone."
Decoding Finger Flexion from Electrocorticographic Signals with Knowledge Based Prior Model,"Decoding by incorporating domain knowledge about the target variable (body movements in BCI) has been shown to be able to significantly improve the performance \cite{WangSJ11}. In the existing model, training a prior model to capture domain knowledge relies on training samples about the target variable. However, in most real BCI applications, brain signals are only collected under thoughts without actual body movements. Even though training sampels for the target variable are available, the model trained on which tends to be biased and has difficulty to generalize. In this paper, we train prior model by explicitly incorporating the domain knowledge without resorting to the training data. The experiment demonstrates its competitive performance."
Stochastic gradient descent  confers resistance to label noise,"This paper explains why machine learning algorithms using thestochastic gradient descent (SGD) algorithm sometimes generalizebetter than algorithms using other optimization techniques.  Weillustrate our point with artificial data sources on which using SGDwith the SVM objective function generalizes much more accurately thanan algorithm which performs more intensive optimization, over a widevariety of choices of the regularization parameters.  We also reporton some similar effects on natural data."
Stochastic gradient descent  confers resistance to label noise,"This paper explains why machine learning algorithms using thestochastic gradient descent (SGD) algorithm sometimes generalizebetter than algorithms using other optimization techniques.  Weillustrate our point with artificial data sources on which using SGDwith the SVM objective function generalizes much more accurately thanan algorithm which performs more intensive optimization, over a widevariety of choices of the regularization parameters.  We also reporton some similar effects on natural data."
Stochastic gradient descent  confers resistance to label noise,"This paper explains why machine learning algorithms using thestochastic gradient descent (SGD) algorithm sometimes generalizebetter than algorithms using other optimization techniques.  Weillustrate our point with artificial data sources on which using SGDwith the SVM objective function generalizes much more accurately thanan algorithm which performs more intensive optimization, over a widevariety of choices of the regularization parameters.  We also reporton some similar effects on natural data."
Distributed large-scale natural graph factorization,"More and more natural graphs arise in numerous Web applications, such as socialnetworks, email, and instant messaging. Such large-scalegraphs today easily contain hundreds of millions of nodes and billions of edges,and are expected to grow even larger in the future. While severaltheoretical models have been proposed for such networks, their analysisis still difficult due to their scale and nature. Inthis paper we propose a distributed framework for large-scale graph factorizationthat allows for easier analysis and more compact representation oflarge natural graphs. "
Distributed large-scale natural graph factorization,"More and more natural graphs arise in numerous Web applications, such as socialnetworks, email, and instant messaging. Such large-scalegraphs today easily contain hundreds of millions of nodes and billions of edges,and are expected to grow even larger in the future. While severaltheoretical models have been proposed for such networks, their analysisis still difficult due to their scale and nature. Inthis paper we propose a distributed framework for large-scale graph factorizationthat allows for easier analysis and more compact representation oflarge natural graphs. "
Distributed large-scale natural graph factorization,"More and more natural graphs arise in numerous Web applications, such as socialnetworks, email, and instant messaging. Such large-scalegraphs today easily contain hundreds of millions of nodes and billions of edges,and are expected to grow even larger in the future. While severaltheoretical models have been proposed for such networks, their analysisis still difficult due to their scale and nature. Inthis paper we propose a distributed framework for large-scale graph factorizationthat allows for easier analysis and more compact representation oflarge natural graphs. "
Distributed large-scale natural graph factorization,"More and more natural graphs arise in numerous Web applications, such as socialnetworks, email, and instant messaging. Such large-scalegraphs today easily contain hundreds of millions of nodes and billions of edges,and are expected to grow even larger in the future. While severaltheoretical models have been proposed for such networks, their analysisis still difficult due to their scale and nature. Inthis paper we propose a distributed framework for large-scale graph factorizationthat allows for easier analysis and more compact representation oflarge natural graphs. "
Perceptron Learning of SAT,"Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science.  In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem.  In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space.  Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm.  Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates.  We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT.  Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task."
Perceptron Learning of SAT,"Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science.  In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem.  In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space.  Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm.  Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates.  We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT.  Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task."
Learning Networks of Heterogeneous Influence,"Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities. "
Expectation maximization for average reward decentralized POMDPs,"Planning for multiple agents under uncertainty is an important task where proposed solutions are often based on decentralized partially observable Markov decision processes (DEC-POMDPs). In current DEC-POMDP approaches, long-term effects of actions need to be de-emphasized by a discount factor. However, in real-life problems such as wireless networking, the agents (wireless devices) will be evaluated by their average performance over time, hence both short-term and long-term effects of actions are important and solutions based on discounting can perform poorly. We introduce a new DEC-POMDP method that optimizes average reward, based on a modified expectation-maximization approach. The method yields improved performance in benchmark problems compared to a state of the art discounted-reward DEC-POMDP approach."
Expectation maximization for average reward decentralized POMDPs,"Planning for multiple agents under uncertainty is an important task where proposed solutions are often based on decentralized partially observable Markov decision processes (DEC-POMDPs). In current DEC-POMDP approaches, long-term effects of actions need to be de-emphasized by a discount factor. However, in real-life problems such as wireless networking, the agents (wireless devices) will be evaluated by their average performance over time, hence both short-term and long-term effects of actions are important and solutions based on discounting can perform poorly. We introduce a new DEC-POMDP method that optimizes average reward, based on a modified expectation-maximization approach. The method yields improved performance in benchmark problems compared to a state of the art discounted-reward DEC-POMDP approach."
Scalable Matrix-valued Kernel Learning and  High-dimensional Nonlinear Causal Inference,"We propose a general matrix-valued multiple kernel learning framework for high-dimensional nonlinear multivariate regression problems. This framework allows a broad class of mixed norm regularizers, including those that induce sparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel Hilbert Spaces~\cite{MichelliPontil05} induced by a collection of separable kernels. We develop a highly scalable and eigendecomposition-free Block coordinate descent procedure that orchestrates two inexact solvers: a Conjugate Gradient (CG) based Sylvester equation solver for solving vector-valued Regularized Least Squares (RLS) problems, and a specialized Sparse approximate SDP solver~\cite{HazanSDP} for learning output kernels. We show how high-dimensional causal inference tasks can be naturally cast as sparse function estimation problems within our framework, leading to novel nonlinear extensions of Grouped Graphical Granger Causality techniques. The algorithmic developments and extensive empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds."
Scalable Matrix-valued Kernel Learning and  High-dimensional Nonlinear Causal Inference,"We propose a general matrix-valued multiple kernel learning framework for high-dimensional nonlinear multivariate regression problems. This framework allows a broad class of mixed norm regularizers, including those that induce sparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel Hilbert Spaces~\cite{MichelliPontil05} induced by a collection of separable kernels. We develop a highly scalable and eigendecomposition-free Block coordinate descent procedure that orchestrates two inexact solvers: a Conjugate Gradient (CG) based Sylvester equation solver for solving vector-valued Regularized Least Squares (RLS) problems, and a specialized Sparse approximate SDP solver~\cite{HazanSDP} for learning output kernels. We show how high-dimensional causal inference tasks can be naturally cast as sparse function estimation problems within our framework, leading to novel nonlinear extensions of Grouped Graphical Granger Causality techniques. The algorithmic developments and extensive empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds."
FastEx: Fast Clustering with Exponential Families," Clustering is a key component in data analysis toolbox. Despite its  importance, scalable algorithms often eschew rich statistical models  in favor of simpler descriptions such as $k$-means clustering. In  this paper we present a sampler, capable of estimating  mixtures of exponential families. At its heart lies a novel proposal distribution using random  projections to achieve high throughput in generating proposals, which is crucial  for clustering models with large numbers of clusters. "
FastEx: Fast Clustering with Exponential Families," Clustering is a key component in data analysis toolbox. Despite its  importance, scalable algorithms often eschew rich statistical models  in favor of simpler descriptions such as $k$-means clustering. In  this paper we present a sampler, capable of estimating  mixtures of exponential families. At its heart lies a novel proposal distribution using random  projections to achieve high throughput in generating proposals, which is crucial  for clustering models with large numbers of clusters. "
FastEx: Fast Clustering with Exponential Families," Clustering is a key component in data analysis toolbox. Despite its  importance, scalable algorithms often eschew rich statistical models  in favor of simpler descriptions such as $k$-means clustering. In  this paper we present a sampler, capable of estimating  mixtures of exponential families. At its heart lies a novel proposal distribution using random  projections to achieve high throughput in generating proposals, which is crucial  for clustering models with large numbers of clusters. "
A Latent Multi-Frame Image Model,"We consider the general problem of understanding the latent structures of videosequences which encode the motion patterns, spatial-temporal relationships andalso the appearance models of objects. Beyond previous models for video segmentation,we learn an appearance representation of the discovered segments acrossvideos. This is achieved by allowing video segments to share global appearancemodels but possibly having different positions and motion in the different videovolume. We propose an unsupervised framework to automatically infer the hierarchicalmodel of video segments and shared appearance models that also relate toobjects moving in the video. The model is a flexible extension of the HierarchicalDirichlet Process and it can be easily applied to different applications. We showa quantitative evaluation on a recent video segmentation task as well as exemplifyother use cases by demonstrating the application to video indexing. How videoscan be retrieved by a sketch that can be matched to the latent video structure.At time of publication we will release the code of our extendible, probabilisticframework for video analysis and segmentation."
A Latent Multi-Frame Image Model,"We consider the general problem of understanding the latent structures of videosequences which encode the motion patterns, spatial-temporal relationships andalso the appearance models of objects. Beyond previous models for video segmentation,we learn an appearance representation of the discovered segments acrossvideos. This is achieved by allowing video segments to share global appearancemodels but possibly having different positions and motion in the different videovolume. We propose an unsupervised framework to automatically infer the hierarchicalmodel of video segments and shared appearance models that also relate toobjects moving in the video. The model is a flexible extension of the HierarchicalDirichlet Process and it can be easily applied to different applications. We showa quantitative evaluation on a recent video segmentation task as well as exemplifyother use cases by demonstrating the application to video indexing. How videoscan be retrieved by a sketch that can be matched to the latent video structure.At time of publication we will release the code of our extendible, probabilisticframework for video analysis and segmentation."
Using Both Supervised and Latent Shared Topics for Multitask Learning,"Since its introduction, Latent Dirichlet Allocation (LDA) has been extended to include two different types ofdocument-level supervision: topic labels and category labels. We introduce a new framework, Doubly SupervisedLatent Dirichlet Allocation (DSLDA), that integrates both types of supervision. We demonstrate thatthis approach is particularly useful for multitask learning, in which both supervised and latent (unsupervised)topics are shared between multiple categories. Experimental results on document classification show thatboth types of supervision improve the performance of DSLDA and that sharing both latent and supervisedtopics allows for better multitask learning."
A tree-decomposed EM algorithm for covariance selection in noisy graphical models,"Gaussian graphical models (GGMs) are widely used in computer science, and have also enjoyed wide applicability in a number of scientific areas. We consider the problem of covariance selection, i.e. estimation of the (inverse) covariance matrix of the joint probability distribution of random variables on a high dimensional graph. To extend the applicability of GGMs, we consider the case where observations for variables are also subject to additional measurement noise. Unfortunately, the the estimation of model parameters in this setting becomes complicated by the fact that the structure of the underlying graph no longer provides direct information about the location of zeros in the inverse covariance matrix. We propose an efficient EM algorithm which uses the tree decomposition of the underlying graph in order to perform the parameter estimation through local operations. We also explore the effect of the treelike structure of the graph on computational performance of the algorithm as well as the accuracy of the estimates by applying it to a wide range of random graph models."
A tree-decomposed EM algorithm for covariance selection in noisy graphical models,"Gaussian graphical models (GGMs) are widely used in computer science, and have also enjoyed wide applicability in a number of scientific areas. We consider the problem of covariance selection, i.e. estimation of the (inverse) covariance matrix of the joint probability distribution of random variables on a high dimensional graph. To extend the applicability of GGMs, we consider the case where observations for variables are also subject to additional measurement noise. Unfortunately, the the estimation of model parameters in this setting becomes complicated by the fact that the structure of the underlying graph no longer provides direct information about the location of zeros in the inverse covariance matrix. We propose an efficient EM algorithm which uses the tree decomposition of the underlying graph in order to perform the parameter estimation through local operations. We also explore the effect of the treelike structure of the graph on computational performance of the algorithm as well as the accuracy of the estimates by applying it to a wide range of random graph models."
Max-Product Particle Belief Propagation,"Belief Propagation (BP) is a popular message passing algorithm for inference in factored probabilistic graphical models. Sum-Product BP computes marginal distributions for node variables, while Max-Product BP outputs a solution that maximizes the joint distribution of all variables, and is used for MAP (maximum a posteriori) inference.BP is commonly applied to problems that assume a discrete (or discretized) state space. When the state space of a variables cannot be enumerated in practice, and the messages cannot be computed in closed form, methods based on sampling are considered. Several nonparametric Sum-Product approximations exist, however many inference problems of scientific interest require MAP inference for high-dimensional, continuous and multimodal distributed random variables, calling for nonparametric algorithms for Max-Product BP. In this paper we formulate a Max-Product version of the PBP algorithm, and analyze its behavior in performing inference for a model with continuous variables that do not easily admit a discrete representation. "
Max-Product Particle Belief Propagation,"Belief Propagation (BP) is a popular message passing algorithm for inference in factored probabilistic graphical models. Sum-Product BP computes marginal distributions for node variables, while Max-Product BP outputs a solution that maximizes the joint distribution of all variables, and is used for MAP (maximum a posteriori) inference.BP is commonly applied to problems that assume a discrete (or discretized) state space. When the state space of a variables cannot be enumerated in practice, and the messages cannot be computed in closed form, methods based on sampling are considered. Several nonparametric Sum-Product approximations exist, however many inference problems of scientific interest require MAP inference for high-dimensional, continuous and multimodal distributed random variables, calling for nonparametric algorithms for Max-Product BP. In this paper we formulate a Max-Product version of the PBP algorithm, and analyze its behavior in performing inference for a model with continuous variables that do not easily admit a discrete representation. "
Topic-Partitioned Multinetwork Embeddings,"We introduce a joint model of network content and context designed forexploratory analysis of email networks via visualization oftopic-specific communication patterns. Our model is an admixture modelfor text and network attributes which uses multinomial distributionsover words as mixture components for explaining text and latentEuclidean positions of actors as mixture components for explainingnetwork attributes.  We validate the appropriateness of our model byachieving state-of-the-art performance on a link prediction task andby achieving semantic coherence equivalent to that of latent Dirichletallocation. We demonstrate the capability of our model fordescriptive, explanatory, and exploratory analysis by investigatingthe inferred topic-specific communication patterns of a new governmentemail dataset, the New Hanover County email corpus."
Topic-Partitioned Multinetwork Embeddings,"We introduce a joint model of network content and context designed forexploratory analysis of email networks via visualization oftopic-specific communication patterns. Our model is an admixture modelfor text and network attributes which uses multinomial distributionsover words as mixture components for explaining text and latentEuclidean positions of actors as mixture components for explainingnetwork attributes.  We validate the appropriateness of our model byachieving state-of-the-art performance on a link prediction task andby achieving semantic coherence equivalent to that of latent Dirichletallocation. We demonstrate the capability of our model fordescriptive, explanatory, and exploratory analysis by investigatingthe inferred topic-specific communication patterns of a new governmentemail dataset, the New Hanover County email corpus."
Topic-Partitioned Multinetwork Embeddings,"We introduce a joint model of network content and context designed forexploratory analysis of email networks via visualization oftopic-specific communication patterns. Our model is an admixture modelfor text and network attributes which uses multinomial distributionsover words as mixture components for explaining text and latentEuclidean positions of actors as mixture components for explainingnetwork attributes.  We validate the appropriateness of our model byachieving state-of-the-art performance on a link prediction task andby achieving semantic coherence equivalent to that of latent Dirichletallocation. We demonstrate the capability of our model fordescriptive, explanatory, and exploratory analysis by investigatingthe inferred topic-specific communication patterns of a new governmentemail dataset, the New Hanover County email corpus."
Topic-Partitioned Multinetwork Embeddings,"We introduce a joint model of network content and context designed forexploratory analysis of email networks via visualization oftopic-specific communication patterns. Our model is an admixture modelfor text and network attributes which uses multinomial distributionsover words as mixture components for explaining text and latentEuclidean positions of actors as mixture components for explainingnetwork attributes.  We validate the appropriateness of our model byachieving state-of-the-art performance on a link prediction task andby achieving semantic coherence equivalent to that of latent Dirichletallocation. We demonstrate the capability of our model fordescriptive, explanatory, and exploratory analysis by investigatingthe inferred topic-specific communication patterns of a new governmentemail dataset, the New Hanover County email corpus."
Learning Label Trees for Probabilistic Modelling of Implicit Feedback,"User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. In the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data."
Learning Label Trees for Probabilistic Modelling of Implicit Feedback,"User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. In the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data."
Learning with Recursive Perceptual Representations,"Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks."
Learning with Recursive Perceptual Representations,"Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks."
Link Prediction in Graphs with Autoregressive Features,"In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm."
Link Prediction in Graphs with Autoregressive Features,"In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm."
Link Prediction in Graphs with Autoregressive Features,"In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm."
Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images,"We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity.To segment {\em biological} neuron membranes, we use a special type of deep {\em artificial} neural network as a pixel classifier.The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it.The input layer maps each window pixel to a neuron. It is followed by a succession ofconvolutional and max-pooling layers which preserve 2D information and extract features withincreasing levels of abstraction. The output layer produces a calibrated probability for each class.The classifier is trained by plain gradientdescent on a $512 \times 512 \times 30$ stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \emph{rand error}, \emph{warping error} and \emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer."
Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images,"We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity.To segment {\em biological} neuron membranes, we use a special type of deep {\em artificial} neural network as a pixel classifier.The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it.The input layer maps each window pixel to a neuron. It is followed by a succession ofconvolutional and max-pooling layers which preserve 2D information and extract features withincreasing levels of abstraction. The output layer produces a calibrated probability for each class.The classifier is trained by plain gradientdescent on a $512 \times 512 \times 30$ stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \emph{rand error}, \emph{warping error} and \emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer."
Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images,"We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity.To segment {\em biological} neuron membranes, we use a special type of deep {\em artificial} neural network as a pixel classifier.The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it.The input layer maps each window pixel to a neuron. It is followed by a succession ofconvolutional and max-pooling layers which preserve 2D information and extract features withincreasing levels of abstraction. The output layer produces a calibrated probability for each class.The classifier is trained by plain gradientdescent on a $512 \times 512 \times 30$ stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \emph{rand error}, \emph{warping error} and \emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer."
Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images,"We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity.To segment {\em biological} neuron membranes, we use a special type of deep {\em artificial} neural network as a pixel classifier.The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it.The input layer maps each window pixel to a neuron. It is followed by a succession ofconvolutional and max-pooling layers which preserve 2D information and extract features withincreasing levels of abstraction. The output layer produces a calibrated probability for each class.The classifier is trained by plain gradientdescent on a $512 \times 512 \times 30$ stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \emph{rand error}, \emph{warping error} and \emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer."
Scalable imputation of genetic data with a discrete fragmentation-coagulation process,"We present a Bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a Markov model of  partitions. The partitions at consecutive locations in the genome are related by their clusters first splitting and then merging.  Our model can be thought of as a discrete time analogue of continuous time fragmentation-coagulation processes [Teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable. We apply this model to the problem of genotype imputation, showing improved computational efficiency while maintaining the same accuracies as in [Teh et al 2011]."
Scalable imputation of genetic data with a discrete fragmentation-coagulation process,"We present a Bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a Markov model of  partitions. The partitions at consecutive locations in the genome are related by their clusters first splitting and then merging.  Our model can be thought of as a discrete time analogue of continuous time fragmentation-coagulation processes [Teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable. We apply this model to the problem of genotype imputation, showing improved computational efficiency while maintaining the same accuracies as in [Teh et al 2011]."
An MDL/Bayesian Approach without Assuming either Discrete or Continuous,"In the minimum description length (MDL) and Bayesian criteria, we construct description length  of data $z^n=z_1\cdots z_n$ of length $n$ such that the length divided by $n$ almost converges to its entropy rate as $n\rightarrow \infty$, assuming $z_i$ is in a finite set $A$. In model selection, if we knew the true probability $P$ of $z^n\in A^n$, we would choose a model $F$ such that the posterior probability of $F$ given $z^n$ is maximized. But, in many situations, we use $Q:A^n\rightarrow [0,1]$ such that $\sum_{z^n\in A^n}Q(z^n)\leq 1$ rather than $P$ because only data $z^n$ are available. In this paper, we consider an extension such that each of the attributes in data can be either discrete or continuous. The main issue is what $Q$ is qualified to be an alternative to $P$ in the generalized situations. We propose the condition in terms of the Radon-Nikodym derivative of $P$ with respect to $Q$, and give the procedure of constructing $Q$ in the general setting. As a result, we obtain the MDL/Bayesian criteria in a general sense. Numerical experiments demonstrate that the novel algorithm works efficiently enough to deal with many practical estimations."
Noise Never Helps ? Revisited !,"Compensating changes between a subjects? training and feedback sessions in Brain Computer Interfacing is challenging but of great importance for a robust BCI operation. We contribute by noting that such individual changes can be reliably estimated using data from other subjects. Surprisingly it is the non-discriminative ?noise? signal subspace that can aid to construct features invariant to the change. This is in contrast to e.g. averaging the covariance matrices or construction of a common feature space between users. Notably, the prominent directions of change are very similar between subjects in the noise subspaces, whereas in the most discriminative directions they are not. Our noise harvesting method compares favourably to other state-of-the-art methods on toy data and EEG recordings from five subjects performing motor imagery. We show that not only a significant increase in performance can be achieved, but also that the changes observed in the non-discriminative noise subspace allow for a neurophysiologically meaningful interpretation."
Noise Never Helps ? Revisited !,"Compensating changes between a subjects? training and feedback sessions in Brain Computer Interfacing is challenging but of great importance for a robust BCI operation. We contribute by noting that such individual changes can be reliably estimated using data from other subjects. Surprisingly it is the non-discriminative ?noise? signal subspace that can aid to construct features invariant to the change. This is in contrast to e.g. averaging the covariance matrices or construction of a common feature space between users. Notably, the prominent directions of change are very similar between subjects in the noise subspaces, whereas in the most discriminative directions they are not. Our noise harvesting method compares favourably to other state-of-the-art methods on toy data and EEG recordings from five subjects performing motor imagery. We show that not only a significant increase in performance can be achieved, but also that the changes observed in the non-discriminative noise subspace allow for a neurophysiologically meaningful interpretation."
Gradient Weights help Nonparametric Regressors,"In regression problems over $\real^d$, the unknown function $f$ often varies more in some coordinates than in others.We show that weighting each coordinate $i$ with the estimated norm of the $i$th derivative of $f$ is an efficient way to significantly improve the performance of distance-based regressors, e.g. kernel and $k$-NN regressors. We propose a simple estimator of these derivative norms and prove its consistency. Moreover, the proposed estimator is efficiently learned online. "
Gradient Weights help Nonparametric Regressors,"In regression problems over $\real^d$, the unknown function $f$ often varies more in some coordinates than in others.We show that weighting each coordinate $i$ with the estimated norm of the $i$th derivative of $f$ is an efficient way to significantly improve the performance of distance-based regressors, e.g. kernel and $k$-NN regressors. We propose a simple estimator of these derivative norms and prove its consistency. Moreover, the proposed estimator is efficiently learned online. "
Monte Carlo Tree Search Using Goal-Directed and Constraint-Biased Action Abstractions,"Monte Carlo Tree Search (MCTS) is a family of methods for planning in large domains. It focuses on finding a good action for a particular state, making its complexity independent of the size of the state space. Effective application of MCTS requires both a good pruning heuristic and a roll-out policy. In this paper we leverage the human tendency to decompose tasks into 1) goal-directed action sequences and 2) sets of constraints to generate stochastic policies that facilitateefficient exploration in MCTS.We demonstrate the efficacy of our approach in the PacMan domain and highlight its advantages over traditional MCTS."
Monte Carlo Tree Search Using Goal-Directed and Constraint-Biased Action Abstractions,"Monte Carlo Tree Search (MCTS) is a family of methods for planning in large domains. It focuses on finding a good action for a particular state, making its complexity independent of the size of the state space. Effective application of MCTS requires both a good pruning heuristic and a roll-out policy. In this paper we leverage the human tendency to decompose tasks into 1) goal-directed action sequences and 2) sets of constraints to generate stochastic policies that facilitateefficient exploration in MCTS.We demonstrate the efficacy of our approach in the PacMan domain and highlight its advantages over traditional MCTS."
Tree Learning Strategies for Large-Scale Taxonomies,"Standard linear models for multi-class categorization have a decision-time complexity which is linear in the number of categories, whereas approximating them by a tree-based sequence of decisions can reducethe prediction time to the logarithm of the number of categories. In this paper, we review several heuristics to build the best hierarchical taxonomyand propose a novel tree-learning approach by formulating the problem asa sequence of max-cut problems where the categories are split intosubcategories in a top-down fashion. We provide an empirical comparison on five different tree-building approaches on multiple datasets,showing that the previous approaches to learn the tree can significantly failif one is interested in the predictive log-likelihoods or on theaverage classification accuracies of the classifiers. "
Tree Learning Strategies for Large-Scale Taxonomies,"Standard linear models for multi-class categorization have a decision-time complexity which is linear in the number of categories, whereas approximating them by a tree-based sequence of decisions can reducethe prediction time to the logarithm of the number of categories. In this paper, we review several heuristics to build the best hierarchical taxonomyand propose a novel tree-learning approach by formulating the problem asa sequence of max-cut problems where the categories are split intosubcategories in a top-down fashion. We provide an empirical comparison on five different tree-building approaches on multiple datasets,showing that the previous approaches to learn the tree can significantly failif one is interested in the predictive log-likelihoods or on theaverage classification accuracies of the classifiers. "
Tree Learning Strategies for Large-Scale Taxonomies,"Standard linear models for multi-class categorization have a decision-time complexity which is linear in the number of categories, whereas approximating them by a tree-based sequence of decisions can reducethe prediction time to the logarithm of the number of categories. In this paper, we review several heuristics to build the best hierarchical taxonomyand propose a novel tree-learning approach by formulating the problem asa sequence of max-cut problems where the categories are split intosubcategories in a top-down fashion. We provide an empirical comparison on five different tree-building approaches on multiple datasets,showing that the previous approaches to learn the tree can significantly failif one is interested in the predictive log-likelihoods or on theaverage classification accuracies of the classifiers. "
Tree Learning Strategies for Large-Scale Taxonomies,"Standard linear models for multi-class categorization have a decision-time complexity which is linear in the number of categories, whereas approximating them by a tree-based sequence of decisions can reducethe prediction time to the logarithm of the number of categories. In this paper, we review several heuristics to build the best hierarchical taxonomyand propose a novel tree-learning approach by formulating the problem asa sequence of max-cut problems where the categories are split intosubcategories in a top-down fashion. We provide an empirical comparison on five different tree-building approaches on multiple datasets,showing that the previous approaches to learn the tree can significantly failif one is interested in the predictive log-likelihoods or on theaverage classification accuracies of the classifiers. "
Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks,"Many social network datasets consist of a sequence of relational observations through time.  Latent variable models for these networks are popular Bayesian approaches to handle our uncertainty over the unobserved dynamics in the network and how these dynamics influence the observed data.  However, current approaches in this Bayesian modeling framework have not been able to capture the reciprocal influence of past observations on future latent representations.  In this paper, we introduce a new probabilistic model for dynamic social network data based on a semi-hidden Markov model, which assumes that the evolution of latent features are influenced by the local topology of the network in past observations.  We show that the particular form of our model is very interpretable in the context of social networks and describe how it captures a phenomenon which we call latent feature propagation.  We present a Markov-Chain Monte-Carlo inference procedure and experimentally show that our model improves upon current methods on link pediction performance using synthetic and real datasets of social networks."
Behaviorally decoding search targets from gaze fixations,"Using a technique that we refer to as behavioral decoding, we demonstrate that the information available in the fixation behavior of subjects is often sufficient todecode the category of their search target?essentially reading a person?s mind by analyzing what they look at. One group of subjects searched for teddy bear targets among random category distractors, another group searched for butterflies among the same distractors. Two SVM-based classifiers trained to recognize teddy bears and butterflies were then used to classify the distractors that were preferentially fixated by subjects during search as either teddy bears or butterflies, based on their distance from the SVM decision boundary. Two methods of preferential fixationwere explored, the object first fixated during search and the object fixated the longest. Using the longest-fixation method, we found that the target of a person?s search could be decoded perfectly when one of the distractors were rated as being visually similar to the target category. Even with completely random distractors, the target category could still be decoded for 75-80% of the subjects. The much harder task of decoding the target on individual trials (from a single object fixation) resulted in much lower classification rates, although targets were stilldecoded above chance. These findings have implications for the visual similarity relationships underlying search guidance and distractor rejection, and demonstratethe feasibility in using these relationships to decode a person?s task or goal."
Behaviorally decoding search targets from gaze fixations,"Using a technique that we refer to as behavioral decoding, we demonstrate that the information available in the fixation behavior of subjects is often sufficient todecode the category of their search target?essentially reading a person?s mind by analyzing what they look at. One group of subjects searched for teddy bear targets among random category distractors, another group searched for butterflies among the same distractors. Two SVM-based classifiers trained to recognize teddy bears and butterflies were then used to classify the distractors that were preferentially fixated by subjects during search as either teddy bears or butterflies, based on their distance from the SVM decision boundary. Two methods of preferential fixationwere explored, the object first fixated during search and the object fixated the longest. Using the longest-fixation method, we found that the target of a person?s search could be decoded perfectly when one of the distractors were rated as being visually similar to the target category. Even with completely random distractors, the target category could still be decoded for 75-80% of the subjects. The much harder task of decoding the target on individual trials (from a single object fixation) resulted in much lower classification rates, although targets were stilldecoded above chance. These findings have implications for the visual similarity relationships underlying search guidance and distractor rejection, and demonstratethe feasibility in using these relationships to decode a person?s task or goal."
Behaviorally decoding search targets from gaze fixations,"Using a technique that we refer to as behavioral decoding, we demonstrate that the information available in the fixation behavior of subjects is often sufficient todecode the category of their search target?essentially reading a person?s mind by analyzing what they look at. One group of subjects searched for teddy bear targets among random category distractors, another group searched for butterflies among the same distractors. Two SVM-based classifiers trained to recognize teddy bears and butterflies were then used to classify the distractors that were preferentially fixated by subjects during search as either teddy bears or butterflies, based on their distance from the SVM decision boundary. Two methods of preferential fixationwere explored, the object first fixated during search and the object fixated the longest. Using the longest-fixation method, we found that the target of a person?s search could be decoded perfectly when one of the distractors were rated as being visually similar to the target category. Even with completely random distractors, the target category could still be decoded for 75-80% of the subjects. The much harder task of decoding the target on individual trials (from a single object fixation) resulted in much lower classification rates, although targets were stilldecoded above chance. These findings have implications for the visual similarity relationships underlying search guidance and distractor rejection, and demonstratethe feasibility in using these relationships to decode a person?s task or goal."
Overlapping Decomposition for High-Order Directed Graphical Modeling,"We propose to estimate the dependence structure in high-order directed graph by decomposing it into subgraphswith overlaps. We first introduce a lasso type model to formulate this problem, and further transfer it into estimating a set of group variable selection problems.Specifically, we establish a generic hierarchical lasso method for the estimation, where scalable norms are employed for controlling the structure of subgraphs flexibly. The asymptotic properties of the proposed method are discussed with detailed analysis. We also develop an efficient algorithm to compute such model. Finally, we evaluate our model on both synthetic data and real traffic data."
Overlapping Decomposition for High-Order Directed Graphical Modeling,"We propose to estimate the dependence structure in high-order directed graph by decomposing it into subgraphswith overlaps. We first introduce a lasso type model to formulate this problem, and further transfer it into estimating a set of group variable selection problems.Specifically, we establish a generic hierarchical lasso method for the estimation, where scalable norms are employed for controlling the structure of subgraphs flexibly. The asymptotic properties of the proposed method are discussed with detailed analysis. We also develop an efficient algorithm to compute such model. Finally, we evaluate our model on both synthetic data and real traffic data."
Overlapping Decomposition for High-Order Directed Graphical Modeling,"We propose to estimate the dependence structure in high-order directed graph by decomposing it into subgraphswith overlaps. We first introduce a lasso type model to formulate this problem, and further transfer it into estimating a set of group variable selection problems.Specifically, we establish a generic hierarchical lasso method for the estimation, where scalable norms are employed for controlling the structure of subgraphs flexibly. The asymptotic properties of the proposed method are discussed with detailed analysis. We also develop an efficient algorithm to compute such model. Finally, we evaluate our model on both synthetic data and real traffic data."
Memory-based Pipelined Hardware Architecture for Communication-free Neural Computation,"Communication has an important impact on the performance of neural simulation systems. In this paper, we propose a neurocomputing architecture in which synapses are computed in parallel, and communication between neurons is carried out simply by accessing memories. In the proposed architecture, a large set of memories produce a wide stream of data for which large-scale pipelining is can be obtained. We also describe a method for translating functional specifications of computations into fine-grained pipelined circuits. Furthermore, we present the design of a simulator for spiking neural networks (SNNs), in order to show that the proposed architecture can be used to build simulators supporting various neural models. Without using the low activation property of SNNs, the performance of our system is comparable to that of event-driven systems."
Flexible Temporal Structure Learning,"This paper extends the recently introduced structure learning methods for Gaussian random fields and ``nonparanormal'' distributions to a multivariate non-Gaussian time series setting. Our approach is based on discriminative state-space models, and introduces sparsity constraints on dependence structures of multivariate outcomes, as well as other parameters of emission and transition distributions. This combines feature selection with time series modelling, giving rise to explainable representations of data and dependence structures at each latent state. In contrast to the recent literature on sparse graphical models, our approach allows for an easy integration of multi-modality and input variables. We apply our method to multivariate financial time series data. We show that it helps to uncover meaningful dependencies between stock prices and significantly outperforms common approaches for predicting share prices, offering potential for real applications."
Flexible Temporal Structure Learning,"This paper extends the recently introduced structure learning methods for Gaussian random fields and ``nonparanormal'' distributions to a multivariate non-Gaussian time series setting. Our approach is based on discriminative state-space models, and introduces sparsity constraints on dependence structures of multivariate outcomes, as well as other parameters of emission and transition distributions. This combines feature selection with time series modelling, giving rise to explainable representations of data and dependence structures at each latent state. In contrast to the recent literature on sparse graphical models, our approach allows for an easy integration of multi-modality and input variables. We apply our method to multivariate financial time series data. We show that it helps to uncover meaningful dependencies between stock prices and significantly outperforms common approaches for predicting share prices, offering potential for real applications."
Dimensionality reduction for data visualisation using Taylor network,"It is well known that any continuous derivable function can be expanded to a Taylor series and an artificial neural network is used to approximate an unknown function, so a neural network is theoretically equivalent to a polynomial. In this paper we prose a structure of such a polynomial that can be trained quickly as an alternative to  traditional artificial neural network. We test it by applying on to data dimensionality reduction such as Sammon's mapping as well as its new extensions on both synthetic and real world data sets."
Dimensionality reduction for data visualisation using Taylor network,"It is well known that any continuous derivable function can be expanded to a Taylor series and an artificial neural network is used to approximate an unknown function, so a neural network is theoretically equivalent to a polynomial. In this paper we prose a structure of such a polynomial that can be trained quickly as an alternative to  traditional artificial neural network. We test it by applying on to data dimensionality reduction such as Sammon's mapping as well as its new extensions on both synthetic and real world data sets."
Dimensionality reduction for data visualisation using Taylor network,"It is well known that any continuous derivable function can be expanded to a Taylor series and an artificial neural network is used to approximate an unknown function, so a neural network is theoretically equivalent to a polynomial. In this paper we prose a structure of such a polynomial that can be trained quickly as an alternative to  traditional artificial neural network. We test it by applying on to data dimensionality reduction such as Sammon's mapping as well as its new extensions on both synthetic and real world data sets."
Online Sum-Product Computation,"We consider the problem of performing efficient sum-product computations in an online setting over a tree.  A natural application of our methods is to compute the marginal distribution at a vertex in a tree-structured Markov random field.  Belief propagation can be used to solve this problem.  However, belief propagation requires time linear in the size of the tree.  This is too slow in an online setting where we are continuously receiving new data and computing individual marginals.  With our method we aim to update the data and compute marginals in time that is no more than logarithmic in the size of the tree, and is often significantly less.  We accomplish this via a hierarchical covering structure that caches previous local sum-product computations.  Our contribution is three-fold: we i) give a linear time algorithm to find an optimal hierarchical cover of a tree; ii) give a sum-product-like algorithm to efficiently compute marginals with respect to this cover; and iii) apply ``i'' and ``ii'' to find an efficient algorithm with a regret bound for the online {\em allocation} problem in a multi-task setting."
Online Sum-Product Computation,"We consider the problem of performing efficient sum-product computations in an online setting over a tree.  A natural application of our methods is to compute the marginal distribution at a vertex in a tree-structured Markov random field.  Belief propagation can be used to solve this problem.  However, belief propagation requires time linear in the size of the tree.  This is too slow in an online setting where we are continuously receiving new data and computing individual marginals.  With our method we aim to update the data and compute marginals in time that is no more than logarithmic in the size of the tree, and is often significantly less.  We accomplish this via a hierarchical covering structure that caches previous local sum-product computations.  Our contribution is three-fold: we i) give a linear time algorithm to find an optimal hierarchical cover of a tree; ii) give a sum-product-like algorithm to efficiently compute marginals with respect to this cover; and iii) apply ``i'' and ``ii'' to find an efficient algorithm with a regret bound for the online {\em allocation} problem in a multi-task setting."
Online Sum-Product Computation,"We consider the problem of performing efficient sum-product computations in an online setting over a tree.  A natural application of our methods is to compute the marginal distribution at a vertex in a tree-structured Markov random field.  Belief propagation can be used to solve this problem.  However, belief propagation requires time linear in the size of the tree.  This is too slow in an online setting where we are continuously receiving new data and computing individual marginals.  With our method we aim to update the data and compute marginals in time that is no more than logarithmic in the size of the tree, and is often significantly less.  We accomplish this via a hierarchical covering structure that caches previous local sum-product computations.  Our contribution is three-fold: we i) give a linear time algorithm to find an optimal hierarchical cover of a tree; ii) give a sum-product-like algorithm to efficiently compute marginals with respect to this cover; and iii) apply ``i'' and ``ii'' to find an efficient algorithm with a regret bound for the online {\em allocation} problem in a multi-task setting."
Adaptive Methods for Online Learning with Kernels,"In online convex optimization, adaptive algorithms, which can utilize the second-order information of the lossfunction's (sub)gradient, have shown improvements over standard gradient methods. However, existing adaptive algorithms mainly consider linear models, and are often designed only for classification problems. In this paper,we first provide a general framework that unifies various existing adaptive algorithms. Based on this, new adaptive algorithms can be easily derived.  Next, we show that adaptive learning can be generalized to nonlinear models byusing the kernel trick in a computationally efficient manner. Regret guarantee of the proposed methods are analyzed, and experiments on various benchmark data sets demonstrate their outstanding performance."
Adaptive Methods for Online Learning with Kernels,"In online convex optimization, adaptive algorithms, which can utilize the second-order information of the lossfunction's (sub)gradient, have shown improvements over standard gradient methods. However, existing adaptive algorithms mainly consider linear models, and are often designed only for classification problems. In this paper,we first provide a general framework that unifies various existing adaptive algorithms. Based on this, new adaptive algorithms can be easily derived.  Next, we show that adaptive learning can be generalized to nonlinear models byusing the kernel trick in a computationally efficient manner. Regret guarantee of the proposed methods are analyzed, and experiments on various benchmark data sets demonstrate their outstanding performance."
Rounding Methods for Discrete Linear Classification,"Learning discrete linear functions, whose weights represent indivisible properties, is a notoriously difficult challenge.In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space,the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set.Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem.Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods.These methods are compared on both synthetic and real-world data."
Rounding Methods for Discrete Linear Classification,"Learning discrete linear functions, whose weights represent indivisible properties, is a notoriously difficult challenge.In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space,the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set.Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem.Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods.These methods are compared on both synthetic and real-world data."
Rounding Methods for Discrete Linear Classification,"Learning discrete linear functions, whose weights represent indivisible properties, is a notoriously difficult challenge.In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space,the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set.Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem.Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods.These methods are compared on both synthetic and real-world data."
Sparse Approximate Manifolds for Differential Geometric MCMC,"One of the enduring challenges in Markov chain Monte Carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost. The recent introduction of locally adaptive MCMC methods based on the natural underlying Riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration.In this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid MCMC algorithm that extends the applicability of Riemannian Manifold MCMC methods to statistical models that do not admit an analytically computable metric tensor. Secondly, we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made. We demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust student-t error model, for which the expected Fisher Information is analytically intractable."
Kernel-based Distance Metric Learning in the Output Space,"In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2- or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches."
Kernel-based Distance Metric Learning in the Output Space,"In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2- or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches."
Learning with Multiple Models,"The standard approach to analyzing data in supervised or unsupervised learning is to assume that a certain specific model generated the data. The goal of the learning process is typically to recover the generating model, or a good approximation thereof. But in many cases, the data are generated by multiple models rather than a single one. In this paper we study the problem of learning when multiple models are considered, generalizing well known schemes such as clustering and multi-subspace approximation. The objective is to learn several models that explain the data best, and the loss for any given data point is the minimal loss among all considered models. We develop an efficient iterative optimization based procedure for the multiple model setup and provide sample complexity bounds."
Learning with Multiple Models,"The standard approach to analyzing data in supervised or unsupervised learning is to assume that a certain specific model generated the data. The goal of the learning process is typically to recover the generating model, or a good approximation thereof. But in many cases, the data are generated by multiple models rather than a single one. In this paper we study the problem of learning when multiple models are considered, generalizing well known schemes such as clustering and multi-subspace approximation. The objective is to learn several models that explain the data best, and the loss for any given data point is the minimal loss among all considered models. We develop an efficient iterative optimization based procedure for the multiple model setup and provide sample complexity bounds."
Fast Variational Inference in the Conjugate Exponential Family ,We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our methodunifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. Weexploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equationshave been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.
Fast Variational Inference in the Conjugate Exponential Family ,We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our methodunifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. Weexploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equationshave been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.
Fast Variational Inference in the Conjugate Exponential Family ,We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our methodunifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. Weexploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equationshave been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.
Variance Inflation in High Dimensional Support Vector Machines,"Many important machine learning models, supervised and unsupervised, are based on simple Euclidean distances or projections in some high dimensional spaces. When estimating such models from small training sets we may face the problem that the span of the training data set input vectors is not the full input space. Hence, when applying the model to future data the model is effectively blind to the missed orthogonal subspace. This can lead to an inflated variance of hidden variables estimated within training set subspace and when applying the model to test data we may find that the hidden variables follow a different probability law with less variance. While the problem and simple means to reconstruct and deflate arewell understood in unsupervised learning, the case of supervised learning is less well understood. We here investigate the effect of variance inflation in supervised learning including the case of Support Vector Machines (SVM) and we propose non-parametric scheme to restore the generalizability. We illustrate the algorithm and its ability to restore performance on a wide range of data sets."
Variance Inflation in High Dimensional Support Vector Machines,"Many important machine learning models, supervised and unsupervised, are based on simple Euclidean distances or projections in some high dimensional spaces. When estimating such models from small training sets we may face the problem that the span of the training data set input vectors is not the full input space. Hence, when applying the model to future data the model is effectively blind to the missed orthogonal subspace. This can lead to an inflated variance of hidden variables estimated within training set subspace and when applying the model to test data we may find that the hidden variables follow a different probability law with less variance. While the problem and simple means to reconstruct and deflate arewell understood in unsupervised learning, the case of supervised learning is less well understood. We here investigate the effect of variance inflation in supervised learning including the case of Support Vector Machines (SVM) and we propose non-parametric scheme to restore the generalizability. We illustrate the algorithm and its ability to restore performance on a wide range of data sets."
Hypergraph Complexity from Directed Line Graphs,"In this paper, we aim to characterize hypergraphs in terms of structural complexities. Measuring the complexity of a hypergraph in straightforward way tends to be elusive since hypergraph may exhibit varying relational orders. We thus transform a hypergraph into a line graph which not only accurately reflects the multiple relationships exhibited by the hypergraph but is also easy to be manipulated for complexity analysis. To locate dominant substructure within a line graph, we identify a centroid vertex by computing the minimum variance of its shortest path lengths. A family of centroid expansion subgraphs of the line graph is derived from the centroid vertex in an attempt to capture dominant structural characteristics of a hypergraph. We then compute the complexity traces of a hypergraph by measuring entropies on the centroid expansion subgraphs. The Shannon or von Neumann entropy measured on the condensed subgraph family enables an efficient characterisation of the complexity trace. We perform hypergraph clustering in the principal components space of the complexity trace vectors.Experiments on (hyper)graph datasets abstracted from bioinformatic and image data demonstrate effectiveness and efficiency of the hypergraphs complexity traces."
Hypergraph Complexity from Directed Line Graphs,"In this paper, we aim to characterize hypergraphs in terms of structural complexities. Measuring the complexity of a hypergraph in straightforward way tends to be elusive since hypergraph may exhibit varying relational orders. We thus transform a hypergraph into a line graph which not only accurately reflects the multiple relationships exhibited by the hypergraph but is also easy to be manipulated for complexity analysis. To locate dominant substructure within a line graph, we identify a centroid vertex by computing the minimum variance of its shortest path lengths. A family of centroid expansion subgraphs of the line graph is derived from the centroid vertex in an attempt to capture dominant structural characteristics of a hypergraph. We then compute the complexity traces of a hypergraph by measuring entropies on the centroid expansion subgraphs. The Shannon or von Neumann entropy measured on the condensed subgraph family enables an efficient characterisation of the complexity trace. We perform hypergraph clustering in the principal components space of the complexity trace vectors.Experiments on (hyper)graph datasets abstracted from bioinformatic and image data demonstrate effectiveness and efficiency of the hypergraphs complexity traces."
Hypergraph Complexity from Directed Line Graphs,"In this paper, we aim to characterize hypergraphs in terms of structural complexities. Measuring the complexity of a hypergraph in straightforward way tends to be elusive since hypergraph may exhibit varying relational orders. We thus transform a hypergraph into a line graph which not only accurately reflects the multiple relationships exhibited by the hypergraph but is also easy to be manipulated for complexity analysis. To locate dominant substructure within a line graph, we identify a centroid vertex by computing the minimum variance of its shortest path lengths. A family of centroid expansion subgraphs of the line graph is derived from the centroid vertex in an attempt to capture dominant structural characteristics of a hypergraph. We then compute the complexity traces of a hypergraph by measuring entropies on the centroid expansion subgraphs. The Shannon or von Neumann entropy measured on the condensed subgraph family enables an efficient characterisation of the complexity trace. We perform hypergraph clustering in the principal components space of the complexity trace vectors.Experiments on (hyper)graph datasets abstracted from bioinformatic and image data demonstrate effectiveness and efficiency of the hypergraphs complexity traces."
Accelerated Training for Matrix-norm Regularization: A Boosting Approach,"Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees $\epsilon$ accuracy within $O(1/\epsilon)$ iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle."
Controlled Recognition Bounds for Visual Learning and Exploration,"We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of ?visual search? of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a ?passive? agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an ?omnipotent? agent, capable of infinite control authority, can achieve arbitrarily good performance(asymptotically)."
Controlled Recognition Bounds for Visual Learning and Exploration,"We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of ?visual search? of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a ?passive? agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an ?omnipotent? agent, capable of infinite control authority, can achieve arbitrarily good performance(asymptotically)."
Controlled Recognition Bounds for Visual Learning and Exploration,"We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of ?visual search? of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a ?passive? agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an ?omnipotent? agent, capable of infinite control authority, can achieve arbitrarily good performance(asymptotically)."
Sparse Probit Factor Analysis for Learning Analytics,"Providing personalized instructions requires a significant amount of effort spent organizing educational material, and analyzing each student's strength and weakness. This, in turn, places an enormous burden on course instructors. Intelligent tutoring systems (ITS) using machine-learning techniques are a novel way to reduce the instructor's efforts. Specifically, ITS consist of two parts, i.e., learning analytics (LA) and scheduling. LA corresponds to the analysis of student response data, whereas scheduling corresponds to the automatic suggestion of learning materials to the student based on the database retrieved through LA. In this work, we propose a statistical approach towards LA based on sparse probit binary factor analysis, and propose two novel algorithms to analyze student response data obtained in a course or test. The first algorithm utilizes convex optimization techniques, whereas the second utilizes a Bayesian latent feature framework. We demonstrate for synthetic and real-world student data that the proposed framework enables us to recover a question--concept association map, as well as a profile of the concept understanding for each student. This proposed framework represents a first step towards an ITS that alleviates the course instructor's workload and  improves the efficacy of student learning."
Distributed Probabilistic Learning for Camera Networks with Missing Data,"Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points.  However, many problems in wide-area surveillance can benefit from distributed modeling, either because of physical or computational constraints.  Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data.  In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing.  In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors.  We demonstrate the utility of this approach on the problem of distributed affine structure from motion.  Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations."
Distributed Probabilistic Learning for Camera Networks with Missing Data,"Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points.  However, many problems in wide-area surveillance can benefit from distributed modeling, either because of physical or computational constraints.  Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data.  In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing.  In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors.  We demonstrate the utility of this approach on the problem of distributed affine structure from motion.  Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations."
A Statistical Model for Recreational Trails in Aerial Images,"We present a statistical model of aerial images of recreational trails, and a method to infer trail routes in such images. We learn a set of textons describing the images, and use them to divide the image into super-pixels represented by their texton. We then learn, for each texton, the frequency of generating on-trail and off-trail pixels, and the direction of trail through on-trail pixels. From these, we derive an image likelihood function. We combine that with a prior model of trail length and smoothness, yielding a posterior distribution for trails, given an image. We search for good values of this posterior using a novel stochastic variation of Dijkstra?s algorithm. Our experiments on trail images and groundtruth collected in the western continental USA, show substantial improvement over those of the previous best trail-finding method"
On the importance of initialization and momentum in deep learning,"In this work, we show that using carefully crafted (but fairly simple) random initializations, deep autoencoders and recurrent neural networks (RNNs) can be trained effectively using stochastic gradient descent (with suitably small learning rates), and that these results can be significantly improved through the use of aggressive momentum-based acceleration.   For deep autoencoders we show that using any one of a variety of recently proposed random initializations schemes, deep autoencoders can be trained to a level of performance exceeding that reported by Hinton and Salakhutdinov, and with the addition of Nesterov-type momentum, the results can be further improved to surpass those reported by Martens.  For RNNs we give a simple initialization scheme related to the one used for Echo State Networks and successfully train them on various datasets exhibiting pathological long range dependencies (spanning 50-200 timesteps, depending on the problem).Our results suggest that previous attempts to train deep and recurrent neural networks from random initializations failed mostly due to poor choices for such initializations, and that the curvature issues which are present in the training objectives of deep models can be addressed through the use of aggressive momentum-based acceleration, without the need for 2nd-order methods."
On the importance of initialization and momentum in deep learning,"In this work, we show that using carefully crafted (but fairly simple) random initializations, deep autoencoders and recurrent neural networks (RNNs) can be trained effectively using stochastic gradient descent (with suitably small learning rates), and that these results can be significantly improved through the use of aggressive momentum-based acceleration.   For deep autoencoders we show that using any one of a variety of recently proposed random initializations schemes, deep autoencoders can be trained to a level of performance exceeding that reported by Hinton and Salakhutdinov, and with the addition of Nesterov-type momentum, the results can be further improved to surpass those reported by Martens.  For RNNs we give a simple initialization scheme related to the one used for Echo State Networks and successfully train them on various datasets exhibiting pathological long range dependencies (spanning 50-200 timesteps, depending on the problem).Our results suggest that previous attempts to train deep and recurrent neural networks from random initializations failed mostly due to poor choices for such initializations, and that the curvature issues which are present in the training objectives of deep models can be addressed through the use of aggressive momentum-based acceleration, without the need for 2nd-order methods."
On the importance of initialization and momentum in deep learning,"In this work, we show that using carefully crafted (but fairly simple) random initializations, deep autoencoders and recurrent neural networks (RNNs) can be trained effectively using stochastic gradient descent (with suitably small learning rates), and that these results can be significantly improved through the use of aggressive momentum-based acceleration.   For deep autoencoders we show that using any one of a variety of recently proposed random initializations schemes, deep autoencoders can be trained to a level of performance exceeding that reported by Hinton and Salakhutdinov, and with the addition of Nesterov-type momentum, the results can be further improved to surpass those reported by Martens.  For RNNs we give a simple initialization scheme related to the one used for Echo State Networks and successfully train them on various datasets exhibiting pathological long range dependencies (spanning 50-200 timesteps, depending on the problem).Our results suggest that previous attempts to train deep and recurrent neural networks from random initializations failed mostly due to poor choices for such initializations, and that the curvature issues which are present in the training objectives of deep models can be addressed through the use of aggressive momentum-based acceleration, without the need for 2nd-order methods."
On the importance of initialization and momentum in deep learning,"In this work, we show that using carefully crafted (but fairly simple) random initializations, deep autoencoders and recurrent neural networks (RNNs) can be trained effectively using stochastic gradient descent (with suitably small learning rates), and that these results can be significantly improved through the use of aggressive momentum-based acceleration.   For deep autoencoders we show that using any one of a variety of recently proposed random initializations schemes, deep autoencoders can be trained to a level of performance exceeding that reported by Hinton and Salakhutdinov, and with the addition of Nesterov-type momentum, the results can be further improved to surpass those reported by Martens.  For RNNs we give a simple initialization scheme related to the one used for Echo State Networks and successfully train them on various datasets exhibiting pathological long range dependencies (spanning 50-200 timesteps, depending on the problem).Our results suggest that previous attempts to train deep and recurrent neural networks from random initializations failed mostly due to poor choices for such initializations, and that the curvature issues which are present in the training objectives of deep models can be addressed through the use of aggressive momentum-based acceleration, without the need for 2nd-order methods."
Probability-One Homotopy Maps for Tracking Constrained Clustering Solutions,"Modern machine learning problems typically have multiple criteria, but there iscurrently no systematic mathematical theory to guide the design of formulationsand exploration of alternatives. Homotopy methods are a promising approach tocharacterize solution spaces by smoothly tracking solutions from one formulation(typically an ?easy? problem) to another (typically a ?hard? problem). We presentnew results in constructing homotopy maps for constrained clustering problems,which combine quadratic loss functions with discrete evaluations of constraintviolations. Our maps help balance requirements of locality in clusters as well asthose of discrete must-link and must-not-link constraints. Our experimental resultsdemonstrate significant advantages in tracking solutions compared to state-of-theartconstrained clustering algorithms."
Probability-One Homotopy Maps for Tracking Constrained Clustering Solutions,"Modern machine learning problems typically have multiple criteria, but there iscurrently no systematic mathematical theory to guide the design of formulationsand exploration of alternatives. Homotopy methods are a promising approach tocharacterize solution spaces by smoothly tracking solutions from one formulation(typically an ?easy? problem) to another (typically a ?hard? problem). We presentnew results in constructing homotopy maps for constrained clustering problems,which combine quadratic loss functions with discrete evaluations of constraintviolations. Our maps help balance requirements of locality in clusters as well asthose of discrete must-link and must-not-link constraints. Our experimental resultsdemonstrate significant advantages in tracking solutions compared to state-of-theartconstrained clustering algorithms."
Probability-One Homotopy Maps for Tracking Constrained Clustering Solutions,"Modern machine learning problems typically have multiple criteria, but there iscurrently no systematic mathematical theory to guide the design of formulationsand exploration of alternatives. Homotopy methods are a promising approach tocharacterize solution spaces by smoothly tracking solutions from one formulation(typically an ?easy? problem) to another (typically a ?hard? problem). We presentnew results in constructing homotopy maps for constrained clustering problems,which combine quadratic loss functions with discrete evaluations of constraintviolations. Our maps help balance requirements of locality in clusters as well asthose of discrete must-link and must-not-link constraints. Our experimental resultsdemonstrate significant advantages in tracking solutions compared to state-of-theartconstrained clustering algorithms."
Probability-One Homotopy Maps for Tracking Constrained Clustering Solutions,"Modern machine learning problems typically have multiple criteria, but there iscurrently no systematic mathematical theory to guide the design of formulationsand exploration of alternatives. Homotopy methods are a promising approach tocharacterize solution spaces by smoothly tracking solutions from one formulation(typically an ?easy? problem) to another (typically a ?hard? problem). We presentnew results in constructing homotopy maps for constrained clustering problems,which combine quadratic loss functions with discrete evaluations of constraintviolations. Our maps help balance requirements of locality in clusters as well asthose of discrete must-link and must-not-link constraints. Our experimental resultsdemonstrate significant advantages in tracking solutions compared to state-of-theartconstrained clustering algorithms."
Risk-sensitive Reinforcement Learning for Applications in Human Decision Making,"This paper proposes a general framework for measuring risk in the context of Markov decision processes by introducing valuation maps, which can model both economically rational and irrational behaviors. Our framework covers most of existing literature in various fields as special cases. The induced risk-preferences are controlled by manipulating the forms of maps. To solve the derived infinite-stage discounted risk-sensitive optimization problems, we provide a dynamic programming algorithm for all maps within our framework and a generalized Q-learning algorithm for a sufficiently rich subfamily, called utility-based shortfall, by which the results in prospect theory can be well replicated. To test its applicability in real data, we apply the algorithm to analyze human behaviors in a sequential investment game. Our method outperforms standard models and the individual risk-preferences revealed by the model are consistent with behavioral data."
Risk-sensitive Reinforcement Learning for Applications in Human Decision Making,"This paper proposes a general framework for measuring risk in the context of Markov decision processes by introducing valuation maps, which can model both economically rational and irrational behaviors. Our framework covers most of existing literature in various fields as special cases. The induced risk-preferences are controlled by manipulating the forms of maps. To solve the derived infinite-stage discounted risk-sensitive optimization problems, we provide a dynamic programming algorithm for all maps within our framework and a generalized Q-learning algorithm for a sufficiently rich subfamily, called utility-based shortfall, by which the results in prospect theory can be well replicated. To test its applicability in real data, we apply the algorithm to analyze human behaviors in a sequential investment game. Our method outperforms standard models and the individual risk-preferences revealed by the model are consistent with behavioral data."
Risk-sensitive Reinforcement Learning for Applications in Human Decision Making,"This paper proposes a general framework for measuring risk in the context of Markov decision processes by introducing valuation maps, which can model both economically rational and irrational behaviors. Our framework covers most of existing literature in various fields as special cases. The induced risk-preferences are controlled by manipulating the forms of maps. To solve the derived infinite-stage discounted risk-sensitive optimization problems, we provide a dynamic programming algorithm for all maps within our framework and a generalized Q-learning algorithm for a sufficiently rich subfamily, called utility-based shortfall, by which the results in prospect theory can be well replicated. To test its applicability in real data, we apply the algorithm to analyze human behaviors in a sequential investment game. Our method outperforms standard models and the individual risk-preferences revealed by the model are consistent with behavioral data."
Risk-sensitive Reinforcement Learning for Applications in Human Decision Making,"This paper proposes a general framework for measuring risk in the context of Markov decision processes by introducing valuation maps, which can model both economically rational and irrational behaviors. Our framework covers most of existing literature in various fields as special cases. The induced risk-preferences are controlled by manipulating the forms of maps. To solve the derived infinite-stage discounted risk-sensitive optimization problems, we provide a dynamic programming algorithm for all maps within our framework and a generalized Q-learning algorithm for a sufficiently rich subfamily, called utility-based shortfall, by which the results in prospect theory can be well replicated. To test its applicability in real data, we apply the algorithm to analyze human behaviors in a sequential investment game. Our method outperforms standard models and the individual risk-preferences revealed by the model are consistent with behavioral data."
Risk-sensitive Reinforcement Learning for Applications in Human Decision Making,"This paper proposes a general framework for measuring risk in the context of Markov decision processes by introducing valuation maps, which can model both economically rational and irrational behaviors. Our framework covers most of existing literature in various fields as special cases. The induced risk-preferences are controlled by manipulating the forms of maps. To solve the derived infinite-stage discounted risk-sensitive optimization problems, we provide a dynamic programming algorithm for all maps within our framework and a generalized Q-learning algorithm for a sufficiently rich subfamily, called utility-based shortfall, by which the results in prospect theory can be well replicated. To test its applicability in real data, we apply the algorithm to analyze human behaviors in a sequential investment game. Our method outperforms standard models and the individual risk-preferences revealed by the model are consistent with behavioral data."
A Truncated Variational EM Approach for Spike-and-Slab Sparse Coding,"We study the recovery of sparse hidden dimensions based on sparse coding with `spike-and-slab' prior. As standard sparse coding, the used model assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse  prior, we study the application of a more flexible `spike-and-slab' prior which models the absence or presence of a source's contribution independently of its strenghts if it contributs. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: firstly, a novel truncated variational EM approach; and, secondly, a recently suggested approach based on standard factored variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of `spike-and-slab' priors for this domain. Furthermore, we find the truncated variational approach to improve on the standard factored approach in most of these tasks -- which may hint to biases introduced by assuming posterior independence in the factored variational approach. Likewise, we find the truncated variational approach to improve on the factored variational approach in applications to a standard denoising task.  While the performance of the factored approach saturates with increasing number of hidden dimensions, performance of the truncated approach improves. For higher noise levels, the truncated approach finally improves the state-of-the-art on this standard benchmark."
A Truncated Variational EM Approach for Spike-and-Slab Sparse Coding,"We study the recovery of sparse hidden dimensions based on sparse coding with `spike-and-slab' prior. As standard sparse coding, the used model assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse  prior, we study the application of a more flexible `spike-and-slab' prior which models the absence or presence of a source's contribution independently of its strenghts if it contributs. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: firstly, a novel truncated variational EM approach; and, secondly, a recently suggested approach based on standard factored variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of `spike-and-slab' priors for this domain. Furthermore, we find the truncated variational approach to improve on the standard factored approach in most of these tasks -- which may hint to biases introduced by assuming posterior independence in the factored variational approach. Likewise, we find the truncated variational approach to improve on the factored variational approach in applications to a standard denoising task.  While the performance of the factored approach saturates with increasing number of hidden dimensions, performance of the truncated approach improves. For higher noise levels, the truncated approach finally improves the state-of-the-art on this standard benchmark."
A Truncated Variational EM Approach for Spike-and-Slab Sparse Coding,"We study the recovery of sparse hidden dimensions based on sparse coding with `spike-and-slab' prior. As standard sparse coding, the used model assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse  prior, we study the application of a more flexible `spike-and-slab' prior which models the absence or presence of a source's contribution independently of its strenghts if it contributs. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: firstly, a novel truncated variational EM approach; and, secondly, a recently suggested approach based on standard factored variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of `spike-and-slab' priors for this domain. Furthermore, we find the truncated variational approach to improve on the standard factored approach in most of these tasks -- which may hint to biases introduced by assuming posterior independence in the factored variational approach. Likewise, we find the truncated variational approach to improve on the factored variational approach in applications to a standard denoising task.  While the performance of the factored approach saturates with increasing number of hidden dimensions, performance of the truncated approach improves. For higher noise levels, the truncated approach finally improves the state-of-the-art on this standard benchmark."
Querying Discriminative and Representative Samples for Batch Mode Active Learning,"Empirical risk minimization (ERM) provides a principal guideline for many machine learning algorithms. Under the ERM principle, we minimize an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, the training data should be i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where we select the most informative samples to label and these data may come from a distribution different with the source. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. Our active learning method queries the most informative samples which also preseve the source distribution as much as possible, thus identifying most uncertain and representative queries. Experiments on benchmark data sets and real world applications show our method outperforms the existing state-of-the-art batch mode active learning methods."
Divide and prosper --- fault tolerant scalable sketches,"We describe a family of algorithms that can be used to extend  sketches such as the CountMin sketch and SpaceSaving, to settings  where fault tolerance and scalability are crucial. We show how tools  from systems research, namely consistent and proportional hashing  can be used to increase accuracy and throughput linearly in the  number of processors while simultaneously decreasing the failure  probability exponentially. We provide both tight theoretical  guarantees and experimental results that corroborate our findings."
Divide and prosper --- fault tolerant scalable sketches,"We describe a family of algorithms that can be used to extend  sketches such as the CountMin sketch and SpaceSaving, to settings  where fault tolerance and scalability are crucial. We show how tools  from systems research, namely consistent and proportional hashing  can be used to increase accuracy and throughput linearly in the  number of processors while simultaneously decreasing the failure  probability exponentially. We provide both tight theoretical  guarantees and experimental results that corroborate our findings."
Minimizing Uncertainty in Pipelines,"In this paper, we consider the problem of debugging large pipelines by human labeling. We represent the execution of a pipeline using a directed acyclic graph of AND and OR nodes, where each node represents a data item produced by some operator in the pipeline. We assume that each operator assigns a confidence to each of its output data. We want to reduce the uncertainty in the output by issuing queries to a human expert, where a query consists of checking if a given data item is correct. In this paper, we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty. We perform a detailed evaluation of the complexity of the problem for various classes of graphs. We give efficient algorithms for the problem for trees, and show that, for a general dag, the problem is intractable."
Minimizing Uncertainty in Pipelines,"In this paper, we consider the problem of debugging large pipelines by human labeling. We represent the execution of a pipeline using a directed acyclic graph of AND and OR nodes, where each node represents a data item produced by some operator in the pipeline. We assume that each operator assigns a confidence to each of its output data. We want to reduce the uncertainty in the output by issuing queries to a human expert, where a query consists of checking if a given data item is correct. In this paper, we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty. We perform a detailed evaluation of the complexity of the problem for various classes of graphs. We give efficient algorithms for the problem for trees, and show that, for a general dag, the problem is intractable."
Sequential Inference with Compact Posterior Representations for the Indian Buffet Process,"Infinite latent feature models, such as those based on the Indian Buffet Process (IBP), provide a flexible way to learn latent features underlying observed data, without having to specify their number a priori. Scalable and accurate posterior inference in these models however remains a challenge. Moreover, in many settings, data arrives sequentially and batch methods such as Gibbs sampling are no longer an option. We present a scalable, sequential MCMC inference method for the IBP that can process one observation at a time. As opposed to the standard particle filter for the IBP, our method incorporates the current observation in the proposal distribution and in the computation of the particle weights. This leads to our method achieving better or comparable inference quality as compared to the standard particle filter for the IBP, while requiring far fewer number of particles (therefore yielding compact posterior representations) and being comparable in terms of inference speed.  Moreover, our method also yields competitive accuracies as compared to the state-of-the-art batch methods based on MCMC and variational inference, while being considerably faster."
Practical Bayesian Optimization of Machine Learning Algorithms ,"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ?black art? requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm?s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieveexpert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including LatentDirichlet Allocation, Structured SVMs and convolutional neural networks."
Practical Bayesian Optimization of Machine Learning Algorithms ,"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ?black art? requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm?s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieveexpert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including LatentDirichlet Allocation, Structured SVMs and convolutional neural networks."
Sparse Bayesian unsupervised learning,"This paper is about variable selection, clustering and estimation in an unsupervised high-dimensional setting. Our approach is based on fitting constrained Gaussian mixture models, where we learn the number of clusters $K$ and the set of relevant variables $S$ using a generalized Bayesian posterior with a sparsity inducing prior. We prove a sparsity oracle inequality which shows that this procedure selects the optimal parameters $K$ and $S$. This result is the first of its kind for sparse model-based clustering. Our procedure is implemented using a Metropolis-Hastings algorithm, based on a clustering-oriented greedy proposal, which makes the convergence to the posterior very fast."
Sparse Bayesian unsupervised learning,"This paper is about variable selection, clustering and estimation in an unsupervised high-dimensional setting. Our approach is based on fitting constrained Gaussian mixture models, where we learn the number of clusters $K$ and the set of relevant variables $S$ using a generalized Bayesian posterior with a sparsity inducing prior. We prove a sparsity oracle inequality which shows that this procedure selects the optimal parameters $K$ and $S$. This result is the first of its kind for sparse model-based clustering. Our procedure is implemented using a Metropolis-Hastings algorithm, based on a clustering-oriented greedy proposal, which makes the convergence to the posterior very fast."
Keyword-supervised topic models,"Supervised topic models are able to use document labels to find topics that are predictive of the labels. However, these models only predict labels through indirect topic allocations and do not account for the fact that different words can have different degrees of effect on the label of a document. In this paper, we present the keyword-supervised latent Dirichlet allocation (ksLDA) model as a supervised model that allows different words to have a more direct effect on the model of a document's label and for the effect of the words to be perturbed by their context. In this paper, we show that this new model performs better than supervised latent Dirichlet allocation (sLDA) on real-world classification and regression tasks and on limited-size training sets."
Probabilistic Latent Component Analysis for Inputs with Real-Valued Dimensions,"We present a probabilistic latent component model which operates on inputs whose dimension indices are real-valued.  Unlike traditional probabilistic latent parameter models, that assume discrete-valued dimensions, the model we present generalizes that idea so that such decompositions can be applied on data with real-valued indices, i.e. data that cannot be represented as a matrix or as lying on a regular grid.  We derive a hierarchical Expectation-Maximization learning process for such a model and present its application on decomposing inputs which exhibit such a structure.  We demonstrate the utility of that approach with some results on latent audio component analysis, results that are otherwise unattainable using existing decompositions."
Probabilistic Latent Component Analysis for Inputs with Real-Valued Dimensions,"We present a probabilistic latent component model which operates on inputs whose dimension indices are real-valued.  Unlike traditional probabilistic latent parameter models, that assume discrete-valued dimensions, the model we present generalizes that idea so that such decompositions can be applied on data with real-valued indices, i.e. data that cannot be represented as a matrix or as lying on a regular grid.  We derive a hierarchical Expectation-Maximization learning process for such a model and present its application on decomposing inputs which exhibit such a structure.  We demonstrate the utility of that approach with some results on latent audio component analysis, results that are otherwise unattainable using existing decompositions."
First-Order Models for POMDPs,"Interest in relational and first-order languages for probabilitymodels has grown rapidly in recent years, and with it the possibilityof extending such languages to handle decision processes---both fullyand partially observable.  We examine the problem of extending afirst-order, open-universe language to describe POMDPs and identifynon-trivial representational issues in describing an agent'scapability for observation and action---issues that were avoided inprevious work only by making strong and restrictive assumptions. Wepresent a solution based on ideas from modal logic, and show how tohandle cases like being able to act upon an object thathas been detected through one's observations."
Comparative Locally Linear Classifiers,"A common issue in recent successful locally classifiers is the relatively high computational complexity in testing because nearest neighbor search always be involved for localizing the data points using anchor points. In this paper, we introduce the idea of locally sensitive hashing (LSH) into the linear classifiers to speed up the localization process, and thus propose another large-margin based locally linear classifier. The features generated by the LSH process are called comparative features. We learn the comparative features by transforming the data points into a new feature space and performing threshold on them using the max-operator. The transformation matrix is actually the anchor point matrix, which is learned in an unsupervised manner by enforcing the corresponding coefficients are the elements on a hypercubic structure in the new feature space. The nearest neighbor search forlocalization is approximated by the max-operator. Compared to other local classifiers, the computational complexity of our classifier in testing is almost identical to linear support vector machines, and only 4-line simple MATLAB code is needed for the binary classification. Experimental results demonstrate that during testing our method is not only comparable or even better than other state-of-the-art local classifiers in terms of accuracy but also much faster in testing."
Comparative Locally Linear Classifiers,"A common issue in recent successful locally classifiers is the relatively high computational complexity in testing because nearest neighbor search always be involved for localizing the data points using anchor points. In this paper, we introduce the idea of locally sensitive hashing (LSH) into the linear classifiers to speed up the localization process, and thus propose another large-margin based locally linear classifier. The features generated by the LSH process are called comparative features. We learn the comparative features by transforming the data points into a new feature space and performing threshold on them using the max-operator. The transformation matrix is actually the anchor point matrix, which is learned in an unsupervised manner by enforcing the corresponding coefficients are the elements on a hypercubic structure in the new feature space. The nearest neighbor search forlocalization is approximated by the max-operator. Compared to other local classifiers, the computational complexity of our classifier in testing is almost identical to linear support vector machines, and only 4-line simple MATLAB code is needed for the binary classification. Experimental results demonstrate that during testing our method is not only comparable or even better than other state-of-the-art local classifiers in terms of accuracy but also much faster in testing."
Learnable Pooling Regions for Image Classification,"From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition. Grouping of local features and their codes is part of most recent recognition pipelines and equips these methods with a certain degree of robustness to translations and deformation yet preserving the same spatial layout of the local image features.Despite the predominance of this approach, we have seen little progress to fully adapt  the pooling strategy to the task at hand. This paper proposes a learning method that allows for learning a task dependent pooling scheme -- which includes previously proposed pooling schemes as a particular instantiation of our method.In contrast to previous work we allow different pooling strategies for each code, which shows in particular beneficial for small codes. We propose a batch-based optimization strategy that allows our approach to scale up to sizable dictionary. In this manner we discover new pooling regions that have not been previously used in computer vision."
Learnable Pooling Regions for Image Classification,"From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition. Grouping of local features and their codes is part of most recent recognition pipelines and equips these methods with a certain degree of robustness to translations and deformation yet preserving the same spatial layout of the local image features.Despite the predominance of this approach, we have seen little progress to fully adapt  the pooling strategy to the task at hand. This paper proposes a learning method that allows for learning a task dependent pooling scheme -- which includes previously proposed pooling schemes as a particular instantiation of our method.In contrast to previous work we allow different pooling strategies for each code, which shows in particular beneficial for small codes. We propose a batch-based optimization strategy that allows our approach to scale up to sizable dictionary. In this manner we discover new pooling regions that have not been previously used in computer vision."
The Time-Marginal Coalescent Prior for Hierarchical Clustering,"We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clus-tering. The prior is constructed by marginalizing out the time information ofKingman?s coalescent, providing a prior over tree structures which we call theTime-Marginalized Coalescent (TMC). This allows for models which factorizethe tree structure and times, providing two benefits: more flexible priors may beconstructed and more efficient Gibbs type inference can be used. We demonstratethis on an example model and show we get competitive experimental results."
The Time-Marginal Coalescent Prior for Hierarchical Clustering,"We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clus-tering. The prior is constructed by marginalizing out the time information ofKingman?s coalescent, providing a prior over tree structures which we call theTime-Marginalized Coalescent (TMC). This allows for models which factorizethe tree structure and times, providing two benefits: more flexible priors may beconstructed and more efficient Gibbs type inference can be used. We demonstratethis on an example model and show we get competitive experimental results."
Fusion with Diffusion for Robust Visual Tracking,"A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. The edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations.In this paper, a novel approach to integrate multiple similarity measures is pro-posed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the dif-fusion on the TPG is the same as the diffusion process on each of the original graphs, Moreover, it is not necessary to explicitly construct the TPG in our frame-work. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods."
Fusion with Diffusion for Robust Visual Tracking,"A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. The edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations.In this paper, a novel approach to integrate multiple similarity measures is pro-posed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the dif-fusion on the TPG is the same as the diffusion process on each of the original graphs, Moreover, it is not necessary to explicitly construct the TPG in our frame-work. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods."
Fusion with Diffusion for Robust Visual Tracking,"A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. The edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations.In this paper, a novel approach to integrate multiple similarity measures is pro-posed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the dif-fusion on the TPG is the same as the diffusion process on each of the original graphs, Moreover, it is not necessary to explicitly construct the TPG in our frame-work. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods."
Top-down particle filtering for Bayesian decision trees,"Decision trees are a fundamental tool in machine learning and statistics, and Bayesian variants, which introduce a prior distribution on the decision tree itself, have demonstrated the utility of full posterior inference.  Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, existing Bayesian decision tree algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection) iteratively through local Monte Carlo modifications to the structure of the tree.  We present a Sequential Monte Carlo (SMC) algorithm that works in a top-down manner, mimicking the behavior and speed of classic algorithms. Through empirical comparisons with existing methods, we demonstrate the potential of this new approach and conclude that it represents a better computation-accuracy tradeoff."
Top-down particle filtering for Bayesian decision trees,"Decision trees are a fundamental tool in machine learning and statistics, and Bayesian variants, which introduce a prior distribution on the decision tree itself, have demonstrated the utility of full posterior inference.  Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, existing Bayesian decision tree algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection) iteratively through local Monte Carlo modifications to the structure of the tree.  We present a Sequential Monte Carlo (SMC) algorithm that works in a top-down manner, mimicking the behavior and speed of classic algorithms. Through empirical comparisons with existing methods, we demonstrate the potential of this new approach and conclude that it represents a better computation-accuracy tradeoff."
Top-down particle filtering for Bayesian decision trees,"Decision trees are a fundamental tool in machine learning and statistics, and Bayesian variants, which introduce a prior distribution on the decision tree itself, have demonstrated the utility of full posterior inference.  Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, existing Bayesian decision tree algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection) iteratively through local Monte Carlo modifications to the structure of the tree.  We present a Sequential Monte Carlo (SMC) algorithm that works in a top-down manner, mimicking the behavior and speed of classic algorithms. Through empirical comparisons with existing methods, we demonstrate the potential of this new approach and conclude that it represents a better computation-accuracy tradeoff."
Automated Gaussian process modelling for many instances of small data sets,"We present techniques for automated Gaussian process (GP) modelling with many small data sets. These problems are common when applying GP models independently to each gene in a gene expression time series data set. Such sets typically contain very few time points. Naive application of common GP modelling techniques can lead to severe over-fitting or under-fitting in a significant fraction ofthe fitted models, depending on the details of the data set. We propose avoiding over-fitting by constraining the GP length-scale to values that focus most of the energy spectrum to frequencies above the Nyquist frequency corresponding to the sampling frequency in the data set. Under-fitting can be avoided by more informative priors on observation noise. Combining these methods allows applying GP methods reliably automatically to large numbers of independent instances of short time series. This is illustrated with experiments with both synthetic data and real gene expression data."
Automated Gaussian process modelling for many instances of small data sets,"We present techniques for automated Gaussian process (GP) modelling with many small data sets. These problems are common when applying GP models independently to each gene in a gene expression time series data set. Such sets typically contain very few time points. Naive application of common GP modelling techniques can lead to severe over-fitting or under-fitting in a significant fraction ofthe fitted models, depending on the details of the data set. We propose avoiding over-fitting by constraining the GP length-scale to values that focus most of the energy spectrum to frequencies above the Nyquist frequency corresponding to the sampling frequency in the data set. Under-fitting can be avoided by more informative priors on observation noise. Combining these methods allows applying GP methods reliably automatically to large numbers of independent instances of short time series. This is illustrated with experiments with both synthetic data and real gene expression data."
A nonparametric variable clustering model,"Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a clustering, of observed variables so that variables in a cluster are highly correlated. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. "
A Quantum Algorithm for PCA,"In this paper, we present a ?quantization? of classical dimensionality reduction techniques. ?Quantization? refers to the use of a quantum subroutine, or classical computer with access to a quantum computer, to enhance a classical algorithm. Here, we quantize a probabilistic Principal Component Analysis (PCA) algorithm by using a quantum subroutine based on a seminal quantum algorithm for numerical gradient estimation due to Jordan. We show that our quantized PCA algorithm requires only O(kn) queries to a black box on a quantum computer, while the classical approach requires O(knd) queries to a black box on a classical computer, where k is the lower dimension, n is the number of points, and d is the dimension. A significant result of our work is that it is independent of the numberof high dimensions d, and d is typically extremely large (the dominant factor)."
A Quantum Algorithm for PCA,"In this paper, we present a ?quantization? of classical dimensionality reduction techniques. ?Quantization? refers to the use of a quantum subroutine, or classical computer with access to a quantum computer, to enhance a classical algorithm. Here, we quantize a probabilistic Principal Component Analysis (PCA) algorithm by using a quantum subroutine based on a seminal quantum algorithm for numerical gradient estimation due to Jordan. We show that our quantized PCA algorithm requires only O(kn) queries to a black box on a quantum computer, while the classical approach requires O(knd) queries to a black box on a classical computer, where k is the lower dimension, n is the number of points, and d is the dimension. A significant result of our work is that it is independent of the numberof high dimensions d, and d is typically extremely large (the dominant factor)."
Different aproaches to feature selection in MDPs,"In problems modeled as Markov Decision Processes (MDP), knowledge transfer is related to the notion of generalization and state abstraction. Abstraction can be obtained through factored representation by describing states with a set of features. Thus, the definition of the best action to be taken in a state can be easily transferred to similar states, i.e., states with similar features. In this paper we present two approaches to find an appropriate compact set of features for such abstraction, thus facilitating the transfer of knowledge to new problems. We also present heuristic versions of both approaches and compare all of the approaches within a discrete simulated navigation problem. "
Different aproaches to feature selection in MDPs,"In problems modeled as Markov Decision Processes (MDP), knowledge transfer is related to the notion of generalization and state abstraction. Abstraction can be obtained through factored representation by describing states with a set of features. Thus, the definition of the best action to be taken in a state can be easily transferred to similar states, i.e., states with similar features. In this paper we present two approaches to find an appropriate compact set of features for such abstraction, thus facilitating the transfer of knowledge to new problems. We also present heuristic versions of both approaches and compare all of the approaches within a discrete simulated navigation problem. "
24 Parallel Codes for Sparse PCA,"Given a multivariate data set, sparse principal component analysis aims to extract several linear combinations of the variables which together explain the variance in the data as much as possible, while controlling the number of nonzero loadings in these combinations. In this paper we consider 8 different optimization formulations for computing a single sparse loading vector; these are obtained by combining the following factors: we employ two norms for measuring  variance (L2, L1) and two sparsity-inducing norms (L0, L1), which are used in two different ways (constraint, penalty). Three of our formulations, notably the one with L0 constraint and L1 variance, have not been considered in the literature. We give a unifying reformulation which we propose to solve  via a natural alternating maximization method. Besides this, we provide one serial (single-core) and three parallel (multi-core, GPU, cluster) codes for each of the 8 problems. Parallelism in the methods is aimed at i) speeding up computations (our GPU code can be 100 times faster than an efficient serial code written in C), ii) obtaining solutions explaining more variance and iii) dealing with large-scale problems (our cluster code is able to solve a 357 GB problem in about a minute)."
24 Parallel Codes for Sparse PCA,"Given a multivariate data set, sparse principal component analysis aims to extract several linear combinations of the variables which together explain the variance in the data as much as possible, while controlling the number of nonzero loadings in these combinations. In this paper we consider 8 different optimization formulations for computing a single sparse loading vector; these are obtained by combining the following factors: we employ two norms for measuring  variance (L2, L1) and two sparsity-inducing norms (L0, L1), which are used in two different ways (constraint, penalty). Three of our formulations, notably the one with L0 constraint and L1 variance, have not been considered in the literature. We give a unifying reformulation which we propose to solve  via a natural alternating maximization method. Besides this, we provide one serial (single-core) and three parallel (multi-core, GPU, cluster) codes for each of the 8 problems. Parallelism in the methods is aimed at i) speeding up computations (our GPU code can be 100 times faster than an efficient serial code written in C), ii) obtaining solutions explaining more variance and iii) dealing with large-scale problems (our cluster code is able to solve a 357 GB problem in about a minute)."
A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function,"We propose a novel Bayesian approach to solve stochastic optimization problems that involve ?nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ?rst, doing inference over the function space and second, ?nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf?cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function."
A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function,"We propose a novel Bayesian approach to solve stochastic optimization problems that involve ?nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ?rst, doing inference over the function space and second, ?nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf?cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function."
A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function,"We propose a novel Bayesian approach to solve stochastic optimization problems that involve ?nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ?rst, doing inference over the function space and second, ?nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf?cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function."
A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function,"We propose a novel Bayesian approach to solve stochastic optimization problems that involve ?nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ?rst, doing inference over the function space and second, ?nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf?cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function."
A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function,"We propose a novel Bayesian approach to solve stochastic optimization problems that involve ?nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ?rst, doing inference over the function space and second, ?nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf?cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function."
Model Class Priors in Reinforcement Learning,"Many reinforcement learning problems have such a structure that estimation of the optimal policy may be significantly simpler than in the general case. While Bayesian approaches would naturally be suited to discovering and exploiting such a structure, previous work has concentrated priors which could model arbitrary environments. This paper proposes instead a hierarchical model where beliefs over different classes of priors are maintained. The model is fundamentally different from classical hierarchical Bayesian approaches, since the evidence depends on the policy followed so far. This obstacle can be removed by an appropriate decomposition of the posterior. Finally, we derive a number of online decision making algorithms, in conjunction with the class prior distribution, which maintain a distribution of value functions. Their performance is examined in a number of reinforcement learning problems and we show that it is at least as good as the performance of the model that knows the correct model class a priori."
Model Class Priors in Reinforcement Learning,"Many reinforcement learning problems have such a structure that estimation of the optimal policy may be significantly simpler than in the general case. While Bayesian approaches would naturally be suited to discovering and exploiting such a structure, previous work has concentrated priors which could model arbitrary environments. This paper proposes instead a hierarchical model where beliefs over different classes of priors are maintained. The model is fundamentally different from classical hierarchical Bayesian approaches, since the evidence depends on the policy followed so far. This obstacle can be removed by an appropriate decomposition of the posterior. Finally, we derive a number of online decision making algorithms, in conjunction with the class prior distribution, which maintain a distribution of value functions. Their performance is examined in a number of reinforcement learning problems and we show that it is at least as good as the performance of the model that knows the correct model class a priori."
Modeling Laminar Recordings from Visual Cortex with Semi-Restricted Boltzmann Machines,"The proliferation of high density recording techniques presents us with new challenges for characterizing the statistics of neural activity over populations of many neurons. The Ising model, which is the maximum entropy model for pairwise correlations, has been used to model the instantaneous state of a population of neurons.  This model suffers from two major limitations: 1) Estimation for large models becomes computationally intractable, and 2) it cannot capture higher-order dependencies.  We propose applying a more general maximum entropy model, the semi-restricted Boltzmann machine (sRBM), which extends the Ising model to capture higher order dependencies using hidden units. Estimation of large models is made practical using minimum probability flow, a recently developed parameter estimation method for energy-based models. The partition functions of the models are estimated using annealed importance sampling, which allows for comparing models in terms of likelihood.  Applied to 32-channel polytrode data recorded from cat visual cortex, these higher order models significantly outperform Ising models. In addition, extending the model to spatiotemporal sequences of states allows us to predict spiking based on network history. Our results highlight the importance of modeling higher order interactions across space and time to characterize activity in cortical networks."
Structure inference in cryo-electron microscopy using Gaussian mixture models,"The reconstruction problem in cryo-electron microscopy is to infer an unknown 3D electron density from a set of its 2D projections, where the projection directions are also unknown. Most existing algorithms need to be initialized with a 3D density, and therefore produce biased results. Futhermore, they typically use non-probabilistic approaches that depend on many user-specified hyperparameters. In this paper we introduce a new reconstruction algorithm that places the entire problem in a probabilistic framework with a very small number of hyperparameters. Our algorithm does not require an initial 3D model. Most importantly, it makes use of a novel representation of 3D densities using Gaussian mixture models. Model parameters are estimated using an expectation maximization type algorithm. The algorithm is applied to synthetic and real datasets."
Structure inference in cryo-electron microscopy using Gaussian mixture models,"The reconstruction problem in cryo-electron microscopy is to infer an unknown 3D electron density from a set of its 2D projections, where the projection directions are also unknown. Most existing algorithms need to be initialized with a 3D density, and therefore produce biased results. Futhermore, they typically use non-probabilistic approaches that depend on many user-specified hyperparameters. In this paper we introduce a new reconstruction algorithm that places the entire problem in a probabilistic framework with a very small number of hyperparameters. Our algorithm does not require an initial 3D model. Most importantly, it makes use of a novel representation of 3D densities using Gaussian mixture models. Model parameters are estimated using an expectation maximization type algorithm. The algorithm is applied to synthetic and real datasets."
Shortest stochastic path with risk sensitive evaluation,"In an environment of uncertainty where decisions must be taken, how to make adecision considering the risk? The shortest stochastic path (SSP) problem modelsthe problem of reaching a goal with the least cost. However under uncertainty, abest decision may: minimize expected cost, minimize variance, minimize worstcase, maximize best case, etc. Markov Decision Processes (MDPs) defines optimaldecision in the shortest stochastic path problem as the decision that minimizesexpected cost, however MDPs does not care about the risk. An extension of MDPwhich has few works in Artificial Intelligence literature is Risk Sensitive MDP.RSMDPs considers the risk and integrates expected cost, variance, worst case andbest case in a simply way. We show theoretically the differences between MDPsand RSMDPs for modeling the SSP problem and show the results of each modelin an artificial scenario."
Shortest stochastic path with risk sensitive evaluation,"In an environment of uncertainty where decisions must be taken, how to make adecision considering the risk? The shortest stochastic path (SSP) problem modelsthe problem of reaching a goal with the least cost. However under uncertainty, abest decision may: minimize expected cost, minimize variance, minimize worstcase, maximize best case, etc. Markov Decision Processes (MDPs) defines optimaldecision in the shortest stochastic path problem as the decision that minimizesexpected cost, however MDPs does not care about the risk. An extension of MDPwhich has few works in Artificial Intelligence literature is Risk Sensitive MDP.RSMDPs considers the risk and integrates expected cost, variance, worst case andbest case in a simply way. We show theoretically the differences between MDPsand RSMDPs for modeling the SSP problem and show the results of each modelin an artificial scenario."
The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification," Linear models are often much faster to learn and test than non-linear models. To enable non-linear (with  respect to the original feature space) learning with efficient linear learning algorithms, explicit  embeddings that approximate popular kernels have recently been proposed. However, the kernels are usually  designed so that their dot product in the high dimensional space is efficient, while we want the embedding  itself to be efficient (and rich enough). We propose a simple and effective pairwise piecewise-linear  embedding to approximate models under a factorization-like assumption. The method is based on discretization  and interpolation of individual features values and feature pairs.  The discretization allows us to model  different regimes of the feature space separately, while the interpolation preseves the original continuous  values.  Pairs allows us to approximate cross-feature relationships. Using this embedding within an SVM  strictly generalizes linear SVM. Additionally, some cross-features relations such as feature similarity can  be modeled exactly, while other cross-feature relations are approximated.  We conducted an extensive  experimental study and show results for a large number of datasets. We compared our method to linear,  polynomial, $\chi^2$-like and RBF kernels and embeddings. Our method consistently achieves good performance  significantly outperforming all other methods, including the RBF kernel on the majority of the  datasets. This is in contrast to other proposed embeddings that were faster than kernel methods, but with  lower accuracy. Additionally, our method is as efficient as the second polynomial explicit feature map. The  code will be made available if the paper is accepted."
The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification," Linear models are often much faster to learn and test than non-linear models. To enable non-linear (with  respect to the original feature space) learning with efficient linear learning algorithms, explicit  embeddings that approximate popular kernels have recently been proposed. However, the kernels are usually  designed so that their dot product in the high dimensional space is efficient, while we want the embedding  itself to be efficient (and rich enough). We propose a simple and effective pairwise piecewise-linear  embedding to approximate models under a factorization-like assumption. The method is based on discretization  and interpolation of individual features values and feature pairs.  The discretization allows us to model  different regimes of the feature space separately, while the interpolation preseves the original continuous  values.  Pairs allows us to approximate cross-feature relationships. Using this embedding within an SVM  strictly generalizes linear SVM. Additionally, some cross-features relations such as feature similarity can  be modeled exactly, while other cross-feature relations are approximated.  We conducted an extensive  experimental study and show results for a large number of datasets. We compared our method to linear,  polynomial, $\chi^2$-like and RBF kernels and embeddings. Our method consistently achieves good performance  significantly outperforming all other methods, including the RBF kernel on the majority of the  datasets. This is in contrast to other proposed embeddings that were faster than kernel methods, but with  lower accuracy. Additionally, our method is as efficient as the second polynomial explicit feature map. The  code will be made available if the paper is accepted."
Calibration in Cost-sensitive Multiclass Classification: an Application to Reinforcement Learning,"In this paper we propose a computationally efficient version of classification based policy iteration. The key idea of these algorithms is to view the problem of coming up with the next policy in policy iteration as a classification problem, where a policy is viewed as a classifier.The main novelty is that we propose to replace the non-convex optimization problem of earlier algorithms with a convex one, where a new cost-sensitive surrogate loss is optimized in each iteration.The new loss is shown to be classification calibrated, which makes it a ``sound'' surrogate loss. As far as we know, this is the first calibration result in the context of multiclass classification. As a result, we are able to extend  theoretical guarantees that existed for the previous inefficient classification-based policy algorithms to our efficient method, thereby giving the first computationally efficient, theoretically sound version of classification-based policy iteration."
Adaptive Radial Filtering for Multi-Oriented Character Recognition,"The recognition of fully multi-oriented handwritten characters is a difficult problem. Contrary to univarite signals where the shift invariance property in the Fourier transform can be used, multivariate signals like images require special care for extracting rotation invariant features. The proposed method considers first input features obtained by the Polar transform. A convolutional neural network is then used for extracting features of higher level. This classifier includes in addition the Fast Fourier Transform for extracting shift invariant features at the neural level. The convolutional layers process the image at the pixel level while the Fourier transform and the upper layers of the neural networks process rotation invariant features. The average recognition rate for multi-oriented characters is 91.68\% for the Latin digits."
Near-Tight Bounds for Cross-Validation via Loss Stability,Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds.  In this work we introduce a new and weak measure of stability (\emph{loss stability}) and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal.  Our work thus quantitatively improves the currentbest bounds on cross-validation.
Near-Tight Bounds for Cross-Validation via Loss Stability,Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds.  In this work we introduce a new and weak measure of stability (\emph{loss stability}) and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal.  Our work thus quantitatively improves the currentbest bounds on cross-validation.
Near-Tight Bounds for Cross-Validation via Loss Stability,Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds.  In this work we introduce a new and weak measure of stability (\emph{loss stability}) and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal.  Our work thus quantitatively improves the currentbest bounds on cross-validation.
A new perspective on convex relaxations of sparse SVM,This paper proposes a convex relaxation of a sparse support vector machine (SVM) based on the perspective relaxation of mixed-integer nonlinear programs. We seek to minimize the zero-norm of the hyperplane normal vector with a standard SVM hinge-loss penalty and extend our approach to a zero-one loss penalty. The relaxation that we propose is a second-order cone formulation that can be efficiently solved by standard conic optimization solvers. We compare the optimization properties and classification  performance of the second-order cone formulation with previous sparse SVM formulations suggested in the literature.
A new perspective on convex relaxations of sparse SVM,This paper proposes a convex relaxation of a sparse support vector machine (SVM) based on the perspective relaxation of mixed-integer nonlinear programs. We seek to minimize the zero-norm of the hyperplane normal vector with a standard SVM hinge-loss penalty and extend our approach to a zero-one loss penalty. The relaxation that we propose is a second-order cone formulation that can be efficiently solved by standard conic optimization solvers. We compare the optimization properties and classification  performance of the second-order cone formulation with previous sparse SVM formulations suggested in the literature.
A new perspective on convex relaxations of sparse SVM,This paper proposes a convex relaxation of a sparse support vector machine (SVM) based on the perspective relaxation of mixed-integer nonlinear programs. We seek to minimize the zero-norm of the hyperplane normal vector with a standard SVM hinge-loss penalty and extend our approach to a zero-one loss penalty. The relaxation that we propose is a second-order cone formulation that can be efficiently solved by standard conic optimization solvers. We compare the optimization properties and classification  performance of the second-order cone formulation with previous sparse SVM formulations suggested in the literature.
Bayesian Inference Reveals Synapse-Specific Short-Term Plasticity in Neocortical Microcircuits,"Short-term synaptic plasticity is highly diverse and varies according to brain area, cortical layer, and developmental stage. Since this form of plasticity shapes neural dynamics, its diversity suggests a specific and essential role in neural information processing. Therefore, a correct identification of short-term plasticity is an important step towards understanding and modeling neural systems. Although accurate phenomenological models have been developed, they are usually fitted to experimental data using least-mean square methods. We demonstrate that, for typical synaptic dynamics, such fitting gives unreliable results. Instead, we introduce a Bayesian approach based on a Markov Chain Monte Carlo method, which provides the full posterior distribution over the parameters of the model. We test the approach on simulated data over different regimes and show that common short-term plasticity protocols yield broad distributions over some of the parameters. Finally, we infer the model parameters using experimental data from three different neocortical excitatory connection types, revealing novel synapse-specific distributions and synaptic transfer functions, while the approach yields more robust clustering results. We conclude that ? because short-term plasticity presumably provides key computational features ? our approach to demarcate synapse-specific synaptic dynamics is an important improvement on the state of the art."
Kuhn meets Rosenblatt: Combinatorial Algorithms for Online Structured Prediction,"Online algorithms have been successful at a variety of prediction tasks.  In structured prediction settings, the model produced by an online learner is fed as input to some combinatorial algorithm for producing structured outputs.  This combinatorial algorithm is predominantly considered a black box, which severely limits the control available to the learner.  In this paper, we break open this black box.  For each example, it aims to change its model minimally subject to a margin-based optimality condition on the output.  We define a flexible linear framework that exploits the combinatorial properties of the desired structured output to achieve this in a convex optimization framework. We demonstrate the efficacy of this framework in two applications: dependency parsing via maximum spanning trees and word alignment via bipartite matching."
Kuhn meets Rosenblatt: Combinatorial Algorithms for Online Structured Prediction,"Online algorithms have been successful at a variety of prediction tasks.  In structured prediction settings, the model produced by an online learner is fed as input to some combinatorial algorithm for producing structured outputs.  This combinatorial algorithm is predominantly considered a black box, which severely limits the control available to the learner.  In this paper, we break open this black box.  For each example, it aims to change its model minimally subject to a margin-based optimality condition on the output.  We define a flexible linear framework that exploits the combinatorial properties of the desired structured output to achieve this in a convex optimization framework. We demonstrate the efficacy of this framework in two applications: dependency parsing via maximum spanning trees and word alignment via bipartite matching."
Kuhn meets Rosenblatt: Combinatorial Algorithms for Online Structured Prediction,"Online algorithms have been successful at a variety of prediction tasks.  In structured prediction settings, the model produced by an online learner is fed as input to some combinatorial algorithm for producing structured outputs.  This combinatorial algorithm is predominantly considered a black box, which severely limits the control available to the learner.  In this paper, we break open this black box.  For each example, it aims to change its model minimally subject to a margin-based optimality condition on the output.  We define a flexible linear framework that exploits the combinatorial properties of the desired structured output to achieve this in a convex optimization framework. We demonstrate the efficacy of this framework in two applications: dependency parsing via maximum spanning trees and word alignment via bipartite matching."
Convergence Rate Analysis of MAP Coordinate Minimization Algorithms,"Finding maximum aposteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used.Solving these relaxations efficiently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However,these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence.Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima."
Convergence Rate Analysis of MAP Coordinate Minimization Algorithms,"Finding maximum aposteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used.Solving these relaxations efficiently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However,these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence.Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima."
Dual-view Dirichlet Process Mixture Models for Cross-modal Data Analysis,"We propose Dual-view Dirichlet Process Mixture Models for analyzing cross-modal data. This model is a Bayesian nonparametric model incorporating a prior of infinite mixture distribution of data in any single modality, and it also captures the correspondences between mixture components from different modalities. We develop an efficient variational inference algorithm for learning the joint distribution of cross-modal data which can contribute to identifying latent structures. For prediction tasks, we provide fast approximated methods based on a latent subspace derived from this generative model and kernel regression. Comparisons of experimental results to other state of the art models on benchmark datasets demonstrate the superiority of our model in significantly improving performances on cross-modal information retrieval and image annotation."
Dual-view Dirichlet Process Mixture Models for Cross-modal Data Analysis,"We propose Dual-view Dirichlet Process Mixture Models for analyzing cross-modal data. This model is a Bayesian nonparametric model incorporating a prior of infinite mixture distribution of data in any single modality, and it also captures the correspondences between mixture components from different modalities. We develop an efficient variational inference algorithm for learning the joint distribution of cross-modal data which can contribute to identifying latent structures. For prediction tasks, we provide fast approximated methods based on a latent subspace derived from this generative model and kernel regression. Comparisons of experimental results to other state of the art models on benchmark datasets demonstrate the superiority of our model in significantly improving performances on cross-modal information retrieval and image annotation."
A nonparametric Bayesian approach to learning directed acylic graph structures,"The learning of graph structure is an important problem in machine learning. To this effect, we present a new stochastic process defining a probability distribution on infinite directed acyclic graph structures. This distribution can be used as a nonparametric Bayesian prior on the structure of graphical models having an unbounded number of hidden random variables. The proposed stochastic process is an extension of the cascading Indian buffet process that removes the limitation of purely layered structures. We evaluate the performance of both approaches in discovering the structure of belief networks and compare the structure complexity of the posterior distribution, showing that our approach can extract graphs with fewer units without scarifying predictive precision."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Mixing-time Regularized Policy Gradient,"Policy gradient reinforcement learning (PGRL) methods have received substantial attention as a mean for seeking stochastic policies that maximize a cumulative reward. However, PRRL methods can often take a huge number of learning steps before it finds a reasonable stochastic policy. This learning speed depends on the mixing time of the Markov chains that are given by the policies that PGRL explores. In this paper, we give a new PGRL approach that regularizes the rule of updating the policy with the hitting time that bounds the mixing time.  In particular, hitting-time regressions based on temporal-difference learning are proposed. This will keep the Markov chain compact and can improve the learning efficiency. Numerical experiments show the proposed method outperforms the conventional PG methods."
Mixing-time Regularized Policy Gradient,"Policy gradient reinforcement learning (PGRL) methods have received substantial attention as a mean for seeking stochastic policies that maximize a cumulative reward. However, PRRL methods can often take a huge number of learning steps before it finds a reasonable stochastic policy. This learning speed depends on the mixing time of the Markov chains that are given by the policies that PGRL explores. In this paper, we give a new PGRL approach that regularizes the rule of updating the policy with the hitting time that bounds the mixing time.  In particular, hitting-time regressions based on temporal-difference learning are proposed. This will keep the Markov chain compact and can improve the learning efficiency. Numerical experiments show the proposed method outperforms the conventional PG methods."
Bounded Gaussian Process Regression,"We extend the Gaussian process (GP) framework for regression by introducing two bounded likelihood functions and by considering a specific choice of link function in the previously suggested warped GP. In contrast to warped GP, the extension allows for an explicit specification of the noise distribution within the observational space. We approximate the intractable posterior distributions by the Laplace approximation and expectation propagation and show the properties of the models on a artificial example. We finally consider two real-world datasets originating from perceptual rating experiments which indicate a significant gain obtained with the proposed explicit noise model extension."
Bounded Gaussian Process Regression,"We extend the Gaussian process (GP) framework for regression by introducing two bounded likelihood functions and by considering a specific choice of link function in the previously suggested warped GP. In contrast to warped GP, the extension allows for an explicit specification of the noise distribution within the observational space. We approximate the intractable posterior distributions by the Laplace approximation and expectation propagation and show the properties of the models on a artificial example. We finally consider two real-world datasets originating from perceptual rating experiments which indicate a significant gain obtained with the proposed explicit noise model extension."
Bounded Gaussian Process Regression,"We extend the Gaussian process (GP) framework for regression by introducing two bounded likelihood functions and by considering a specific choice of link function in the previously suggested warped GP. In contrast to warped GP, the extension allows for an explicit specification of the noise distribution within the observational space. We approximate the intractable posterior distributions by the Laplace approximation and expectation propagation and show the properties of the models on a artificial example. We finally consider two real-world datasets originating from perceptual rating experiments which indicate a significant gain obtained with the proposed explicit noise model extension."
Multi-Task Approximate Bayes-Optimal Dual Adaptive Control,"Optimally controlling a system with unknown properties requires trading off the dual requirements of exploration by generating probe signals and exploiting any obtained knowledge to minimize costs. Bayes-optimal approaches to dual adaptive control make this trade-off optimally, but in general have no closed form solution. Furthermore, they require an informative prior distribution to be maximally efficient. An agent with experience in multiple related tasks can exploit its knowledge about the distribution the tasks are drawn from to improve the quality of its control actions. In this paper, we propose extending an approximate scheme for Bayes-optimal dual-adaptive control to a multi-task setting. Evaluations of ourapproach on an object-pushing task show the approximate dual-adaptive controller is able to take advantage of its experience with similar tasks."
Multi-Task Approximate Bayes-Optimal Dual Adaptive Control,"Optimally controlling a system with unknown properties requires trading off the dual requirements of exploration by generating probe signals and exploiting any obtained knowledge to minimize costs. Bayes-optimal approaches to dual adaptive control make this trade-off optimally, but in general have no closed form solution. Furthermore, they require an informative prior distribution to be maximally efficient. An agent with experience in multiple related tasks can exploit its knowledge about the distribution the tasks are drawn from to improve the quality of its control actions. In this paper, we propose extending an approximate scheme for Bayes-optimal dual-adaptive control to a multi-task setting. Evaluations of ourapproach on an object-pushing task show the approximate dual-adaptive controller is able to take advantage of its experience with similar tasks."
Novelty Detection in Multi-Instance Multi-Label Learning,"Novelty detection plays an important role in machine learning and signal processing. However, this problem has not been previously studied in multi-instance multi-label learning (MIML). In MIML, the training dataset consists of bags of instances associated with sets of labels. It is a common assumption that every instance in a bag is associated with one of the labels in the set. In this paper, we focus on the scenario where a bag may contain novel class instances, which are not associated with the bag-level label set. The goal is to determine for any given instance in a new bag whether it belongs to a known class or not. Detecting novelty in the MIML setting captures manyreal-world phenomena and has many potential applications. For example, in a collection of tagged images not all objects in a given image are represented in the image tag set. Discovering an object which has not been taggedbefore can be useful for the purpose of soliciting a label for the new object. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed methods, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in unseen bags."
Online Learning of Hierarchical Balancing Strategies for Bipedal Humanoid Robots,"  Bipedal humanoid robots will fall under unforeseen perturbations without active stabilization.  Humans use dynamic full body behaviors in response to perturbations, and recent bipedal robot controllers for balancing are based upon human biomechanical responses.  These controllers assume simple physical models and require very accurate state information, making them less effective on physical robots in uncertain environments.  To address this issue, we propose a hierarchical control architecture that learns to switch between three low-level biomechanically-motivated strategies in response to perturbations.  The high level strategy is learned in an online fashion from state trajectory information gathered during experimental trials.  This learning approach is evaluated in physics-based simulations as well as on a small humanoid robot. Our results demonstrate how well this method stabilizes the robot during walking and whole body manipulation tasks."
Online Learning of Hierarchical Balancing Strategies for Bipedal Humanoid Robots,"  Bipedal humanoid robots will fall under unforeseen perturbations without active stabilization.  Humans use dynamic full body behaviors in response to perturbations, and recent bipedal robot controllers for balancing are based upon human biomechanical responses.  These controllers assume simple physical models and require very accurate state information, making them less effective on physical robots in uncertain environments.  To address this issue, we propose a hierarchical control architecture that learns to switch between three low-level biomechanically-motivated strategies in response to perturbations.  The high level strategy is learned in an online fashion from state trajectory information gathered during experimental trials.  This learning approach is evaluated in physics-based simulations as well as on a small humanoid robot. Our results demonstrate how well this method stabilizes the robot during walking and whole body manipulation tasks."
Human memory search as a random walk in a semantic network,"The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single, undirected process rather than one process for exploring a cluster and one process for switching between clusters."
Modeling Two Functionally Distinct Ventral Pathways Representing Static Form and Color/Texture,"Anatomical and physiological data suggests that the ventral visual pathway of the primate brain is subdivided into specialized processing modalities. We combine a model of color/texture processing with a separately developed model of shape/form processing to determine whether such pathways can be both functionally independent and complimentary. Our hypothesis was that the combination would yield better performance on an invariant object localization and classification task than either model alone. Functional independence is established if the optimal combination corresponds to the Boolean rules used for combining two statistically independent binary classifiers. To extract color/texture information, we learned a sparse dictionary of features from representative training data, pooled over the dictionary elements using a winner-take-all heuristic and then clustered the pooled data groups using a k-means algorithm. In tandem, we extracted shape information by pre-processing the image with a canny edge filter, then computed difference kernels based on co-occurrence statistics for edge combinations characteristic of the target category. We represented the two pathways as binary classifiers and combined them with optimal Boolean operators, as defined using a Neyman-Pearson theorem for Receiver Operating Characteristics (ROC) curves. Using ground-truth for a high-definition video-stream from a helicopter flying over Los Angeles, CA, we demonstrate that the two pathways are functionally independent and when combined perform substantially better than either pathway alone. Our results suggest that the separate processing modalities found in the primate ventral visual pathway represent functionally independent and complimentary approaches to viewpoint invariant object detection and localization."
Modeling Two Functionally Distinct Ventral Pathways Representing Static Form and Color/Texture,"Anatomical and physiological data suggests that the ventral visual pathway of the primate brain is subdivided into specialized processing modalities. We combine a model of color/texture processing with a separately developed model of shape/form processing to determine whether such pathways can be both functionally independent and complimentary. Our hypothesis was that the combination would yield better performance on an invariant object localization and classification task than either model alone. Functional independence is established if the optimal combination corresponds to the Boolean rules used for combining two statistically independent binary classifiers. To extract color/texture information, we learned a sparse dictionary of features from representative training data, pooled over the dictionary elements using a winner-take-all heuristic and then clustered the pooled data groups using a k-means algorithm. In tandem, we extracted shape information by pre-processing the image with a canny edge filter, then computed difference kernels based on co-occurrence statistics for edge combinations characteristic of the target category. We represented the two pathways as binary classifiers and combined them with optimal Boolean operators, as defined using a Neyman-Pearson theorem for Receiver Operating Characteristics (ROC) curves. Using ground-truth for a high-definition video-stream from a helicopter flying over Los Angeles, CA, we demonstrate that the two pathways are functionally independent and when combined perform substantially better than either pathway alone. Our results suggest that the separate processing modalities found in the primate ventral visual pathway represent functionally independent and complimentary approaches to viewpoint invariant object detection and localization."
Modeling Two Functionally Distinct Ventral Pathways Representing Static Form and Color/Texture,"Anatomical and physiological data suggests that the ventral visual pathway of the primate brain is subdivided into specialized processing modalities. We combine a model of color/texture processing with a separately developed model of shape/form processing to determine whether such pathways can be both functionally independent and complimentary. Our hypothesis was that the combination would yield better performance on an invariant object localization and classification task than either model alone. Functional independence is established if the optimal combination corresponds to the Boolean rules used for combining two statistically independent binary classifiers. To extract color/texture information, we learned a sparse dictionary of features from representative training data, pooled over the dictionary elements using a winner-take-all heuristic and then clustered the pooled data groups using a k-means algorithm. In tandem, we extracted shape information by pre-processing the image with a canny edge filter, then computed difference kernels based on co-occurrence statistics for edge combinations characteristic of the target category. We represented the two pathways as binary classifiers and combined them with optimal Boolean operators, as defined using a Neyman-Pearson theorem for Receiver Operating Characteristics (ROC) curves. Using ground-truth for a high-definition video-stream from a helicopter flying over Los Angeles, CA, we demonstrate that the two pathways are functionally independent and when combined perform substantially better than either pathway alone. Our results suggest that the separate processing modalities found in the primate ventral visual pathway represent functionally independent and complimentary approaches to viewpoint invariant object detection and localization."
Lifted Parameter Learning for Markov Logic,"Statistical relational learning (SRL) augments probabilistic models with relational representations and facilitates reasoning over sets of objects. When learning the probabilistic parameters for SRL models, however, one often resorts to reasoning over individual objects. We propose to harness the full power of relational representations in the learning phase, by using lifted inference. For this we compile a Markov logic network into a compact and efficient first-order data structure and use weighted first-order model counting (WFOMC) to calculate the likelihood of the data in a lifted manner. By exploiting the relational structure in the model, it is possible to dramatically improve the run time of the likelihood calculation and learn parameters more accurately. This allows us to calculate the exact likelihood of the data for models where previously only approximate inference was feasible. Results on real-world data sets shows that this approach learns more accurate models."
Lifted Parameter Learning for Markov Logic,"Statistical relational learning (SRL) augments probabilistic models with relational representations and facilitates reasoning over sets of objects. When learning the probabilistic parameters for SRL models, however, one often resorts to reasoning over individual objects. We propose to harness the full power of relational representations in the learning phase, by using lifted inference. For this we compile a Markov logic network into a compact and efficient first-order data structure and use weighted first-order model counting (WFOMC) to calculate the likelihood of the data in a lifted manner. By exploiting the relational structure in the model, it is possible to dramatically improve the run time of the likelihood calculation and learn parameters more accurately. This allows us to calculate the exact likelihood of the data for models where previously only approximate inference was feasible. Results on real-world data sets shows that this approach learns more accurate models."
Lifted Parameter Learning for Markov Logic,"Statistical relational learning (SRL) augments probabilistic models with relational representations and facilitates reasoning over sets of objects. When learning the probabilistic parameters for SRL models, however, one often resorts to reasoning over individual objects. We propose to harness the full power of relational representations in the learning phase, by using lifted inference. For this we compile a Markov logic network into a compact and efficient first-order data structure and use weighted first-order model counting (WFOMC) to calculate the likelihood of the data in a lifted manner. By exploiting the relational structure in the model, it is possible to dramatically improve the run time of the likelihood calculation and learn parameters more accurately. This allows us to calculate the exact likelihood of the data for models where previously only approximate inference was feasible. Results on real-world data sets shows that this approach learns more accurate models."
Bayesian n-Choose-k Models for Classification and Ranking,"In categorical data there is often structure in the number ofvariables that take on each label.  For example, the total number of objects in an image and the number of highlyrelevant documents per query in web search both tend to follow a structured distribution.In this paper, we study a probabilistic model that explicitly includes a priordistribution over such counts, along with a count-conditional likelihood thatdefines probabilities over all subsets of a given size.When labels are binary and the prior over counts is a Poisson-Binomialdistribution, a standard logistic regression model is recovered, but for othercount distributions, such priors induce global dependencies and combinatoricsthat appear to complicate learning and inference. However, we demonstrate that simple, efficient learning procedures can bederived for more general forms of this model.We show the utility of the formulation by exploring multi-object classification asmaximum likelihood learning, and ranking and top-K classification asloss-sensitive learning. "
Bayesian n-Choose-k Models for Classification and Ranking,"In categorical data there is often structure in the number ofvariables that take on each label.  For example, the total number of objects in an image and the number of highlyrelevant documents per query in web search both tend to follow a structured distribution.In this paper, we study a probabilistic model that explicitly includes a priordistribution over such counts, along with a count-conditional likelihood thatdefines probabilities over all subsets of a given size.When labels are binary and the prior over counts is a Poisson-Binomialdistribution, a standard logistic regression model is recovered, but for othercount distributions, such priors induce global dependencies and combinatoricsthat appear to complicate learning and inference. However, we demonstrate that simple, efficient learning procedures can bederived for more general forms of this model.We show the utility of the formulation by exploring multi-object classification asmaximum likelihood learning, and ranking and top-K classification asloss-sensitive learning. "
Bayesian n-Choose-k Models for Classification and Ranking,"In categorical data there is often structure in the number ofvariables that take on each label.  For example, the total number of objects in an image and the number of highlyrelevant documents per query in web search both tend to follow a structured distribution.In this paper, we study a probabilistic model that explicitly includes a priordistribution over such counts, along with a count-conditional likelihood thatdefines probabilities over all subsets of a given size.When labels are binary and the prior over counts is a Poisson-Binomialdistribution, a standard logistic regression model is recovered, but for othercount distributions, such priors induce global dependencies and combinatoricsthat appear to complicate learning and inference. However, we demonstrate that simple, efficient learning procedures can bederived for more general forms of this model.We show the utility of the formulation by exploring multi-object classification asmaximum likelihood learning, and ranking and top-K classification asloss-sensitive learning. "
Bayesian n-Choose-k Models for Classification and Ranking,"In categorical data there is often structure in the number ofvariables that take on each label.  For example, the total number of objects in an image and the number of highlyrelevant documents per query in web search both tend to follow a structured distribution.In this paper, we study a probabilistic model that explicitly includes a priordistribution over such counts, along with a count-conditional likelihood thatdefines probabilities over all subsets of a given size.When labels are binary and the prior over counts is a Poisson-Binomialdistribution, a standard logistic regression model is recovered, but for othercount distributions, such priors induce global dependencies and combinatoricsthat appear to complicate learning and inference. However, we demonstrate that simple, efficient learning procedures can bederived for more general forms of this model.We show the utility of the formulation by exploring multi-object classification asmaximum likelihood learning, and ranking and top-K classification asloss-sensitive learning. "
No More Pesky Learning Rates,"The performance of stochastic gradient descent (SGD) dependscritically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations accross samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained throughsystematic search, and effectively removes the need for learning rate tuning. "
No More Pesky Learning Rates,"The performance of stochastic gradient descent (SGD) dependscritically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations accross samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained throughsystematic search, and effectively removes the need for learning rate tuning. "
Consensus Ranking with Signed Permutations,Signed permutations (also known as the hyperoctahedral group) are used in modeling genome rearrangements. The algorithmic problems they raise are computationally demanding when not NP-hard. This paper presents an algorithm for learning consensus ranking between signed permutations under the inversion distance. This can be extended to estimate a natural class of exponential models over the group of signed permutations. We investigate experimentally the efficiency of our algorithm for modeling data generated by random reversals.
Consensus Ranking with Signed Permutations,Signed permutations (also known as the hyperoctahedral group) are used in modeling genome rearrangements. The algorithmic problems they raise are computationally demanding when not NP-hard. This paper presents an algorithm for learning consensus ranking between signed permutations under the inversion distance. This can be extended to estimate a natural class of exponential models over the group of signed permutations. We investigate experimentally the efficiency of our algorithm for modeling data generated by random reversals.
Explanation with Causal Logic Models,"Despite their success in transferring the powerful human faculty of causal reasoning to a mathematical and computational form, causal models have not been widely used in the context of core AI applications such as robotics.  In this paper, we define Causal Logic Models (CLMs), a new discrete-time, probabilistic, first-order representation which uses causality as a fundamental building block. Rather than merely converting causal rules to first-order logic as various methods in Statistical Relational Learning have done, we treat the causal rules as basic primitives which cannot be altered without changing the system. We present an algorithm using CLMs for one type of causal reasoning known as causal explanation, i.e., understanding the causal links between events spaced out in time. Using CLMs rather than traditional fixed causal models allows causal explanation to be performed in dynamic situations where variables of interest are not necessarily known a priori. We show empirically that CLMs produce intuitive and succinct explanations given an evidence set, more in line with human causal reasoning. We also discuss how CLMs other types of causal reasoning such as prediction and counterfactuals can look qualitatively different from their counterparts with other representations."
Explanation with Causal Logic Models,"Despite their success in transferring the powerful human faculty of causal reasoning to a mathematical and computational form, causal models have not been widely used in the context of core AI applications such as robotics.  In this paper, we define Causal Logic Models (CLMs), a new discrete-time, probabilistic, first-order representation which uses causality as a fundamental building block. Rather than merely converting causal rules to first-order logic as various methods in Statistical Relational Learning have done, we treat the causal rules as basic primitives which cannot be altered without changing the system. We present an algorithm using CLMs for one type of causal reasoning known as causal explanation, i.e., understanding the causal links between events spaced out in time. Using CLMs rather than traditional fixed causal models allows causal explanation to be performed in dynamic situations where variables of interest are not necessarily known a priori. We show empirically that CLMs produce intuitive and succinct explanations given an evidence set, more in line with human causal reasoning. We also discuss how CLMs other types of causal reasoning such as prediction and counterfactuals can look qualitatively different from their counterparts with other representations."
Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models,"Recent experiments have demonstrated that humans and animals typically reasonprobabilistically about their environment. This ability requires a neural codethat represents probability distributions and neural circuits that are capable ofimplementing the operations of probabilistic inference. The proposed probabilisticpopulation coding (PPC) framework provides a statistically efficient neuralrepresentation of probability distributions that is both broadly consistent withphysiological measurements and capable of implementing some of the basic operationsof probabilistic inference in a biologically plausible way. However, theseexperiments and the corresponding neural models have largely focused on simple(tractable) probabilistic computations such as cue combination, coordinate transformations,and decision making. As a result it remains unclear how to generalizethis framework to more complex probabilistic computations. Here we addressthis short coming by showing that a very general approximate inference algorithmknown as Variational Bayesian Expectation Maximization can be implementedwithin the linear PPC framework. We apply this approach to a generic problemfaced by any given layer of cortex, namely the identification of latent causes ofcomplex mixtures of spikes. We identify a formal equivalent between this spikepattern demixing problem and topic models used for document classification, inparticular Latent Dirichlet Allocation (LDA). We then construct a neural networkimplementation of variational inference and learning for LDA that utilizes a linearPPC. This network relies critically on two non-linear operations: divisive normalizationand super-linear facilitation, both of which are ubiquitously observed inneural circuits. We also demonstrate how online learning can be achieved using avariation of Hebb?s rule and describe an extesion of this work which allows us todeal with time varying and correlated latent causes."
Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models,"Recent experiments have demonstrated that humans and animals typically reasonprobabilistically about their environment. This ability requires a neural codethat represents probability distributions and neural circuits that are capable ofimplementing the operations of probabilistic inference. The proposed probabilisticpopulation coding (PPC) framework provides a statistically efficient neuralrepresentation of probability distributions that is both broadly consistent withphysiological measurements and capable of implementing some of the basic operationsof probabilistic inference in a biologically plausible way. However, theseexperiments and the corresponding neural models have largely focused on simple(tractable) probabilistic computations such as cue combination, coordinate transformations,and decision making. As a result it remains unclear how to generalizethis framework to more complex probabilistic computations. Here we addressthis short coming by showing that a very general approximate inference algorithmknown as Variational Bayesian Expectation Maximization can be implementedwithin the linear PPC framework. We apply this approach to a generic problemfaced by any given layer of cortex, namely the identification of latent causes ofcomplex mixtures of spikes. We identify a formal equivalent between this spikepattern demixing problem and topic models used for document classification, inparticular Latent Dirichlet Allocation (LDA). We then construct a neural networkimplementation of variational inference and learning for LDA that utilizes a linearPPC. This network relies critically on two non-linear operations: divisive normalizationand super-linear facilitation, both of which are ubiquitously observed inneural circuits. We also demonstrate how online learning can be achieved using avariation of Hebb?s rule and describe an extesion of this work which allows us todeal with time varying and correlated latent causes."
Hierarchical Estimation of Locomotion Mode and Gait Cycle using Switching Unscented Kalman Filters,"As we walk, the state of our limbs varies cyclically, while other variables such as walking speed vary along continuous axes and all are nonlinearly related to one another and to potentially observed aspects of gait. Moreover, our locomotion may switch between modes such as walking and standing. Here we present an efficient solution to nonlinear estimation problems with both cyclical and contin- uous state variables with dynamics that undergo switches. This solution is based on a Hidden Markov Model (HMM) over Unscented Kalman Filters (UKF). The resulting algorithm captures the total variability of the gait parameters with a to- tal variance accounted for (R-squared) of 0.99, outperforming existing linear regression estimators (R-squared = 0.92)"
Cost-Sensitive Exploration in Bayesian Reinforcement Learning,"In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems."
Modeling Expertise of Crowd by Normalized Gamma Decomposition,"We develop a flexible framework for modeling the expertise of a crowd, called normalized gamma decomposition of a confusion matrix.The proposed framework enables us to model the ability of workers, the labeling tendency (confusion) of workers, and the items' difficulties to correctly annotate.Moreover, we can apply our framework to a heterogynous labeling problem where we have to analyze a task that includes different types of labels."
Modeling Expertise of Crowd by Normalized Gamma Decomposition,"We develop a flexible framework for modeling the expertise of a crowd, called normalized gamma decomposition of a confusion matrix.The proposed framework enables us to model the ability of workers, the labeling tendency (confusion) of workers, and the items' difficulties to correctly annotate.Moreover, we can apply our framework to a heterogynous labeling problem where we have to analyze a task that includes different types of labels."
Modeling Expertise of Crowd by Normalized Gamma Decomposition,"We develop a flexible framework for modeling the expertise of a crowd, called normalized gamma decomposition of a confusion matrix.The proposed framework enables us to model the ability of workers, the labeling tendency (confusion) of workers, and the items' difficulties to correctly annotate.Moreover, we can apply our framework to a heterogynous labeling problem where we have to analyze a task that includes different types of labels."
Learning with Partially Absorbing Random Walks,"We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$, and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\mathcal{S}$, while dropping sharply outside $\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning."
Instance Level Multiple Instance Learning Using Similarity Preserving Quasi Cliques,In this paper we introduce an instance-level approach to multiple instance learning. Our bottom-up approach learns a discriminative notion of similarity between instances in positive bags and use it to form a discriminative similarity graph. We then introduce the notion of similarity preserving quasi-cliques that aims at discovering large quasi-cliques with high scores of within-clique similarities. We argue that such large cliques provide clue to infer the underlying structure between positive instances. We use a ranking function that takes into account pairwise similarities coupled with prospectiveness of edges to score all positive instances. We show that these scores yield to positive instance discovery. Our experimental evaluations show that our method outperforms state-of-the-art MIL methods both at the bag-level and instance-level predictions in standard benchmarks and image and text datasets.
Instance Level Multiple Instance Learning Using Similarity Preserving Quasi Cliques,In this paper we introduce an instance-level approach to multiple instance learning. Our bottom-up approach learns a discriminative notion of similarity between instances in positive bags and use it to form a discriminative similarity graph. We then introduce the notion of similarity preserving quasi-cliques that aims at discovering large quasi-cliques with high scores of within-clique similarities. We argue that such large cliques provide clue to infer the underlying structure between positive instances. We use a ranking function that takes into account pairwise similarities coupled with prospectiveness of edges to score all positive instances. We show that these scores yield to positive instance discovery. Our experimental evaluations show that our method outperforms state-of-the-art MIL methods both at the bag-level and instance-level predictions in standard benchmarks and image and text datasets.
Diagnose and Decide: An Optimal Bayesian Approach 004,"Many real-world scenarios require making informed choices after some sequence of actions that yield noisy information about a latent state.  Prior research has mostly focused on generic methods that struggle to scale to large domains. We focus on a subclass of such problems with two particular characteristics. First, though information gathering actions or tests only provide noisy information about the hidden state, once performed a test will always yield the same result. This means it is sufficient to perform each test once. Second, we assume that test costs can be expressed in the same units as costs of the final decisions made. We call such scenarios diagnose-and-decide problems. We prove diagnose-and-decide problems are a special subclass of POMDPs for which the optimal policy can be computed in time polynomial in the set of possible tests' outcomes. We develop a new simple algorithm which is able to take advantage of the unique structure in our problem while guaranteeing optimality. We demonstrate the advantages of our approach over greedy and traditional POMDP methods in two simulations based on real-world data (colon cancer screening and object recognition) as well as a large synthetic domain. "
Robust Distance Metric Learning via Simultaneous $\ell_1$-Norm Minimization and Maximization,"Traditional distance metric learning with side information often formulates the learning objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Since covariance matrices are prone to outliers, it is desirable to develop a robust distance metric learning method. In this paper, motivated by existing studies that improve the robustness of machine learning models via the L1-norm, we propose a robust formulation of distance metric learning using the L1-norm distances. However, solving the formulated objective is very challenging because it simultaneously minimizes and maximizes (minmax) the non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is scarcely studied in literature. Extensive empirical evaluations on the proposed robust distance metric learning method are performed, in which our new method outperforms related state-of-the-art methods in a variety of experimental settings and demonstrate their effectiveness in the clustering tasks on both noiseless and noisy data.  "
Robust Distance Metric Learning via Simultaneous $\ell_1$-Norm Minimization and Maximization,"Traditional distance metric learning with side information often formulates the learning objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Since covariance matrices are prone to outliers, it is desirable to develop a robust distance metric learning method. In this paper, motivated by existing studies that improve the robustness of machine learning models via the L1-norm, we propose a robust formulation of distance metric learning using the L1-norm distances. However, solving the formulated objective is very challenging because it simultaneously minimizes and maximizes (minmax) the non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is scarcely studied in literature. Extensive empirical evaluations on the proposed robust distance metric learning method are performed, in which our new method outperforms related state-of-the-art methods in a variety of experimental settings and demonstrate their effectiveness in the clustering tasks on both noiseless and noisy data.  "
Relating Structural MRI and Behavioral Measure to Functional MRI Measures,"Structural magnetic resonance imaging (MRI) and behavioral measures in the elderly have been shown to be associated in someway or another with functional MRI measures in past studies. However, the goal of this study is to analyze the ability of structural MRI and behavioral measures to predict functional measures in relationship to one another. This study uses both linear regression and artificial neural networks to achieve this goal. The results show that linear regression performs better and the features that best relate to functional measures include age, mini-mental state examination scores, total regional volume, number of tracks connecting regions, and regional white matter hyperintensities volume."
Extending generalized delta rules for efficient Hessian calculations through backpropagation,"Recent extensions of first-order backpropagation (BP), also known asgeneralized delta rules, of Rumelhart et al. (1986) lead to the development of efficient Hessian calculations for second-orderoptimization (e.g., Levenberg-Marquardt methods).  Consider, for instance, the evaluation of  the so-called Gauss-Newton Hessian matrix J'*J of size n x n when optimizing a multi-layer neural network that has multipleZ outputs (Z > 1).  Fairbank & Alonso~(2012) described how to use first-order BPfor (Z+n) times per data pattern in forming Z rows of J and then J'*Jexplicitly column by column.  Their claim is thatthe proposed method works faster than the ``standard'' algebraic method bya factor of Z. Yet, their analysis totally ignores several key factors thatare already discussed individually in other computational techniques.Even under their assumption 1 << Z <= square root of n,the standard method can work faster in some situations.By combining the strengths of existing algorithms,we have derived an efficient algorithm that performs backward passes only forB times, followed by some algebraic manipulations, where B denotes the total number of hidden nodes.Since B is approximately equal to sqaure root of n, our improvement would be significant.We also show its further extensions and an efficient matrix-freealgorithm that combines BP with a forward mode of automatic differentiation."
Learning mixture models with the hierarchical expectation maximization algorithm,"Driven by the need for computationally efficient parameter estimation from large, web-scale data sets, the hierarchical EM (HEM) algorithm has been proposed and proven effective for a variety of modeling tasks and applications. In this paper, we investigate the benefits of HEM as a general-purpose algorithm for parameter estimation in mixture models, compared to regular EM. First, we re-derive the algorithm in more generality, for generic exponential family distributions, with and without unobserved variables. Second, we discuss and experimentally verify its benefits across a broad spectrum of model classes and applications. Besides scalability, HEM's implicit regularization and adaptation for multiple instance learning make it an appealing alternative to standard EM, for practitioners."
An Integrated Method for Causality Structure and Hidden Causes Discovery,"When we focus on causality research of real-life application, we may need to discover causal structure accurately and detecting hidden causes automatically without a priori restriction of hidden variable (non)existence in the underlying causal network. Unfortunately, most existing causal discovery algorithms may confuse direction between cause and effect and measure the hidden causes dif?cultly. Motivated by these facts, we present an integrated method for causal discovery with automatic detection of probable hidden causes, where the causal structure is induced by conditional independency testing and ?cliques? of initial variables with relationships between ?cliques?. Clues of hidden cause are obtained when we identify redundant edges.  In this paper, we distinguish causality of be-to-be and not be-to-not. As a sample application, we present our integrated method (ICIC)for LUCAS released in NIPS 2008 and show the results ?nally."
An Integrated Method for Causality Structure and Hidden Causes Discovery,"When we focus on causality research of real-life application, we may need to discover causal structure accurately and detecting hidden causes automatically without a priori restriction of hidden variable (non)existence in the underlying causal network. Unfortunately, most existing causal discovery algorithms may confuse direction between cause and effect and measure the hidden causes dif?cultly. Motivated by these facts, we present an integrated method for causal discovery with automatic detection of probable hidden causes, where the causal structure is induced by conditional independency testing and ?cliques? of initial variables with relationships between ?cliques?. Clues of hidden cause are obtained when we identify redundant edges.  In this paper, we distinguish causality of be-to-be and not be-to-not. As a sample application, we present our integrated method (ICIC)for LUCAS released in NIPS 2008 and show the results ?nally."
An Integrated Method for Causality Structure and Hidden Causes Discovery,"When we focus on causality research of real-life application, we may need to discover causal structure accurately and detecting hidden causes automatically without a priori restriction of hidden variable (non)existence in the underlying causal network. Unfortunately, most existing causal discovery algorithms may confuse direction between cause and effect and measure the hidden causes dif?cultly. Motivated by these facts, we present an integrated method for causal discovery with automatic detection of probable hidden causes, where the causal structure is induced by conditional independency testing and ?cliques? of initial variables with relationships between ?cliques?. Clues of hidden cause are obtained when we identify redundant edges.  In this paper, we distinguish causality of be-to-be and not be-to-not. As a sample application, we present our integrated method (ICIC)for LUCAS released in NIPS 2008 and show the results ?nally."
Learning Latent Factor Models of Human Travel,"This paper describes probability models for human travel data, using learned latent factors.  	Latent factors represent interpretable properties including travel distance, desirability of destinations, and affinity between locations.  Individuals are clustered into distinct classes of travel models. The latent factors combine in a multiplicative manner, and are learned using maximum likelihood.  The resulting models exhibit significant improvements in predictive power over previous methods, while also using far fewer parameters than histogram-based methods.  The method is demonstrated from travel datasets collected from Flickr data and from taxi travel, and demonstrates improved predictive power over previous approaches."
Learning Latent Factor Models of Human Travel,"This paper describes probability models for human travel data, using learned latent factors.  	Latent factors represent interpretable properties including travel distance, desirability of destinations, and affinity between locations.  Individuals are clustered into distinct classes of travel models. The latent factors combine in a multiplicative manner, and are learned using maximum likelihood.  The resulting models exhibit significant improvements in predictive power over previous methods, while also using far fewer parameters than histogram-based methods.  The method is demonstrated from travel datasets collected from Flickr data and from taxi travel, and demonstrates improved predictive power over previous approaches."
Sparse projections onto the simplex,"The past decade has seen the rise of $\ell_1$-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard $\ell_1$-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation."
Sparse projections onto the simplex,"The past decade has seen the rise of $\ell_1$-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard $\ell_1$-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation."
Sparse projections onto the simplex,"The past decade has seen the rise of $\ell_1$-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard $\ell_1$-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation."
Coordinated collision avoidance of multiple agents with continuous stochastic plant dynamics,"We describe an approach to multi-agent planning under continuous stochastic dynamics. The approach yields collision-free state trajectories with adjustably high certainty while aiming for lowsocial cost.  To this end we describe a collision-detection module based on a distribution-independent probabilistic bound and compare fixed priority and auction-based coordination protocols to resolve collisions.While our experiments were conducted with agents governed by linear stochastic dynamics with state-independent noise, our methods extend to more general settings of state-dependent noise and with non-linear dynamics."
Coordinated collision avoidance of multiple agents with continuous stochastic plant dynamics,"We describe an approach to multi-agent planning under continuous stochastic dynamics. The approach yields collision-free state trajectories with adjustably high certainty while aiming for lowsocial cost.  To this end we describe a collision-detection module based on a distribution-independent probabilistic bound and compare fixed priority and auction-based coordination protocols to resolve collisions.While our experiments were conducted with agents governed by linear stochastic dynamics with state-independent noise, our methods extend to more general settings of state-dependent noise and with non-linear dynamics."
Coordinated collision avoidance of multiple agents with continuous stochastic plant dynamics,"We describe an approach to multi-agent planning under continuous stochastic dynamics. The approach yields collision-free state trajectories with adjustably high certainty while aiming for lowsocial cost.  To this end we describe a collision-detection module based on a distribution-independent probabilistic bound and compare fixed priority and auction-based coordination protocols to resolve collisions.While our experiments were conducted with agents governed by linear stochastic dynamics with state-independent noise, our methods extend to more general settings of state-dependent noise and with non-linear dynamics."
Locating Changes in Highly-Dependent Data with Unknown Number of Change Points,"The problem of multiple change-point estimation is considered for time-series sequences with unknown number of change points. A consistency framework is suggested that is suitable for highly-dependent time series, and an asymptotically consistent algorithm is proposed.  The only assumption that we need to establish consistency is  that the data are generated by stationary ergodic time-series distributions. No modeling, independence or parametric assumptions are made; the data are allowed to be dependent and the dependence can be of arbitrary form. The theoretical result is complemented with an experimental evaluation."
Locating Changes in Highly-Dependent Data with Unknown Number of Change Points,"The problem of multiple change-point estimation is considered for time-series sequences with unknown number of change points. A consistency framework is suggested that is suitable for highly-dependent time series, and an asymptotically consistent algorithm is proposed.  The only assumption that we need to establish consistency is  that the data are generated by stationary ergodic time-series distributions. No modeling, independence or parametric assumptions are made; the data are allowed to be dependent and the dependence can be of arbitrary form. The theoretical result is complemented with an experimental evaluation."
Modeling Salient Object-Object Interactions to Generate Textual Descriptions for Natural Images,"In this paper we propose a new method for automatically generating textual descriptions of images. Our method consists of two main steps. Using saliency maps, it detects the areas of interests in the image and then creates the description by recognizing the interactions between detected objects within those areas. These interactions are modeled using the pose(body parts configuration) of the objects. To create sentences a syntactic model is used that builds subtrees around the detected objects and then combines those subtrees using recognized interaction. Our Results show the improved accuracy of the descriptions generated by our algorithm."
Probabilistic Event Cascades for Alzheimer's disease," Accurate and detailed models of the progression of neurodegenerative diseases such as  Alzheimer's (AD) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments. In this paper, we introduce the ALPACA (Alzheimer's disease Probabilistic Cascades) model, a generative model linking latent Alzheimer's progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a fixed ordering of events, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed disease progression models. We describe efficient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer's patients from the  Alzheimer's Disease Neuroimaging Initiative."
Modelling the Lexicon in Unsupervised Part of Speech Induction,"Automatically inducing the syntactic part-of-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single  part-of-speech tag. This one-tag-per-type heuristic, which counters the tendency of Hidden Markov Model based taggers to over generate tags for a given word type, is clearly incompatible with basic syntactic theory. In this paper we extend the current state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model of the lexicon. In doing so we are able to incorporate a soft bias towards inducing few tags per type. We develop a novel particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with the state-of-the-art without making any unrealistic restrictions."
Modelling the Lexicon in Unsupervised Part of Speech Induction,"Automatically inducing the syntactic part-of-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single  part-of-speech tag. This one-tag-per-type heuristic, which counters the tendency of Hidden Markov Model based taggers to over generate tags for a given word type, is clearly incompatible with basic syntactic theory. In this paper we extend the current state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model of the lexicon. In doing so we are able to incorporate a soft bias towards inducing few tags per type. We develop a novel particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with the state-of-the-art without making any unrealistic restrictions."
Efficient and direct estimation of a neural subunit model for sensory coding,"Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a `subunit' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (``convolutional??) copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the `subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency."
Efficient and direct estimation of a neural subunit model for sensory coding,"Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a `subunit' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (``convolutional??) copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the `subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency."
Efficient and direct estimation of a neural subunit model for sensory coding,"Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a `subunit' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (``convolutional??) copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the `subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency."
Accurate Deterministic Parsing with Syntax-Directed Embeddings,"Distributional analysis of words by embedding in continuous vector spaces has been effective at supplying features for many NLP tasks.  In contrast to many applications that use fixed context windows, we present continuous embeddings based on words' contexts in syntactic dependency trees. We employ these embeddings within a deterministic dependency parser and achieve the state of the art performance for this class of model.To our knowledge, this work is the first application of continuous vector embeddings in dependency parsing."
Accurate Deterministic Parsing with Syntax-Directed Embeddings,"Distributional analysis of words by embedding in continuous vector spaces has been effective at supplying features for many NLP tasks.  In contrast to many applications that use fixed context windows, we present continuous embeddings based on words' contexts in syntactic dependency trees. We employ these embeddings within a deterministic dependency parser and achieve the state of the art performance for this class of model.To our knowledge, this work is the first application of continuous vector embeddings in dependency parsing."
Spectral Learning of Latent-Variable HMMs ,We derive a spectral algorithm for learning the parameters of a latent-variable HMM. This method avoids the problem of local optima and provides a consistent estimate of the parameters. We demonstrate the method on a phoneme recognition task and show that it performs competitively with EM. 
A template model for fine-grained object recognition,"Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms. "
Multi-Task Learning in Low-Dimensional Manifolds for Successful Generalization from Few Samples,"We consider multi-task regression with parametric function approximators. A common approach to multi-task learningis to use Hierarchical Bayesian Models (HBMs) to learn the parameters of each task as well as the priordistribution over the parameter vectors of the tasks. For high dimensional parameter vectors as they occur for most interestingtasks the estimation of this prior distribution is challenging as it either requires a large amount of tasks-datasets or we need to resortto simplistic, less-expressive distributions. In this paper we propose to model the prior distribution not in the original high-dimensional space but in a low-dimensional manifold. This allows us to estimate the prior distribution accurately already from few datasets.In addition, our approach allows us to extract a latent variable for each task which can be used to characterize the task. We will test our method on a standard multi-task learningbenchmark problem as well as on a multi-task motor control problem. The experiments show that our method can exploit the low-dimensional structure of the tasks and outperforms other methods."
Multi-Task Learning in Low-Dimensional Manifolds for Successful Generalization from Few Samples,"We consider multi-task regression with parametric function approximators. A common approach to multi-task learningis to use Hierarchical Bayesian Models (HBMs) to learn the parameters of each task as well as the priordistribution over the parameter vectors of the tasks. For high dimensional parameter vectors as they occur for most interestingtasks the estimation of this prior distribution is challenging as it either requires a large amount of tasks-datasets or we need to resortto simplistic, less-expressive distributions. In this paper we propose to model the prior distribution not in the original high-dimensional space but in a low-dimensional manifold. This allows us to estimate the prior distribution accurately already from few datasets.In addition, our approach allows us to extract a latent variable for each task which can be used to characterize the task. We will test our method on a standard multi-task learningbenchmark problem as well as on a multi-task motor control problem. The experiments show that our method can exploit the low-dimensional structure of the tasks and outperforms other methods."
Getting the First Page Right: Bayesian Active Retrieval under Uncertainty,"Triggered by the idea of an information retrieval system for objects with noisy and missing features, we investigate the general problem of actively learning a similarity function of complex objects when the inputs to this function are not known exactly. To reduce the uncertainty in the inputs, and in turn improve the similarity function, we are interested in acquiring more information about the input objects. As gathering clean and complete information is costly or even impossible, it is important to carefully select the information needed and to be able to deal with uncertainty in order to retrieve meaningful results fast and with low total cost. Hence, we propose a Bayesian active learning approach to efficiently learn the most similar objects to a given query object in the setting where only partial and noisy information about entities is available. In our information retrieval case this corresponds to the task of getting the first page (of retrieval results) right. We evaluate the proposed Bayesian decision theoretic framework to actively acquire information on several retrieval problems, including a real-world document retrieval task."
Getting the First Page Right: Bayesian Active Retrieval under Uncertainty,"Triggered by the idea of an information retrieval system for objects with noisy and missing features, we investigate the general problem of actively learning a similarity function of complex objects when the inputs to this function are not known exactly. To reduce the uncertainty in the inputs, and in turn improve the similarity function, we are interested in acquiring more information about the input objects. As gathering clean and complete information is costly or even impossible, it is important to carefully select the information needed and to be able to deal with uncertainty in order to retrieve meaningful results fast and with low total cost. Hence, we propose a Bayesian active learning approach to efficiently learn the most similar objects to a given query object in the setting where only partial and noisy information about entities is available. In our information retrieval case this corresponds to the task of getting the first page (of retrieval results) right. We evaluate the proposed Bayesian decision theoretic framework to actively acquire information on several retrieval problems, including a real-world document retrieval task."
A new edge selection method for preserving the topology of persistent brain network,"The functional brain connectivity at the macro-scale studies how the localized areas of brain, i.e., the regions of interest (ROIs), work together during the specific mental functions.Their inter-regional connections estimated by the dissimilarity measure between observations in ROIs are usually too dense to visualize and interpret.So, we need to select the important edges in the network.Thresholding the edge weight matrix is most popular way to select the edges because it is assumed that only the strongly connected edges are important.However, there is no widely accepted rule for determining the threshold as well as the weakly connected edges also have the information which discriminates networks.In this paper, we propose a new edge selection method which preserves the topological structures of brain network based on the persistent homology.The persistent homology scans the topological structures by increasing the threshold and transforms the topological invariants to the algebraic form, known as Betti numbers.We seek the edges which affect the changes of the zeroth and first Betti numbers, i.e., connected components and holes.We applied the proposed method to the functional brain network based on FDG-PET data consisting of 24 attention deficit hyperactivity disorder (ADHD), 26 autism spectrum disorder (ASD) children and 11 pediatric control (PedCon) subjects.We showed that our edge selection method finds the minimum number of edges preserving the origianl geodesic distance of network."
Large-Scale Sparse PCA through Low-rank Approximations,We introduce a novel scheme for sparse PCA that has provable approximation guarantees.We first introduce an algorithm that can exactly solve sparse PCA for matrices of constant rank in polynomial time. Given a full-rank frequency matrix we obtain a constant rank matrix approximation and subsequently execute the exact sparse PCA solver on this low-rank approximation. We surprisingly see that this low-rank approximation step introduces very small errors. We theoretically explain this behavior by showing that data sets with few high-degree words must have data matrices that are close to low-rank. Our formalism allows us to show that our sparse PCA algorithm is asymptotically tight. Experimentally we evaluate our algorithm in a very large Twitter data set. A feature elimination step allows us to perform sparse PCA in millions of Tweets in a few minutes. Our scheme typically captures $80-90\%$ of the data variance by using rank-3 or rank-4 approximations. 
On a Mechanical Doctor for Prognostication,"We study an adaptive strategy for a doctor to learn how to make a prognosis from processing event data and adjusting his skills based on past experience. This is an example of decision-making in the face of uncertainty, and fits well within the framework of online learning and the recently proposed technique of aggregation by mirror averaging. The particulars of this approach are threefold. (1) feedback (rewards or loss) comes in necessarily delayed (i.e., the size of the delay is the very subject of study) (2) Since making a prognosis is necessarily random, we introduce the notion of an individual Probability Density Function (iPDF). This is a curve for an individual patient which is expected to give a large value when evaluated on the value of its associated event.This approach is to be contrasted with the common statistical population-based approach which tries to uncover the probabilistic rules underlying all events. A final crucial ingredient (3) is the presence of censored observations, particular to such setting of survival analysis.This paper designs a strategy ('A mechanical Doctor, or MD') based on recent developments in the area  of machine and statistical learning, and illustrates the technique on a challenging problem in bioinformatics."
On a Mechanical Doctor for Prognostication,"We study an adaptive strategy for a doctor to learn how to make a prognosis from processing event data and adjusting his skills based on past experience. This is an example of decision-making in the face of uncertainty, and fits well within the framework of online learning and the recently proposed technique of aggregation by mirror averaging. The particulars of this approach are threefold. (1) feedback (rewards or loss) comes in necessarily delayed (i.e., the size of the delay is the very subject of study) (2) Since making a prognosis is necessarily random, we introduce the notion of an individual Probability Density Function (iPDF). This is a curve for an individual patient which is expected to give a large value when evaluated on the value of its associated event.This approach is to be contrasted with the common statistical population-based approach which tries to uncover the probabilistic rules underlying all events. A final crucial ingredient (3) is the presence of censored observations, particular to such setting of survival analysis.This paper designs a strategy ('A mechanical Doctor, or MD') based on recent developments in the area  of machine and statistical learning, and illustrates the technique on a challenging problem in bioinformatics."
Sequence Learning via Hypergraph Particle Filtering,"Variable-order Markov models create a tree of partitions to represent higher-order contexts of discrete sequences. Though efficient in lossless compression, the partitioning approaches are not very flexible, and difficult for sequential update. Here we propose a hypergraph-based sparse population coding method for learning the higher-order Markov models of sequence data. This representation facilitates fast and flexible update of the transition probability matrix of non-uniform variable-order Markov chains. We develop a particle filter-like sequential Bayesian estimation algorithm that learns and predicts sequences using online Monte Carlo sampling on the sparse hypergraph space. The parsimony vs. accuracy tradeoff in this ?hypergraph particle filter? can be made conveniently by Bayesian priors on model order and population size. The flexibility and sparseness of the Markov model structures learned is experimentally demonstrated in the domains of music, language, and robot motion generation. Our results are somewhat surprising since the Markov models of higher-order k with non-uniform, sparse connectivity often outperform the lower-order j < k Markov models with full parameterization both in reconstruction error and novelty of patterns discovered."
Sequence Learning via Hypergraph Particle Filtering,"Variable-order Markov models create a tree of partitions to represent higher-order contexts of discrete sequences. Though efficient in lossless compression, the partitioning approaches are not very flexible, and difficult for sequential update. Here we propose a hypergraph-based sparse population coding method for learning the higher-order Markov models of sequence data. This representation facilitates fast and flexible update of the transition probability matrix of non-uniform variable-order Markov chains. We develop a particle filter-like sequential Bayesian estimation algorithm that learns and predicts sequences using online Monte Carlo sampling on the sparse hypergraph space. The parsimony vs. accuracy tradeoff in this ?hypergraph particle filter? can be made conveniently by Bayesian priors on model order and population size. The flexibility and sparseness of the Markov model structures learned is experimentally demonstrated in the domains of music, language, and robot motion generation. Our results are somewhat surprising since the Markov models of higher-order k with non-uniform, sparse connectivity often outperform the lower-order j < k Markov models with full parameterization both in reconstruction error and novelty of patterns discovered."
Sequence Learning via Hypergraph Particle Filtering,"Variable-order Markov models create a tree of partitions to represent higher-order contexts of discrete sequences. Though efficient in lossless compression, the partitioning approaches are not very flexible, and difficult for sequential update. Here we propose a hypergraph-based sparse population coding method for learning the higher-order Markov models of sequence data. This representation facilitates fast and flexible update of the transition probability matrix of non-uniform variable-order Markov chains. We develop a particle filter-like sequential Bayesian estimation algorithm that learns and predicts sequences using online Monte Carlo sampling on the sparse hypergraph space. The parsimony vs. accuracy tradeoff in this ?hypergraph particle filter? can be made conveniently by Bayesian priors on model order and population size. The flexibility and sparseness of the Markov model structures learned is experimentally demonstrated in the domains of music, language, and robot motion generation. Our results are somewhat surprising since the Markov models of higher-order k with non-uniform, sparse connectivity often outperform the lower-order j < k Markov models with full parameterization both in reconstruction error and novelty of patterns discovered."
Sequence Learning via Hypergraph Particle Filtering,"Variable-order Markov models create a tree of partitions to represent higher-order contexts of discrete sequences. Though efficient in lossless compression, the partitioning approaches are not very flexible, and difficult for sequential update. Here we propose a hypergraph-based sparse population coding method for learning the higher-order Markov models of sequence data. This representation facilitates fast and flexible update of the transition probability matrix of non-uniform variable-order Markov chains. We develop a particle filter-like sequential Bayesian estimation algorithm that learns and predicts sequences using online Monte Carlo sampling on the sparse hypergraph space. The parsimony vs. accuracy tradeoff in this ?hypergraph particle filter? can be made conveniently by Bayesian priors on model order and population size. The flexibility and sparseness of the Markov model structures learned is experimentally demonstrated in the domains of music, language, and robot motion generation. Our results are somewhat surprising since the Markov models of higher-order k with non-uniform, sparse connectivity often outperform the lower-order j < k Markov models with full parameterization both in reconstruction error and novelty of patterns discovered."
Is Matching Pursuit Solving Convex Problems?,"Matching pursuit  (\texttt{MP}) algorithms have been successfullyapplied in signal processing and pattern recognition areas. However,as far as we know, it is still not clear whether any \texttt{MP}algorithm can solve a convex problem or not. In this paper, a novelconvex relaxation is proposed for a class of matching pursuitalgorithms, which includes the classical orthogonal matching pursuit(\texttt{OMP}) as a special case.  Based on the proposed scheme, ageneral matching pursuit (\texttt{GMP}) algorithm can be naturallyobtained. As it solves a convex problem, \texttt{GMP}  guarantees toconverge globally. In addition, a subspace exploratory search canfurther improve the performance. Finally, we show that \texttt{GMP}with an $\ell_1$ regularization term can recover the $k$-sparsesignals if the restricted isometry constant $\sigma_k\leq 0.307-\nu$,where $\nu$ can be arbitrarily close to 0. The proposed method can beeasily parallelized and the efficiency can be further improved.Simulations on an 8-core machine show that the proposed method cansuccessfully decode the problems of scale $2^{13} \times 2^{17}$within 10 seconds and scale $2^{10} \times 2^{20}$ within 2 seconds."
Approximate Gaussian process inference for the drift function in stochastic differential equations,"We introduce a nonparametric approach for estimating drift functions in stochastic differential equations from incomplete observations of the state variable. Using a Gaussian process prior over the drift as a function of the state variable, we develop an approximate EM algorithm to deal with the unobserved, latent state dynamics between observations. The posterior dynamics over states is approximated by a piecewise linearized process and the MAP estimation of the drift is facilitated by a sparse Gaussian process approximation."
Approximate Gaussian process inference for the drift function in stochastic differential equations,"We introduce a nonparametric approach for estimating drift functions in stochastic differential equations from incomplete observations of the state variable. Using a Gaussian process prior over the drift as a function of the state variable, we develop an approximate EM algorithm to deal with the unobserved, latent state dynamics between observations. The posterior dynamics over states is approximated by a piecewise linearized process and the MAP estimation of the drift is facilitated by a sparse Gaussian process approximation."
Approximate Gaussian process inference for the drift function in stochastic differential equations,"We introduce a nonparametric approach for estimating drift functions in stochastic differential equations from incomplete observations of the state variable. Using a Gaussian process prior over the drift as a function of the state variable, we develop an approximate EM algorithm to deal with the unobserved, latent state dynamics between observations. The posterior dynamics over states is approximated by a piecewise linearized process and the MAP estimation of the drift is facilitated by a sparse Gaussian process approximation."
Imitation Learning by Coaching,"Imitation Learning has been shown to be successful in solving many challenging real-world problems.Some recent approaches give strong performance guarantees by training the policy iteratively.However, it is important to note that these guarantees depend on  how well the policy we found can imitate the oracle on the training data. When there is a substantial difference between the oracle's ability and the learner's policy space,we may fail to find a policy that has low error on the training set.In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner and gradually approaches the oracle.By a reduction of learning by demonstration to online learning, we prove that coaching can yield a lower regret bound than using the oracle.We apply our algorithm to a novel cost-sensitive dynamic feature selection problem,a hard decision problem that considers a user-specified accuracy-cost trade-off. Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods."
Imitation Learning by Coaching,"Imitation Learning has been shown to be successful in solving many challenging real-world problems.Some recent approaches give strong performance guarantees by training the policy iteratively.However, it is important to note that these guarantees depend on  how well the policy we found can imitate the oracle on the training data. When there is a substantial difference between the oracle's ability and the learner's policy space,we may fail to find a policy that has low error on the training set.In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner and gradually approaches the oracle.By a reduction of learning by demonstration to online learning, we prove that coaching can yield a lower regret bound than using the oracle.We apply our algorithm to a novel cost-sensitive dynamic feature selection problem,a hard decision problem that considers a user-specified accuracy-cost trade-off. Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods."
Imitation Learning by Coaching,"Imitation Learning has been shown to be successful in solving many challenging real-world problems.Some recent approaches give strong performance guarantees by training the policy iteratively.However, it is important to note that these guarantees depend on  how well the policy we found can imitate the oracle on the training data. When there is a substantial difference between the oracle's ability and the learner's policy space,we may fail to find a policy that has low error on the training set.In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner and gradually approaches the oracle.By a reduction of learning by demonstration to online learning, we prove that coaching can yield a lower regret bound than using the oracle.We apply our algorithm to a novel cost-sensitive dynamic feature selection problem,a hard decision problem that considers a user-specified accuracy-cost trade-off. Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods."
Stochastically Emerging Medoids: Application of Classical Problems in Probability Theory for Clustering Massive Data Sets,"K-medoid methods for clustering data have many desirable properties such as robustness and the ability to use non-numerical values, but their typically high computational complexity has made their application to large data sets difficult. In this paper, we present AGORAS, a novel stochastic algorithm for the k-medoids problem that is especially well-suited to clustering massive data sets. Our approach involves taking a sequence of uniform sample sets and a heuristic for determining the sample size and identifying cluster medoids from the sampled items. As a result, computing the final solution only involves solving k trivial sub-problems of centrality, which can be done much more efficiently on large data sets than searching a combinatorial space for the optimal value of an objective function. As a result, the complexity of AGORAS is effectively independent of the full data size, and it can scale to arbitrarily large data sets.  We evaluate AGORAS experimentally against PAM and CLARANS, the best-known existing algorithms for the k- medoids problem, across a variety of published and synthetic data sets.  We find that AGORAS outperforms PAM by up to four orders of magnitude for data sets with less than 10,000 points, and it outperforms CLARANS by two orders of magnitude on a data set of just 64,000 points.  Moreover, we find in some cases that AGORAS also outperforms these algorithms in terms of cluster quality. "
An online learning rule for a recurrent neural network with hidden neurons,Recent experimental finding suggest that the brain can spontaneously generate spiking patterns with the same statistics as stimulus-evoked activity patterns. Here we propose an online learning rule for a network of stochastic visible and hidden neurons that is able to adapt its spontaneous firing statistics to the stimulus-evoked firing statistics. We show furthermore that learning synaptic weights towards hidden neurons enables the network to bridge a silency gap in the activity pattern of visible neurons.
An online learning rule for a recurrent neural network with hidden neurons,Recent experimental finding suggest that the brain can spontaneously generate spiking patterns with the same statistics as stimulus-evoked activity patterns. Here we propose an online learning rule for a network of stochastic visible and hidden neurons that is able to adapt its spontaneous firing statistics to the stimulus-evoked firing statistics. We show furthermore that learning synaptic weights towards hidden neurons enables the network to bridge a silency gap in the activity pattern of visible neurons.
An online learning rule for a recurrent neural network with hidden neurons,Recent experimental finding suggest that the brain can spontaneously generate spiking patterns with the same statistics as stimulus-evoked activity patterns. Here we propose an online learning rule for a network of stochastic visible and hidden neurons that is able to adapt its spontaneous firing statistics to the stimulus-evoked firing statistics. We show furthermore that learning synaptic weights towards hidden neurons enables the network to bridge a silency gap in the activity pattern of visible neurons.
Modeling Scientific Impact with Citation Influence Regression,"When reviewing the scientific literature in a specific subject area, it would be usefulto have automatic tools that identify the most influential scientific articles aswell as how ideas propagate between articles. Bibliometric measures based on citationcounts, such as impact factors, provide some indication of the influence ofan article or the prestige of its publication venue. However, citations can occur fordifferent reasons and may not always indicate the transfer of ideas, so that citationcounts alone can be misleading. In this paper we develop latent variable probabilisticmodels for inferring influence in scientific corpora. The models operateon a collection of documents embedded in a citation graph with articles as nodesand citations as edges, where the latent topics of cited papers influence the priordistribution over topics in citing papers. We show how the proposed models canbe used to automatically determine the degree of influence of scientific articles,and their influence along the edges of the citation graph."
Modeling Scientific Impact with Citation Influence Regression,"When reviewing the scientific literature in a specific subject area, it would be usefulto have automatic tools that identify the most influential scientific articles aswell as how ideas propagate between articles. Bibliometric measures based on citationcounts, such as impact factors, provide some indication of the influence ofan article or the prestige of its publication venue. However, citations can occur fordifferent reasons and may not always indicate the transfer of ideas, so that citationcounts alone can be misleading. In this paper we develop latent variable probabilisticmodels for inferring influence in scientific corpora. The models operateon a collection of documents embedded in a citation graph with articles as nodesand citations as edges, where the latent topics of cited papers influence the priordistribution over topics in citing papers. We show how the proposed models canbe used to automatically determine the degree of influence of scientific articles,and their influence along the edges of the citation graph."
Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models,"Links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics, i.e., letting the variance of particular distributions in a graphical model go to zero. For instance, in the context of clustering, such an approach yields precise connections between the k-means and EM algorithms.  In thispaper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models.  Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models.  We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis."
Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models,"Links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics, i.e., letting the variance of particular distributions in a graphical model go to zero. For instance, in the context of clustering, such an approach yields precise connections between the k-means and EM algorithms.  In thispaper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models.  Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models.  We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis."
A latent factor model for highly multi-relational data,"Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations."
A latent factor model for highly multi-relational data,"Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations."
A latent factor model for highly multi-relational data,"Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations."
A latent factor model for highly multi-relational data,"Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations."
Simultaneously Leveraging Output and Task Structures  for Multiple-Output Regression,"Multiple-output regression models require estimating multiple functions, one for each output. To improve parameter estimation in such models, methods based on structural regularization of the model parameters are usually needed. In this paper, we present a multiple-output regression model that leverages the covariance structure of the functions (i.e., how the multiple functions are related with each other) as well as the conditional covariance structure of the outputs. This is in contrast with existing methods that usually take into account only one of these structures. More importantly, unlike most of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data. Several previously proposed structural regularization based  multiple-output regression models turn out to be special cases of our model. Moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs). Experimental results on both synthetic and real datasets demonstrate the effectiveness of our method."
Simultaneously Leveraging Output and Task Structures  for Multiple-Output Regression,"Multiple-output regression models require estimating multiple functions, one for each output. To improve parameter estimation in such models, methods based on structural regularization of the model parameters are usually needed. In this paper, we present a multiple-output regression model that leverages the covariance structure of the functions (i.e., how the multiple functions are related with each other) as well as the conditional covariance structure of the outputs. This is in contrast with existing methods that usually take into account only one of these structures. More importantly, unlike most of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data. Several previously proposed structural regularization based  multiple-output regression models turn out to be special cases of our model. Moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs). Experimental results on both synthetic and real datasets demonstrate the effectiveness of our method."
Continuous Relaxations for Discrete Hamiltonian Monte Carlo,"Continuous relaxations play an important role in discreteoptimization, but have not seen much use in approximate probabilisticinference. Here we show that a general form of the GaussianIntegral Trick makes it possible to transform a wide class ofdiscrete variable undirected models into fully continuous systems. Thecontinuous representation allows the use of gradient-based HamiltonianMonte Carlo for inference,  results in new ways of estimatingnormalization constants (partition functions), and in general opens upa number of new avenues for inference in difficult discretesystems. We demonstrate some of these continuous relaxation inference algorithmson a number of illustrative problems."
Continuous Relaxations for Discrete Hamiltonian Monte Carlo,"Continuous relaxations play an important role in discreteoptimization, but have not seen much use in approximate probabilisticinference. Here we show that a general form of the GaussianIntegral Trick makes it possible to transform a wide class ofdiscrete variable undirected models into fully continuous systems. Thecontinuous representation allows the use of gradient-based HamiltonianMonte Carlo for inference,  results in new ways of estimatingnormalization constants (partition functions), and in general opens upa number of new avenues for inference in difficult discretesystems. We demonstrate some of these continuous relaxation inference algorithmson a number of illustrative problems."
Continuous Relaxations for Discrete Hamiltonian Monte Carlo,"Continuous relaxations play an important role in discreteoptimization, but have not seen much use in approximate probabilisticinference. Here we show that a general form of the GaussianIntegral Trick makes it possible to transform a wide class ofdiscrete variable undirected models into fully continuous systems. Thecontinuous representation allows the use of gradient-based HamiltonianMonte Carlo for inference,  results in new ways of estimatingnormalization constants (partition functions), and in general opens upa number of new avenues for inference in difficult discretesystems. We demonstrate some of these continuous relaxation inference algorithmson a number of illustrative problems."
Mechanism Design for Machine Learning Problems,"While machine learning competitions like the Netflix Prize have had relative success on their own, they pave the way to think about procedural aspects of developing predictors. We believe that applying optimal structures designed using game theoretical thinking can make the process of development of machine learning solution much more efficient. However, there are some special features thatare specific to learning scenarios. In this paper, we make the initial steps towards achieving this. In particular, we address the issue that in a prediction problem the outcome of a mechanism must depend on a quantity unknown to all parties (i.e., how well the proposed algorithms will perform). We also propose a specific auction where the developers can submit multiple proposed predictors."
Deep Learning of invariant features via tracked video sequences,"We use video sequences produced by tracking as training data to learn invariant features. These features are spatial instead of temporal, and well suited to extract from still images. With a temporal coherence objective, a multi-layer neural network encodes invariance that grow increasingly complex with layer hierarchy. Without fine-tuning with labels, we achieve competitive performance on five non-temporal image datasets and state-of-the-art classification accuracy 61% on STL-10 object recognition dataset."
Deep Learning of invariant features via tracked video sequences,"We use video sequences produced by tracking as training data to learn invariant features. These features are spatial instead of temporal, and well suited to extract from still images. With a temporal coherence objective, a multi-layer neural network encodes invariance that grow increasingly complex with layer hierarchy. Without fine-tuning with labels, we achieve competitive performance on five non-temporal image datasets and state-of-the-art classification accuracy 61% on STL-10 object recognition dataset."
Visual Object Classification is Consistent with Bayesian Generative vs Discriminative Representations,"The ability to learn and distinguish categories is essential for human behavior, and the underlying neural computations are actively investigated. Taking a normative view, we can relate categorisation to the distinction between generative and discriminative classification in machine learning. Generative approaches solve the categorization problem by building a probabilistic model of how each cate- gory was formed and infer then category labels. In contrast, the discriminative approach learns a direct mapping between input and label. Recent work shows how human classification is consistent with discriminative and generative classifi- cation depending on conditions. We hypothesize that humans employ generative mechanisms for classification, when not encouraged otherwise. To test this we exploit a counterintuitive prediction for generative classification, namely how the discrimination boundary between two classes shifts if one category?s distribution is revealed to be broader during learning. We tested N=20 subjects to distinguish two classes, A and B in two tasks (two artificial-script, armadillo-horse stick- drawings). The classes in each task were parameterized by two scalars; objects for each class are drawn from Gaussian parameter distributions, with equal variance and different means (class ?prototypes?). Next, subjects classify unlabelled examples drawn between the classes, so we can infer their discrimination boundary. This process is then repeated but includes training data for class A, which lie far away from B. Counter-intuitively, generative classification predicts a shift of the discrimination boundary closer to B. Conversely, discriminative classifiers will show either no shift of the boundary or a shift of the boundary away from class B. Our results show that categorization in both tasks is consistent with generative and not discriminative classifiers, as classification boundaries shifted towards B for both tasks in all subjects."
Visual Object Classification is Consistent with Bayesian Generative vs Discriminative Representations,"The ability to learn and distinguish categories is essential for human behavior, and the underlying neural computations are actively investigated. Taking a normative view, we can relate categorisation to the distinction between generative and discriminative classification in machine learning. Generative approaches solve the categorization problem by building a probabilistic model of how each cate- gory was formed and infer then category labels. In contrast, the discriminative approach learns a direct mapping between input and label. Recent work shows how human classification is consistent with discriminative and generative classifi- cation depending on conditions. We hypothesize that humans employ generative mechanisms for classification, when not encouraged otherwise. To test this we exploit a counterintuitive prediction for generative classification, namely how the discrimination boundary between two classes shifts if one category?s distribution is revealed to be broader during learning. We tested N=20 subjects to distinguish two classes, A and B in two tasks (two artificial-script, armadillo-horse stick- drawings). The classes in each task were parameterized by two scalars; objects for each class are drawn from Gaussian parameter distributions, with equal variance and different means (class ?prototypes?). Next, subjects classify unlabelled examples drawn between the classes, so we can infer their discrimination boundary. This process is then repeated but includes training data for class A, which lie far away from B. Counter-intuitively, generative classification predicts a shift of the discrimination boundary closer to B. Conversely, discriminative classifiers will show either no shift of the boundary or a shift of the boundary away from class B. Our results show that categorization in both tasks is consistent with generative and not discriminative classifiers, as classification boundaries shifted towards B for both tasks in all subjects."
Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence,"We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to an algorithm called, unified gap-based exploration (UGapE), with common structure and theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms. "
Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence,"We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to an algorithm called, unified gap-based exploration (UGapE), with common structure and theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms. "
Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence,"We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to an algorithm called, unified gap-based exploration (UGapE), with common structure and theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms. "
Forest Graph Estimation with Constraints,"We consider the problem of learning high dimensional forest graphical models.Unlike previous methods, we impose different structural constraints on the obtainedforest graphs: including (i) bounded tree constraint, (ii) bounded star constraint,and (iii) bounded path constraint. These constraints are well motivatedby empirical applications and theoretical considerations. We systematically studythese different constraints and illustrate their relationships. We develop hardnessresults and propose novel approximation algorithms with provable guarantees."
Forest Graph Estimation with Constraints,"We consider the problem of learning high dimensional forest graphical models.Unlike previous methods, we impose different structural constraints on the obtainedforest graphs: including (i) bounded tree constraint, (ii) bounded star constraint,and (iii) bounded path constraint. These constraints are well motivatedby empirical applications and theoretical considerations. We systematically studythese different constraints and illustrate their relationships. We develop hardnessresults and propose novel approximation algorithms with provable guarantees."
Mapping Overlapping Functional Networks via Multiple Relational Embedding,"Studying the modular composition of the functional cerebral architecture is a challenging line of research. Of particular interest are network structures that are active during specific cognitive tasks. In this paper we address the question of identifying partially overlapping networks that are active across different fMRI experiment conditions. We propose to use Multiple Relational Embedding (MRE) on functional brain imaging data acquired during different cognitive tasks. Multiple functional relationships are embedded into a single joint latent embedding, that encodes both joint, and individual network structure. Experiments demonstrate that this approach can identify shared-, and individual functional connectivity structure, and that it recovers functional networks with higher stability compared to the embedding of individual fMRI sequences."
Mapping Overlapping Functional Networks via Multiple Relational Embedding,"Studying the modular composition of the functional cerebral architecture is a challenging line of research. Of particular interest are network structures that are active during specific cognitive tasks. In this paper we address the question of identifying partially overlapping networks that are active across different fMRI experiment conditions. We propose to use Multiple Relational Embedding (MRE) on functional brain imaging data acquired during different cognitive tasks. Multiple functional relationships are embedded into a single joint latent embedding, that encodes both joint, and individual network structure. Experiments demonstrate that this approach can identify shared-, and individual functional connectivity structure, and that it recovers functional networks with higher stability compared to the embedding of individual fMRI sequences."
Inferring ground truth from multi-annotator ordinal data: a probabilistic approach,"A popular approach for large scale data annotation tasks is crowdsourcing, wherein each data point is labeled by multiple noisy annotators. We consider the problem of inferring ground truth based on noisy ordinal labels from multiple annotators of varying and unknown expertise levels. We propose a new model for crowd sourced ordinal data that accounts for instance difficulty as well as annotator expertise, and derive a variational inference algorithm for parameter estima- tion. We analyze the ordinal extensions of several state-of-the-art annotator models for binary/categorical labels and evaluate the performance of all the models on a large real world dataset containing query-url relevance scores, collected through Amazon?s Mechanical Turk. Our results indicate that the proposed model performs better or as well as existing state-of-the-art methods and is more resistant to ?spammy? ratings than popular baselines such as mean, median, and majority vote which do not account for annotator expertise."
Inferring ground truth from multi-annotator ordinal data: a probabilistic approach,"A popular approach for large scale data annotation tasks is crowdsourcing, wherein each data point is labeled by multiple noisy annotators. We consider the problem of inferring ground truth based on noisy ordinal labels from multiple annotators of varying and unknown expertise levels. We propose a new model for crowd sourced ordinal data that accounts for instance difficulty as well as annotator expertise, and derive a variational inference algorithm for parameter estima- tion. We analyze the ordinal extensions of several state-of-the-art annotator models for binary/categorical labels and evaluate the performance of all the models on a large real world dataset containing query-url relevance scores, collected through Amazon?s Mechanical Turk. Our results indicate that the proposed model performs better or as well as existing state-of-the-art methods and is more resistant to ?spammy? ratings than popular baselines such as mean, median, and majority vote which do not account for annotator expertise."
Constrained-based Human Body Shape Analysis,"We develop a framework for constrained dimensionality reduction that is highly flexible and we illustrate the approach by analyzing 3D human body shape. Previous approaches to body shape analysis reduced the dimensionality of the body shape using principal component analysis and then analyzed shape in a low-dimensional subspace, for example, by relating principle component directions to body measurements like height and weight. We show that the principal components of body shape are not always the best way to capture human shape variation. Instead we define a flexible optimization framework to describe variation in 3D human body shape using orthogonal shape subspaces related to different linear or non-linear constraints. We formulate several different objective functions in this framework and show how to learn models of body shape tailored to specific applications. In particular, we find directions of body shape variation that are more directly related to human measurements that enable improved measurement prediction when compared to previous methods. Additionally we define a {\em null space} of human body shape variation.  This null space captures how body shape varies in ways that are orthogonal to the measurement space for example. The null space reveals interesting properties of body shape that are not captured by standard tailoring measurements such as inter- and intra-person posture variation, ``bow legs'', and body shape asymmetries. Finally, our framework makes it easy to add sparsity constraints and we find that these improve measurement prediction from body scans."
Constrained-based Human Body Shape Analysis,"We develop a framework for constrained dimensionality reduction that is highly flexible and we illustrate the approach by analyzing 3D human body shape. Previous approaches to body shape analysis reduced the dimensionality of the body shape using principal component analysis and then analyzed shape in a low-dimensional subspace, for example, by relating principle component directions to body measurements like height and weight. We show that the principal components of body shape are not always the best way to capture human shape variation. Instead we define a flexible optimization framework to describe variation in 3D human body shape using orthogonal shape subspaces related to different linear or non-linear constraints. We formulate several different objective functions in this framework and show how to learn models of body shape tailored to specific applications. In particular, we find directions of body shape variation that are more directly related to human measurements that enable improved measurement prediction when compared to previous methods. Additionally we define a {\em null space} of human body shape variation.  This null space captures how body shape varies in ways that are orthogonal to the measurement space for example. The null space reveals interesting properties of body shape that are not captured by standard tailoring measurements such as inter- and intra-person posture variation, ``bow legs'', and body shape asymmetries. Finally, our framework makes it easy to add sparsity constraints and we find that these improve measurement prediction from body scans."
Active Inference for Brain-Computer Interfaces with Application to an SSVEP Speller,"We view a brain-computer interface as the means by which human users may communicate their intent to a computer. Based on this view, we cast the problem of brain-computer interface design as the problem of inferring the user?s intent by asking a sequence of queries to the human user. The approach is to use an active inference model to choose the next query to be asked based on the observed responses from the previous queries and expected response times for each query. We demonstrate this approach by developing a brain-computer interface for spelling English sentences based on steady-state visually evoked potentials (SSVEP). Results show that the interface allows subjects to spell more than 10 letters per minute, which provides an improvement of performance over previous SSVEP spellers."
Active Inference for Brain-Computer Interfaces with Application to an SSVEP Speller,"We view a brain-computer interface as the means by which human users may communicate their intent to a computer. Based on this view, we cast the problem of brain-computer interface design as the problem of inferring the user?s intent by asking a sequence of queries to the human user. The approach is to use an active inference model to choose the next query to be asked based on the observed responses from the previous queries and expected response times for each query. We demonstrate this approach by developing a brain-computer interface for spelling English sentences based on steady-state visually evoked potentials (SSVEP). Results show that the interface allows subjects to spell more than 10 letters per minute, which provides an improvement of performance over previous SSVEP spellers."
Active Inference for Brain-Computer Interfaces with Application to an SSVEP Speller,"We view a brain-computer interface as the means by which human users may communicate their intent to a computer. Based on this view, we cast the problem of brain-computer interface design as the problem of inferring the user?s intent by asking a sequence of queries to the human user. The approach is to use an active inference model to choose the next query to be asked based on the observed responses from the previous queries and expected response times for each query. We demonstrate this approach by developing a brain-computer interface for spelling English sentences based on steady-state visually evoked potentials (SSVEP). Results show that the interface allows subjects to spell more than 10 letters per minute, which provides an improvement of performance over previous SSVEP spellers."
Learning Stable Non-linear Features in Contractive Auto-encoders,"Unsupervised learning of feature hierarchies is often a good initialization for supervised training of deep architectures.  In existing deep learning methods, these feature hierarchies are built layer by layer in a greedy fashion using auto-encoders or restricted Boltzmann machines.  Both yield encoders, which compute linear projections followed by a smooth thresholding function.  We point out that these encoders fail to find stable features when the required computation is in the exclusive-or class.  To overcome this limitation, we propose a two-layer encoder which is not restricted in the type of features it can learn.  The proposed encoder can be regularized by an extension of previous work on contractive regularization.  We demonstrate the advantages of two-layer encoders qualitatively, as well as on commonly used benchmark datasets.  "
Learning Stable Non-linear Features in Contractive Auto-encoders,"Unsupervised learning of feature hierarchies is often a good initialization for supervised training of deep architectures.  In existing deep learning methods, these feature hierarchies are built layer by layer in a greedy fashion using auto-encoders or restricted Boltzmann machines.  Both yield encoders, which compute linear projections followed by a smooth thresholding function.  We point out that these encoders fail to find stable features when the required computation is in the exclusive-or class.  To overcome this limitation, we propose a two-layer encoder which is not restricted in the type of features it can learn.  The proposed encoder can be regularized by an extension of previous work on contractive regularization.  We demonstrate the advantages of two-layer encoders qualitatively, as well as on commonly used benchmark datasets.  "
Conditional conjugate priors,"We extend the notion of conjugates in exponential families to  conditional distributions. This yields new sets of priors which are  computationally attractive. Moreover, we show that the commonly  used $\ell_1$ and $\ell_2$ priors are special cases of our  framework. We also point out the connections to the inference with Universum method. Experiments confirm the  efficiency of our approach."
Conditional conjugate priors,"We extend the notion of conjugates in exponential families to  conditional distributions. This yields new sets of priors which are  computationally attractive. Moreover, we show that the commonly  used $\ell_1$ and $\ell_2$ priors are special cases of our  framework. We also point out the connections to the inference with Universum method. Experiments confirm the  efficiency of our approach."
Conditional Likelihood Inference on Overlapping Figure-Ground Segment Hypotheses,"In this paper we present an inference procedure for the semantic segmentation of images, namely identifying the spatial layout and class labels of the objects present.Different from many CRF approaches that rely on dependencies modeled with unary and pairwisepixel or superpixel potentials, our method is entirely based on overlap estimates on diverse, potentially overlapping segments in the image. This enablesus to solve the statistical inference problem without using a random field and its associated dependencies. In the approach, posteriorsuperpixels are obtained by intersections of the overlapping regions.Then random variables are defined on such superpixels so that the overlap between each segment and the ground truth can be constructed from them. Inferenceis then performed using conditional maximum likelihood based on an EM formulation. In the PASCAL VOC challenge, the proposed approach is comparable with the 3-times winner SVRSEGM system, but in addition it successfully recognizes additional images with multiple interacting objects."
Conditional Likelihood Inference on Overlapping Figure-Ground Segment Hypotheses,"In this paper we present an inference procedure for the semantic segmentation of images, namely identifying the spatial layout and class labels of the objects present.Different from many CRF approaches that rely on dependencies modeled with unary and pairwisepixel or superpixel potentials, our method is entirely based on overlap estimates on diverse, potentially overlapping segments in the image. This enablesus to solve the statistical inference problem without using a random field and its associated dependencies. In the approach, posteriorsuperpixels are obtained by intersections of the overlapping regions.Then random variables are defined on such superpixels so that the overlap between each segment and the ground truth can be constructed from them. Inferenceis then performed using conditional maximum likelihood based on an EM formulation. In the PASCAL VOC challenge, the proposed approach is comparable with the 3-times winner SVRSEGM system, but in addition it successfully recognizes additional images with multiple interacting objects."
Phylogenetic inference based on alignment of etymological data,"We apply models developed for population genetics to induce phylogeniesbased on linguistic data, specifically, on a large corpus of geneticallyrelated, or {\em cognate}, words from languages within a languagefamily.  This is achieved via a novel and natural projection of thelinguistic data into genetic primitives.  First, we process the cognatesets to obtain a globally-optimal alignment of the corpus.  Thealignments then serve as input to the model for phylogeneticreconstruction, which produces family tree structures that stronglymatch the ``true'' (or expected) structures.  We place our methods inthe context of those reported in the literature and illustrate themusing data from Uralic language family.  A suite of etymologicalsoftware is released for public use.  "
Hierarchical Model-based Control of Non-Linear Dynamical Systems using Reinforcement Learning,"Non-adaptive methods are currently state of the art in approximating solutions to non-linear optimal control problems.These carry a large computational  cost associated with iterative calculations and have to be solved individually for different start and end points.In addition they may not scale well for real-world problems and require considerable tuning to converge.As an alternative, we present a novel hierarchical approach to non-Linear Control using Reinforcement Learning to choose between locally linear controllers. These are dynamically learnt and repositioned in state space using Linear Dynamic Systems. We illustrate our approach with a solution to a benchmark problem.We show that our approach, Reinforcement Learning Optimal Control (RLOC) competes in terms of solution quality with a state-of-the-art control algorithm iLQR, and offers a robust, flexible framework to address large scale non-linear control problems with unknown dynamics."
Hierarchical Model-based Control of Non-Linear Dynamical Systems using Reinforcement Learning,"Non-adaptive methods are currently state of the art in approximating solutions to non-linear optimal control problems.These carry a large computational  cost associated with iterative calculations and have to be solved individually for different start and end points.In addition they may not scale well for real-world problems and require considerable tuning to converge.As an alternative, we present a novel hierarchical approach to non-Linear Control using Reinforcement Learning to choose between locally linear controllers. These are dynamically learnt and repositioned in state space using Linear Dynamic Systems. We illustrate our approach with a solution to a benchmark problem.We show that our approach, Reinforcement Learning Optimal Control (RLOC) competes in terms of solution quality with a state-of-the-art control algorithm iLQR, and offers a robust, flexible framework to address large scale non-linear control problems with unknown dynamics."
Hierarchical Model-based Control of Non-Linear Dynamical Systems using Reinforcement Learning,"Non-adaptive methods are currently state of the art in approximating solutions to non-linear optimal control problems.These carry a large computational  cost associated with iterative calculations and have to be solved individually for different start and end points.In addition they may not scale well for real-world problems and require considerable tuning to converge.As an alternative, we present a novel hierarchical approach to non-Linear Control using Reinforcement Learning to choose between locally linear controllers. These are dynamically learnt and repositioned in state space using Linear Dynamic Systems. We illustrate our approach with a solution to a benchmark problem.We show that our approach, Reinforcement Learning Optimal Control (RLOC) competes in terms of solution quality with a state-of-the-art control algorithm iLQR, and offers a robust, flexible framework to address large scale non-linear control problems with unknown dynamics."
Hierarchical Model-based Control of Non-Linear Dynamical Systems using Reinforcement Learning,"Non-adaptive methods are currently state of the art in approximating solutions to non-linear optimal control problems.These carry a large computational  cost associated with iterative calculations and have to be solved individually for different start and end points.In addition they may not scale well for real-world problems and require considerable tuning to converge.As an alternative, we present a novel hierarchical approach to non-Linear Control using Reinforcement Learning to choose between locally linear controllers. These are dynamically learnt and repositioned in state space using Linear Dynamic Systems. We illustrate our approach with a solution to a benchmark problem.We show that our approach, Reinforcement Learning Optimal Control (RLOC) competes in terms of solution quality with a state-of-the-art control algorithm iLQR, and offers a robust, flexible framework to address large scale non-linear control problems with unknown dynamics."
Model-based Decision Strategy Recognition,"We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of decision agents on the basis of observation of their actions in solving sequential decision problems.  We model the problem faced by the decision agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of degrees of rationality with respect to optimal forward planning for the MDP.  To recognize the agents, we first use IRL to learn reward functions consistent with observed actions and then use these reward functions as the basis for clustering or classification models.   Experimental studies with GridWorld, a navigation problem,  and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for classifying automated decision rules (e.g., cutoff rule, successive first candidates), even in the presence of action noise and variations in the parameters of the rule.  We propose a new Bayesian IRL approach in which the likelihood function can be interpreted in terms of rationality models.  Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for recognition problems. "
Model-based Decision Strategy Recognition,"We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of decision agents on the basis of observation of their actions in solving sequential decision problems.  We model the problem faced by the decision agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of degrees of rationality with respect to optimal forward planning for the MDP.  To recognize the agents, we first use IRL to learn reward functions consistent with observed actions and then use these reward functions as the basis for clustering or classification models.   Experimental studies with GridWorld, a navigation problem,  and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for classifying automated decision rules (e.g., cutoff rule, successive first candidates), even in the presence of action noise and variations in the parameters of the rule.  We propose a new Bayesian IRL approach in which the likelihood function can be interpreted in terms of rationality models.  Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for recognition problems. "
Toward improving the visual stimulus meaning for increasing the P300 detection,"The P300 speller is a well known Brain-Computer Interface paradigm that has been used for over two decades. A new P300 speller paradigm (XP300) is proposed. It includes several characteristics: (i) the items are not intensified by using rows and columns, (ii) the order of the visual stimuli is pseudo-random,(iii) a visual feedback is added on each item to increase the stimulus meaning, which is the main novelty. XP300 has been tested on ten healthy subjects on copy spelling mode, with only eight sensors. It has been compared with the classical P300 paradigm (CP300). With five repetitions, the average recognition rate across subjects is 85.25\% for XP300 and 77.25\% for CP300. Single-trial detection is significantly higher with XP300 by comparing the AUC (Area Under Curve) of the ROC (Receiver Operating Characteristic) curve. The mean AUC is 0.86 for XP300, 0.80 for CP300. More importantly, XP300 has also been judged as more convenient and user-friendly than CP300, hence being able to allow longer sessions."
Toward improving the visual stimulus meaning for increasing the P300 detection,"The P300 speller is a well known Brain-Computer Interface paradigm that has been used for over two decades. A new P300 speller paradigm (XP300) is proposed. It includes several characteristics: (i) the items are not intensified by using rows and columns, (ii) the order of the visual stimuli is pseudo-random,(iii) a visual feedback is added on each item to increase the stimulus meaning, which is the main novelty. XP300 has been tested on ten healthy subjects on copy spelling mode, with only eight sensors. It has been compared with the classical P300 paradigm (CP300). With five repetitions, the average recognition rate across subjects is 85.25\% for XP300 and 77.25\% for CP300. Single-trial detection is significantly higher with XP300 by comparing the AUC (Area Under Curve) of the ROC (Receiver Operating Characteristic) curve. The mean AUC is 0.86 for XP300, 0.80 for CP300. More importantly, XP300 has also been judged as more convenient and user-friendly than CP300, hence being able to allow longer sessions."
Bayesian Learning in Bayesian Networks of Moderate Size,"We study the problem of learning Bayesian network structures from data.Koivisto and Sood (2004)presented a DP algorithm that can computethe exact posterior probabilities of modular features in Bayesian networks of moderate size.In this paper, we propose a new algorithm which is able to efficiently sample network structuresby using the results of the DP algorithm.The network samples can then be used to efficiently estimatethe posteriors of any features.We empirically show that our algorithm considerably outperforms previous state-of-the-art methods."
Bayesian Learning in Bayesian Networks of Moderate Size,"We study the problem of learning Bayesian network structures from data.Koivisto and Sood (2004)presented a DP algorithm that can computethe exact posterior probabilities of modular features in Bayesian networks of moderate size.In this paper, we propose a new algorithm which is able to efficiently sample network structuresby using the results of the DP algorithm.The network samples can then be used to efficiently estimatethe posteriors of any features.We empirically show that our algorithm considerably outperforms previous state-of-the-art methods."
Convex Adversarial Collective Classification,"Many real-world domains, such as web spam, auctionfraud, and counter-terrorism, are both relational and adversarial.Existing work on adversarial machine learning assumes that theattributes of each instance can be manipulated independently.Collective classification violates this assumption, since objectlabels depend on the labels of related objects as well as their ownattributes.  In this paper, we present a novel method for robustlyperforming collective classification in the presence of a maliciousadversary that can modify up to a fixed number of binary-valuedattributes.  Our method is formulated as a convex quadratic programthat guarantees optimal weights against a worst-case adversary inpolynomial time.  In addition to increased robustness against activeadversaries, this kind of adversarial regularization can also lead toimproved generalization even when no adversary is present.  Inexperiments on real and simulated data, our method consistentlyoutperforms both non-adversarial and non-relational baselines."
Convex Adversarial Collective Classification,"Many real-world domains, such as web spam, auctionfraud, and counter-terrorism, are both relational and adversarial.Existing work on adversarial machine learning assumes that theattributes of each instance can be manipulated independently.Collective classification violates this assumption, since objectlabels depend on the labels of related objects as well as their ownattributes.  In this paper, we present a novel method for robustlyperforming collective classification in the presence of a maliciousadversary that can modify up to a fixed number of binary-valuedattributes.  Our method is formulated as a convex quadratic programthat guarantees optimal weights against a worst-case adversary inpolynomial time.  In addition to increased robustness against activeadversaries, this kind of adversarial regularization can also lead toimproved generalization even when no adversary is present.  Inexperiments on real and simulated data, our method consistentlyoutperforms both non-adversarial and non-relational baselines."
Hierarchical Classification with strutured SVMs,"Multi-label classification from hierarchical structure rises from  many real world applications, for instance documents can  be labeled with multiple categories or topics. It is  difficult to learn most probable label configuration efficiently, which involves searching over large combinatory label space, specially label set size is large, namely over 10000.Also  incorporating hierarchical structure has shown notable improvement on prediction accuracyand reduction of learning complexity. We present efficient multi-labellearning method utilizing hierarchical label structure using  linear structuralSVM. Learning is done over entire label space, which infers most probable labels from all possible label configurations. We show an efficient but simpleto implement learning  method using stochastic primal  optimization,PEGASOS, and dynamic programming. The number of variables is independentof the number of instances in the training dataset, which suits large size problem. Experiments show improved results benefiting fromhierarchy."
Hierarchical Classification with strutured SVMs,"Multi-label classification from hierarchical structure rises from  many real world applications, for instance documents can  be labeled with multiple categories or topics. It is  difficult to learn most probable label configuration efficiently, which involves searching over large combinatory label space, specially label set size is large, namely over 10000.Also  incorporating hierarchical structure has shown notable improvement on prediction accuracyand reduction of learning complexity. We present efficient multi-labellearning method utilizing hierarchical label structure using  linear structuralSVM. Learning is done over entire label space, which infers most probable labels from all possible label configurations. We show an efficient but simpleto implement learning  method using stochastic primal  optimization,PEGASOS, and dynamic programming. The number of variables is independentof the number of instances in the training dataset, which suits large size problem. Experiments show improved results benefiting fromhierarchy."
The trace norm constrained matrix-variate Gaussian process for the prioritization of disease genes,"We propose the trace norm regularized matrix-variate Gaussian process model for low-rank matrix data.A variational constraint is enforced on the model inference; resulting in aposterior matrix-variate Gaussian process with a mean function of constrainedtrace norm. We show that the resulting inference is convex, and the meaninference may be interpreted as the matrix analogue of elastic netregularization; striking a balance between the Hilbert norm and the trace norm.The proposed approach is able to significantly improve the predictionquality when the matrix is partially observed , and all the observedentries have the same value. This is a known failure case for trace normconstrained matrix estimation with Dirac kernels.Our motivating application is the prioritization of candidate disease genes.This tasks seeks to identify new associations between human genes and humandiseases, using known associations as well as kernels induced by gene-geneinteraction networks and disease ontologies,"
Minimax vs. UCT: A Comparative Study Using Synthetic Games,"Upper Confidence bounds for Trees (UCT) and Minimax are two of themost prominent tree-search based adversarial reasoning strategies fora variety of challenging domains, such as Chess and Go. Theircomplementary strengths in different domains have been the motivationfor several works attempting to achieve a better understanding oftheir behaviors. In this paper, rather than using complex games as atestbed for deriving indirect insights into UCT and Minimax, wepropose the study of relatively simple synthetic trees that permitanalysis and afford a greater degree of experimental freedom. Using anovel tree model that does not suffer from the shortcomings ofpreviously studied models, we provide a relatively straightforwardcharacterization of the kinds of games where UCT is superior toMinimax, and vice versa --- to the best of our knowledge, this is thefirst time such an effort has been successful. In particular, we showthat UCT shines in games where heuristics are accurately modeled usingadditive Gaussian noise, that contrary to previous work, earlyterminal states by themselves do not necessarily hurt UCT, and that intrees with heuristic dispersion lag, UCT is outperformed byMinimax."
Laplacian and Distance Covariance Maps: Algorithms for Supervised and Unsupervised Manifold Learning,We propose algorithms for supervised and unsupervised manifold learning. In a supervised setting the dimensionality of the features is reduced while simultaneously preserving the neighborhood structure of the features and also maximizing a statistical measure of dependence known as distance covariance between the features and a response variable. This helps in solving the prediction problem with a low-dimensional set of features. In an unsupervised setting the manifold learning algorithm produces low-dimensional representations of a high-dimensional dataset while preserving the local geometric information. The algorithms are fomulated as majorization minimization and concave convex optimization problems and are iterative.
Laplacian and Distance Covariance Maps: Algorithms for Supervised and Unsupervised Manifold Learning,We propose algorithms for supervised and unsupervised manifold learning. In a supervised setting the dimensionality of the features is reduced while simultaneously preserving the neighborhood structure of the features and also maximizing a statistical measure of dependence known as distance covariance between the features and a response variable. This helps in solving the prediction problem with a low-dimensional set of features. In an unsupervised setting the manifold learning algorithm produces low-dimensional representations of a high-dimensional dataset while preserving the local geometric information. The algorithms are fomulated as majorization minimization and concave convex optimization problems and are iterative.
Interpreting prediction markets: a stochastic approach,"We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution.This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved.In particular, we show that the stationary point of the stochastic process of prices generated by the market is equal to the market's Walrasian equilibrium of classic market analysis.Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour."
MCMC algorithms for near deterministic systems,"Markov Chain Monte Carlo (MCMC) methods are used ubiquitously to generate samples from a probability distribution where exact sampling is not feasible. However, in the presence of near-deterministic components in a joint distribution, it is well-known that mixing is very slow and jumping from one mode to another can take very large amount of time.In this work, we explore one possible fix to this problem by visiting the deterministic problem corresponding to the near-deterministic components. If there exists an efficient algorithm to identify the solutions to this deterministic problem, then we show that it is often possible to design an MCMC algorithm where the proposal distribution is shaped by the efficient algorithm to the deterministic problem."
Copula Discriminant Analysis and Dynamic Time Wrapping for Isolated Acoustic Based Sketch Recognition,"Sketch recognition is an active research field, whose goalis to automatically recognize hand-drawn diagrams drawn on the digital device. However, one interesting finding is that the sound that is generated while people are sketching on the physical surface also provides rich information. Imagine a person sketching simple shapes on the table using fingernail or key, then it is possible for one to guess what that person has drawn by hearing the sound. This technique is cheap and only need one built-in microphone. In this paper, we will investigate this new area, which we called acoustic based sketch recognition, and proposed a novelrecognition algorithm to recognize the sketch through sound. The algorithm works by aggregating the results from copula discriminant analysis and dynamic time wrapping. The result shows that the aggregated decision has higher accuracy than using dynamic time wrapping alone.  "
Copula Discriminant Analysis and Dynamic Time Wrapping for Isolated Acoustic Based Sketch Recognition,"Sketch recognition is an active research field, whose goalis to automatically recognize hand-drawn diagrams drawn on the digital device. However, one interesting finding is that the sound that is generated while people are sketching on the physical surface also provides rich information. Imagine a person sketching simple shapes on the table using fingernail or key, then it is possible for one to guess what that person has drawn by hearing the sound. This technique is cheap and only need one built-in microphone. In this paper, we will investigate this new area, which we called acoustic based sketch recognition, and proposed a novelrecognition algorithm to recognize the sketch through sound. The algorithm works by aggregating the results from copula discriminant analysis and dynamic time wrapping. The result shows that the aggregated decision has higher accuracy than using dynamic time wrapping alone.  "
Risk-Aversion in Multi-armed Bandits,"In stochastic multi--armed bandits the objective is to solve the exploration--exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk--aversion where the objective is to compete against the arm with the best risk--return trade--off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, we investigate their theoretical guarantees, and we report preliminary empirical results."
Risk-Aversion in Multi-armed Bandits,"In stochastic multi--armed bandits the objective is to solve the exploration--exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk--aversion where the objective is to compete against the arm with the best risk--return trade--off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, we investigate their theoretical guarantees, and we report preliminary empirical results."
Risk-Aversion in Multi-armed Bandits,"In stochastic multi--armed bandits the objective is to solve the exploration--exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk--aversion where the objective is to compete against the arm with the best risk--return trade--off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, we investigate their theoretical guarantees, and we report preliminary empirical results."
Confusion-Based Online Learning and a Passive-Aggressive Scheme,"This paper provides the first ---to the best of our knowledge---analysis of online learning algorithms for multiclass problems whenthe {\em confusion} matrix is taken as a performance measure. The workbuilds upon recent and elegant results on noncommutativeconcentration inequalities, i.e. concentration inequalities that applyto matrices, and more precisely to matrix martingales. We do establish generalization bounds for online learningalgorithm and show how the theoretical study motivate the propositionof a new confusion-friendly learning procedure. This learningalgorithm, called \copa (for COnfusion Passive-Aggressive) is apassive-aggressive learning algorithm; it is shown that the updateequations for \copa can be computed analytically, thus allowing theuser from having to recours to any optimization package to implement it. "
A Hierarchical Motion Model for Short- and Long Range Motion,"Recent work [28] presented a unified model of motion perception inspired by theability of the human visual system to simultaneously perceive both short-rangeand long-range motion in dynamic scenes. Their model differed from conven-tional optical flow techniques because it performed inference compositionally bycombining local hypotheses for optical flow to build non-local hypotheses whichexploit non-local context and hence resolve local ambiguities. The model wasable to account for a range of psychophysical phenomena using random dot kine-matograms as stimuli. In this paper, we extend the model so that it can be appliedto natural image sequences. We test its performance on images from the KITTIoptical flow database [10], which include large ranges of motion. Our performanceis comparable to the performance of more standard algorithms on this database.To make the task even more challenging we introduce additional stimuli, mov-ing balls, into the KITTI dataset to give a richer motion field. We show that thehierarchical model still gives accurate results for these stimuli while more con-ventional algorithms degrade. The algorithm is naturally parallelizable and ourimplementation using a Graphics Processing Card (GPU) has a runtime of a fewminutes."
A Hierarchical Motion Model for Short- and Long Range Motion,"Recent work [28] presented a unified model of motion perception inspired by theability of the human visual system to simultaneously perceive both short-rangeand long-range motion in dynamic scenes. Their model differed from conven-tional optical flow techniques because it performed inference compositionally bycombining local hypotheses for optical flow to build non-local hypotheses whichexploit non-local context and hence resolve local ambiguities. The model wasable to account for a range of psychophysical phenomena using random dot kine-matograms as stimuli. In this paper, we extend the model so that it can be appliedto natural image sequences. We test its performance on images from the KITTIoptical flow database [10], which include large ranges of motion. Our performanceis comparable to the performance of more standard algorithms on this database.To make the task even more challenging we introduce additional stimuli, mov-ing balls, into the KITTI dataset to give a richer motion field. We show that thehierarchical model still gives accurate results for these stimuli while more con-ventional algorithms degrade. The algorithm is naturally parallelizable and ourimplementation using a Graphics Processing Card (GPU) has a runtime of a fewminutes."
MODELING THE DYNAMIC SYSTEM OF WIND IN MIDWEST USING MONTE CARLO HIDDEN MARKOV MODEL,"The paper presents a new method of modeling the dynamic of wind in Midwest using HMM clustering coupled with Monte Carlo. After testing the method with simulated data, the method is applied on wind data series in Midwest and get interesting results. The HMM model and cluster result can be used to help the wind forecast for wind mill production. One of the problem with the current method is the time for estimating the model"
Generalized Least Squares for Principled Complex Backups in Temporal Difference Learning,"We derive the form of an estimator that uses generalized least squares to obtaina principled form of the eligibility trace. We show that both the $\lambda$-return and $\gamma$-return can be thought of as assuming that the inverse covariance matrix of $n$-step returns has specific values on its diagonal, and is zero elsewhere.  The new weighting scheme has a single parameter that can easily be set from data,closely matches the empirical covariance weights, and performs very well across several settings of $\gamma$ and $\epsilon$. "
Generalized Least Squares for Principled Complex Backups in Temporal Difference Learning,"We derive the form of an estimator that uses generalized least squares to obtaina principled form of the eligibility trace. We show that both the $\lambda$-return and $\gamma$-return can be thought of as assuming that the inverse covariance matrix of $n$-step returns has specific values on its diagonal, and is zero elsewhere.  The new weighting scheme has a single parameter that can easily be set from data,closely matches the empirical covariance weights, and performs very well across several settings of $\gamma$ and $\epsilon$. "
Branch-and-Bound Prediction for Large Data,"For the problems requiring instance-based learning, nearest neighbor is one of the most popular learning methods. While a linear search for the nearest neighbor can be improved upon by building an efficient structure such as a kd-tree, nearest neighbor method suffers from several shortcomings. These shortcomings include lack of generalization, issue of overfitting, sensitivity to outliers, and inability to triage inputs appropriately. Assuming we have probabilistic equivalence of kd-tree, this paper describes how efficiently we are able to find the most likely prediction through our branch-and-bound approach. The experiments show that branch-and-bound becomes more effective as tree size grows and works significantly faster in big trees. In order to argue this, we implicitly build a tree containing more than 30 billion training examples, and launch experiments to find the most likely example among those 30 billion subclasses. The results show that the branch-and-bound approach allows searches that are roughly 1.41 times deeper than possible with linear search for large binary trees. Considering that the branch-and-bound approach we employed here is parallelizable, these results open a possibility to develop a new class of machine learning algorithms for large data that delay some parts of training process to prediction time while efficiently solving critical issues such as overfitting."
Branch-and-Bound Prediction for Large Data,"For the problems requiring instance-based learning, nearest neighbor is one of the most popular learning methods. While a linear search for the nearest neighbor can be improved upon by building an efficient structure such as a kd-tree, nearest neighbor method suffers from several shortcomings. These shortcomings include lack of generalization, issue of overfitting, sensitivity to outliers, and inability to triage inputs appropriately. Assuming we have probabilistic equivalence of kd-tree, this paper describes how efficiently we are able to find the most likely prediction through our branch-and-bound approach. The experiments show that branch-and-bound becomes more effective as tree size grows and works significantly faster in big trees. In order to argue this, we implicitly build a tree containing more than 30 billion training examples, and launch experiments to find the most likely example among those 30 billion subclasses. The results show that the branch-and-bound approach allows searches that are roughly 1.41 times deeper than possible with linear search for large binary trees. Considering that the branch-and-bound approach we employed here is parallelizable, these results open a possibility to develop a new class of machine learning algorithms for large data that delay some parts of training process to prediction time while efficiently solving critical issues such as overfitting."
Focus of Attention for Linear Predictors,"We present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious. This trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets. We observe that some examples are easier to classify than others, a phenomenon which is characterized by the event when most of the features agree on the class of an example.By stopping the feature evaluation when encountering an easy-to-classify example, the predictor can achieve substantial gains in computation. Our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to include our attentive method we prove that the average number of features computed is $O(\sqrt{n \log \delta^{-0.5}})$ where $n$ is the original number of features, and $\delta$ is the error rate incurred due to early stopping. We demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim, Gisette, and synthetic datasets."
A Convex Extension of MKL to Local Mixtures,"In the context of metric learning, localized multiple kernel learning algorithms have been proposed, featuring a richer model than MKL and improved accuracy. The optimization problem is not solved exactly because of non-convexities or approximations. We present a class of convex problems based on a generalized hinge loss, which can be solved accurately and whose solution is a sparse nonparametric local mixture of kernels. Consistency results of SVMs are generalized, and combined with convexity provide theoretical guarenties. We study the optimization of two examples, and adapt the SMO solver with colsed-form line-search to one of them, achieving fast and scalable learning. Additionally, we extend MKL to our approach, thus combining global and local kernel weights, which further improves performance and scales better with the number of kernels. Promising experiments are conducted on 3 algorithms."
A Convex Extension of MKL to Local Mixtures,"In the context of metric learning, localized multiple kernel learning algorithms have been proposed, featuring a richer model than MKL and improved accuracy. The optimization problem is not solved exactly because of non-convexities or approximations. We present a class of convex problems based on a generalized hinge loss, which can be solved accurately and whose solution is a sparse nonparametric local mixture of kernels. Consistency results of SVMs are generalized, and combined with convexity provide theoretical guarenties. We study the optimization of two examples, and adapt the SMO solver with colsed-form line-search to one of them, achieving fast and scalable learning. Additionally, we extend MKL to our approach, thus combining global and local kernel weights, which further improves performance and scales better with the number of kernels. Promising experiments are conducted on 3 algorithms."
A Convex Extension of MKL to Local Mixtures,"In the context of metric learning, localized multiple kernel learning algorithms have been proposed, featuring a richer model than MKL and improved accuracy. The optimization problem is not solved exactly because of non-convexities or approximations. We present a class of convex problems based on a generalized hinge loss, which can be solved accurately and whose solution is a sparse nonparametric local mixture of kernels. Consistency results of SVMs are generalized, and combined with convexity provide theoretical guarenties. We study the optimization of two examples, and adapt the SMO solver with colsed-form line-search to one of them, achieving fast and scalable learning. Additionally, we extend MKL to our approach, thus combining global and local kernel weights, which further improves performance and scales better with the number of kernels. Promising experiments are conducted on 3 algorithms."
Using context and phonetic features for etymological alignment and reconstruction,"This paper presents methods for investigating etymological data. First, we introducealignment algorithms which explicitly utilize phonetic features and learnlong-range contextual rules that condition recurrent correspondences within a languagefamily. Second, we present an imputation procedure which allows us comparethe quality of alignment models, as well as the goodness of the data sets.We present evaluations to demonstrate that the new model yields improvements inperformance, compared to those previously reported in the literature. We releasethe suite of etymological software for public use."
SMO based Optimization for Quadratic Programming Feature Selection,"Domains such as vision, bioinformatics, web search and web rankings invariably produce datasets where number of features is very large. To carry out classification/regression, feature selection is commonly employed to deal with the curse of dimensionality.  Recently, Quadratic Programming Feature Selection (QPFS) has been shown to outperform many of the existing feature selection methods. A quadratic program solver is used to solve the problem. This requires time complexity cubic in the number of features. Further, the algorithm needs to store the entire feature similarity matrix in memeory.In this paper, we propose an SMO based framework for QPFS (SMO-QPFS). We develop the formulation for working set selection for SMO-QPFS using second order approximations.  Our proposed approach has computaional time quadratic in the number of features in the worst case.In practice, it is shown to take linear time to converge as demonstarted by our experiments.Further, we only need to store feature similarities for the current working set (size $2\times 2$), in contrast to QPFS.This memory saving can be critical for doing feature selection in the datasets with tens of thousands of features.The performance of the SMO-QPFS is evaluated using  eight publicly available benchmark microarray datasets. From our experimental study, it is found that the SMO-QPFS is many times faster than the QPFS approach while retaining the same level of performance."
SMO based Optimization for Quadratic Programming Feature Selection,"Domains such as vision, bioinformatics, web search and web rankings invariably produce datasets where number of features is very large. To carry out classification/regression, feature selection is commonly employed to deal with the curse of dimensionality.  Recently, Quadratic Programming Feature Selection (QPFS) has been shown to outperform many of the existing feature selection methods. A quadratic program solver is used to solve the problem. This requires time complexity cubic in the number of features. Further, the algorithm needs to store the entire feature similarity matrix in memeory.In this paper, we propose an SMO based framework for QPFS (SMO-QPFS). We develop the formulation for working set selection for SMO-QPFS using second order approximations.  Our proposed approach has computaional time quadratic in the number of features in the worst case.In practice, it is shown to take linear time to converge as demonstarted by our experiments.Further, we only need to store feature similarities for the current working set (size $2\times 2$), in contrast to QPFS.This memory saving can be critical for doing feature selection in the datasets with tens of thousands of features.The performance of the SMO-QPFS is evaluated using  eight publicly available benchmark microarray datasets. From our experimental study, it is found that the SMO-QPFS is many times faster than the QPFS approach while retaining the same level of performance."
Bayesian Inference from Non-Ignorable Network Sampling Designs,"Consider individuals interacting in a social network and a response that can be measured on each individual. We are interested in making inferences on a population quantity that is a function of both the response and the social interactions. In this paper, working within Rubin's inferential framework, we introduce a new notion of non-ignorable sampling design for the case of missing covariates. This notion is the key element for developing valid inferences in applications to epidemiology and healthcare in which hard-to-reach populations are sampled using link-tracing designs, including respondent-driven sampling, that carry information about the quantify of interest."
Cardinality Restricted Boltzmann Machines ,"The Restricted Boltzmann Machine (RBM) is a popular density model thatis also good for extracting features.A main source of tractability in RBM models is the model's assumptionthat given an input, hidden units activate independently from one another.Sparsity and competition in the hidden representation is believed tobe beneficial,and while an RBM with competition among its hidden units would acquire someof the attractive properties of sparse coding, such constraints are not added due to the widespread belief that the resulting model would become intractable.In this work, we show how a dynamic programming algorithm developed in 1981can be used to implement exact sparsity in the RBM's hidden units.We then expand on this and show how to pass derivatives through alayer of exact sparsity, which makes it possible to fine-tune adeep belief network (DBN) consisting of RBMs with sparse hiddenlayers.  We show that sparsity in the RBM's hidden layer improves theperformance of both the pre-trained representations and of thefine-tuned model."
Cardinality Restricted Boltzmann Machines ,"The Restricted Boltzmann Machine (RBM) is a popular density model thatis also good for extracting features.A main source of tractability in RBM models is the model's assumptionthat given an input, hidden units activate independently from one another.Sparsity and competition in the hidden representation is believed tobe beneficial,and while an RBM with competition among its hidden units would acquire someof the attractive properties of sparse coding, such constraints are not added due to the widespread belief that the resulting model would become intractable.In this work, we show how a dynamic programming algorithm developed in 1981can be used to implement exact sparsity in the RBM's hidden units.We then expand on this and show how to pass derivatives through alayer of exact sparsity, which makes it possible to fine-tune adeep belief network (DBN) consisting of RBMs with sparse hiddenlayers.  We show that sparsity in the RBM's hidden layer improves theperformance of both the pre-trained representations and of thefine-tuned model."
Cardinality Restricted Boltzmann Machines ,"The Restricted Boltzmann Machine (RBM) is a popular density model thatis also good for extracting features.A main source of tractability in RBM models is the model's assumptionthat given an input, hidden units activate independently from one another.Sparsity and competition in the hidden representation is believed tobe beneficial,and while an RBM with competition among its hidden units would acquire someof the attractive properties of sparse coding, such constraints are not added due to the widespread belief that the resulting model would become intractable.In this work, we show how a dynamic programming algorithm developed in 1981can be used to implement exact sparsity in the RBM's hidden units.We then expand on this and show how to pass derivatives through alayer of exact sparsity, which makes it possible to fine-tune adeep belief network (DBN) consisting of RBMs with sparse hiddenlayers.  We show that sparsity in the RBM's hidden layer improves theperformance of both the pre-trained representations and of thefine-tuned model."
Cardinality Restricted Boltzmann Machines ,"The Restricted Boltzmann Machine (RBM) is a popular density model thatis also good for extracting features.A main source of tractability in RBM models is the model's assumptionthat given an input, hidden units activate independently from one another.Sparsity and competition in the hidden representation is believed tobe beneficial,and while an RBM with competition among its hidden units would acquire someof the attractive properties of sparse coding, such constraints are not added due to the widespread belief that the resulting model would become intractable.In this work, we show how a dynamic programming algorithm developed in 1981can be used to implement exact sparsity in the RBM's hidden units.We then expand on this and show how to pass derivatives through alayer of exact sparsity, which makes it possible to fine-tune adeep belief network (DBN) consisting of RBMs with sparse hiddenlayers.  We show that sparsity in the RBM's hidden layer improves theperformance of both the pre-trained representations and of thefine-tuned model."
Cardinality Restricted Boltzmann Machines ,"The Restricted Boltzmann Machine (RBM) is a popular density model thatis also good for extracting features.A main source of tractability in RBM models is the model's assumptionthat given an input, hidden units activate independently from one another.Sparsity and competition in the hidden representation is believed tobe beneficial,and while an RBM with competition among its hidden units would acquire someof the attractive properties of sparse coding, such constraints are not added due to the widespread belief that the resulting model would become intractable.In this work, we show how a dynamic programming algorithm developed in 1981can be used to implement exact sparsity in the RBM's hidden units.We then expand on this and show how to pass derivatives through alayer of exact sparsity, which makes it possible to fine-tune adeep belief network (DBN) consisting of RBMs with sparse hiddenlayers.  We show that sparsity in the RBM's hidden layer improves theperformance of both the pre-trained representations and of thefine-tuned model."
Learning Multi-Label Scene Classification using Asymmetric SIMPLS Classifier,"In this paper, we propose asymmetric SIMPLS classifier for automatic learning multi-label scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). We show that asymmetric SIMPLS is a sub-optimal solution of a joint learning framework in which we perform dimensionality reduction and multi-label classification simultaneously. Asymmetric SIMPLS and other five state-of-the-artlearning algorithms are evaluated and compared on a public dataset, which consists of a set of 2000 images with 5 clusters of basic scenes. Experimental results validate the effectiveness of our asymmetric SIMPLS compared toother methods. Furthermore, our work appears to generalize to other classification problems of the same nature."
Information Driven Exploration using Poisson Sampling over Ising Marginals,"We describe an information-gathering approach for exploring an unknown scene using a range sensor. It relies on an efficient approximation of the uncertainty in the map due to visibility (occlusions). The reduction in uncertainty due to a control action is represented on an Ising model using a Poisson covering. Our algorithm improves the performance of recent visibility-based planning approaches that come with guaranteed performance bounds on the expected path length to complete exploration, and extends them to allow exploration of an unbounded region, with an extension of the bounds to exploration rate rather than complete exploration."
Information Driven Exploration using Poisson Sampling over Ising Marginals,"We describe an information-gathering approach for exploring an unknown scene using a range sensor. It relies on an efficient approximation of the uncertainty in the map due to visibility (occlusions). The reduction in uncertainty due to a control action is represented on an Ising model using a Poisson covering. Our algorithm improves the performance of recent visibility-based planning approaches that come with guaranteed performance bounds on the expected path length to complete exploration, and extends them to allow exploration of an unbounded region, with an extension of the bounds to exploration rate rather than complete exploration."
Information Driven Exploration using Poisson Sampling over Ising Marginals,"We describe an information-gathering approach for exploring an unknown scene using a range sensor. It relies on an efficient approximation of the uncertainty in the map due to visibility (occlusions). The reduction in uncertainty due to a control action is represented on an Ising model using a Poisson covering. Our algorithm improves the performance of recent visibility-based planning approaches that come with guaranteed performance bounds on the expected path length to complete exploration, and extends them to allow exploration of an unbounded region, with an extension of the bounds to exploration rate rather than complete exploration."
