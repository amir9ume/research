sid,title,abstract,country,university,top
12,Efficient Local Image Description and Matching Based on Permutation Distances,"Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a randomsampling of pairwise comparisons of pixel intensities in an image patch.  Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a descriptor based on permutation distances between the ordering of intensities or RGB values between two patches. LUCID is computable in linear time of patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF yet up to five times more accuate, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster.",United States,"University of California, San Diego",1
13,Clustering the Stochastic Block Model via Positions in the Network,"We consider the stochastic block model and analyze methods based on the positions of the nodes in the network.  This perspective was introduced by Burt (1976) and algorithmically amounts to embedding the graph by mapping the vertices to the corresponding rows of the adjacency matrix.  Once this is done, off-the-shelf methods for clustering points in Euclidean (or Hamming) space can be applied.  We study some popular dissimilarities in this context and provide theoretical guarantees for them that are sufficient for hierarchical clustering to succeed in correctly clustering the nodes.  The analysis is relatively simple in the context of a stochastic block model, yielding competitive performance bounds, particularly when the number of communities in the network grows with the number of nodes.  We evaluate some of these methods in some simulations, comparing them with spectral clustering.  ",United States,"University of California, San Diego",1
19,Multiscale Hidden Conditional Neural Fields with Adaptive-Rate Latent Variable Grouping,"We hypothesize that considering multiple levels of abstraction with adaptive-rate time quantization improves temporal sequence learning. To prove this empirically, we developed multiscale hidden conditional neural fields with adaptive-rate latent variable grouping. Our model is comprised of multiple layers, where each layer is recursively built from the preceding layer by aggregating observations that are similar in the latent space, representing the sequence at a coarser scale. We extract a nonlinear combination of features from grouped variables using a set of gate functions, learning the optimal abstraction of the sequence at each scale. This allows our model to learn higher level abstractions at ever more coarse-grained time scale as the layer gets higher. Optimization is performed layer-wise, making the complexity grow linearly with the number of layers. We evaluate our approach on three human activity datasets: ArmGesture, NATOPS, and Canal9. Our method achieves a near perfect recognition accuracy on the ArmGesture dataset, and outperforms all baseline models on the NATOPS and Canal9 dataset.",United States,Massachusetts Institute of Technology,1
23,Jointly Segmenting Multiple Web Photo Streams,"As online sharing of personal photo streams is becoming popular and many of such photo streams often share overlapping contents, the cosegmentation can potentiate a wide range of intriguing Web applications. However, existing cosegmentation algorithms are still far limited for these new opportunities in that input images must be carefully prepared by human. In this paper, we address the problem of jointly segmenting an arbitrary number of unaligned and uncalibrated Web photo streams from multiple anonymous users. Given that the main difficulty of cosegmenting such photo streams lies in their extreme diversity in visual contents, we propose a multi-round segmentation algorithm that consists of the companion sampling  and cosegmentation  steps. Theoretically, we show that the developed method achieves the sublinear bound of regrets (i.e. the cumulative sum of difference between unknown ideal cosegmentation scores and actual scores). With experiments on more than 16K images of Flickr dataset and LabelMe dataset, wedemonstrate that our algorithm is more successful in both segmentation quality and scalability over other state-of-art methods.",United States,Carnegie Mellon University,1
29,Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition,We present data-driven techniques to augment Bag of Word (BoW) models within a Vector Space Model framework. This approach allows for more robust modeling and recognition of complex activities for instances where the structure and topology of the activities are not known a priori. Our approach addresses the limitation of standard BoW approaches that fail to represent the temporal information inherent in activity streams. We also introduce the use of randomly sampled Regular Expressions to capture the global structure of activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in three complex data-sets. ,United States,Georgia Institute of Technology,1
33,A Unified 3D Scene Parsing Framework,"We propose a unified framework for parsing an image to predict the scene category, the 3D boundary of the space, camera parameters, and all objects in the scene, represented by their 3D bounding boxes and categories. Using a structural SVM, we build a complete end-to-end system which learns all parameters together in a single step. We encode many novel image features and context rules into the structural SVM feature function and our framework automatically weighs the relative importance of all these rules based on training data. By optimizing a unified objective function, we do not require extra training of other models, nor an additional fusion step. We design an intuitive web-based tool to annotate 3D bounding boxes for objects, build our ?SUN3D? database and demonstrate that our model outperforms the state-of-the-art algorithms on several individual subtasks.",United States,Massachusetts Institute of Technology,1
35,Coupling Nonparametric Mixtures via Latent Dirichlet Processes,"Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Specifically, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures, allowing mixture models to be coupled more flexibly. In addition, we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling.",United States,Massachusetts Institute of Technology,1
72,Learning Mixed Graphical Models,We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables. The structure and parameters of this model are learned using the pseudo-likelihood approximation with group-sparsity regularization. The pairwise model is also extended to incorporate features. Two algorithms for solving the resulting optimization problem are presented. The proposed models are compared with competing methods on synthetic data and a survey dataset.,United States,Stanford University,1
73,Efficient Robust Recovery of Low-rank Tensors,"Robust tensor recovery in the presence of outliers, gross corruptions and missing values plays an instrumental role in robustifying tensor decompositions for multilinear data analysis and has a diverse array of applications.  In this paper, we study the problem of robust low-rank tensor recovery in a convex optimization framework, drawing upon the recent advance in robust PCA and tensor completion.  We propose tailored optimization algorithms with global convergence guarantees for solving both the constrained and the Lagrangian formulations of the problem.  These algorithms are based on the highly efficient alternating direction augmented Lagrangian and accelerated proximal gradient methods.  We investigate the empirical recoverability properties of the convex formulation and compare the recovery and computational performance of the algorithms on both simulated and real data.",United States,Columbia University,1
85,ForeCA: Forecastable Component Analysis,"Blind source separation (BSS) techniques are often applied to multivariate time series with the goal to obtain better forecasts. But BSS and the need for better forecasts are often treated separately, in the sense that finding an optimally transformed (sub-)space has nothing to do with the aim to predict well. Here I introduce Forecastable \textbf{C}omponent Analysis (ForeCA), a new BSS technique for temporally dependent signals that uses forecastability as the explicit objective in finding an optimal transformation. It separates the signal into the forecastable, $\mathbf{F}$, and the orthogonal white noise space, $\mathbf{F}^{\bot}$. Simulations and applications to financial data show that ForeCA successfully finds signals that can be used to forecast. ForeCA therefore automatically discovers informative structure in multivariate signals.",United States,Carnegie Mellon University,1
127,Constructing a Design Matrix by Stepping Vertex Features on Multiple Networks,"Suppose there are $n$ vertices which are embedded in $p$ networks and which take values $\mathbf{z}$.  We are interested in the simultaneous interactions between the vertices and the networks in which they are embedded.  In pursuit of this, we propose constructing an $n$ by $p$ design matrix by simply taking a step from $\mathbf{z}$ on each adjacency matrix (premultiplying each adjacency matrix by $\mathbf{z}$).  We then rely on traditional statistical techniques to analyze this constructed design matrix either with exploratory techniques or predictive techniques if the vertices take on a value $\mathbf{y}$ (we also consider the autoregressive case where $\mathbf{y} = \mathbf{z}$).  We compare this design matrix construction approach to methods specialized to handle the network analysis.",United States,Stanford University,1
141,Ordered Rules for Classification: A Discrete Optimization Approach to Associative Classification,"We aim to design classifiers that have the interpretability of association rules yet have predictive power on par with the top machine learning algorithms for classification. We propose a novel mixed integer optimization (MIO) approach called Ordered Rules for Classification (ORC) for this task. Our method has two parts. The first part mines a particular frontier of solutions in the space of rules, and we show that this frontier contains the best rules according to a variety of interestingness measures. The second part learns an optimal ranking for the rules to build a decision list classifier that is simple and insightful. We report empirical evidence using several different datasets to demonstrate the performance of this method.",United States,Massachusetts Institute of Technology,1
154,Modeling the Forgetting Process using Image Regions,"While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten over time has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten over time, using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. ",United States,Massachusetts Institute of Technology,1
157,Object Focused Q-learning for Autonomous Agents ,"We present Object Focused Q-learning (OF-Q), a novel reinforcement learning algorithm that can offer exponential speedups over classic Q-learning on domains composed of nearly-independent objects. OF-Q treats the state space as a collection of different objects organized into different object classes. Our key contribution is to estimate the risk of different objects by learning non-optimal Q-functions that we incorporate into a control policy. We compare our algorithm to traditional Q-learning and previous arbitration algorithms in two domains, including a version of Space Invaders.",United States,Georgia Institute of Technology,1
181,Textual features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks.",United States,"University of California, San Diego",1
196,Scalable Manifold Learning,"High computational costs of manifold learning make its application prohibitive for large point sets. A common strategy to overcome this problem is to sample a subset of points, called landmarks, on which the dimensionality reduction is performed and to reconstruct the embedding of all points using the Nystr?m method. In this paper, we address the two main challenges that arise in this setup. First, the selected subset of landmarks in non-Euclidean geometries must result in a low reconstruction error. Second, the nearest neighbor graph construction on sparsely sampled subsets must be robust and approximate the original data well. We propose an extension for sampling from determinantal distributions on non-Euclidean spaces by opearting on the geodesic distance on the manifold. Since current determinantal sampling algorithms have the same complexity as manifold learning, we propose an efficient approximation running in $\mO(ndk)$. We achieve excellent results with the proposed algorithm for manifold sampling by restricting the probability update to local neighborhoods. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Mahalanobis distance increases the robustness of dimensionality reduction on sparsely sampled manifolds. ",United States,Massachusetts Institute of Technology,1
200,Sampling GMRFs by Subgraph Correction,"The problem of efficiently drawing samples from a Gaussian Markov random field is studied. In this paper, we introduce the subgraph correction sampling algorithm, which makes use of any pre-existing tractable sampling algorithm for a subgraph by perturbing this algorithm so as to yield asymptotically exact samples for the intended distribution. The subgraph can have any structure for which efficient sampling algorithms exist: for example, tree-structured, with low tree-width, or with a small feedback vertex set. Experimental results demonstrate that the subgraph correction algorithm yields accurate samples much faster than many traditional sampling methods---such as Gibbs sampling---for many graph topologies.",United States,Massachusetts Institute of Technology,1
201,Multidimensional Membership Mixture Models,"We present the multidimensional membership mixture (M3) models where every dimension of the membership represents an independent mixture model and each data point is generated from the selected mixture components jointly. This is helpful when the data has a certain shared structure. For example, three unique means and three unique variances can effectively form a Gaussian mixture model with nine components, while requiring only six parameters to fully describe it. In this paper, we present three instantiations of M3 models (together with the learning and inference algorithms): infinite, finite, and hybrid, depending on whether the number of mixtures is fixed or not. They are built upon Dirichlet process mixture models, latent Dirichlet allocation, and a combination respectively. We then consider two applications: topic modeling and learning 3D object arrangements. Our experiments show that our M3 models achieve better performance using fewer topics than many classic topic models. We also observe that topics from the different dimensions of M3 models are meaningful and orthogonal to each other. ",United States,Cornell University,1
204,Optimal Regularized Dual Averaging Methods for Stochastic Optimization,"This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth.  We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the optimal  rate of $O(\frac{1}{N}+\frac{1}{N^2})$ for $N$  iterations, which improves the best known rate $O(\frac{\log N }{N})$ of previous stochastic dual averaging algorithms. In addition, our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., $\ell_1$-norm), it has the advantage of encouraging sparser solutions. We further develop a multi-stage extension using the proposed algorithm as a subroutine, which achieves the uniformly-optimal rate $O(\frac{1}{N}+\exp\{-N\})$ for strongly convex loss.",United States,Carnegie Mellon University,1
205,Learning global properties of scene images from conditional correlational structure,"Scene images with similar spatial layout properties often display characteristic statistical regularities on a global scale. In order to develop an efficient code for these global properties that reflects their inherent regularities, we train a hierarchical probabilistic model to infer conditional correlational information from scene images. Fitting a model to a scene database yields a compact representation of global information that encodes salient visual structures with low dimensional latent variables. Using perceptual ratings and scene similarities based on spatial layouts of scene images, we demonstrate that the model representation is more consistent with perceptual similarities of scene images than the metrics based on the state-of-the-art visual features. ",United States,Carnegie Mellon University,1
206,The variational hierarchical EM algorithm for clustering hidden Markov models.,"In this paper, we derive a novel algorithm to cluster  hidden Markov models (HMMs) according to their probability distributions.We propose a variational hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ``cluster center'', i.e., a novel HMM that is representative for the group.We illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging.",United States,"University of California, San Diego",1
212,3D Gaze Concurrences from Head-mounted Cameras,"A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanating from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the fixed relationship between the primary gaze ray and the head-mounted camera pose. The resulting gaze model enables us to build a social saliency field in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent mode-seeking in the social saliency field. Our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth.",United States,Carnegie Mellon University,1
236,Learning Useful Abstractions from the Web: Case Study on Patient Medications and Outcomes,"The successful application of machine learning to electronic medical records typically turns on the construction of an appropriate feature vector.  That often depends upon the ability to find an appropriate way to abstract the large number of variables found in such records.  In this paper, we explore the use of topic modeling to design feature vectors in an automated manner by harnessing expertise available on the Web. We test the proposed methods on the task of inferring useful abstractions from a list of thousands of medications. Using Latent Dirichlet Allocation we learn a topic model based on Web entries corresponding to each drug in the list. Using only knowledge from Wikipedia pages, we were able to learn a model that is similar to the curated drug classification scheme that serves as an industry standard. We further demonstrate the utility of these learned abstractions through the construction of a kernel based on the earth mover's distance and derived from the learned topic model. Applied to a corpus of 25,000 patient admissions, we use this kernel to predict three different adverse outcomes (death, an abnormally long stay, or admission through the emergency room) for the next hospital admission.  Somewhat surprisingly,  the classifiers built using the learned abstractions outperform classifiers learned from the curated drug classification scheme.",United States,Massachusetts Institute of Technology,1
237,Learning Model-Based Sparsity via Projected Gradient Descent,"Several convex formulation methods have been proposed previously for statistical estimation with structured sparsity as the prior. These methods often require a carefully tuned regularization parameter, often a cumbersome or heuristic exercise. Furthermore, the estimate that these methods produce might not belong to the desired sparsity model, albeit accurately approximating the true parameter. Therefore, greedy-type algorithms could often be more desirable in estimating structured-sparse parameters. So far, these greedy methods have mostly focused on linear statistical models. In this paper we study the projected gradient descent with non-convex structured-sparse parameter model as the constraint set. Should the cost function have a Stable Model-Restricted Hessian the algorithm converges to the desired minimizer up to an approximation error. As an example we elaborate on application of the main results to estimation in Generalized Linear Model.",United States,Carnegie Mellon University,1
240,Discriminatively Activated Sparselets,"As the number of object classes becomes large, redundancy amonglearned object models increases substantially and thus naturallymotivates the idea of compact intermediate representations that canbe shared across classes for efficient multiclass inference. Recently,a new universal intermediate representation for multiclass objectdetection, sparselets, was introduced yieldingone to two orders of magnitude reduction in inference time andenabling real-time multiclass object detection. However, as computationalefficiency is gained by making the sparselet activations increasingly sparse, the task performance of reconstructive sparseletmodels degrades unfavorably.  This paper provides a generalformalism where sparselet activations are learned discriminativelyin a structured output prediction framework. Our experimental resultson multiclass object detection and multiclass image classificationdemonstrate that the proposed discriminative sparselet activationsmaintain high task performance while achieving greater sparsity,which in turn significantly improves inference efficiency.",United States,"University of California, Berkeley",1
241,On Consistent Classification with Imbalanced Classes,"We consider the problem of imbalanced classes in binary classification, where one class is rare compared to the other. This problem arises frequently in practice and has been widely studied. However very little is understood in terms of the theoretical properties of the problem or of the algorithms proposed: what performance measures are appropriate, how these affect the learning process, and whether the algorithms are statistically consistent with respect to the desired performance measures. In this paper, we initiate a formal study of these issues, focusing on the balanced 0-1 error that evaluates errors on the majority and minority classes separately and effectively balances the two. The underlying balanced 0-1 loss bears similarity to cost-sensitive losses; however a critical difference between the two is that the balanced loss depends on the underlying distribution, while cost-sensitive losses are defined independent of the distribution. We establish statistical consistency of two types of algorithms with respect to the balanced 0-1 error: plug-in rules that use an empirically determined threshold, and certain types of empirically balanced risk minimization algorithms. Our experiments support our theoretical results, showing that both these approaches perform as well as (or better than) under-/over-sampling methods that are currently viewed as the state of the art.",United States,"University of California, San Diego",1
247,Patient Risk Stratification for Hospital-Associated C. Diff as a Time-Series Classification Task,"A patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state. We explore representing patient risk as a time series. In doing so, patient risk stratification becomes a time-series classification task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted. Thus, we begin by defining and extracting approximate \textit{risk processes}, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns. We apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \textit{Clostridium Difficile}. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05). ",United States,Massachusetts Institute of Technology,1
252,Human Activity Learning using Object Affordances from RGB-D Videos,"Human activities comprise several sub-activities performed in a sequence and involve interactions with various objects. This makes reasoning about the object affordances a central task for activity recognition. In this work, we consider the problem of jointly labeling the object affordances and human activities from RGB-D videos. We frame the problem as a Markov Random Field where the nodes represent  objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural SVM approach, where labeling over various alternate temporal segmentations are considered as latent variables. We tested our method on a dataset comprising 120 activity videos collected from four subjects, and obtained an end-to-end precision of 81.8% and recall of 80.0% for labeling the activities.",United States,Cornell University,1
259,Fast Max-kernel Search,"The wide applicability of kernels make the problem of max-kernel searchubiquitous and more general than the usual similarity search. We focus onsolving this problem efficiently. We begin by characterizing the inherenthardness of the max-kernel search problem with a novel notion of {\emdirectional concentration}. Following that, we present a method to use an $O(n\log n)$ scheme to index any set of objects (points in $\Real^\dims$ or abstractobjects) \textit{directly in the kernel space} without any explicitrepresentation of the points in this kernel space. A provably $O(\log n)$branch-and-bound algorithm is presented using this index for exact max-kernelsearch. Empirical results for a variety of data sets as well as abstract objectsdemonstrate up to 4 orders of magnitude speedup in some cases. Extensions forapproximate max-kernel search are also presented.",United States,Georgia Institute of Technology,1
274,Understanding Trees via Margins,"Building off recent work such as [Fruend, et.al.,2007], we seek to further understanding of  the behavior of multidimensional trees in high dimensional data. These trees are used in nearest-neighbor search, vector quantization, classification and other tasks. Usual analysis of trees investigate the capability of trees to adapt to some notion of intrinsic dimensions. In this paper, we provide an alternate avenue to mitigate high dimension effects -- margins. To this end, (1) we quantify the contribution of margins to the performance of tree, and (2) we formalize an intuitive notion of robustness and present its dependence on the margins.  We also provide empirical evidence showing that large margin splits can result in good quality indexing in high dimensions while being fairly robust to perturbations. ",United States,Georgia Institute of Technology,1
279,Majorization for CRFs and Latent Likelihoods,"The partition function plays a key role in probabilistic modeling including conditional random fields, graphical models, and maximum likelihood estimation. To optimize partition functions, this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Suchbounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems, low-rank versions of the bound are provided and outperformLBFGS as well as first-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods.",United States,Columbia University,1
280,Ensemble weighted kernel estimators for multivariate entropy estimation,"The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, kernel plug-in estimators suffer from the curse of dimensionality, wherein the MSE rate of convergence is glacially slow - of order  $O(T^{-{\gamma}/{d}})$, where $T$ is the number of samples, and $\gamma>0$ is a rate parameter. In this paper, it is shown that for sufficiently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order $O(T^{-1})$. Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed offline. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates.",United States,University of Michigan - Ann Arbor,1
285,Efficient high dimensional maximum entropy modeling via symmetric partition functions,"  The application of the maximum entropy principle to sequence  modeling has been popularized by methods such as Conditional Random  Fields (CRFs).  However, these approaches are generally limited to  modeling paths in discrete spaces of low dimensionality.  We  consider the problem of modeling distributions over paths in  continuous spaces of high dimensionality---a problem for which  inference is generally intractable.  Our main contribution is to  show that maximum entropy modeling of high-dimensional, continuous  paths is tractable as long as the constrained features   possess a certain kind of low dimensional structure.  In this case, we show that the associated {\em partition function} is  symmetric and that this symmetry can be exploited to compute the  partition function efficiently in a compressed form.  Empirical  results are given showing an application of our method to maximum  entropy modeling of high dimensional human motion capture data.",United States,Carnegie Mellon University,1
291,Structured  Sparse Learning of Multiple Gaussian Graphical Models,"We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data.",United States,University of Washington,1
298,Shifting Weights: Adapting Object Detectors from Image to Video,"Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video.",United States,Stanford University,1
306,Weighted regret-based likelihood: a new approach to describing uncertainty,"A new representation of likelihood is proposed, based on the notion ofregret, and is completely characterized.",United States,Cornell University,1
313,Semantic GIST: Probabilistic Modelling of Scenes using Scenelet,"In this paper, we propose a probabilistic modeling framework for scenes to encode semantic information of images into a compact Semantic Gist representation. The representation is based on a key concept called {\em scenelets}, which serves as building blocks for scenes. We learn these scenelets using a topic model to group correlated objects such that the learned set of scenelets maximally retain the semantic saliency of images in terms of KL-divergence. Our model also integrates information from individual discriminative object detectors and global image features by coding them as priors. Empirical results demonstrate the power of our model. We first show that using a small set of scenelet classifiers, we can predict the existence of a large set of objects without running individual object detectors.Furthermore, we can even predict the presence of objects without running large sets of object detectors by MAP estimation using our model.We also show that the framework can improve the performance of individual detectors by incorporating the contextual object and scenelet information. Experiments on challenging datasets including PASCAL and SUN09  demonstrate that our model outperforms other state-of-the-art ones.",United States,Stanford University,1
341,Search-Based Understanding of Hierarchical Dirichlet Process Topic Models,"Bayesian nonparametric models are widely used to allow unsupervised learning of statistical model structure from data.  Conventional learning algorithms, based on Gibbs sampling or variational Bayesian approximations, can be undesirably prone to local optima and sensitive to initialization.  We explore this issue in the context of the hierarchical Dirichlet process, which with combined with Dirichlet-multinomial likelihoods provides a nonparametric topic model.  We present a more efficient learning algorithm based on the Maximization-Expectation (ME) algorithm, based on a novel combinatorial formulation of the HDP marginal likelihood and implemented with carefully crafted search moves.  Experiments on synthetic data with statistics similar to real text corpora, and four real document collections, show consistent improvement over sampling algorithms in both recovery of the true number of topics, and predictive likelihood of test data.",United States,Massachusetts Institute of Technology,1
342,Localizing 3D cuboids in single-view images,"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners.",United States,Massachusetts Institute of Technology,1
343,Active Metric Learning For Ground Level To Aerial Image Matching,"Image-based geolocation is a challenging problem that has recently captured the interest of computer vision and machine learning researchers.  In this work we focus on the specific problem of matching ground level images to 45 degree aerial images. Matching ground level and aerial images is a very hard problem due to wide disparities in viewpoint and imaging conditions, the combination of which leads to low level feature matching failure.  To overcome this problem, we propose an active metric learning framework that allows a human user to solve the problem collaboratively with the machine.  Our approach selects pairs of regions of interest based on an information gain criterion and asks the human user to establish correspondences between them.  Those pairs are subsequently used to update a metric to improve the correspondences. This process continues until the system finds the correct location of the ground level image.  We introduce a new Ground Level to Aerial Image Dataset (GLAID) to assess strengths and weaknesses of our proposed framework. Our experiments show that our system allows user to find the correct location with significantly reduced effort.",United States,"University of California, San Diego",1
359,Diagnosing learners' knowledge from their actions using inverse reinforcement learning,"The use of computerized environments in which students complete complex tasks is increasingly common in education. Data about students' actions in these environments has the potential to provide information about these students' knowledge, but can be difficult to interpret.In this paper, we focus on instances in which a student must take a series of actions to complete a goal, and develop a framework for automatically inferring the student's underlying beliefs based on these observed actions. This framework relies on modeling how student actions follow from beliefs about the effects of those actions. By framing the problem in terms of a Markov decision process, we specify a general model that can be applied to a wide range of situations. We first validate that this model can recover learners' beliefs in a lab experiment, and then use it to model data from an educational game. In the lab experiment, the model's inferences reflect participants' stated beliefs, and for the educational game, the model's inferences are consistent with conventional assessment measures.",United States,"University of California, Berkeley",1
366,Adaptive Compressive Network Modeling,"Network data is ubiquitous nowadays, such as social networks, bio-networks, computer networks, and complex information network. However, the research of network data is still mostly heuristic and lacks rigorous theoretical underpinnings. Until very recently, a new line of work named compressive network modeling sheds light on this problem.  Specifically, [13] proposes a general framework to use compressed sensing techniques to analyze network data. This method explores a large latent dictionary which is able to recover the hidden structure within the network. However, one limitation of this framework is that the dictionary used to model the network must be pre-given, e.g. clique spaces. Such a pre-determined dictionary is not adaptive to unknown datasets. In this paper, instead of assuming the dictionary is prefixed, we propose a method to automatically learn a dictionary based on the observed networks, which better adapts to the empirical data. Our approach, named adaptive compressive network modeling, is amenable to theoretical analysis, computationally tractable and can be successfully applied to many areas. ",United States,Stanford University,1
387,Sparse Principal Component Analysis with missing observations,"In this paper, we study the problem of sparse Principal Component Analysis(PCA) in the high-dimensional setting with missing observations. Our goal isto estimate the first principal component when we only have access to partial ob-servations. Existing estimation techniques are usually derived for fully observeddata sets and require a prior knowledge of the sparsity of the first principal compo-nent in order to achieve good statistical guarantees. Our contributions is threefold.First, we establish the first information-theoretic lower bound for the sparse PCAproblem with missing observations. Second, we propose a simple procedure thatdoes not require any prior knowledge on the sparsity of the unknown first principalcomponent or any imputation of the missing observations, adapts to the unknownsparsity of the first principal component and achieves the optimal rate of estima-tion up to a logarithmic factor. Third, if the covariance matrix of interest admits asparse first principal component and is in addition approximately low-rank, thenwe can derive a completely data-driven procedure computationally tractable inhigh-dimension, adaptive to the unknown sparsity of the first principal componentand statistically optimal (up to a logarithmic factor).",United States,Georgia Institute of Technology,1
388,Proximal Newton-type Methods for Minimizing Convex Objective Functions in Composite Form,"We consider minimizing convex objective functions in \emph{composite form}\begin{align*}  \minimize_{x\in\R^n} f(x) := g(x) + h(x),\end{align*}where $g$ is convex and twice-continuously differentiable and $h:\R^n\to\R$ is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efficiently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. Many problems of relevance in high-dimensional statistics, machine learning, and signal processing can be formulated in composite form. We prove such methods are globally convergent to a minimizer and achieve quadratic rates of convergence in the vicinity of a unique minimizer. We also demonstrate the performance of such methods using problems of relevance in machine learning and high-dimensional statistics.",United States,Stanford University,1
395,Multi-criteria Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.",United States,University of Michigan - Ann Arbor,1
401,Max-Margin Transforms for Visual Domain Adaptation,"We present a new algorithm for training linear support vector machine classifiers across image domains. Our algorithm learns a linear transformation that maps points from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce a novel cost function for transformation learning based on the misclassification loss of the target points transformed into the source domain. One advantage of our method over previous SVM-based domain adaptation algorithms is that it performs multi-task adaptation, learning a shared component of the domain shift across all categories.  Experiments on both synthetic data and real image datasets demonstrate strong performance and computational advantages compared to previous approaches.",United States,"University of California, Berkeley",1
404,Fast Exact MAP Inference by Passing Incomplete Messages,"We propose a novel approach to faster exact MAP inference that builds on max-product (min-sum) message passing on clique trees and exploits the branch-and-bound idea. The high level procedure is to propagate incomplete messages over the clique tree while maintaining local upper bounds at sepsets. Our algorithm is guaranteed to converge and find the global optimal solution. Empirically we show that our method consistently demonstrates large savings over state-of-the-art methods on several different models with both synthetic and real world data. Our approach also suggests an interesting connection between exact and approximate MAP inference: If we can find tighter relaxations (over sub-graphs), we can make use of the lower bounds to perform exact inference even faster.",United States,Stanford University,1
409,Efficient Random Walk with Gaussian Kernels,"Implicit manifolds is a technique used with random walk-based semi-supervised learning and graph clustering methods to implicitly construct a dense similarity matrix, reducing the cost of manipulations on the matrix from $O(n^2)$ to linear. Specifically, propagating labels through an $O(n^2)$ matrix can be replaced with propagating through a series of sparse matrices. While similarity functions such as cosine similarity are easily ``plugged into'' the implicit manifold framework, it is not straightforward to integrate the Gaussian kernel. In this paper we propose two methods to do this and provide experimental results.",United States,Carnegie Mellon University,1
413,Timely Object Recognition,"In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the eminent PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains $66\%$ better AP than a random ordering, and $14\%$ better performance than an intelligent baseline. On the timeliness measure, our method obtains at least $11\%$ better performance. Our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning.",United States,"University of California, Berkeley",1
448,Region-of-Interest-Constrained Discriminant Analysis for MEG Decoding,"Brain state decoding based on whole-head MEG has been extensively studied over the past decade. However, recent MEG applications pose an emerging need of decoding brain states by the signals originating from specific cortical regions. Towards this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA closely integrates linear classification and beamspace transformation into a unified framework by formulating a constrained non-convex optimization problem. A numerical solver is developed to solve the non-convex optimization problem posed by RDA with guaranteed global convergence. Our experimental results based on a human subject demonstrate that RDA can efficiently extract the discriminant information from pre-specified cortical regions to accurately distinguish different brain states.",United States,Carnegie Mellon University,1
482,Near-optimal Differentially Private Principal Components,"Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension.  Many current data sets of interest contain private or sensitive information about individuals.  Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs.  Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs.  In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output.  We demonstrate that on real data, there this a large performance gap between the existing methods and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling.",United States,"University of California, San Diego",1
483,Graph Estimation From Multi-attribute Data,"Many real world network problems often concern multivariate nodal  attributes such as image, textual, and multi-view feature vectors on  nodes, rather than simple univariate nodal attributes. The existing  graph estimation methods built on Gaussian graphical models and  covariance selection algorithms can not handle such data, neither can  the theories developed around such methods be directly  applied. In this paper, we propose a new principled framework for  estimating multi-attribute networks. Instead of estimating the  partial correlation as in current literature, our method estimates  the {\it partial canonical correlations} that naturally accommodate  complex nodal features.  Computationally, we provide an efficient  algorithm which utilizes the multi-attribue  structure. Theoretically, we provide sufficient conditions which  guarantee consistent graph recovery. Empirically, we apply our  method on a genomic dataset to illustrate its usefulness. ",United States,Carnegie Mellon University,1
486,Recovering Block-structured Activations Using Compressive Measurements ,"We consider the problem of detection and localization of a small block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. However, contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that in our problem the magnitude of the weakest signals one can reliably localize is strongly influenced by both structure and the ability to choose measurements adaptively. We derive tight upper and lower bounds for the detection and estimation problems, under both adaptive and non-adaptive measurement schemes. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation.",United States,Carnegie Mellon University,1
492,Classification with Asymmetric Label Noise,"We consider the problem of binary classification when the training labels are noisy. Previous theoretical work on this problem assumes that the two classes are separable, or that the label noise is independent of the class label. We present a general framework that allows for overlapping class-conditional distributions (so that the true labels are not deterministic) and that accommodates asymmetric label noise. This problem is motivated by several applications including nuclear particle classification, where background radiation and other environmental factors make it impossible to obtain pure training samples. We first identify a necessary and sufficient condition that makes the unknown noise proportions identifiable. Under this assumption, we describe estimation of the Type I and Type II errors, and use these estimates to design a classification rule that is consistent with respect to the minmax criterion. Our approach depends critically on recent results for mixture proportion estimation, which is the problem of estimating the percentage of one distribution that is present in another.",United States,University of Michigan - Ann Arbor,1
527,A Gaussian Latent Variable Model for Ranking,"We describe a Gaussian latent variable model for ranking. The model learns a linear scoring function to maximize the probability that randomly chosen pairs of examples are correctly ranked. We show how to perform inference in this model and derive the Expectation-Maximization (EM) algorithm that monotonically increases the likelihood of correct ranking. We also highlight the intuitive form of the EM algorithm: at each iteration, the weight vector is re-estimated by a simple least-squares update. Finally, we explore two extensions of the model, based on kernels and boosting, to learn nonlinear ranking functions. The model?s effectiveness is demonstrated on problems in AUC maximization and information retrieval.",United States,"University of California, San Diego",1
535,Recognizing Activities by Attribute Dynamics ,"The problem of modeling the dynamic structure of the attributes of human activities is considered. Video is first represented in a semantic feature space, where each feature encodes the probability of occurrence of an action attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining a binary observation variable with a hidden Gauss-Markov state process. In this way, it combines the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by the popular dynamic texture method for LDS learning, is proposed. A similarity measure between BDSs, which generalizes the Binet-Cauchy LDS kernel, is then introduced and used to design activity classifiers. These are shown to outperform similar classifiers derived from the kernel-LDS and state-of-the-art approaches to dynamics-based or attribute-based action recognition.",United States,"University of California, San Diego",1
537,Graph Estimation with Joint Additive Models,"In recent years, there has been considerable interest in estimating conditional independence graphs in the high-dimensional setting in which the number of features exceeds the number of observations.  Most prior work in this area has assumed that the observations are drawn from a multivariate Gaussian distribution, or that conditional dependence relations among variables are linear, which as we will see are roughly equivalent. Unfortunately, if this assumption is violated, then the resulting conditional independence estimates can be inaccurate. Here we present a semi-parametric method, Sparse Conditional Estimation with Joint Additive Models (SpaCE JAM),  which allows for arbitrary additive conditional relationships among the features.  We present an efficient algorithm for its computation, and prove that our estimator is consistent.  We also  extend our method to estimation of  directed graphs.  SpaCE JAM enjoys superior performance to existing methods when there are non-linear relationships among the features, and is comparable to methods that assume multivariate normality when the features are linearly related.",United States,University of Washington,1
547,A Mixed-Membership Model for Learning Genomic Subtype Signatures,We address the problem of identification of genomic signatures from mixed tumor samples. Most methods for identifying tumors on the basis of their genomic mutations are all-or-none classifiers whereas real tumors are complex mixtures. We present a method for identifying sparse subtype signatures from mixed samples using a hierarchical Bayesian model. We apply this method to identify signatures for subtypes of glioblastoma from RNA expression data obtained as part of the Cancer Genome Atlas (TCGA) project and find one subtype is associated with genes involved in DCX-mediated invasion and another subtype is associated with high POSTN expression and mesenchymal-like tumors.,United States,Stanford University,1
831,High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning,"Joint sparsity regularization in multi-task learning has attracted much attentionin recent years. The traditional convex formulation employs the group Lasso relaxationto achieve joint sparsity across tasks. Although this approach leads to asimple convex formulation, we argue in this paper that the quadratic regularizerinduced by the group Lasso formulation is suboptimal. To remedy this problem,we view jointly sparse multi-task learning as a specialized random effects model,and derive a convex relaxation approach that involves two steps. The first steplearns the covariance matrix of the coefficients using a convex formulation whichwe refer to as sparse covariance coding; the second step solves a ridge regressionproblem with a sparse quadratic regularizer based on the covariance matrix obtainedin the first step. It is shown that this approach produces an asymptoticallyoptimal quadratic regularizer in the multitask learning setting if the number oftasks approaches infinity. Experimental results demonstrate that the convex formulationobtained via the proposed model significantly outperforms group Lasso.",United States,Georgia Institute of Technology,1
845,Shape Priors for Weakly Labeled Segmentation,"In this paper we tackle the problem of weakly labeled image segmentation.  Towards this goal, we propose a novel generative model of segmentation based on transformed hierarchical Pitman-Yor processes, where we augment each object class with a shape prior.  Our model exploits weakly label data, as it does not require a training set composed of  pixel-wise annotations. Instead, it learns appearance models for each object using  as labels only bounding boxes around the object of interest as well as a  shape prior. We demonstrate the effectiveness of our approach on the PASCAL 2010 dataset and show that we outperformed a set of baselines, improving $8\%$ absolute error over the unsupervised version of our model as well as $9\%$ over the detector, which is theinput to our approach. Importantly, our approach performs similarly to fully-supervised approaches.  ",United States,"University of California, Berkeley",1
867,Sampling with Deterministic Constraints,"Deterministic and near-deterministic relationships among subsets of variables in multivariate systems are known to causeserious problems for Monte Carlo algorithms. We examine the family of problems in whichthe relationship $Z = f(X_1,\ldots,X_k)$ holds and we wish to obtain exact samples from the conditional distribution $P(X_1,\ldots,X_k\mid Z= z)$. We begin with the case where $f$ is addition,showing that the problem is NP-hard even when the $X_i$s are independent and each has only two possible values.In more restricted cases---for example, i.i.d. Boolean or uniform continuous $X_i$s---efficient exact samplers have been obtained previously.For the case where each $X_i$ has a bounded range of integer values, we derive an $O(k)$ dynamic programming algorithm called {\em exact constrained sequential sampling} (ECSS).For the more general, continuous case, we propose a {\em dynamic scaling} algorithm (DYSC),a form of importance sampling. We evaluate these algorithms on several examplesand derive generalized forms that operate with any function $f$ that satisfies certain natural conditions.",United States,"University of California, Berkeley",1
877,Modeling Human-Object Interactions for Action Recognition in Real-World Videos,"This paper deals with the interesting problem of recognizing human actions in real-world videos. Such videos usually present large variation in background and camera motion, which makes the performance of low-level appearance and motion features unsatisfactory, particularly in the case of video classes sharing similar objects and background (e.g. ``snatch'' and ``clean-jerk'' weightlifting actions). In this paper, we tackle the problem through representation of action classes as human and object interactions (HOI). HOI is modeled as the spatio-temporal relationship between human and object tracks along with their appearance descriptions in a video. However, such a representation requires accurate detection of human and object tracks. This is a difficult task in its own right when dealt separately. We address the issue by extracting candidate tracks from a video and modeling the choice of correct tracks as latent variables in a latent SVM framework. This formulation enables the task of HOI modeling and action recognition without accurate initialization of human and object tracks. We demonstrate promising action classification results on the challenging Olympic Sports [1] and TRECVID11-MED [2] datasets, where our method outperforms state-of-the-art approaches.",United States,Stanford University,1
911,Regularization of Latent Variable Models to Obtain Sparsity,"We present a pseudo-observed variable based regularization technique for latent variable mixed-membership models that provides a mechanism to impose preferences on the characteristics of aggregate functions of latent and observed variables.  The regularization framework is used to regularize topic models, which are latent variable mixed membership models for language modeling.  In many domains, documents and words often exhibit only a slight degree of mixed-membership behavior that is inadequately modeled by topic models which are overly liberal in permitting mixed-membership behavior.  The regularization introduced in the paper is used to control the degree of polysemy of words permitted by topic models and to prefer sparsity in topic distributions of documents.  The utility of the regularization is evaluated internally using document perplexity and externally by using the models to predict star counts in movie and product reviews based on the content of the reviews.  Results of our experiments show that using the regularization to finely control the behavior of topic models leads to better perplexity and lower mean squared error rates in the star-prediction task.",United States,Carnegie Mellon University,1
917,Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ) for Multi-voxel Pattern Analysis in fMRI,"We present an efficient algorithm for simultaneously training elastic-net-regularized generalized linear models across many related problems, which may arise from bootstrapping, cross-validation and nonparametric permutation testing.  Our approach leverages the redundancies across problems to obtain approximately 10x computational improvements relative to solving the problems sequentially by the glmnet algorithm. We demonstrate our fast simultaneous training of generalized linear models (FaSTGLZ) algorithm,  for multivariate analysis of fMRI and run otherwise computationally intensive bootstrapping and permutation test analyses that are typically necessary for obtaining statistically rigorous classification results and  meaningful interpretation.  We also use our algorithmic framework to propose a new algorithm for stabilizing feature selection in sparse regression models.",United States,Columbia University,1
918,Estimating Unknown Sparsity in Compressed Sensing,"Within the framework of compressed sensing, many theoretical guarantees for signal reconstruction require that the number of linear of measurements $n$ exceed the sparsity $\|x\|_0$ of the unknown signal $x\in\R^p$. However, when the sparsity parameter $\|x\|_0$ is unknown, the choice of $n$ remains problematic. In this paper, we consider the problem of directly estimating $\|x\|_0$ from a small number of linear measurements---without making any prior assumptions about the sparsity of $x$. Although we show that estimation of $\|x\|_0$ is generally intractable in this framework, we consider an alternative measure of sparsity $s(x):=\|x\|_1^2\big/\|x\|_2^2$, which is a sharp lower bound on $\|x\|_0$, and is more amenable to estimation. When $x$ is a non-negative signal, we propose a computationally inexpensive estimator $\hat{s}(x)$ for $s(x)$, and derive concentration bounds that imply $\hat{s}(x)/s(x)\to 1$ almost surely as $(n,p)\to\infty$. Remarkably, the quality of estimation is \emph{dimension-free}, which ensures that $\hat{s}(x)$ is well-suited to the high-dimensional regime where $n\ll p$. These results also extend naturally to the problems of using linear measurements to estimate the rank of a positive semidefinite matrix, or the sparsity of a non-negative matrix. Finally, we show that if no structural assumption (such as non-negativity) is made on the signal $x$, then the quantity $s(x)$ cannot generally be estimated when $n\ll p$.",United States,"University of California, Berkeley",1
925,A Spectral Learning Approach to Range-Only SLAM,"We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our  approach does not need to linearize a transition or measurement model; such linearizations can cause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly for the highly non-Gaussian posteriors encountered in range-only SLAM. We provide a theoretical analysis of our method, including finite-sample error bounds.  Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost.",United States,Carnegie Mellon University,1
953,An Alternative Kernel Method for the Two-Sample Problem,"We present an alternative kernel method for the two-sample problem that is based on Friedman's approach of using any binary classification learning machine to score the data.  When the learning machine is chosen to be a support vector machine, we show that this approach is a generalization of the permutation $t$-test.  Previous work has yielded a normal rate of convergence bound using Stein's Method in the simple setting of univariate data and a linear kernel with simulations, suggesting that this proof technique may be extended to address a more general setting.  Despite a lack of tuning of the SVM parameters, this method is shown to be competitive with the Maximum Mean Discrepancy (MMD) test.",United States,Stanford University,1
980,Neural Network Models for Multilabel Learning,"Multilabel learning is an extension of standard binary classification where the goal is to predict a set of labels (which we call tags) for a given input example. There have been many models proposed for this problem, such as subspace learning, kernel methods, and nearest neighbour schemes. Recently, the probabilistic classifier chain method was proposed, which learns a series of probabilistic models that attempt to capture tag correlations. In this paper, we show how this model may be interpreted as a neural network with connections amongst output nodes. We argue that using an explicit hidden layer instead brings several advantages, such as tractable test-time inference, and removing the need for fixing a tag ordering. Further, the hidden units capture nonlinear latent structure that both improves classification performance and allows for interpretability. Compared to previous neural network models for multilabel learning, we discuss several design decisions that have a significant impact on training the network. Empirical results show that the model outperforms several existing methods.",United States,"University of California, San Diego",1
992,Learning features for image classification from text,"The principle of cross-modal regularization, where data from an auxiliary modality is used to regularize classifiers of a principal modality, is exploited to improve image classification. Images and text are first represented by semantic descriptors, composed of their classification scores under classical image and text classifiers.A measure of cross-modal similarity, which defines the similarity of any image to the training texts, is then learned, and used to implement a soft label transfer mechanism, that transfers labels from training texts to the image. This mechanism is finally used to learn a set of cross-modal image classifiers, i.e. classifiers that classify images according to 1) their similarity to training text, and 2) the labels of the latter. The scores of these cross-modal image classifiers are used to augment the semantic image descriptors, acting as {\it cross-modalregularizing features\/}.  This regularization is shown to significantly improve the state-of-the-art semantics based image classification, on three challenging datasets.",United States,"University of California, San Diego",1
1003,Sample selection bias in unsupervised clustering,Sample selection bias has been studied in supervised settings when the training and test data are drawn from different distributions. We consider the effect of sample selection bias in an unsupervised setting when mixed-membership models are used for population stratification and topic modeling.We examined the effect of biased sampling on the accuracy of unsupervised clustering in terms of learning the low-dimensional representations of objects (documents or individual genotypes). We found that the accuracy of unsupervised clustering using a mixed-membership model is affected to a large extent by the sample used for analysis and how representative it is of the underlying populations. We also propose a correction for sample selection bias that is effective in real applications.,United States,Carnegie Mellon University,1
1005,Learning and Decision Making,"This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization.",United States,Massachusetts Institute of Technology,1
1013,Spike triggered covariance for strongly correlated Gaussian stimuli,"Characterizing feature selectivity is an important problem because it can shed light on how neurons process their inputs. The spike triggered covariance method (STCM) is a very commonly used method to extract the relevant set of stimulus features to which a neuron responds. One of the main advantages of STCM is that it can determine the dimensionality of the cell's relevant subspace. The method has been previously thought to be applicable when stimuli are drawn from a Gaussian ensemble, with or without stimulus correlations.  Here we use random matrix theory to show that when STCM is used with strongly correlated Gaussian stimuli, the null distribution of eigenvalues has a large outstanding mode. As a result, STCM can either yield an extra feature, which often corresponds to the strongest eigenvalue, or fail to yield any significant dimensions. We present a simple correction scheme that removes this artifact and illustrate its effectiveness by analyzing model neurons and recordings from retinal ganglion cells probed with correlated Gaussian stimuli whose second-order statistics was matched to natural stimuli. Our results can serve as guidelines for design of reverse correlation experiments that can help illuminate how neurons are optimized to code natural stimuli.",United States,"University of California, San Diego",1
1027,No voodoo here! Learning discrete graphical models via inverse covariance estimation,"We investigate the relationship between the support of the inverses of generalized covariance matrices and the structure of a discrete graphical model. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph. Our work extends results which were previously established only for multivariate Gaussian distributions, and partially answers an open question about the meaning of the inverse covariance matrix of a non-Gaussian distribution. We propose graph selection methods for a general discrete graphical model with bounded degree based on possibly corrupted observations, and verify our theoretical results via simulations. Along the way, we also establish new results for support recovery in the setting of sparse high-dimensional linear regression based on corrupted and missing observations.",United States,"University of California, Berkeley",1
1038,Rational impatience in perceptual decision-making: a Bayesian account of discrepancy between two-alternative forced choice and Go/NoGo Behavior,"Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes.",United States,"University of California, San Diego",1
1047,3D Scene Grammar for Parsing RGB-D Pointclouds,"We pose 3D scene-understanding as a problem of parsing in a grammar.  A grammar helps us capture the compositional structure of real-word objects, e.g., a chair is composed of a seat, a back-rest and some legs. Having multiple rules for an object helps us capture structural variations in objects, e.g., a  chair can optionally also have arm-rests. Finally, having rules to capture composition at different levels helps us formulate the  entire scene-processing pipeline as a single problem of finding most likely parse-tree---small segments combine to form parts of objects, parts to objects and objects to a scene. We attach a generative probability model to our grammar by having a feature-dependent probability function for every rule. Our model can be trained very efficiently (within seconds), and it scales only linearly in with the number of rules in the grammar.   We show that we obtain good parse trees on 84 real point-clouds obtained from RGB-D cameras.",United States,Cornell University,1
1048,On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks,"In this paper, we argue for representing networks as a bag of {\it triangular motifs},particularly for important network problems that current model-based approaches handle poorlydue to computational bottlenecks incurred by using edge representations.Such approaches require both 1-edges and 0-edges(missing edges) to be provided as input, and as a consequence, approximate inference algorithms for thesemodels usually require $\Omega(N^2)$ time per iteration, precluding their application to larger real-world networks.In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality.A triangular motif is a vertex triple containing 2 or 3 edges, andthe number of such motifs is $\Theta(\sum_{i}D_{i}^{2})$ (where $D_i$ is the degree of vertex $i$),which is much smaller than $N^2$ for low-maximum-degree networks. Using this representation, we develop a novelmixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networkswith high maximum degree, the triangular motifs can be naturally subsampled in a {\it node-centric}fashion, allowing for much faster inference at a small cost in accuracy.Empirically, we demonstrate that our approach, when compared to that of an edge-based model,has faster runtime and improved accuracy for mixed-membership community detection.We conclude with a large-scale demonstration on an $N\approx 280,000$-node network, which isinfeasible for network models with $\Omega(N^2)$ inference cost.",United States,Carnegie Mellon University,1
1054,Learning a network from multiple data sources,"Recent methods on estimating the structure of undirected Gaussian graphical models have focused on estimation from a single data source. However, in many real world applications, multiple data sources are available that give information about the same set of nodes. We propose NP-MuScL (non-paranormal multi source learning) to estimate the structure of a sparse undirected graphical model that is consistent with multiple sources of data, having the same underlying relationships between the nodes. We use the semiparametric Gaussian copula to model the distribution of the different data sources, and show how to estimate such a model in the high dimensional scenario. Results are reported on synthetic data, where NP-MuScL outperforms baseline algorithms significantly, even in the presence of noisy data sources. Experiments are also run on two different data sets of yeast microarray, where NP-MuScL predicts a higher number of known gene interactions than existing techniques. ",United States,Carnegie Mellon University,1
1061,Shadow Densities for Speeding Up Kernel Methods ,"This paper presents an approach to improve the training and evaluation of kernel manifold learning algorithms relying on spectral decomposition. The approach, called the shadow method, exploits research regarding the spectral decomposi-tion of kernel operators applied to probability distributions. It is used to define the shadow of a kernel density estimate. The shadow density estimate (ShDE)in turn defines shadow KPCA (ShKPCA). For large, redundant datasets ShKPCA improves training and evaluation time of KPCA by an order of magnitude or more, each. A single parameter $\ell$ controls the computational gains. The shadow method is justified through bounds on the density estimate error, on the spectral decomposition error, and on the spectral operator error, all in terms of $\ell$. For low $\ell$ there are large improvements but the method is lossy. Increasing approaches baseline performance and leads to lower speed improvements. Experimentally $\ell$ =  4 works well across a broad spectrum of datasets. Modifications to  improve $\ell$ low performance are given, but with reduced computational gains during training.",United States,Georgia Institute of Technology,1
1070,Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL,"Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.  We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.",United States,Georgia Institute of Technology,1
1071,Active Learning on Low-Rank Matrices,"Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. With various criteria, we can actively choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many large points as possible. We evaluate our methods on simulated data and show their applicability on movie ratings prediction as well as discovering drug-target interactions.",United States,Carnegie Mellon University,1
567,Tracking 3-D Rotations with the Quaternion Bingham Filter,"A deterministic method for sequential estimation of 3-D rotationsis presented.  The Bingham distribution is used to representuncertainty directly on the unit quaternion hypersphere.  Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions.  Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic.  We present two versions of the  QBF--suitable for tracking the state of first- and second-order rotating dynamical systems.",United States,Massachusetts Institute of Technology,1
581,Unsupervised Structure Discovery for Semantic Analysis of Audio,"Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines.",United States,Carnegie Mellon University,1
582,Identification of Spike-Processing Neural Circuits,"Reverse engineering of neural circuits requires the development of sound experimental and theoretical methods for determining the circuit connectivity and for estimating the processing of both spiking and continuous sensory signals. Here we present a new approach for identification of receptive fields in spiking neuron models that admit both continuous  signals and multidimensional spike trains as input stimuli. We consider circuit models of the sensory periphery in olfaction, audition and  vision as well as models of spike processing in higher brain centers, including models with lateral connectivity and feedback. We present algorithms for identifying temporal, spectrotemporal and spatiotemporal receptive fields directly from spike times produced by a neuron. The algorithms obviate the need to repeat experiments in order to compute the neuron's rate of response, rendering our methodology of interest to both experimental and theoretical neuroscientists.",United States,Columbia University,1
589,Growing a List,"We would like to intelligently grow a long list, starting from a small seed of examples. Our algorithm for solving this problem takes advantage of the wisdom of the crowd, in the sense that there are many experts who post lists of things on the Internet. We want to find these experts and aggregate their lists in an intelligent way in order to produce a single concise, complete, and meaningful list. Our solution to the list aggregation problem has several simple components: i) a combinatorial search over pairs of seed items, leveraging the speed of search engines, ii) a fast clustering algorithm (Bayesian Sets), and iii) an implicit feedback loop where the most relevant terms are added to the seed. The algorithm is extremely fast, and we show experimental results on two problems: creating a list of planned events in and around Boston, and creating a list of Jewish foods. We find that Bayesian Sets clusters well even when its underlying independence assumption is violated, and we provide a theoretical generalization bound to explain its ability to cluster well in general.",United States,Massachusetts Institute of Technology,1
590,Building an Attribute based Semantic Hierarchy,"We propose a new framework to build attribute based hierarchies from visual datasets. Our desiderata is to construct a tree structure in which attributes which are used more frequently are associated with nodes which are closer to the root, whereas attributes which are used less frequently are associated with lower levels in the tree. Most of the existing works that are concerned with learning visual and semantic taxonomies are based on hierarchical topic models which entail the bag of features representation. Such approaches are therefore not suitable for dealing with an attribute based representation. An attribute based representation can facilitate information transfer from previously observed instances into new images, and therefore an attribute based hierarchy can capture richer semantics while limiting the use of costly annotation data. We develop a new generative model for hierarchical clustering of binary vectors, which we refer to as the attribute tree process (ATP), and which is based on a tree-structured stick breaking process. The ATP allows us to estimate the entire structure of the hierarchy and the model parameters in an unsupervised fashion. We evaluate the proposed framework using several widely available datasets, and demonstrate that the ATP is capable of constructing semantically meaningful hierarchical representations of the data. ",United States,University of Michigan - Ann Arbor,1
591,Refining Models for Percutaneous Coronary Intervention through Transfer Learning,"Identifying patients at risk of complications during percutaneous coronary intervention (PCI), such as arrhythmias and bleeding, is essential in guiding patient care at the bedside and in streamlining healthcare delivery across hospitals with varying clinical resources (e.g., with or without on-site cardiac surgery). The traditional approach to develop models for PCI care is largely centralized and uses patient data aggregated across a growing number of hospitals. While this approach is effective in increasing the amount of training data available for model training, and in improving the generality of the models learned, it suffers from the pooled data abstracting the PCI population in a way that fails to reflect variations across individual hospitals in terms of both patients and caregivers. We address this shortcoming by exploring the hypothesis that models for PCI care can be improved through the use of transfer learning. In particular, we study how transfer learning can be applied to adapt models derived from multi-hospital PCI data for use at individual hospitals, in an effort to simultaneously leverage the benefits of both aggregating clinical datasets and fitting to the specific characteristics of the patient/caregiver mix at individual hospitals. When studied on a registry of patients undergoing PCI, this approach of model adaptation through transfer learning improved reclassification at the individual healthcare provider level for many complications associated with PCI.",United States,University of Michigan - Ann Arbor,1
605,Posterior contraction of the population polytope in finite admixture models,"We study the posterior contraction behavior of the latent population structure that arises in admixture models as the amount of data increases. An admixture model  --- alternatively known as a topic model --- specifies $k$ populations (or topics), each of which is characterized by vector of frequencies for generating a set of discrete values of observations. The population polytope is defined as the convex hull of the $k$ frequency vectors. Given a prior distribution over the space of population polytopes, we establish rates at which the posterior distribution contracts to $G_0$, under the Hausdorff metric and a minimum matching Euclidean metric, as the amount of data tends to infinity. Rates are obtained for the overfitted setting, i.e., when the number of extreme points of $G_0$ is bounded above by $k$, and for the setting in which the number of extreme points of $G_0$ is known. Minimax lower bounds are also established. Our analysis combines posterior asymptotics techniques for the estimation of mixing measures in hierarchical models with arguments in convex geometry.",United States,University of Michigan - Ann Arbor,1
617,How Prior Probability Influences Decision Making: A Unifying Probabilistic Model,"How does the brain combine prior knowledge with sensory evidence whenmaking decisions under uncertainty? Two competing descriptive modelshave been proposed based on experimental data.  The first posits anadditive offset to a decision variable, implying a static effect ofthe prior. However, this model is inconsistent with recent data from amotion discrimination task involving temporal integration of uncertainsensory evidence. To explain this data, a second model has beenproposed which assumes a time-varying influence of the prior. Here wepresent a normative model of decision making that incorporates priorknowledge in a principled way.  We show that the additive offset modeland the time-varying prior model emerge naturally when decision makingis viewed within the framework of partially observable Markov decisionprocesses (POMDPs).  Decision making in the model reduces to (1)computing beliefs given observations and prior information in a Bayesianmanner, and (2) selecting actions based on these beliefs to maximize the expected sum of future rewards. We show that the model can explain both data previously explained using the additive offset model as well as more recent data on the time-varying influence of prior knowledge on decision making.",United States,University of Washington,1
631,Dimension Independent Similarity Computation,"We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO)to compute all pairwise similarities between very high dimensional sparse vectors.All of our results are provably independent of dimension, meaningapart from the initial cost of trivially reading in the data, all subsequentoperations are independent of the dimension, thus the dimension can be very large.We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash.Our results are geared toward the MapReduce framework. We empirically validate ourtheorems at large scale using data from the social networking site Twitter.",United States,Stanford University,1
635,Infinite Structured Hidden Markov Model,"We present the infinite structured hidden Markov model (ISHMM). An ISHMM is an HMM that possesses an unbounded number of states, parameterizes state dwell-time distributions explicitly, and can constrain what kinds of state transitions are possible. We present two parameterizations of the ISHMM. The first is a novel construction for an infinite explicit duration HMM. The second is an entirely novel infinite left-to-right HMM. We provide inference algorithms for the ISHMM and show results from using the ISHMM to analyze both real and synthetic data.",United States,Massachusetts Institute of Technology,1
646,Understanding Indoor Scenes with Latent Interaction Template Models,"Visual scene understanding is a difficult problem, interleaving object detection, geometric reasoning and scene classification. In this paper, we present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the latent Interaction Template Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings, while also improving individual object detections.",United States,University of Michigan - Ann Arbor,1
649,Eliciting Predictions from a Connected Crowd of Traders,"We study an online trading community where traders can communicate with each other as well as perform trades. We discuss characteristics of social influence on trading decisions within this connected crowd. We discover traders are still heavily affected by social influence even when every trade is with their own money, and social influence often negatively affects their returns. Based on our observations, we implement three trading strategies to elicit the crowd?s prediction. In particular, we design a novel way of inferring predictions by modeling the crowd reasoning process under social influence. We find that even complex social dynamics can dramatically effect trades, it is still possible to infer knowledge from the crowd. Our novel algorithm achieves the best performance by modeling decision making processes under influence rather than the decisions from the crowd.",United States,Massachusetts Institute of Technology,1
655,Link Prediction in Biological Networks using Penalized Multi-Attribute ERGMs,"Reconstruction of genetic networks is an important, yet challenging problem in systems biology. Gene networks often include different interaction mechanisms, such as transcriptional regulatory and protein-protein interactions. Further, different data sources provide valuable information about the relationships among genes, which motivate methods that allow for data integration.We propose a novel multi-attribute exponential random graph model for supervised prediction of gene networks, coupled with a penalized estimation framework for improved prediction performance. The proposed framework facilitates the analysis of gene networks with multiple edge types, and provides a systematic method for incorporating multiple sources of biological data, as well as diverse attributes regarding the function and location of genes, and structure of observed networks. Results of numerical experiments indicate that the method enjoys superior performance compared to state-of-the-art reconstruction methods.",United States,University of Washington,1
664,Visually-grounded Bayesian Word Learning,"Learning the meaning of a novel noun from a few labelled objects is one of the simplest aspects of learning a language, but approximating human performance on this task is still a significant challenge for current machine learning systems. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for given visual stimulus. Recent work in cognitive science on Bayesian models of word learning partially addresses this challenge, but assumes that objects are perfectly recognized and has only been evaluated in small domains. We present a system for learning words directly from images, using probabilistic predictions generated by visual classifiers as the input to Bayesian word learning, and compare this system to human performance in a large-scale automated experiment. The system captures a significant proportion of the variance in human responses. Combining the uncertain outputs of the visual classifiers with the ability to identify an appropriate level of abstraction that comes from Bayesian word learning allows the system to outperform alternatives that assume perfect recognition or use a more conventional computer vision approach.",United States,"University of California, Berkeley",1
668,A Sparse and Adaptive Prior for Time-Dependent Model Parameters,"We consider the scenario where the parameters of a probabilistic model are expected to vary over time. We construct a novel prior distribution that promotessparsity and adapts the strength of correlation between parameters at successivetimesteps, based on the data. We derive approximate variational inference procedures for learning and prediction with this prior. We test the approach on twotasks: forecasting ?nancial quanitities from relevant text, and modeling languagecontingent on time-varying ?nancial measurements.",United States,Carnegie Mellon University,1
671,Leveraging for Fitting Linear Models in Large-scale Data,"Recent empirical and theoretical work has focused on using the empirical statistical leverage scores of data matrices in order to develop improved algorithms for common matrix problems such as least-squares approximation and low-rank matrix approximation.  Existing work focuses on algorithmic issues such as worst-case running times or on the usefulness of this approach in downstream data applications.  Here, we examine the statistical properties of this leveraging paradigm in the context of fitting a linear model to data.  We derive the mean squared errors for two related leveraging-based estimates and for uniform sampling estimates.  Depending on the the mean, variance and skewness of the leverage scores, one procedure or another is preferred.  We also describe the empirical behavior of these procedures on several synthetic and real data sets.",United States,Stanford University,1
678,Dual Semi-Supervised Co-Clustering Informed by Geometry,"Co-clustering algorithms, which group a data matrix based on the similarities of both rows (samples) and columns (features), often yield impressive performanceimprovement over traditional one-side clustering approaches. Efficient utilizing partial supervision in the form of row labels as well as column labels is still a challenge, especially when the number of labels is small. Moreover, since many real world data are sampled from a low dimensional manifold, effective co-clusteringalgorithms will depend upon the intrinsic structures of rows as well as columns of the data matrix. In this paper we propose dual semi-supervised co-clusteringinformed by geometry (DSCIG) to address these two issues. First, we provide a general framework for co-clustering that incorporates partial supervision informationand preserve local geometry. Second, we augment this framework with an additional step for similarity propagation that generate richer supervision information.To manage the different applications of DSCIG, we derive an alternative optimization procedure and show the convergence is guaranteed theoretically .",United States,"University of California, San Diego",1
682,Privacy Aware Learning,"We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.  In this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator.",United States,"University of California, Berkeley",1
687,Smooth and Monotone Covariance Regularization,"The dangers of using the sample covariance matrix obtained from scarce data in high-dimensional settings are well recognized. In particular, the inconsistency of its eigenvalue spectrum has grave implications for modeling risk within the Markowitz portfolios framework. A variety of approaches to improve the covariance estimates exploit knowledge of structure in the data, including low-rank models (principal component and factor analysis), banded models, sparse inverse covariances, and parametric models. We investigate a different nonparametric prior for random vectors indexed along a low-dimensional manifold: we assume that the covariance matrix is monotone and smooth with respect to this indexing. This fits a variety of problems including interest-rate risk modeling in econometrics, and sensor array noise modeling. We formulate the estimation problem in a convex-optimization framework as a semidefinite-programming problem, and develop efficient first-order methods to solve it. We apply our framework on a number of examples with limited, missing and asynchronous data, and show that it has the potential to provide more accurate covariance matrix estimates than existing methods, and exhibits a desirable eigenvalue-spectrum correction effect.",United States,Massachusetts Institute of Technology,1
689,Discovering Voxel-Level Functional Connectivity Between Cortical Regions,"Functional connectivity patterns are known to exist in the human brain at the millimeter scale, but the standard fMRI connectivity measure only computes functional correlations at a coarse level. We present the first method which identifies fine-grained functional connectivity between any two brain regions by simultaneously learning voxel-level connectivity maps over both regions. We show how to formulate this problem as a constrained least-squared optimization, with a spatial regularization term that allows connectivity maps to be learned much more efficiently. This optimization problem can be solved using a trust region approach, and can automatically discover connectivity between multiple distinct voxel clusters in the two regions. We validate our method in two experiments, demonstrating that we can successfully learn subregion connectivity structures from a small amount of training data. Our approach is shown to be substantially better at estimating fine-grained connectivity differences than state-of-the-art subregion connectivity methods, all of which learn maps over only one region at a time.",United States,Stanford University,1
692,Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods,"We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{\dim}$ in convergence rate over traditional stochastic gradient methods, where $\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors.",United States,"University of California, Berkeley",1
705,Robust Structural Metric Learning,"Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degradesaccordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation,while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methodsin both high- and low-noise settings.",United States,"University of California, San Diego",1
711,Multiclass Clustering using a Semidefinite Relaxation,"Spectral and other cut-based relaxations have been applied to graph clustering problems. In this paper, we propose a novel semidefinite relaxation for graph clustering known as Max-cut clustering. The clustering problem is formulated in terms of a discrete optimization problem and then relaxed to a SDP. To make the optimization scalable, we represent the SDP by a low-rank factorized approximation that reduces the number of variables, and then use a simple projected gradient method to solve it. To obtain the clustering, we propose a reweighted rounding scheme to get integral solutions. We also extend this formulation to a global approach to multi-class clustering and MAP inference in graphical models. Experimental results indicate that we outperform state-of-art several clustering methods. The algorithm is extended to perform MAP inference in graphical models and outperforms competing methods.",United States,Stanford University,1
713,Higher-order Nonparametric Models for Recognition by Analogy,"Nonparametric classification methods such as nearest neighbor offer the ability tolearn by association, leveraging large amounts of data, avoiding a training phase,and placing no assumptions on the structure of label space. However, such meth-ods often perform poorly due to the limited ability of typical distance functions tocapture complex relationships in the data. We propose a method for learning dis-tance functions using higher-order nonparametric models, resulting in better per-formance while still learning by association with no training phase. Our methodreplaces single example association with pair association, and can be interpretedas finding analogies among training and test examples. We test our method onRGB-D [1], a multi-view object data set, which lets us learn implicitly when dif-ferent 2D shapes describe a similar 3D structure. We show that our method isparticularly beneficial in the one-shot transfer regime, where only one exampleis available for a test category. Where traditional supervised learning methodsperform poorly, our method can use the relationships between objects in differentcategories to learn the structure of categories with impoverished training data.",United States,"University of California, Berkeley",1
716,Communication-Efficient Algorithms for Statistical Optimization,"We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$. Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset.",United States,"University of California, Berkeley",1
729,Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions,"We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a $\order(\pdim/T)$ convergencerate for strongly convex objectives in $\pdim$ dimensions and $\order(\sqrt{\spindex( \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our algorithm is based on successively solving a series of $\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error ofour solution after $T$ iterations is at most$\order(\spindex(\log\pdim)/T)$, with natural extensions toapproximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem.",United States,"University of California, Berkeley",1
739,Active Sensing as Bayes-Optimal Sequential Decision-Making,"Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience.  An important but poorly understood aspect of sensory processing is the role of active sensing: the use of self-motion to selectively process the most rewarding or informative aspects of the environment. Here, we present a Bayes-optimal inference and decision-making framework for active sensing, which directly minimizes a cost function that takes into account behavioral costs such as response delay, error, and effort. Unlike previously proposed algorithms that optimize heuristic objectives such as expected entropy reduction [Butko and Movellan, 2010] or one-step look-ahead accuracy [Najemnik and Geisler, 2005], this optimal policy can account for search duration as well as location, and is sensitive to contextual factors such as the relative importance of time, error, and effort.  We implement the optimal policy, along with the two heuristic policies, for an example visual search task, and illustrate how the heuristic policies deviate from optimal performance in various contexts.  We show that the discrepancy is especially large when the cost of time and the cost of switching between sensing locations are high. We demonstrate a potential route for overcoming the computational complexity of the optimal algorithm, especially problematic in large real-world applications, by exploiting the concavity and smoothness of the value function.  We show that a basis function approximation to the value function, several orders of magnitude reduced in dimensionality and complexity, achieves near-optimal performance.",United States,"University of California, San Diego",1
740,Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation,"Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented.",United States,Stanford University,1
760,Analysis of Differential Privacy Based on Importance Weighting,"This paper introduces and analyzes a novel data-publishing mechanism based on computing weights that make an existing dataset, for which there are no confidentiality issues, analogous to the dataset that must be kept private. The existing dataset may be genuine but public already, or it can be synthetic. The only necessary requirement is that it have similar schema as the private dataset. The weights are importance sampling weights, but they are regularized and have noise added. The weights allow statistical queries to be answered approximately while provably guaranteeing differential privacy. We derive expressions for the variance of the approximate answers. Experiments show that the new mechanism performs well even when the public dataset is quite different from the private dataset,and the privacy budget is small.",United States,"University of California, San Diego",1
780,Sparse Optimal Control Signals for Natural Human Movements Using the Infinity Norm,"Optimal control models have been a successful tool in describing many aspects and characteristics of human movements. While such models have a sound theoretical foundation, their interpretation and neuronal implementation in the Central Nervous System (CNS) is not clear. We propose that the CNS not only utilizes control policies that are optimal with respect to a criterion, but also satisfy sparsity constraints. In recent years sparsity has played a pivotal role in theoretical neuroscience for information processing (such as vision). Typically, sparsity is imposed by introducing a cardinality constraint or penalty measured or approximate by the one-norm. In this work, to obtain sparse control signals, however, the $L_{\infty}$ norm is used as a penalty on the control signal. Even though such sparse control signals are discontinuous, the movements that result are continuous and smooth.  In addition, such sparse control signals are more biologically realistic and have a clear neuronal interpretation with a sequence of neuronal spikes. We show that moreover sparse optimal control signals quantitatively describe real human arm movements with high accuracy. ",United States,"University of California, San Diego",1
807,Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model,"We consider linear regression in the high-dimensional regime in which the number of observations $n$ is smaller than the number of parameters $p$. A very successful approach in this setting uses $\ell_1$-penalized least squares (a.k.a. the Lasso) to search for a subset of $s_0< n$  parameters that best explain the data, while setting the other parameters to zero. A considerable amount of work has been devoted to characterizing the estimation and model selection problems within this approach. In this paper we consider instead the fundamental --but  far less understood-- question of statistical significance. Roughly speaking, when the Lasso estimates a specific parameter to be zero (or non-zero), \emph{how certain is this conclusion}? We study this problem under the random design model in which the rows of the design matrix are i.i.d. and drawn from an unknown high-dimensional Gaussian distribution. This situation arises --for instance-- in learning high-dimensional Gaussian graphical models. Leveraging on an asymptotic distributional characterization of regularized least squares estimators, we develop a procedure for computing p-values and hence assessing statistical significance for hypothesis testing. We characterize the power of this procedure, and evaluate it on synthetic and real data, comparing it with earlier proposals.",United States,Stanford University,1
1108,On Pre-training Shallow Networks with Support Vector Machine Primals,"We present a methodology to pre-train shallow neural networks with Support Vector Machine primals. We train a Support Vector Machine and extract the primal weights to embed them as pre-trained prior knowledge in a shallow neural network; we then proceed to apply backpropagation to leverage and fine tune this knowledge. This contrasts with previous work on pre-training, in which unsupervised pre-training has been used as feature extractors in deep learning. In our MNIST experimental results, we find that using even only $\frac{1}{60}$ of the original dataset for the Support Vector Machine primal pre-training yielded a consistently faster convergence in the network. We believe this paper opens up interesting opportunities for pre-training shallow networks using prior knowledge.",United States,Carnegie Mellon University,1
1126,Off-Policy Actor-Critic with Function Approximation,"We present a new off-policy learning algorithm with an actor-critic architecture that is convergent to a locally optimal solution. Off-policy learning---learning about a policy different from the one being followed---plays an important role in reinforcement learning (RL) due to exploration-exploitation tradeoff. Recent advances in off-policy Temporal-Difference (TD) learning, such as Greedy-GQ, have been hitherto  limited  to value-function based methods and have not been fully extended to policy gradient methods,  which can represent a larger class of policies and also can handle problems with large (or continuous) action space.  Among policy gradient methods, actor-critic methods substantially have been considered for large-scale applications due to their desirable algorithmic features---e.g., they use bootstrapping methods such as TD learning that can reduce variance, and generally are easy to use with function approximation. The critic in our algorithm is based on recent gradient-TD prediction (GTD) methods with linear function approximation and the actor updates the policy parameters via stochastic gradient-ascent of a performance measure. Recently, Degris et al. (2012) have presented an off-policy actor-critic algorithm (OPAC) with similar objectives. However, OPAC does not update the actor via gradient-ascent and, as we will establish, does not always converge. In this paper, we address this issue by proposing an algorithm, called GTD-AC, that shares several of OPACs desirable features: online operation, incremental updating, linear complexity both in terms of memory and per-time-step computation, and in addition it maintains the same number of tuning parameters. Most importantly, we establish a convergence guarantee.",United States,Stanford University,1
1134,Semi-Supervised Classification for Intracortical Brain-Computer Interface,"Brain-computer interface (BCI) decoders are typically retrained daily in a supervised manner to maintain performance.  While this is feasible in a laboratory setting, it is not clear  that the burden of collecting daily training data will be viable in a clinical setting.  We propose a novel classifier for intracortical BCI which is initially trained in a supervised manner using labeled data acquired on a set of training days.  After this supervised training, the decoder then learns updated decoding parameters in an online, semi-supervised manner on all following test days without requiring any further labelled data.  This algorithm assumes decoding parameters are randomly drawn anew each day from a fixed prior distribution and uses unlabeled neural activity collected as a subject performs a BCI task to reduce the posterior uncertainty in parameter estimates.  We evaluate the performance of this new decoder using neural activity recorded with a 96-electrode array implanted in the motor cortex of a macaque monkey.  The mean day-to-day accuracy of the new decoder over 31 test days, achieved without any supervised retraining, is not significantly different than standard methods that are retrained daily in a supervised manner. While these results must be reproduced in a closed-loop setting, we believe the development of decoders that can operate for weeks to months without supervised retraining represents a significant step towards the clinical translation of intracortical BCI systems. ",United States,Carnegie Mellon University,1
1142,Conditional Distance Variance and Correlation,"Recently a new dependence measure, the distance correlation, has been proposed to measure the dependence between continuous random variables. A nice property of this measure is that it can be consistently estimated with the empirical average of the products of certain distances between the sample points. Here we generalize this quantity to measure the conditional dependence between random variables, and show that this can also be estimated with a statistic using a weighted empirical average of the products of distances between the sample points. We demonstrate the applicability of the estimators with numerical experiments on real and simulated data sets.",United States,Carnegie Mellon University,1
1148,Learning the Dependency Structure of Latent Factors,"In this paper, we study latent factor models with the dependency structure in the latent space.  We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance.",United States,Georgia Institute of Technology,1
1165,Bayesian models for Large-scale Hierarchical Classification ,"A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods. ",United States,Carnegie Mellon University,1
1166,Recovery of Sparse Probability Measures via Convex Programming,"We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. It's well-known that the classical $\ell_1$ regularizer fails to promote sparsity on the probability simplex since $\ell_1$ norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on $\ell_1$ norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm.",United States,"University of California, Berkeley",1
1181,Optimal Stochastic Convex Optimization Through The Lens Of Active Learning,"The large fields of convex optimization and active learning have been developed fairly independent of each other, from the design of algorithms to the techniques of proof. Given the growing literature in both these subjects, we believe that understanding the connections between them is important to people in both areas. Here, we establish few such interesting relationships in upper and lower bound techniques that bring out these similarities. Our prime result is showing upper and lower bounds for precisely how the minimax rate for optimizing a given function depends solely on a flatness/noise condition for the function around its minimum.",United States,Carnegie Mellon University,1
1184,"Halo, Hyperbole, and the Pragmatic Interpretation of Numbers","Numbers are interpreted flexibly in everyday language: imprecision, exaggeration, and hyperbole are everywhere. We propose a computational model of the pragmatic interpretation of numbers, building upon recent models of pragmatics as rational inference. We assume that speaker and listener perform a social inference regarding the intended meaning, precision, and affective subtext of a numerical utterance. This model predicts two pragmatic effects, pragmatic halo and hyperbole, and their interaction. We demonstrate that the model accurately predicts the qualitative effects of human interpretation of number words in five real-world domains.",United States,Stanford University,1
1191,Learning Manifolds with K-Means and K-Flats,"We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-?ats, and analyze their performance. We extend previous results  for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-?ats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-?ats, both the results and the mathematical tools are  new.",United States,Massachusetts Institute of Technology,1
1196,Iterative ranking from pair-wise comparisons ,"The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR?s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, finding ?scores? for each object (e.g. player?s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efficacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1].",United States,Massachusetts Institute of Technology,1
1198,Learning Probability Measures with respect to  Optimal Transport Metrics,"We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.",United States,Massachusetts Institute of Technology,1
1208,Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs,"We describe an approach to speed-up inference with latent variable PCFGs, which have been shown to  be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature.  We also describe an error bound for this approximation, which bounds the difference between the probabilities calculated by the algorithm and the true probabilities that the approximated model gives. Empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance.",United States,Columbia University,1
1216,A Soft-Label Model with Impact for Active Graph Search,"We consider the problem of active search on a graph where we seek nodes belonging to a certain positive class by iteratively selecting nodes to query for their class label. The problem has similarities with active learning on a graph except that the performance is measured by number of positives identified rather than classification accuracy. Good solutions must tradeoff exploration to better fit a model against exploitation to collect likely positives and thus the problem has similarities with bandit problems as well. However, bandit algorithms are hard to adapt to the  problem since we will never choose the same node more than once.Previous work showed that the optimal active search algorithm requires a look ahead evaluation of expected utility that is exponential in the number of node selections to be made and considered heuristics that do a truncated look ahead [1]. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs.  We test the algorithm empirically on citation and wikipedia graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps.",United States,Carnegie Mellon University,1
1236,Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential ?1-Minimization,"We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where $A_k$ and $e_k$ denote the measurement matrix and noise, respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ \emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.",United States,Massachusetts Institute of Technology,1
1237,Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems ,"We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes.More recently, an asymptotic regret bound of $\tilde{O}(\sqrt{T})$ was shown for $T \gg p$ where $p$ is the dimension of the state space.In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large.We present an adaptive control scheme that for $p \gg 1$ and $T \gg \polylog(p)$ achieves a regret bound of $\tilde{O}(p \sqrt{T})$.In particular, our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$.This is in comparison to previous work on the dense dynamics where the algorithm needs $\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy.We believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks.",United States,Stanford University,1
1248,Emergence of Object-Selective Features in Unsupervised Feature Learning," Recent work in unsupervised feature learning has focused on the goal  of discovering high-level features from unlabeled images.  Much  progress has been made in this direction, but in most cases it is  still standard to use a large amount of labeled data in order to  construct detectors sensitive to object classes or other complex  patterns in the data.  In this paper, we aim to test the hypothesis  that unsupervised feature learning methods, provided with only  unlabeled data, can learn high-level, invariant features that are  sensitive to commonly-occurring objects.  Though a handful of prior  results suggest that this is possible when each object class  accounts for a large fraction of the data (as in many labeled  datasets), it is unclear whether something similar can be  accomplished when dealing with completely unlabeled data.  A major  obstacle to this test, however, is scale: we cannot expect to  succeed with small datasets or with small numbers of learned  features.  Here, we propose a large-scale feature learning system  that enables us to carry out this experiment, learning 150,000  features from tens of millions of unlabeled images.  Based on two  scalable clustering algorithms (K-means and agglomerative  clustering), we find that our simple system can discover features  sensitive to a commonly occurring object class (human faces) and can  also combine these into detectors invariant to significant global  distortions like large translations and scale.",United States,Stanford University,1
1249,Annotation on the cheap,"We consider the task of producing a high-quality labeling of a new data set, given access to a human annotator who is to be used sparingly. Our approach involves the active learning of a classifier that is allowed to abstain on difficult inputs.",United States,"University of California, San Diego",1
1259,Learning-driven Exploration in Embodied Action-Perception Loops,"Extracting the structure underlying observed data points is a recurring problem in machine learning. When data can be actively collected in the context of a closed-action perception loop, behavior becomes a fundamental determinant of learning efficiency. Previous machine learning studies in closed action-perception loops, however, have largely focused on the control problem of maximizing acquisition of rewards and often treat the learning of structure as a secondary objective deriving from the search for rewards. Psychology, in contrast, has long argued that learning itself is a primary motivation in both human and animal behavior. Here, we study explorative behavioral control in the absence of external reward structure. Instead, we take the quality of an agent's internal model as the primary objective. In a simple probabilistic framework, we derive an estimate, predicted information gain (PIG), for the amount of information about an (unknown) environment that an agent can expect to receive by taking an action. We develop an explorative strategy through approximate maximization of information gain by backwards propagation of future PIG using a value-iteration algorithm. Across a range of environments, we demonstrate that the proposed behavioral policy learns significantly faster than previous reward-free explorative strategies. Finally, we address the possible evolutionary advantage of reward-free exploration by demonstrating that agents which explore efficiently when rewards are not available, are later better able to accomplish a range of goal-directed tasks.",United States,"University of California, Berkeley",1
1263,On the Sample Complexity of Ranking,"Learning to rank is a core machine learning problem. When the truescoring functions are hard to learn or training data is scarce, thesample complexity for predicting a ranking with small error is ofconsiderable interest. We present a lower bound for such a samplecomplexity for any algorithm that estimates a broad class of scoringfunction based on randomly sampled binary comparisons. Additionally,we demonstrate two simple algorithms that achieve the bound inexpectation.  While one algorithm predicts rankings with roughlyuniform quality across the ranking, the other predicts more accuratelynear the top of the ranking than the bottom. Results are presented onsynthetic examples and on an application to epitope (peptide) ranking.",United States,"University of California, Berkeley",1
1269,Density Propagation and Improved Bounds on the Partition Function,"Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds.",United States,Cornell University,1
1275,Distributed large-scale natural graph factorization,"More and more natural graphs arise in numerous Web applications, such as socialnetworks, email, and instant messaging. Such large-scalegraphs today easily contain hundreds of millions of nodes and billions of edges,and are expected to grow even larger in the future. While severaltheoretical models have been proposed for such networks, their analysisis still difficult due to their scale and nature. Inthis paper we propose a distributed framework for large-scale graph factorizationthat allows for easier analysis and more compact representation oflarge natural graphs. ",United States,Carnegie Mellon University,1
1277,Learning Networks of Heterogeneous Influence,"Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities. ",United States,Georgia Institute of Technology,1
1279,Statistical Consistency of Finite-dimensional Unregularized Linear Classification,"This manuscript studies statistical properties of linear classifiers obtained through minimization of an unregularized convex risk over a finite sample. Although the results are explicitly finite-dimensional, inputs may be passed through feature maps; in this way, in addition to treating the consistency of logistic regression, this analysis also handles boosting over a finite weak learning class with, for instance, the exponential, logistic, and hinge losses.  In this finite-dimensional setting, it is still possible to fit arbitrary decision boundaries: scaling the complexity of the weak learning class with the sample size leads to the optimal classification risk almost surely.",United States,"University of California, San Diego",1
1282,Multiclass Learning  with Simplex Coding,"In this paper we dicuss a novel  framework for multiclass learning, defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classesa relaxation approach commonly used in binary classification.In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized methods with training/tuning complexity which is {\em independent} to the number of classes.Tools from convex analysis are introduced that can be used beyond the scope of this paper. ",United States,Massachusetts Institute of Technology,1
1283,FastEx: Fast Clustering with Exponential Families," Clustering is a key component in data analysis toolbox. Despite its  importance, scalable algorithms often eschew rich statistical models  in favor of simpler descriptions such as $k$-means clustering. In  this paper we present a sampler, capable of estimating  mixtures of exponential families. At its heart lies a novel proposal distribution using random  projections to achieve high throughput in generating proposals, which is crucial  for clustering models with large numbers of clusters. ",United States,Carnegie Mellon University,1
1290,Learning with Recursive Perceptual Representations,"Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks.",United States,"University of California, Berkeley",1
1293,Changepoint Detection over Graphs with the Spectral Scan Statistic,"We consider the change-point detection problem of deciding, based on noisy measurements, whether an unknown signal over a given graph is constant or is instead piecewise constant over two connected induced subgraphs of relatively low cut size. We analyze the corresponding generalized likelihood ratio (GLR) statistics and relate it to the problem of finding a sparsest cut in a graph. We develop a tractable relaxation of the GLR statistic based on the combinatorial Laplacian of the graph, which we call the spectral scan statistic, and analyze its properties. We show how its performance as a testing procedure depends directly on the spectrum of the graph, and use this  result to explicitly derive its asymptotic properties on few significant graph topologies. Finally, we demonstrate both theoretically and by simulations that the spectral scan statistic can outperform naive testing procedures based on edge thresholding and $\chi^2$ testing. ",United States,Carnegie Mellon University,1
1317,Spatial Coarse-to-Fine Processing in a Neural Model with Cortical Feedback,"Methods utilizing coarse-to-fine processing, in which the general features of a stimulus are processed before more detailed structure, have found success in several applications, such as natural language processing, image processing, and computer vision. Such dynamics have also been observed in several neurological pathways, including the visual system. In this paper, we consider mechanisms of the spatial coarse-to-fine process in the central visual pathway. We present a model of the lateral geniculate  nucleus (LGN) and visual cortex (V1) incorporating both feedforward and feedback connections. We show that cortical feedback has a substantial effect on spatial dynamics in the LGN. Our results suggest that the LGN may use these recurrent connections to ?learn? the coarse-to-fine dynamic during development. We provide an ideal framework within which to explore this process through more computationally intensive simulations.",United States,"University of California, Berkeley",1
1327,Submodular Bregman Divergences with Applications,"We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular Bregman divergences. We  consider two kinds,  defined either from tight modular upper or tight modular lower  bounds of a submodular function. We show that the properties of  these divergences are analogous to the (standard continuous) Bregman  divergence. Further, we demonstrate how they generalize many useful  divergences, including the weighted Hamming distance, squared  weighted Hamming, weighted precision, recall, conditional mutual  information, and a generalized KL-divergence on sets. We also show  that the lower bound submodular Bregman is actually a special case  of the generalized Bregman divergence on the \lovasz{} extension of  a submodular function which we call the \lovasz{} Bregman divergence. We then  point out a number of applications of the submodular Bregman  divergences, and in particular show that a proximal  algorithm defined through the submodular Bregman divergences  provides a framework for many mirror-descent style algorithms related  to submodular function optimization. We also show that a  generalization of the k-means algorithm using the \lovasz{} Bregman divergence is natural in clustering scenarios where  the ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely efficient unlike the other order based distance measures. \extendedv{Finally we provide a    clustering framework for the submodular Bregman, and we derive    fast algorithms for clustering sets of binary vectors    (equivalently sets of sets).",United States,University of Washington,1
1332,Discovering Sparse Networks In Spiking Data,"  In order to reason about the interacting processes giving rise to a given set of data, we must understand the causal relationships between those latent processes. One way to uncover these relationships is to discover a directed network structure from the data. Often, these data have the form of discrete events --- for example, neural spikes --- which can be modeled effectively via interacting point processes; the Hawkes process is the classical example of such a joint process. Here we provide a fully-Bayesian treatment of the Hawkes process, introducing 1)~convenient conjugate priors for the model parameters and 2)~a sparsity-promoting  spike-and-slab prior on the elements of the interaction matrix.  We demonstrate how to perform efficient inference in this model with Markov chain Monte Carlo.  This enables us to both recover posterior samples of the network structure and infer the characteristics of the underlying temporal dynamics.  We validate our approach on a simulated dataset with known ground truth before examining two real-world data sets --- neural spike train data and financial tick streams.  In each case, we uncover sparse networks with meaningful parameters, suggesting that this method is widely applicable for network discovery.",United States,Massachusetts Institute of Technology,1
1343,First-Order Models for POMDPs,"Interest in relational and first-order languages for probabilitymodels has grown rapidly in recent years, and with it the possibilityof extending such languages to handle decision processes---both fullyand partially observable.  We examine the problem of extending afirst-order, open-universe language to describe POMDPs and identifynon-trivial representational issues in describing an agent'scapability for observation and action---issues that were avoided inprevious work only by making strong and restrictive assumptions. Wepresent a solution based on ideas from modal logic, and show how tohandle cases like being able to act upon an object thathas been detected through one's observations.",United States,"University of California, Berkeley",1
1349,Detecting Activations over Graphs using Spanning Tree Wavelet Bases,"We consider the detection activations over graphs under Gaussian noise, where signals are supposed to be peice-wise constant over the graph.Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with proveable theoretical guarantees for general graph topologies.We first provide a universal necessary condition for asymptotic distinguishability of the null and alternative hypotheses.We introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph.We prove that for any spanning tree, we can hope to correctly detect signals in a low signal-to-noise regime using spanning tree wavelets.We propose a randomized test, in which we use a uniform spanning tree in the basis construction.Using electrical network theory, we show that the uniform spanning tree provides strong theoretical guarantees for arbitrary graphs that in many cases match our necessary condition.We prove that for edge transitive graphs, $k$-nearest neighbor graphs, and $\epsilon$-graphs we obtain nearly optimal performance with the uniform spanning tree wavelet detector.",United States,Carnegie Mellon University,1
1356,Nonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction,"We show how to incorporate information from labeled examples into nonnegative matrix factorization (NMF). In addition to mapping the data into a space of lower dimensionality, our approach aims to preserve the nonnegative components of the data that are important for classification. We identify these components from the support vectors of large-margin classifiers and derive iterative updates to preserve them in a semi-supervised version of NMF. These updates have a simple multiplicative form like their unsupervised counterparts. We evaluate these updates for dimensionality reduction when they are used as a precursor to linear classification.  We find that they yield much better performance than their unsupervised counterparts. We also find one unexpected benefit of the low dimensional representations discovered by our approach: often they yield more accurate classifiers than both ordinary and transductive SVMs trained in the original input space.",United States,"University of California, San Diego",1
1365,Modeling Laminar Recordings from Visual Cortex with Semi-Restricted Boltzmann Machines,"The proliferation of high density recording techniques presents us with new challenges for characterizing the statistics of neural activity over populations of many neurons. The Ising model, which is the maximum entropy model for pairwise correlations, has been used to model the instantaneous state of a population of neurons.  This model suffers from two major limitations: 1) Estimation for large models becomes computationally intractable, and 2) it cannot capture higher-order dependencies.  We propose applying a more general maximum entropy model, the semi-restricted Boltzmann machine (sRBM), which extends the Ising model to capture higher order dependencies using hidden units. Estimation of large models is made practical using minimum probability flow, a recently developed parameter estimation method for energy-based models. The partition functions of the models are estimated using annealed importance sampling, which allows for comparing models in terms of likelihood.  Applied to 32-channel polytrode data recorded from cat visual cortex, these higher order models significantly outperform Ising models. In addition, extending the model to spatiotemporal sequences of states allows us to predict spiking based on network history. Our results highlight the importance of modeling higher order interactions across space and time to characterize activity in cortical networks.",United States,"University of California, Berkeley",1
1373,Near-Tight Bounds for Cross-Validation via Loss Stability,Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds.  In this work we introduce a new and weak measure of stability (\emph{loss stability}) and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal.  Our work thus quantitatively improves the currentbest bounds on cross-validation.,United States,"University of California, San Diego",1
1380,Projection Retrieval for Classification,"In many applications classification systems often require in the loop human intervention. In such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution. To tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users.",United States,Carnegie Mellon University,1
1385,Robust Crowd Labeling using Little Expertise,"Crowd Labeling emerged from the need to label large-scale and complex data, atedious, expensive and time-consuming task. Each object to label is generally annotatedby multiple crowd labelers, and the collected labels are combined to inferone final estimated label. One open problem is the quality and integration of differentlabels, especially when the labelers participating to the task are of unknownexpertise. In order to address this challenge, we propose a new framework thatautomatically combines and boosts bulk crowd labels with a limited number ofground truth labels from experts. We show through extensive experiments that,unlike other state-of-the-art approaches, our method is robust to estimate true labelseven with the presence of a large proportion of not-so-good labelers in thecrowd.",United States,Columbia University,1
1387,Buy-in-Bulk Active Learning,"In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time.This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch.In this work, we study the label complexity of active learning algorithms thatrequest labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once.  In particluar, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increasethe total number of labels requested, it reduces the total cost requiredfor learning.",United States,Carnegie Mellon University,1
1389,Forward Model Extraction from Neural Population Activity,"Internal forward models are believed to explain the nervous system's ability to compensate for sensory feedback delays and adapt to changes in effector dynamics.  Single-neuron and behavioral studies have provided evidence of forward models, but to our knowledge it has not yet been possible to extract a full forward model of the effector directly from neural activity.  Here, we develop a novel probabilistic framework for forward model extraction that integrates neural commands with sensory feedback.  Using this framework, we can i) extract the subject's forward model, which is represented as parameters in the probabilistic model, and ii) infer the subject's timestep-by-timestep internal estimates of the motor effector position, which are latent variables in the model.  We leverage brain-computer interface (BCI) infrastructure, in which all neural commands driving the effector and sensory feedback are fully observed. We applied this framework to neural population activity recorded in macaque motor cortex during BCI control of a computer cursor to acquire visual targets. We found that recorded neural commands were more consistent with aiming straight toward targets from the subject's internal estimates of cursor position, as inferred by our probabilistic framework, than from the cursor positions displayed during online control. The extracted forward models explain about 75% of the subject's aiming errors. We believe that the probabilistic framework developed provides a critical link between sensory feedback and motor commands, and will likely facilitate the study of feedback motor control and motor learning.",United States,Carnegie Mellon University,1
1394,Noisy Bayesian Active Learning,"We consider the problem of noisy Bayesian active learning, where we are given a finite set of functions $\mathcal{H}$, and a sample space $\mathcal{X}$. A function in $\mathcal{H}$ assigns a label to a sample in $\mathcal{X}$, and the result of a label query on a sample is corrupted by independent noise. The goal is to identify the function in $\mathcal{H}$ that generates the labels with high confidence using as few label queries as possible, by selecting the queries adaptively in a strategic manner. Previous work in Bayesian active learning considers Generalized Binary Search, and its variants for the noisy case, and analyzes the number of queries required by these sampling strategies. In this paper, we show that these schemes are, in general, suboptimal. Instead we propose and analyze an alternative strategy for sample collection. Our sampling strategy is motivated by a connection between Bayesian active learning and active hypothesis testing, and is based on querying the label of a sample which maximizes the Extrinsic Jensen--Shannon Divergence at each step. We provide upper and lower bounds on the performance of this sampling strategy, and show that these bounds are better than previous bounds.",United States,"University of California, San Diego",1
1395,Human memory search as a random walk in a semantic network,"The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single, undirected process rather than one process for exploring a cluster and one process for switching between clusters.",United States,"University of California, Berkeley",1
1397,Manifold Regularization and Embedding Through Laplacian Low-Rank Correction,"This paper introduces Laplacian low-rank correction (LLRC), a method for manifold regularization  that is generally applicable to arbitrary data sets and learning problems.  Unlike popular embedding algorithms such as Laplacian eigenmaps, locally linear embedding, and ISOMAP, our approach does not explicitly seek a low-dimensional representation of data.  Rather, LLRC leverages the graph Laplacian of a local similarity graph to construct a low-rank linear correction that maintains the original dimension of the feature space. This linear correction eliminates dominant off-manifold noise directions, such that the principal components of the corrected space represent the data manifold.LLRC can be kernelized to account for nonlinear structure. A primary advantage of such a linear approach is that the correction can be easily extended out-of-sample.A low-dimensional embedding may be obtained from LLRC by extracting the (kernel) principal components from the corrected feature representation, and can significantly outperform standard (kernel) PCA at recovering manifold structure. The modified representation may also be applied in a number of other learning problems, including classification, regression, and clustering. As an example, we apply LLRC to SVM classification in a semi-supervised setting. In particular, we develop a low-rank approximation to the standard Laplacian SVM. This approximation offers greatly reduced computation for large data sets and, in some cases, improved performance.",United States,University of Michigan - Ann Arbor,1
1401,Local Support Vector Machines: Formulation and Analysis,"We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs.",United States,Georgia Institute of Technology,1
1410,Learning with Partially Absorbing Random Walks,"We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$, and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\mathcal{S}$, while dropping sharply outside $\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning.",United States,Columbia University,1
1411,Efficient MAP Inference in Binary Pairwise MRFs,"Markov random fields (MRFs) have broad application in many fields including computer vision. In general, however, finding the most likely (MAP) configuration of variables is NP-hard.  If we restrict attention to the class of associative (submodular) models then the task is tractable but further speed improvement would be welcome. Simplifications are revealed by reducing the MAP inference problem to maximum weight stable set (MWSS) on a compiled nand Markov random field (NMRF). This yields two results for general binary pairwise MRFs: (1) A rapid pre-processing algorithm that identifies an exact MAP configuration of a subset of the variables; the subset is larger for sparse MRFs with low associativity and random settings;local sensitivity parameters are also returned for every variable; and (2) We derive necessary and sufficient conditions for when such an MRF maps to a perfect NMRF, which guarantees efficient MAP inference.",United States,Columbia University,1
1414,Diagnose and Decide: An Optimal Bayesian Approach 004,"Many real-world scenarios require making informed choices after some sequence of actions that yield noisy information about a latent state.  Prior research has mostly focused on generic methods that struggle to scale to large domains. We focus on a subclass of such problems with two particular characteristics. First, though information gathering actions or tests only provide noisy information about the hidden state, once performed a test will always yield the same result. This means it is sufficient to perform each test once. Second, we assume that test costs can be expressed in the same units as costs of the final decisions made. We call such scenarios diagnose-and-decide problems. We prove diagnose-and-decide problems are a special subclass of POMDPs for which the optimal policy can be computed in time polynomial in the set of possible tests' outcomes. We develop a new simple algorithm which is able to take advantage of the unique structure in our problem while guaranteeing optimality. We demonstrate the advantages of our approach over greedy and traditional POMDP methods in two simulations based on real-world data (colon cancer screening and object recognition) as well as a large synthetic domain. ",United States,Massachusetts Institute of Technology,1
1420,Learning mixture models with the hierarchical expectation maximization algorithm,"Driven by the need for computationally efficient parameter estimation from large, web-scale data sets, the hierarchical EM (HEM) algorithm has been proposed and proven effective for a variety of modeling tasks and applications. In this paper, we investigate the benefits of HEM as a general-purpose algorithm for parameter estimation in mixture models, compared to regular EM. First, we re-derive the algorithm in more generality, for generic exponential family distributions, with and without unobserved variables. Second, we discuss and experimentally verify its benefits across a broad spectrum of model classes and applications. Besides scalability, HEM's implicit regularization and adaptation for multiple instance learning make it an appealing alternative to standard EM, for practitioners.",United States,"University of California, San Diego",1
1428,Probabilistic Event Cascades for Alzheimer's disease," Accurate and detailed models of the progression of neurodegenerative diseases such as  Alzheimer's (AD) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments. In this paper, we introduce the ALPACA (Alzheimer's disease Probabilistic Cascades) model, a generative model linking latent Alzheimer's progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a fixed ordering of events, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed disease progression models. We describe efficient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer's patients from the  Alzheimer's Disease Neuroimaging Initiative.",United States,Stanford University,1
1434,Spectral Learning of Latent-Variable HMMs ,We derive a spectral algorithm for learning the parameters of a latent-variable HMM. This method avoids the problem of local optima and provides a consistent estimate of the parameters. We demonstrate the method on a phoneme recognition task and show that it performs competitively with EM. ,United States,Massachusetts Institute of Technology,1
1436,One Permutation Hashing,"While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.In this paper, we develop a simple \textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing $k$ permutations to just one  would be much more \textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \& logistic regression also confirm the theoretical results.",United States,Cornell University,1
1437,A template model for fine-grained object recognition,"Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms. ",United States,University of Washington,1
1439,Getting the First Page Right: Bayesian Active Retrieval under Uncertainty,"Triggered by the idea of an information retrieval system for objects with noisy and missing features, we investigate the general problem of actively learning a similarity function of complex objects when the inputs to this function are not known exactly. To reduce the uncertainty in the inputs, and in turn improve the similarity function, we are interested in acquiring more information about the input objects. As gathering clean and complete information is costly or even impossible, it is important to carefully select the information needed and to be able to deal with uncertainty in order to retrieve meaningful results fast and with low total cost. Hence, we propose a Bayesian active learning approach to efficiently learn the most similar objects to a given query object in the setting where only partial and noisy information about entities is available. In our information retrieval case this corresponds to the task of getting the first page (of retrieval results) right. We evaluate the proposed Bayesian decision theoretic framework to actively acquire information on several retrieval problems, including a real-world document retrieval task.",United States,Carnegie Mellon University,1
1445,Near Optimal Chernoff Bounds for Markov Decision Processes,"The expected return is a widely used objective in decision making under uncertainty.  Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize.  We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw connections to previously proposed objectives for risk-aware planing: minmax, exponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded.  Additionally, we present an efficient algorithm for optimizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale.",United States,"University of California, Berkeley",1
1456,Entropy Estimations Using Correlated Symmetric Stable Random Projections,"Methods for efficiently estimating  the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of {\em Compressed Counting (CC)}~\cite{Proc:Li_Zhang_COLT11}  based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two \textbf{correlated} frequency moments estimated from  correlated samples of \textbf{symmetric} stable random variables. Interestingly, the  estimator for the moment we recommend for entropy estimation  barely has bounded variance itself, whereas the  geometric mean estimator (which has bounded higher-order moments) is not sufficient for entropy estimation. Our experiments confirm that this method is able to  well approximate the Shannon entropy using small storage (e.g., $100\sim 1000$ samples). A prior study~\cite{Proc:Zhao_IMC07}  approximated the Shannon entropy using symmetric stable random projections with (e.g.,) $10^5\sim1.6\times10^6$ \textbf{independent} samples.",United States,Cornell University,1
1467,Deep Learning of invariant features via tracked video sequences,"We use video sequences produced by tracking as training data to learn invariant features. These features are spatial instead of temporal, and well suited to extract from still images. With a temporal coherence objective, a multi-layer neural network encodes invariance that grow increasingly complex with layer hierarchy. Without fine-tuning with labels, we achieve competitive performance on five non-temporal image datasets and state-of-the-art classification accuracy 61% on STL-10 object recognition dataset.",United States,Stanford University,1
1475,Structure Learning for Weakly Dependent Observations,"We consider the problem of estimating the graph structure of a certain class of stochastic processes defined on an Ising model. This class contains as special cases the i.i.d. observation model, the geometric $\alpha$-mixing process, and the rapidly-mixing Glauber dynamics. We analyze thenode-based neighborhood estimation algorithm of \cite{RavWaiLaf09}, which reduces to $\ell_1$-regularized logistic regression. Our main result is to provide sufficient conditions on the triple $(\numobs, \pdim, \ddim)$ for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously.",United States,"University of California, Berkeley",1
1477,On the Sample Complexity of Robust PCA,"We estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix.This estimator is used in a convex algorithm for robust subspace recovery (i.e., robust PCA).Our model assumes a sub-Gaussian underlying distribution and an i.i.d.~sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $N$ is of order $O(N^{-0.5+\eps})$ for arbitrarily small $\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation, i.e., $O(N^{-0.5})$. Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is $O(D^{2+\delta})$ for arbitrarily small $\delta>0$ (whereas the sample complexity of direct covariance estimation with Frobenius norm is $O(D^{2})$).These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm, which are close to those of PCA. To the best of our knowledge, this is the only work analyzing the sample complexity of any robust PCA algorithm.",United States,Massachusetts Institute of Technology,1
1478,View-invariance and mirror-symmetric tuning in a model of the macaque face-processing system,"Recent experimental results characterizing the face-processing network in macaque visual cortex pose a major puzzle. View-tuned units (found in patches ML/MF) are a natural step to a view-tolerant representation (found in patch AM), as predicted by several models. However, the observation that cells in patch AL are tuned to faces and their mirror reflections remains unexplained (cf. Freiwald and Tsao (2010)). We show that a model based on the hypothesis that the ventral stream implements a memory-based approach to transformation invariance predicts the main properties of ML/MF, AL and AM. In this view, a major computational goal of the ventral stream is to compute invariant ?signatures? that can be used to recognize novel objects under previously-seen transformations of arbitrary ``templates''.  These invariant signatures can be regarded as encodings of a novel object relative to a compressed memory of the transformation of familiar objects (PCA). ",United States,Massachusetts Institute of Technology,1
1482,Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning,"We describe a latent variable model for supervised dimensionality reduction and distance metric learning. The model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones. The model?s continuous latent variables locate pairs of examples in a latent space of lower dimensionality. The model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate Gaussian. Nevertheless we show that inference is completely tractable and derive an Expectation-Maximization (EM) algorithm for parameter estimation. We also compare the model to other approaches in distance metric learning. The model?s main advantage is its simplicity: at each iteration of the EM algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem. Experiments show that these simple updates are highly effective.",United States,"University of California, San Diego",1
1487,Out-of-Sample Extensions for Manifold Learning Using Sparse Kernel Ridge Regression,"Many manifold learning algorithms do not provide a mapping from the input space to the low-dimensional manifold, requiring an out-of-sample extension to project new points onto the low-dimensional manifold. We propose an out-of-sample extension approach based on a sparse approximation to kernel ridge regression to achieve significant computational savings. In particular, we present a convex program for approximating kernel ridge regression where given a set of points in $\mathbb{R}^d$ and corresponding points in $\mathbb{R}^p$, we find a mapping from $\mathbb{R}^d$ to $\mathbb{R}^p$ that depends only on a subset of the input data points acting as support vectors. Our construction uses group sparsity and guarantees an upper bound on the average squared Euclidean distance between the predicted points of the sparsified mapping and those of kernel ridge regression. We present two medical imaging applications that necessitate a fast projection to a low-dimensional space. The first is respiratory gating in ultrasound, where we assign the breathing state to each ultrasound frame during the acquisition in real-time. The second is the detection of the position of a patient in an MRI scanner while the bed the patient lies on is moving to a target location.",United States,Massachusetts Institute of Technology,1
1488,Trajectory-Based Short-Sighted Probabilistic Planning,"Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [ref] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs.  We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately $10^{70}$ states.",United States,Carnegie Mellon University,1
1490,Jointly Learning and Selecting Features via Conditional Point-wise Mixture RBMs,"Feature selection is an important technique for ?nding relevant features from high-dimensional data. However, the performance of feature selection methods is often limited by the raw feature representation. On the other hand, unsupervised feature learning has recently emerged as promising tools for extracting useful features from data. Although supervised information can be exploited in the process of supervised ?ne-tuning (preceded by unsupervised pre-training), the training becomes challenging when the unlabeled data contain signi?cant amounts of irrelevant information. To address these issues, we propose a new unsupervised feature learning algorithm, the conditional point-wise mixture restricted Boltzmann machine, which attempts to perform feature grouping while learning the features. Our model represents each input coordinate as a mixture model when conditioned on the hidden units, where each group of hidden units can generate the corresponding mixture component. Furthermore, we present an extension of our method that combines bottom-up feature learning and top-down feature selection in a coherent way, which can effectively handle irrelevant input patterns by focusing on relevant signals and thus learn more informative features. Our experiments show that our model is effective in learning separate groups of hidden units (e.g., that correspond to informative signals vs. irrelevant patterns) from complex, noisy data.",United States,University of Michigan - Ann Arbor,1
1493,Tight Bounds on Redundancy and Distinguishability of Label-Invariant Distributions,"The minimax KL-divergence of any distribution from alldistributions in a given collection has several practicalimplications. In compression, it is the least additional number of bitsover the entropy needed in the worst case to encode theoutput of a distribution in the collection. In onlineestimation and learning, it is the lowestexpected log-loss regret when guessing a sequence of randomvalues. In hypothesis testing, it upper bounds the largestnumber of distinguishable distributions in the collection.Motivated by problems ranging from population estimation totext classification and speech recognition, severalmachine-learning and information-theory researchers haverecently considered label-invariant distributions and propertiesof \iid-drawn samples.Using techniques that reveal and exploit the structure of these distributions,we improve on a sequence of previous works and show that theminimax KL-divergence of the collection of label-invariantdistributions over length-$n$ \iid sequencesis between $0.3\cdot n^{1/3}$ and $n^{1/3}\log^2n$.",United States,"University of California, San Diego",1
1494,Conditional Likelihood Inference on Overlapping Figure-Ground Segment Hypotheses,"In this paper we present an inference procedure for the semantic segmentation of images, namely identifying the spatial layout and class labels of the objects present.Different from many CRF approaches that rely on dependencies modeled with unary and pairwisepixel or superpixel potentials, our method is entirely based on overlap estimates on diverse, potentially overlapping segments in the image. This enablesus to solve the statistical inference problem without using a random field and its associated dependencies. In the approach, posteriorsuperpixels are obtained by intersections of the overlapping regions.Then random variables are defined on such superpixels so that the overlap between each segment and the ground truth can be constructed from them. Inferenceis then performed using conditional maximum likelihood based on an EM formulation. In the PASCAL VOC challenge, the proposed approach is comparable with the 3-times winner SVRSEGM system, but in addition it successfully recognizes additional images with multiple interacting objects.",United States,Georgia Institute of Technology,1
1506,Minimax vs. UCT: A Comparative Study Using Synthetic Games,"Upper Confidence bounds for Trees (UCT) and Minimax are two of themost prominent tree-search based adversarial reasoning strategies fora variety of challenging domains, such as Chess and Go. Theircomplementary strengths in different domains have been the motivationfor several works attempting to achieve a better understanding oftheir behaviors. In this paper, rather than using complex games as atestbed for deriving indirect insights into UCT and Minimax, wepropose the study of relatively simple synthetic trees that permitanalysis and afford a greater degree of experimental freedom. Using anovel tree model that does not suffer from the shortcomings ofpreviously studied models, we provide a relatively straightforwardcharacterization of the kinds of games where UCT is superior toMinimax, and vice versa --- to the best of our knowledge, this is thefirst time such an effort has been successful. In particular, we showthat UCT shines in games where heuristics are accurately modeled usingadditive Gaussian noise, that contrary to previous work, earlyterminal states by themselves do not necessarily hurt UCT, and that intrees with heuristic dispersion lag, UCT is outperformed byMinimax.",United States,Cornell University,1
1517,Unfolding Latent Tree Structures using 4th Order Tensors,"Existing approaches for discovering latent structures often require the number ofhidden states as an input, a quantity usually unknown in practice. In this paper, wepropose a quartet based approachwhich is agnostic to this number. Our key contributionis a novel rank characterization of the tensor associated with the marginaldistribution of a quartet; and this characterization allows us to design a nuclearnorm based test for resolving the quartet relations. We then use this quartet testas a subroutine in a divide-and-conquer algorithm for recovering the latent treestructure. We show that, under certain conditions, the algorithm is consistent andits error probability decays exponentially with increasing sample size. In experiments,we demonstrate that our approach compares favorably to alternatives fordiscovering latent structures. In real world datasets, our approach also discoversmeaningful groupings of variables.",United States,Georgia Institute of Technology,1
1520,Generalized Least Squares for Principled Complex Backups in Temporal Difference Learning,"We derive the form of an estimator that uses generalized least squares to obtaina principled form of the eligibility trace. We show that both the $\lambda$-return and $\gamma$-return can be thought of as assuming that the inverse covariance matrix of $n$-step returns has specific values on its diagonal, and is zero elsewhere.  The new weighting scheme has a single parameter that can easily be set from data,closely matches the empirical covariance weights, and performs very well across several settings of $\gamma$ and $\epsilon$. ",United States,Massachusetts Institute of Technology,1
1529,Improved Graph Laplacian by Geometric Consistency,"We consider the problem of setting the parameters used to construct the graph Laplacian for Euclidean data, most notably ?, the kernel bandwidth. We exploit the connection between geometry and the Laplace-Beltrami operator to evaluate how closely the graph Laplacian recovers the geometry of the data. In doing so, we can consider a family of graph Laplacians indexed by a finite number of parameters and select the values that best recover the geometry of the data.",United States,University of Washington,1
