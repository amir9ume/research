title,abstract
High-Dimensional Feature Selection by Kernel-Based Feature-Wise Non-Linear Lasso,"The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features."
Context-Dependent Topic Modeling Using Multinomial Probit Random Effect Regression (MPR),"This paper presents the multinomial probit random effect regression (MPR) topic model that can incorporate document meta-data for topic inference. Document meta-data enter the topic model through regression covariates of a multinomial-probit-like setting that influence the prior of topic distribution. Both discrete and continuous variables can be included. The MPR contains document-specific random effects to allow flexible inference. We developed Gibbs sampling for the non-Dirichlet-conjugate setting. We demonstrate the utility of MPR by incorporating intra-day, weekly, and monthly cyclical patterns into topic models. Experimental results show that MPR can capture interesting short-term cyclical patterns in investor forum postings, newswires, and newspaper articles. Likelihood evaluation shows that MPR outperforms LDA in the three testbeds considered in this study."
Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information,"The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high non-linearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets."
A Covariate dependent HMM for learning hybrid dynamical models,"Our main contribution is a learning system that acquires an hybrid representation of a dynamical system. For this we extend hidden markov models by including a covariate in the transition model. We propose an efficient Gibbs sampler and show that we can reliably estimate such models compute the full posterior model thus having access also to the confidence on our model. In the work of \cite{ghahramani2000variational} we can see a general discussion of many different graphical models with different structures. Our model adds some extra complexity due to the dependency of the switch state transitions on the continuous variables.  This model does not only provide better prediction quality but it is easier to interpret and is suitable for control, filtering and planning that is grounded on the physical system dynamics and on the task goal. We will show a simple example of how planning can be achieved with such model."
Can We Recognize Tiger by Bus Images? ?Robust and Discriminative Self-Taught Image Categorization,"The lack of training data is a common challenge in many real-world image categorization problems, which is often tackled by semi-supervised learning or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught image categorization approach to utilize any unlabeled images (e.g., those randomly downloaded from Internet) without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled images. Because our new objective involves non-smooth terms in both the loss function and the regularization, it is difficult to solve in general. Thus, we derive an efficient iterative algorithm to solve the optimization problem, and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.  "
"Robust Linear Discriminant Analysis Using Ratio Minimization of $l_{1,2}$-Norms","Traditional Linear Discriminant Analysis (LDA) minimizes the ratio of squared $l_2$-norms, which is sensitive to outliers. In recent research, many $l_1$-norm based robust learning models were proposed. However, so far there is no existing work to utilize $l_1$-norm based objective for LDA, due to the difficulty of $l_1$-norm ratio optimization. Meanwhile, trivially replacing $l_2$-norms by $l_1$-norms in LDA objective introduces the $l_1$-norm maximization problem and doesn't provide the robustness. In this paper, we propose a novel robust LDA formulation based on the $l_{1,2}$-norm ratio minimization. Minimizing the $l_{1,2}$-norm ratio is a much more challenging problem than the traditional methods, and existing optimization algorithms cannot solve such a non-smooth terms ratio problem. We derive a new efficient algorithm to solve this challenging problem, and provide the theoretical analysis on the algorithm convergence. Our algorithm is easy to be implemented, and converges fast in practice with the same computational complexity as the trace ratio LDA. Extensive experiments on both synthetic data and nine real benchmark data sets show the effectiveness of the proposed robust LDA method."
A Near Neighbor Scoring Function Improves Error Bounds Based on Worst Likely Assignments,"Error bounds based on worst likely assignments operate in a transductive classification setting, where there are training examples with known class labels and the goal is to classify a set of working examples with known inputs and unknown class labels. These bounds have proven effective even for small data sets. This note describes a method to improve these bounds. The method evaluates potential assignments of class labels to working examples based on whether the assigned labels agree with labels on nearby training examples. The method shows the greatest improvement for validating accurate classifiers."
Efficient Local Image Description and Matching Based on Permutation Distances,"Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a randomsampling of pairwise comparisons of pixel intensities in an image patch.  Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a descriptor based on permutation distances between the ordering of intensities or RGB values between two patches. LUCID is computable in linear time of patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF yet up to five times more accuate, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster."
Clustering the Stochastic Block Model via Positions in the Network,"We consider the stochastic block model and analyze methods based on the positions of the nodes in the network.  This perspective was introduced by Burt (1976) and algorithmically amounts to embedding the graph by mapping the vertices to the corresponding rows of the adjacency matrix.  Once this is done, off-the-shelf methods for clustering points in Euclidean (or Hamming) space can be applied.  We study some popular dissimilarities in this context and provide theoretical guarantees for them that are sufficient for hierarchical clustering to succeed in correctly clustering the nodes.  The analysis is relatively simple in the context of a stochastic block model, yielding competitive performance bounds, particularly when the number of communities in the network grows with the number of nodes.  We evaluate some of these methods in some simulations, comparing them with spectral clustering.  "
Learning from Distributions via Support Measure Machines,"This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework."
Multiscale Hidden Conditional Neural Fields with Adaptive-Rate Latent Variable Grouping,"We hypothesize that considering multiple levels of abstraction with adaptive-rate time quantization improves temporal sequence learning. To prove this empirically, we developed multiscale hidden conditional neural fields with adaptive-rate latent variable grouping. Our model is comprised of multiple layers, where each layer is recursively built from the preceding layer by aggregating observations that are similar in the latent space, representing the sequence at a coarser scale. We extract a nonlinear combination of features from grouped variables using a set of gate functions, learning the optimal abstraction of the sequence at each scale. This allows our model to learn higher level abstractions at ever more coarse-grained time scale as the layer gets higher. Optimization is performed layer-wise, making the complexity grow linearly with the number of layers. We evaluate our approach on three human activity datasets: ArmGesture, NATOPS, and Canal9. Our method achieves a near perfect recognition accuracy on the ArmGesture dataset, and outperforms all baseline models on the NATOPS and Canal9 dataset."
Jointly Segmenting Multiple Web Photo Streams,"As online sharing of personal photo streams is becoming popular and many of such photo streams often share overlapping contents, the cosegmentation can potentiate a wide range of intriguing Web applications. However, existing cosegmentation algorithms are still far limited for these new opportunities in that input images must be carefully prepared by human. In this paper, we address the problem of jointly segmenting an arbitrary number of unaligned and uncalibrated Web photo streams from multiple anonymous users. Given that the main difficulty of cosegmenting such photo streams lies in their extreme diversity in visual contents, we propose a multi-round segmentation algorithm that consists of the companion sampling  and cosegmentation  steps. Theoretically, we show that the developed method achieves the sublinear bound of regrets (i.e. the cumulative sum of difference between unknown ideal cosegmentation scores and actual scores). With experiments on more than 16K images of Flickr dataset and LabelMe dataset, wedemonstrate that our algorithm is more successful in both segmentation quality and scalability over other state-of-art methods."
Statistical Computations Underlying the Dynamics of Memory Updating,"Psychophysical and neurophysiological studies have suggested that the formation of memory traces is sensitive to the temporal structure of the environment. We present a statistical theory of memory formation in a dynamic environment, based on a nonparametric generalization of the switching Kalman filter. We show that this theory can account for existing data on the dynamics of memory updating, as well as the results of a new behavioral experiment. Our behavioral findings suggest that humans use temporal discontinuities in the structure of the environment to determine when to form new memory traces. The statistical perspective provides a coherent account of the conditions under which old memories are modified and new memories are formed."
Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery,"Given pairwise dissimilarities between data points, we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection. We formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming. The solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives. We obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives. When data points are distributed around multiple clusters according to the dissimilarities, we show that the data in each cluster select only representatives from that cluster. Unlike metric-based methods, our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality. We demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text."
Feature Clustering for Accelerating Parallel Coordinate Descent,"Large scale $\ell_1$-regularized loss minimization problemsarise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\ell_1$ regularizedproblems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\ell_1$-regularization problems."
Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA,"Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules in correspondence to components of a monosynaptic spinal loop. Results of emulated activities are shown. The paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions. "
Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition,We present data-driven techniques to augment Bag of Word (BoW) models within a Vector Space Model framework. This approach allows for more robust modeling and recognition of complex activities for instances where the structure and topology of the activities are not known a priori. Our approach addresses the limitation of standard BoW approaches that fail to represent the temporal information inherent in activity streams. We also introduce the use of randomly sampled Regular Expressions to capture the global structure of activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in three complex data-sets. 
Stratified Sensor Network Calibration from TDOA Measurements,"This paper presents a study of the sensor network calibrationthat arise in TOA and TDOA measurements.Such calibration arise in several applications such ascalibration of (acoustic) microphone arrays, calibration ofwifi-transmittor arraysand radio antenna networks. There are at present no solution methodsfor the minimal cases. In the paper we improve on the currentstate-of-the-artby solving several new cases that are 'closer' to the minimal ones. We apply a three-step stratification process, (i) using a novel set ofrank constraintsto determine the unknown offsets, (ii) applying factorization techniquesto determinesenders and receivers up to unknown affine transformation and (iii) determiningthe affine stratification using the remaining constraints. For thetime-of-arrivalcase only steps (ii) and (iii) are needed.Experiments are shown both for simulated and real data with promising results.For simulated data we explore how sensitive the estimated parameters are withrespect to different degrees noise in the data and show that the proposed methods are numerically similar to previous methods. For real data, we test the method on several microphone and sound measurements and verify the results using computer vision based methods. "
A Unified 3D Scene Parsing Framework,"We propose a unified framework for parsing an image to predict the scene category, the 3D boundary of the space, camera parameters, and all objects in the scene, represented by their 3D bounding boxes and categories. Using a structural SVM, we build a complete end-to-end system which learns all parameters together in a single step. We encode many novel image features and context rules into the structural SVM feature function and our framework automatically weighs the relative importance of all these rules based on training data. By optimizing a unified objective function, we do not require extra training of other models, nor an additional fusion step. We design an intuitive web-based tool to annotate 3D bounding boxes for objects, build our ?SUN3D? database and demonstrate that our model outperforms the state-of-the-art algorithms on several individual subtasks."
Coupling Nonparametric Mixtures via Latent Dirichlet Processes,"Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Specifically, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures, allowing mixture models to be coupled more flexibly. In addition, we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling."
Bayesian Nonparametric Maximum Margin Matrix Factorization for Collaborative Prediction,"We present a probabilistic formulation to max-margin matrix factorization and build accordingly an infinite nonparametric Bayesian model to automatically resolve the unknown number of latent factors. Our work demonstrates a successful example that integrates Bayesian nonparametrics and max-margin learning, which are conventionally two separate paradigms and enjoy complementary advantages. We develop an efficient variational learning algorithm for posterior inference, and our extensive empirical studies on large-scale MovieLens and EachMovie data sets appear to demonstrate the advantages inherited from both max-margin matrix factorization and Bayesian nonparametrics."
The Subspace Intersection Problem,"This paper introduces a novel and very general problem termed the subspace intersection problem. The goal is to infer an unknown subspace from a set of given subspaces which are known to each intersect this unknown subspace. The intersections however are unknown, as well. As an example, an intuitive instance of such a subspace intersection problem is given by 4 lines in 3D space where an unknown additional line has to be determined such that the known 4 lines intersect with it. A general algebraic formulation based on the Laplace expansion for determinants is presented enabling the computation of the unknown subspace in closed-form, given sufficiently many constraining known subspaces. Moreover, an efficient numerical scheme based on a partial reduced row-echelon form is introduced. The theory and algorithms for subspace intersection problems are showcased on a structure-from-sound problem. The theory enables the computation of an unknown synchronization of sound sources in closed-form. Furthermore, improving upon previous work, new cases such as missing observations can be handled in closed-form. "
A Bayesian Framework for Low-Rank and Sparse Patterns Inference in Multi-Task Learning,"In this paper we study the low-rank modeling for task relatedness in multi-task learning paradigm. We propose a Bayesian framework to infer the underlying low-rank and sparse patterns from multiple tasks. With the assumption of a shared low-rank hypothesis subspace, the framework treats the low-rank and sparse components as hidden variables which are constrained by certain sparsity inducing priors. We present a principled framework to infer the low-rank and sparse parts alternatively. Unlike optimization based methods, our method doesn't suffer from convex constraints and provides a novel perspective for multi-task learning from Bayesian standpoint. Experimental results on real-world data demonstrate the competitive capability of our method in dealing with multi-task learning problem."
"Greedy Bilateral Sketch, Completion & Smoothing","Recovering a large low-rank matrix from highly corrupted, incomplete or sparse outlier overwhelmed observations is the crux of various intriguing statistical problems. We explore the power of ``greedy bilateral (GreB)'' paradigm in reducing both time and sample complexities for solving these problems. GreB models a low-rank variable as a bilateral factorization, and updates the left and right factors in a mutually adaptive and greedy incremental manner. We detail how to model and solve low-rank approximation, matrix completion and robust PCA in GreB's paradigm. On their MATLAB implementations, approximating a noisy $10^4\times 10^4$ matrix of rank $500$ with SVD accuracy takes $6$s; MovieLens10M matrix can be completed in $10$s from $30\%$ of $10^7$ ratings with RMSE $0.86$ on the rest $70\%$; the low-rank background and sparse moving outliers in a $120\times 160$ video of $500$ frames are accurately separated in $1$s."
Bayesian Hierarchical Reinforcement Learning,"We describe an approach to incorporating Bayesian priors in the maxq framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, given sensible priors, (ii) task hierarchies and Bayesian priors can be complementary sources of information, and using both sources is better than either alone, (iii) taking advantage of the structural decomposition induced by the task hierarchy significantly reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to automatic learning of hierarchically optimal rather than recursively optimal policies. This paper is a resubmission of ICML paper number 660, where we have addressed the reviewer requests for experiments with additional baselines."
Stereoscopic Tracking with Neural Network Hardware,"A stereoscopic learning and tracking system is built using off-the-shelf parts: a pair of Cognimem V1KU neural network boards with onboard CMOS cameras; four HiTec servo motors; a Phidgets servo controller; and a laptop. A simple acrylic mount was constructed. Object learning and recognition occurs primarily within the Cognimem neural network chips, with tracking and triangulation done in software. A Kalman filter is used for predictive tracking. The resulting prototype can learn a new object quickly, usually within a second. It then can track the object?s motion in 3D space with depth-perception, at fairly fast speeds even in the presence of noisy background, without the use of structured light. The software is designed for extension to a variety of practical applications."
Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction,"We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable's marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufficiently certain about any individual decision. Whenever this is the case, we dynamically prune the variable we are confident about from the underlying factor graph. Consequently, at any time only samples of variable whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classification and image inpainting, shows that adaptive sampling can drastically accelerate MMP without sacrificing prediction accuracy."
Local Supervised Learning through Space Partitioning,"We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers. We formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error. We train locally linear classifiers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also show improved robustness to label noise."
Multi-View Clustering and Feature Learning via Structured Sparsity-Inducing Norms,"Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which make the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods."
Localized Gaussian Process Kernel Combining,"This paper investigates learning to combine multiple kernels in the context of Gaussian Process modeling. The fusing of kernels empowers the learner to take advantage of multiple heterogeneous data sources and views but poses the problem of how to best combine them. Unlike many existing algorithms where kernels are linearly combined at the matrix level, we propose an element-wise kernel combining approach with the former being a special case. The lower-level combining scheme is not limited to more flexible data integration, it also motivates new problem settings. We explore one such setting, rejecting noisy instances of MRI data to improve the learning performance. We propose EM and efficient gradient based optimization methods which make it possible to handle large scale problems. The promising experimental results demonstrate the performance of our model, and validate the effectiveness of element-level kernel combining."
A Generative Model for Parts-based Object Segmentation,"The Shape Boltzmann Machine (SBM) has recently been introduced as a state-of-the-art model of foreground/background object shape. We extend the SBM to account for the foreground object's parts. Our model, the Multinomial SBM (MSBM), can capture both local and global statistics of part shapes accurately. We combine the MSBM with an appearance model to form a fully generative model of images of objects. Parts-based image segmentations are obtained simply by performing probabilistic inference in the model. We apply the model to two challenging datasets which exhibit significant shape and appearance variability, and find that it obtains results that are comparable to the state-of-the-art."
Super-Bit Locality-Sensitive Hashing,"Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method towards scalable nearest neighbor search in high dimensional data space, which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments."
Fisher?s Discriminant with Natual Image Priors,"We suggest a Bayesian framework to combine Fisher's discriminant and natural image statistics. The probability structure of Fisher's discriminant is expressed, and the idea of natural image statistics is utilized as prior probabilities (\emph{Natural Image Priors}). Previous methods which directly employed the spatial smoothness assumption of images can be shown as special cases within this framework, but with potentially improper priors which are different from the natural image prior. We also propose a novel method which is a \emph{maximum a posteriori probability} (MAP) estimate with the natural image prior. Experimental results on the Yale face database and the ETH-80 object categorization dataset show that the proposed method significantly outperforms the state-of-the-art methods for general image data."
Accelerated Training of Linear Object Detectors,"We describe a general and exact method to speed up the training of linear object detection systems operating in a sliding, multi-scale window fashion, such as deformable part-based models.Our approach consists of reformulating the computation of the gradient as a convolution, and making use of properties of the Fourier transform to obtain a speedup factor proportional to the linear filters' sizes. This technique does not rely on the sparsity induced by a specific loss, nor on a stochastic sub-sampling of the training examples.Experiments on the PASCAL VOC benchmark show a speedup factor of more than one order of magnitude compared to a standard exact generic method."
The Bethe Partition Function of Log-supermodular Graphical Models,"Sudderth, Wainwright, and Willsky conjectured that the Bethe approximation corresponding to any fixed point of the belief propagation algorithm over an attractive, pairwise binary graphical model provides a lower bound on the true partition function. In this work, we resolve this conjecture in the affirmative by demonstrating that, for any graphical model with binary variables whose potential functions (not necessarily pairwise) are all log-supermodular, the Bethe partition function always lower bounds the true partition function.  The proof of this result follows from a new variant of the ?four functions? theorem that may be of independent interest."
A Tree Based Classifier for Non-Disjoint Classification,"Traditional approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that best describes it, and that all other labels are equally bad. In this paper we investigate the non-disjoint classification problem. In this scenario, each output label is instead a vector representing a data point's affinity for each of the classes. At test time, the class(es) with the highest affinity can be compared probabilistically. To this end, we propose a new supervised learning classifier underpinned by ensembles of decision trees. We show that by exploiting a multidimensional label vector, we can create less complex classifiers that generalize better at test time, even when a single label is sought. We compare our classifier to ensembles of classification and regression trees on both synthetic and real image data, and show the advantages of our model in terms of probabilistic interpretability and accuracy."
Convergence Analysis and Ef?cient Algorithms for Hyper-Graph Matching,"This paper focuses on the theoretical and algorithmic design for hyper graph matching where the affinity is represented by a tensor. We start with the simple Gradient Assignment (HGA) that works in the discrete domain iteratively, and ?nd its m-point loop convergence property under rather weak conditions, where m is the tensor order. Then Hyper Constrained Gradient Assignment (HCGA) is proposed to avoid such unwanted iteration track, and we show this algorithm will ensure to converge to a unique discrete point. Then we further extend HCGA to the continuous domain: HSCGA which plays the role to the classical Graduate Assignment (HGAGM) as HCGA to HGA. Then we explore the underlying connection between HCGA and its continuous counterpart both theoretically and empirically: under weak conditions, we first prove HGAGM will converge to m-discrete point like HGA, then illustrate HSCGA have the same convergence property with HCGA. These findings build the theoretical connection between the proposed two algorithms. Experimental results on both synthetic and real data consent our theoretical analysis: both algorithms perform competitively to state-of-the-arts. While HCGA outstands due to its working in the discrete space, able to handle ill cases when Hungarian method return multiple solutions, and being an ef?cient and anytime algorithm."
Learning Compositional Model with Attributes,"We present a framework for unsupervised learning of a compositional model of human figures from raw images and assigning attributes - text labels associated with images - to the parts of the model. The objectives of our approach are 1) to learn a meaningful part dictionary and unknown structure that can explain the images well, and 2) to assign given set of attributes onto the parts (nodes) of themodel based on three criteria: uniqueness, precision and consistency. The learning process is structure learning and associated parameter estimation driven by images and attributes. For evaluation, we propose a new dataset containing 1000 images of upper bodies of people with attribute annotations. On this dataset, we show that our learning algorithm discovers meaningful compositional parts of human bodies and that the final model captures meaningful information about the given attributes."
Feature Selection using Partial Least Squares Regression and Optimal Experiment Design,We introduce a supervised feature selection criterion based on linear Partial Least Squares (PLS) regression. We show that an optimal feature subset can be identified by employing the optimal experiment design criterion on the loadings covariance matrix obtained from PLS. Our feature selection criterion can be derived in two different ways. We first derive it using the properties of maximum relevance and minimum redundancy on PLS models and then obtain the same criterion by applying optimal experiment design to PLS. To overcome the computational challenges in evaluating this criterion we use an approximate criterion and still obtain superior results. In our experiments we use the D-optimality criterion which maximizes the determinant of loadings covariance matrix. Experimental evaluation on four datasets demonstrates a consistent and better performance of our Optimal Loadings criterion when compared to other supervised feature selection techniques.
"Towards Bridging the Gaps Between Pattern Recognition, Symbolic Representations, and Online Learning","In neural networks, and more specifically perceptrons that can perform pattern recognition, underlying associations are opaque: the weights are sub-symbolic.  This complicates the ability to use symbolic representations, reuse networks, learn, and learn online.  Methods have been proposed to address these difficulties separately, with various degrees of success.  This work shows that by implementing network dynamics differently, during the testing phase instead of the training phase, connection weights can represent the fixed points or solutions of the network.  This allows the weights to be symbolically relevant: by looking at the weights, fixed-points and symbolic-like components can be inferred.  Moreover, it is easier to learn and modify existing representations, and localizes changes required for online learning.  Although this method is functionally analogous to a single layer linear perceptron and has similar limitations, it is an important step towards realizing neural-symbolic representations and online learning using methods optimized for pattern recognition."
Graduated Non-Convexity and Graduated Concavity Procedure (GNCGCP),"In this paper we propose the graduated nonconvexity and graduated concavity procedure (GNCGCP) as a general optimization framework to approximately solve the discrete optimization problems (usually NP-hard) over the set of partial permutation matrices. As implied by its name, the GNCGCP comprises two sub-procedures, the graduated nonconvexity (GNC) which realizes a convex relaxation and graduated concavity which realizes a concave relaxation. It is proved that the GNCGCP is a type of convex-concave relaxation procedure (CCRP), but with a much simpler formulation which does not involve the convex or concave relaxation in an explicit way. Two typical NP-hard problems, (sub)graph matching and quadratic assignment problem (QAP), are employed to demonstrate the simplicity as well as state-of-the-art performance of the GNCGCP."
Implicit Collaborative Filtering with Random Graphs,"Implicit collaborative filtering harnesses the co-occurrence of edges between user and item vertices in a graph, to interpolate the presence of other edges. We advocate a distribution over random graphs as a novel foundation to collaborative filtering. By mimicking the power law properties of real world networks in a model, we achieve state of the art results on large scale problems. Inference is performed with a mean field approximation, and we show how a tractable procedure for the inclusion of the graph-based prior can be derived by drawing on Monte Carlo samples and stochastic optimization."
Variational Inference in Nonconjugate Models," Mean-field variational inference is a powerful algorithm for approximate posterior inference, but is difficult to derive for nonconjugate probabilistic models. We develop two variational strategies for nonconjugate priors---Laplace variational inference and delta method variational inference---which place minimal conditions on the model. These strategies extend and unify existing methods that were derived for specific models.  We illustrate our approach on the correlated topic models, Bayesian logistic regression, and hierarchical Bayesian logistic regression.  Our experimental results show that our methods work well on real-world datasets."
Venue Discovery,Didn't finish writing the abstract yet...
ForeCA: Forecastable Component Analysis,"Blind source separation (BSS) techniques are often applied to multivariate time series with the goal to obtain better forecasts. But BSS and the need for better forecasts are often treated separately, in the sense that finding an optimally transformed (sub-)space has nothing to do with the aim to predict well. Here I introduce Forecastable \textbf{C}omponent Analysis (ForeCA), a new BSS technique for temporally dependent signals that uses forecastability as the explicit objective in finding an optimal transformation. It separates the signal into the forecastable, $\mathbf{F}$, and the orthogonal white noise space, $\mathbf{F}^{\bot}$. Simulations and applications to financial data show that ForeCA successfully finds signals that can be used to forecast. ForeCA therefore automatically discovers informative structure in multivariate signals."
A Schrodinger formalism for simultaneously computing the Euclidean distance transform and its gradient density,"In this paper, we leverage the well-known Hamilton-Jacobi to Schrodinger connection to present a unified framework for computing both the Euclidean distance function and its gradient density in two dimensions. We introduce a novel Schrodinger wave function for representing the Euclidean distance transform from a discrete set of points. An approximate distance transform is computed from themagnitude of the wave function while the gradient density is estimated from the Fourier transform of the phase of the wave function. In addition to its simplicity and efficient O(N log N)computation, we prove that the wave function-based density estimator increasingly, closely approximatesthe distance transform gradient density (as a free parameter approaches zero) without requiring the true distance function."
Stock Clustering through Equity Analyst Hypergraph Partitioning,"Use of industry classifications in the finance community is pervasive. They are critical to deriving a balanced portfolio of stocks and, more broadly, to risk management. Businesses, academics and government agencies have all researched and developed various schemes with mixed success. Recognizing major brokerages and research firms tend to assign their analysts to cover highly similar companies, we propose a scheme that makes use of stock analyst coverage assignments. Although creating coverage groups of highly similar stocks is not the direct goal of research firms, it may be imperative to their success because increasing similarity in coverage helps maximize synergy and derive the most value per analyst. To create our industry scheme, we construct a hypergraph where vertices represent stocks and hyperedges represent analyst coverage, connecting his/her similar companies. Using no additional information, we perform hypergraph partitioning to form clusters of stocks. Our scheme can produce any number of clusters and can dynamically update as research firms change analyst coverage as opposed to  today's leading industry schemes which have only fixed numbers of industries and require periodic expert review. Can our dynamic scheme match the quality of stock groups from the expert-driven schemes? We make head-to-head comparisons to a leading academic and a leading commercial scheme using a methodology from the finance community that measures the coincidence of stock price movements. We also compare our scheme against a clusterer that creates groups based on past return correlations. Our results rival and often exceed all 3 schemes."
Learning Multiple Concepts with Incremental Diverse Density,"We present a novel method of learning multiple disjunct concepts with diverse density using an incremental approach.  We demonstrate that by maximizing the diverse density over individual target concept points and minimizing the probability of their intersection, concepts can be learned incrementally.  This method reduces the complexity of the algorithm from factorial, with respect to the number of targets, to exponential order.  We demonstrate that this greedy approach successfully learns disjunctive target concepts with competitive classification accuracy on a benchmark multiple instance learning dataset in comparison to other common diverse density approaches.  We also introduce a novel application of the multiple instance learning framework to an emotion recognition task using prosodic and spectral speech features."
A Growing Technique for Construction of Conlitron and Multiconlitron,"Based on the concepts of conlitron and multiconlitron, we propose a growing construction technique for improving the performance of piecewise linear classifiers on two-class problems. This growing technique consists of two basic operations: SQUEEZE and INFLATE, it can make the classification boundary adjusted to improve the generalization ability. Experimental evaluation shows that the growing technique can simplify the structure of a conlitron/multiconlitron effectively by reducing the number of linear functions, largely keeping and even greatly improving the level of classification performances. It would come to play an important role in the subsequent development of piecewise linear learning."
A new metric on the manifold of kernel matrices with application to matrix geometric means,"Symmetric positive definite (spd) matrices are remarkably pervasive in a multitude of scientific disciplines, including machine learning and optimization. We consider the fundamental task of measuring distances between two spd matrices; a task that is often nontrivial whenever an application demands the distance function to respect the non-Euclidean geometry of spd matrices. Unfortunately, typical non-Euclidean distance measures such as the Riemannian metric $\riem(X,Y)=\frob{\log(X\inv{Y})}$, are computationally demanding and also complicated to use. To allay some of these difficulties, we introduce a new metric on spd matrices: this metric not only respects non-Euclidean geometry, it also offers faster computation than $\riem$ while being less complicated to use. We support our claims theoretically via a series of theorems that relate our metric to $\riem(X,Y)$, and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances."
Multi-Label Multi-View Laplacian Hashing,"With the advent of the Internet, large scale datasets are available.The data may be high dimensional,  represented by multiple features,and associated with more than one concepts. Hashing is an effectivestrategy for dimensionality reduction and efficient nearest neighborsearch in massive datasets. We propose a novel method to seekcompact hash code that allows efficient retrieval with multi-labelmulti-view data. Based on multi-graph Laplacian, we learn theoptimal combination of heterogeneous features to effectivelydescribe multi-view data, which exploit the feature correlationsbetween different views. We obtain  the hash embedding whichpreserves the neighborhood context in the original spaces, and thesemantic embedding (i.e., multi-label vectors) at the same time.Both labeled and unlabeled data are employed for learning, andinter-label correlations are sufficiently captured to improve theperformance of hash learning with multi-label data. The experimentalevaluation on real-world datasets demonstrates promising resultsthat validate our method."
Learning a Discriminative Isotropic Space from Labeled and Unlabeled Data,"Euclidean distance measure is computationally simple and commonlyused in the task of classification. However, it does not capitalizeon any discriminant information from training samples, which iscrucial to classification performance. Moreover, Euclidean distanceis invalid when the input space is not isotropic, which often occursin many practical applications. In this paper, we learn aDiscriminative Isotropic Space (DIS) from both labeled and unlabeldata to improve classification accuracy as well as generalizationability. Intra-class compactness and inter-class separability areachieved simultaneously in the learned space, so it isdiscriminative and benefits the task of classification. The learnedspace looks as the same in every direction as possible, so Euclideandistance is suitable to measure dissimilarities between samples insuch space. Our regularized objective function implicitly minimizesmutual information between input space and transformed space, whichis reasonable from the perspective of information theory. Our methodis scalable and can be applicable to large dataset. There is nolocal optimum problem in our algorithm since the objective functionis convex and its closed form solution can be easily obtained,therefore the proposed method is more effective and more efficientthan the alternative. Experiments on real data sets demonstrate theefficacy of our method."
Efficient Regularized Isotonic Regression,"Isotonic regression is a nonparametric approach that fits the model subject to a set of isotonic constraints. In structured variable selection and estimation, isotonic constraints can also be used to capture the hierarchical relationships among variables according to the heredity principle. However, isotonic regression solvers cannot handle regularizers (e.g., sparsity regularizers) and constraints (e.g., non-negative constraints) on the parameters, which on the other hand are often important ingredients in learning with structured sparsity. In this paper, we propose a general optimization formulation that addresses these limitations. Efficient solvers are proposed for regression with both tree-ordered and DAG-ordered isotonic constraints. Experiments on a number of large data sets show that they are fast and accurate. Using together with proximal gradient methods, hierarchy information can now be flexibly incorporated, leading to better structured sparse models."
Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification,"In hierarchical classification, the prediction paths may be required to always end at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes. However, while there have been a lot of MLNP methods in hierarchical multiclass classification, performing MLNP in hierarchical multilabel classification is much more difficult. In this paper, we propose a novel MLNP algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and DAGs. We show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm. Moreover, this can be further extended to the minimization of the expected symmetric loss. Experiments are performed on a number of real-world data sets with tree- and DAG-structured label hierarchies. The  proposed method consistently outperforms other hierarchical and flat multilabel classification methods."
Topical Structural Analysis on Social Communications,"The popularity of online social networks has lowered the barrier of online communications, which results in massive number of users using the networks for interaction and friendship making.To characterize the user positions and message content generated by users of different positions, we propose the Dirichlet Allocation Blockmodels (DABM) for topical structural analysis.DABM model allows each pair of users to generate message content following the topic distribution conditioned on the social positions of the two users.Compared with the earlier model, DABM allows users of the same positions to have some variability in their message topic distribution.We evaluate both DABM and the earlier model on tweets generated by a set of Twitter users connected by follow links, and show that DABM achieves better likelihood and perplexity than the earlier model."
An Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks,"We present an axiomatic construction of hierarchical clustering in asymmetric networks where the dissimilarity from node $a$ to node $b$ is not necessarily equal to the dissimilarity from node $b$ to node $a$. The theory is built on the axioms of value, influence, and transformation. The Axiom of Value says that in a two-node network the nodes cluster at resolution equal to the maximum dissimilarity between them. The Axiom of Influence says that no clusters are formed at resolutions that do not allow bidirectional paths to be formed. The Axiom of Transformation states that if we consider a network and do not increase any pairwise dissimilarity, the level at which two nodes become part of the same cluster is not larger than the level at which they were clustered together in the original network. Two asymmetric hierarchical clustering methods that abide to these axioms are derived. Reciprocal clustering requires clusters to form through arcs that are similar in both directions. Nonreciprocal clustering allows clusters to form through cycles of small dissimilarity. We further show that any clustering method that satisfies the axioms of value, influence, and transformation lies between reciprocal and nonreciprocal clustering in the sense that all other methods cluster two points together at resolutions larger than nonreciprocal clustering and smaller than reciprocal clustering. To conclude, we apply this theory to the formation of circles of trust in social networks."
Adaptive Sparseness for function learning using Parametric and Nonparametric Bayesian LASSO Methodology,"One of the most important problems in supervised learning is that of accurately inferring functional mappings based on (typically)  high dimensional training data.  Learning is accomplished by estimating parameters which weight the features used in the inference.  To achieve good generalization,  it is considered desirable, while retaining accuracy, to obtain sparse solutions to this problem (i.e. solutions which properly select the parameters which need to be estimated). The lasso methodology has been used extensively  to solve this problem;  it controls both complexity and accuracy by adding a regularization term to the least squares fit. EM methodology provides non-adaptive solutions which require manual control of complexity.  Parametric Bayesian approaches to the lasso adopt parametric priors to learn the function parameters;  these serve to adaptively control for both the complexity and accuracy of the learned function.   Typically, the function parameters have complicated relationships with one another and with the data.  Parametric priors frequently fail to accurately learn these relationships.Learning in the aforementioned settings can be handled using prior distributions which stipulate mixture models with a potentially infinite number of components;  these are known as nonparametric Bayesian priors.  Dirichlet process priors are a particular example of a nonparametric Bayesian prior.  We propose a Hierarchical Bayesian function learning algorithm which employs Dirichlet process priors to infer functional mappings. Model comparisons between algorithms employing Dirichlet process priors, models employing parametric priors, and models employing expectation-maximization algorithms demonstrate the superiority of the former.  An example illustrates the advantages and disadvantages which accrue from using these models for function learning. "
Efficient Incremental Feature Learning in Manufacturing Environments,"Most heavy duty manufacturing operations consist of hundreds of steps, where multiple measurements are taken at each step to monitor the qualityof the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate stepscan prove to be extremely useful in enhancing productivity. In this paper, we provide an approach for learning regression models in an environmentwhere features (i.e. measurements) and datapoints (i.e. individual products) are added incrementally. At each step, any finite number of features maybeadded and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares,weighted least squares, generalized least squares and ridge regression, but also for generalized linear models and lasso regression that useiterated re-weighted least squares for maximum likelihood estimation. For arbitrary regression methods, even a relaxation of the approach is noworse than using the model from a previous step or using a model that learns on the additional features and optimizes the residual of the modelat the previous step. We further validate these claims through experiments on a real industrial dataset."
Continuous-weight neural networks for learning from sparse continuous dictionary representations,"Sparse feature representations from overdetermined dictionaries are commonly seen in digital signal processing fields, including computer vision and audio processing. Examples of such representations are sinusoidal models, wavelet representations, and interest point sets. Due to the high dimension of the dictionaries involved and the variable number of features per representation, current machine learning algorithms are not designed to train well on these feature sets. In this paper, we present continuous-weight neural networks, a novel machine learning paradigm with universal continuous approximation capabilities that can directly train on these sparse dictionary representations. The algorithm thus allows for sparser feature sets in numerous multimedia applications, and thus improved learning quality in many of these cases, as feature sparsity is strongly correlated with learning robustness. This paper additionally presents a class of training algorithms for the paradigm by demonstrating how to efficiently compute the gradient of the parametric model."
Balanced Relative Margin Machines --- Closing the Gap Between Fisher's Discriminant and SVM Classification,"We approach the class of relative margin classification algorithms from the mathematical programming perspective. In particular, we propose a Balanced Relative Margin Machine and then extend it by a 1-norm regularization. Subsequently, we show the strong relations of the methods to SVMs as well as toregularized discriminant analysis techniques."
Application of moving variance calculation to EMG based movement onset detection,"  Adaptation of human-machine interaction devices by means of physiological data requires online analysis.  To save memory and resources for realtime time series data processing  as, e.g. movement detection based on electromyographic (EMG) data,  new update formulas are needed,  when calculating mean and variance of the signal.  Applications were presented, where the length of the relevant time frame is fixed  and moving average and variance have to be calculated in realtime.  This differs from incremental calculations, which have been largely analyzed.  Formulas for an efficient calculation are introduced and applied on  synthetic and EMG data to show the benefits."
The representer theorem for Hilbert spaces: a necessary and sufficient condition,"A family of regularization functionals is said to admit a linear representer theorem if every member of the family admits minimizers that lie in a fixed finite dimensional subspace. A recent characterization states that a general class of regularization functionals with differentiable regularizer admits a linear representer theorem if and only if the regularization term is a non-decreasing function of the norm. In this paper, we improve over such result by replacing the differentiability assumption with lower semi-continuity and deriving a proof that is independent of the dimensionality of the space."
Acquiring Dynamical Primitives from Unlabeled Demonstrations through Parameter Space Clustering,"In this paper we introduce a method to learn multiple dynamical systems from unlabeled data. This problem has applications in a wide range of domains where labeled data is expensive or even impossible to obtain. Applications include multi-task learning from demonstration in robotics, learning from multiple teachers that may have different strategies,  behavior modeling for surveillance or human motion interpretation. One of the difficulties of this problem is that, due to multiple objectives, trajectories can be very mixed in measurement space. Based on mixture models, we propose to cluster trajectories in a latent representation of potential functions. These potential functions are parameterized as linear combinations of a large number of features. We derive two algorithms based on Expectation Maximization for a known number of clusters and Dirichlet Processes to estimate this number from data. In both cases, we enforce sparsity to perform feature selection while clustering the data. We evaluate the proposed method using 2D synthetic trajectories and real 3D human motion data."
Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress,"Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such asRmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which driveexploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions."
Supervised Learning with Similarity Functions,"In this paper we address the problem of general supervised learning where data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multi-class classification problems. Inspired by an existing work on classification with similarity functions [Balcan-Blum '06], we propose a general ``goodness'' criterion for similarity functions w.r.t. a given supervised learning task. Our definition is generic enough to handle any supervised learning task and also subsumes the goodness condition of [Balcan-Blum '06]. We then adapt a landmarking technique by [Balcan-Blum '06, Balcan et al '08] to provide efficient algorithms for supervised learning using ``good'' similarity functions. In particular, we consider three important supervised learning problems : a) real-valued regression, b) ordinal regression and c) ranking. For each of these problems we show that our goodness definition satisfies the following two key properties : 1) Utility : given good similarity functions, our algorithms guarantee bounded generalization error with polynomial sample complexities, 2) Admissibility : our goodness definitions are flexible enough to at least admit all good PSD kernels; the goodness of a PSD kernel being defined according to standard definitions in literature. Furthermore, for the case of real-valued regression, we provide a natural goodness definition that when used in conjunction with a recent work on sparse vector recovery [Shalev-Schwartz '10], guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs. "
Bilinear Low-Rank Matrix Hashing with Rank-Sensitive Block Permutation,"Conventional locality-sensitive hashing only handles the inputs in the forms of vectors or sets. This paper explores a new topic of matrix hashing. Search of nearest neighbor data in the matrix form can be found in many applications such as detection of image duplicates and geographical distributions. Reducing the task to 1D vector search may incur significant information loss. Our contributions are two-folds: first, under mild assumptions on the matrix, we investigate the relationship between the matrix rank and the difficulty of nearest matrix search. Based on the notation of \emph{random matrix similarity}, we show that low-rank matrices are often more favorable in such a task. Second, we compare different schemes on matrix hashing. Among them, the linear hashing scheme is a natural extension from the conventional vector field to matrices. However, for matrices of size $n \times m$, its complexity is $O(n m)$ for both computation and storage, which is unaffordable for large matrices.To solve this problem, this paper proposes a bilinear matrix hashing scheme which greatly reduces the complexity by exploiting the matrix singular structures. We present very interesting observations on bilinear matrix hashing: although the efficacy of the scheme is mainly determined by the matrix low-rankness, its performance will be enhanced when matrix blocks are permutated to increase the rank. Therefore the final bilinear scheme involves a very interesting and practical tradeoff between block-level high-rankness and inner-block low-rankness. We conduct an in-depth study of this issue based on kurtosis analysis and provide a rank-sensitive algorithm for learning a universal block permutation. Extensive experiments are presented to corroborate the effectiveness of the proposed matrix hashing scheme."
Cocktail Party Processing via Structured Prediction,"While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random fields (CRFs) to classify speech dominance withineach time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises."
Robustness and risk-sensitivity in Markov decision processes,"We uncover relations between robust MDPs and risk-sensitive MDPs.  The objective of a robust MDP is to minimize a function, such as the expectation of cumulative cost, for the worst case when the parameters have uncertainties.  The objective of a risk-sensitive MDP is to minimize a risk measure of the cumulative cost when the parameters are known.  We show that a risk-sensitive MDP of minimizing the expected exponential utility is equivalent to a robust MDP of minimizing the worst-case expectation with a penalty for the deviation of the uncertain parameters from their nominal values, which is measured with the Kullback-Leibler divergence.  We also show that a risk-sensitive MDP of minimizing an iterated risk measure that is composed of certain coherent risk measures is equivalent to a robust MDP of minimizing the worst-case expectation when the possible deviations of uncertain parameters from their nominal values are characterized with a concave function."
Group Bridge Regression is Beta Uniformly Stable,"Sparsity and stability are the desired properties of a machine learning algorithm, especially in high-dimensional data problems. The group Lasso is a sparsity promoting method that exploits grouped variables and has been studied intensively in the literature. However, it has been recently discovered that like the Lasso, the group Lasso does not possess the desirable stability property which is used to establish generalization. As sparsity and uniform stability are conflicting goals, an immediate question of the optimal trade-off is raised: What would be a stable algorithm that trades off sparsity well? In the context of regression with grouped variables, we show that the existing bridge regression method already provides a natural trade-off between stability and sparsity. We show that group bridge regression, where in the group sparsity is controlled via its bridge order, is uniformly beta-stable and thus generalizes. Numerical studies on high-dimensional synthetic and splice detection problems demonstrate that group bridge regression is competitive to the group Lasso in  machine learning contexts."
Dynamical And-Or Graph Learning for Object Shape Modeling and Detection,"This paper studies a novel discriminative part-based model to represent and recognize object shapes with an ``And-Or graph''. We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP, is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to well handle large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization.  We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches."
Bayesian Score for Orders of Variables,"Analysing high-dimensional data is hard. Search spaces of many optimizationproblems grow exponentially with respect to the dimension of data, while at thesame time, a large number of data points is needed in order to havestatistically solid results.  One way to approach the curse of dimensionalityis to impose some structure on the variables. In this paper we study linearorders between variables, a natural structure for many datasets.  The goal ofthis paper is measure the quality of the order for the variables inmulti-dimensional data. Such a score will help us to decide if the order athand is genuinely significant. Our score will be based on the intuition thatthe order should have a good score when variables that depend on each other areclose to each in the given order.More specifically, given a dataset and an order we consider a set of Bayesiannetworks that depends on this order. If the order is good, that is, if thedependent variables are close to each other, then these models will have a goodposterior probability, which we will estimate with Bayesian InformationCriterion (BIC). Since we are not interested in the actual models, wemarginalize them out.  Since there are exponential number of such models forone given order, we will give a non-trivial technique for computing the orderin polynomial time. We also show how to significantly improve the computationaltime if we allow some minimal error in the score."
Some Results About the Vapnik-Chervonenkis Entropy and the Rademacher Complexity,"This paper deals with the problem of identifying a connection between the Vapnik-Chervonenkis (VC) Entropy, a notion of complexity introduced by Vapnik in his seminal work, and the Rademacher Complexity, a more powerful notion of complexity, which has been in the limelight of several works in the recent literature. In order to establish this connection, we refine some previously known relationships and derive a new result. Our proposal allows computing an admissible range for the Rademacher Complexity, given a value of the VC-Entropy, and vice versa, therefore opening new appealing research perspectives in the field of assessing the complexity of an hypothesis space. "
Distributed Non-Stochastic Experts,"We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator. At each time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon T, while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal O(\sqrt{log(n)T}) regret bound at the cost of T communication. (ii) No communication: Each site runs an independent copy ? the regret is O(\sqrt{log(n)kT}) and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than \sqrt{kT} and communication better than T. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret O(\sqrt{k^{5(1+\epsilon)/6} T}) and communication O(T/k^\epsilon), for any value of \epsilon in (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off."
Learning Image Descriptors with the Boosting-Trick,"In this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection. The main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes. This representation can  be improved using machine learning, however, past approaches  have been limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.  While kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive. We propose  to use the {\em  boosting-trick}  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space. The non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance."
Squared-loss Mutual Information Regularization,"The information maximization principle, which prefers classifiers that maximize an information measure between data and labels, is a useful probabilistic alternative to the low-density separation principle. In this paper, we specify the squared-loss mutual information (SMI) as the information measure to be maximized and propose SMI regularization (SMIR) for semi-supervised classification. SMIR offers all of the following four abilities to semi-supervised algorithms: analytical solution, out-of-sample and multi-class classification, and probabilistic output. Furthermore, SMIR results in learning algorithms with data-dependent risk bounds that even incorporate the information of unlabeled data. Experiments demonstrate that SMIR compares favorably with state-of-the-art information-theoretic regularization approaches in terms of both accuracy and computational efficiency. "
Modeling the Forgetting Process using Image Regions,"While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten over time has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten over time, using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. "
Object Focused Q-learning for Autonomous Agents ,"We present Object Focused Q-learning (OF-Q), a novel reinforcement learning algorithm that can offer exponential speedups over classic Q-learning on domains composed of nearly-independent objects. OF-Q treats the state space as a collection of different objects organized into different object classes. Our key contribution is to estimate the risk of different objects by learning non-optimal Q-functions that we incorporate into a control policy. We compare our algorithm to traditional Q-learning and previous arbitration algorithms in two domains, including a version of Space Invaders."
User Distances in Composite Social Networks,"An important challenge in social network analysis is how to measure users' distances or latent similarity as a single measure. It is the basis for link prediction, community detection, social marketing, etc. Due to the sparsity of data, where each user may just have a few connected friends, it is hard to effectively learn distance measures for any given pair of users in a single network. Nowadays however, people engage in multiple social networks, such as Facebook, Twitter, LinkedIn, etc., where these networks form a composite social network. To alleviate the data sparsity problem for a given single network, we propose a transfer metric learning approach to collectively exploit the knowledge from multiple networks, to extract related and richer knowledge through an optimization and boosting-based framework. Then, we use this knowledge for user distance modeling in a target network. It projects and combine social information, behaviors and profile attributes, measured in different magnitudes, in an embedding space that preserves important community information and network structure. We empirically evaluate the effectiveness of the constructed user distance measure on link prediction - an important social modeling task, and state-of-the-art methods are typically based on user distances. Empirical studies demonstrate that the proposed approach significantly improves the link-prediction precision over several state-of-the-art baselines."
A Generalized Theory of PAC-Learning,"In this article we prove that probably approximately correct (PAC) learning is guaranteed under a more general assumption that the loss function has finite variance under the sample distribution. We establish our theory based on the existence of a metric between hypothesis, to which the loss function is Lipschitz continuous. These results will give hope for learning using unbouded loss functions and real-valued hypotheses, such as energy-based models, unbouded loss regression and neural networks. More importantly, even with such generalization, asymptotic bounds as good as previous formulations of PAC-learning have been achieved."
Compressed learning: learning in the smallest capacity machine,"Supervised  learning focuses on using finite input-output samples topredict the intrinsic relationship  between  input and output.Several methods such as neural networks, kernel method and redundantdictionary learning have been developed to tackle the problem andall of them have been proved to be universal approximants anduniversal consistent learners. However, the good approximation andgeneralization capabilities are established on the basis ofimplementing high computational complexity algorithms in largecapacity machines, which results extremely high computationalburdens in the learning processes. Motivated by the well knownKolmogorov width theory, we deduce optimal linear sparserepresentations for a large number of priors. Taking the set ofsparse representations as the desired machine, we construct smallestcapacity machines which achieves the optimal learning rate. Based onthis, we propose a new learning methodology called the compressedlearning that implements learning in the smallest capacity machinewith simple linear problem algorithms. Our analysis reveals that thecompressed learning is a high quality method that possesses palmaryapproximation capability, prominent generalization capability andlow computational burden."
Part-segment Features for Body Pose Estimation,"We propose part-segment (PS) features for estimating an articulated pose in still images. The proposed PS features are developed to evaluate image likelihood of each body part (e.g. head, torso, and arms) robustly to background clutter and nuisance textures on the body and clothing. While general gradient-based features (e.g. HOG) might include many nuisance responses, the PS features represent only the boundaries of the body parts by iterative binary segmentation with updating shape prior on each part. The PS features are fused complementarily with gradient features using discriminative training and adaptive weighting. Comparative experiments with public datasets demonstrate improvement in pose estimation by the PS features compared with conventional features."
Mean Shift by Hebbian Learning for the L0-norm based Sparse Coding Problem,"It is well known that the emergence of Gabor-like receptive fields of simple cells in the visual cortex can be predicted by sparse coding, which has been validated to be a general coding strategy for sensory systems. Until now, the neural mechanism of the learning procedure is far to be understood. In this paper, a novel mean shift algorithm using Hebbian learning for the L0-norm based sparse coding problem is proposed. Different from other studies on sparse coding, our work do not consider the coefficients of basis functions but model the selection of basis functions. We perform an analysis on the spatial distribution of input samples and conclude that the basis functions are related to the local maxima of the distribution. Detailed theoretical investigation affirms this conclusion, showing that the sparse coding problem with the L0-norm is essentially one of mode detection and the basis functions are the modes of the kernel density estimate. The mean shift algorithm is presented for mode detection, and its updating rule is proved to be Hebbian. Experimental results demonstrate the robustness of the algorithm in producing basis functions well tuned for orientation as well as spatial frequency."
Incremental Subspace Learning for Unsupervised Domain Adaptation Using Manifold Optimization,"Developing recognition methods to handle cases in which the distribution of data changes between the training and testing phases of model building is a common problem in many fields. In computer vision, it is often the case that both well-defined geometric changes can be estimated, such as rotation in images, as well as more abstract transformations, such as changes in camera quality. It has recently been proposed that utilizing an incremental framework for adapting between two different domains is beneficial as it is able to encapsulate much of the change in the distribution between training and testing sets, regardless of the domain shift. In this paper, we adopt a general method for learning intermediate subspaces between training and testing domains on which we build models to perform both regression andclassification. We show that utilizing this method to obtain intermediate subspaces is more beneficial than similar methods both from a practical and a theoretical standpoint, as the proposed method admits an attractive interpretation in terms of posing it as an optimization problem. The method is used to improve performance in object recognition tasks when the shift in domain is not a geometric one and in age estimation tasks both when domain shifts are geometric and nongeometric."
Data split strategies for evolving predictive models,"A conventional textbook prescription for building good predictive models is to split the data into three parts: training set (for model fitting), validation set (for model selection), and test set (for final model assessment).  Predictive models can potentially evolve over time as developers improve their performance either by acquiring new data or improving the existing model. The main contribution of this paper is to discuss problems encountered and propose various workflows to manage the allocation of newly acquired data into different sets in such dynamic model building and updating scenarios. We propose three different workflows (parallel dump, serial waterfall, and hybrid) for allocating new data into the existing training, validation, and test splits. Particular emphasis is laid on avoiding the bias due to the repeated use of the existing validation or the test set. "
Multimodal similarity-preserving hashing,"We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable.The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches, our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks."
Sparse Locality Preserving,"In this paper, we introduce a new subspace learning framework called Sparse Locality Preserving (SLP). Compared with the conventional methods considering global data structure, e.g., PCA, LDA, SLP aims at preserving the local neighborhood structure on data manifold and provides a more accurate data representation via locality sparse coding. In addition, it removes the common concerns of many local structure based subspace learning methods e.g., Local Linear Embedding (LLE), Neighborhood Preserving Embedding (NPE), that how to choose appropriate neighbors. SLP adaptively select neighbors based on their distances and importance, which is less sensitive to outliers than NPE. Moreover, the dual-sparse processes, i.e., the locality sparse coding, and sparse eigen-decomposition in graph embedding yield a noise-tolerant framework. Finally, SLP is learned in an inductive fashion, and therefore easily extended to different tests. We exhibit experimental results on several databases and demonstrate the effectiveness of the proposed method."
Active sampling as a curriculum for model selection,Conventionally active learning has been used to query the best example to label in order to reduce the labeling cost. In this paper we show how active learning (uncertainty sampling in particular) can be used as a curriculum strategy for nested model selection problems. We exploit the phase transition like phenomenon observed in active learning for misspecified models to select the number of components in mixture models.
Annotation models for crowdsourced ordinal labels,"In supervised learning scenarios when acquiring good quality labels is hard, practitioners often resort to getting the data labeled by multiple noisy annotators. Various methods have been proposed to estimate the consensus labels for binary and categorical labels bycorrecting for the bias of annotators. A commonly used paradigm to annotate instances when the labels are inherently subjective is to use ordinal scales. Theannotator is asked to rate an instance on a certain discrete ordinal scale. In this paper we propose annotator models based on Receiver Operating Characteristic (ROC) curve analysis to consolidate the ordinal annotations from multiple annotators.  The models lead to simple Expectation-Maximization (EM) algorithms that estimate both the consensus labels and annotator performance jointly. Experiments on data from different domains indicate that the proposed algorithm is superior to the commonly used majority voting rule. The ROC based models have an added advantage that the annotators can be ranked using the area under the estimated ROC curve. "
MILEAGE: Multiple Instance LEArning with Global Embedding," Multiple Instance Learning (MIL) methods generally represent each example as a collection of  individual instances, whereastraditional learning methods typically extract a global featurevector for the whole content of each example. Substantial priorresearch work has been proposed to solve  MIL problems. However, MILmethods do not always perform better than traditional learningmethods in all the cases. Limited research work has studied thisissue. This paper proposes a novel framework -- \emph{MultipleInstance LEArning with Global Embedding (MILEAGE)}, in which theglobal feature vectors for traditional learning methods areintegrated into the MIL setting. MILEAGE can leverage the benefitsderived from both learning settings. Within the proposed framework,a large margin method is formulated. In particular,  the proposedmethod adaptively tunes the weights on the two different kinds offeature representations (i.e., global and multiple instance) foreach example and trains the classifier simultaneously. Analternative algorithm is proposed to solve the resultingoptimization problem, which extends the bundle method to thenon-convex case. Some important properties of the proposed method,such as the convergence rate and the generalization error rate, areanalyzed. A series of experiments on both the image and textclassification tasks have been conducted to demonstrate theadvantages of the proposed method over several state-of-the-artmultiple instance and traditional learning methods."
Automatic Feature Induction for Stagewise Collaborative Filtering,"Recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms."
Learning Heteroscedastic Models via SOCP under Group Sparsity,"Sparse estimation methods based on  $\ell_1$  relaxation, such as the Lasso and the Dantzig selector,are powerful tools for estimating high dimensional linear models. However, in order to properly tune these methods, the variance ofthe noise is often required. This constitutes a major obstacle in applying these methods in several frameworks---such astime series, random fields, inverse problems---for which noise is rarely homoscedastic and the noise level is hard to know in advance.In this paper, we propose a new approach to the joint estimation of the conditional mean andthe conditional variance in a high-dimensional (auto-)regression setting. An attractive feature of the proposed estimator isthat it is efficiently computable even for very large scale problems by solving a second-order cone program (SOCP). We present numericalresults assessing the performance of the proposed procedure. We also establish non-asymptotic risk bounds which are nearly asstrong as those for original $\ell_1$-penalized estimators: the Lasso, the Dantzig selector and their grouped counterparts."
Selective Labeling via Error Bound Minimization,"In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods."
Textual features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks."
Volume Regularization for Binary Classification,"We introduce a large-volume box classification for binary  prediction, which maintains a subset of weight vectors, and  specifically axis-aligned boxes. Our learning algorithm seeks for a  box of large volume that contains ``simple'' weight vectors which  most of are accurate on the training set. Two versions of the  learning process are cast as convex optimization problems, and it  is shown how to solve them efficiently.  The formulation yields a  natural PAC-Bayesian performance bound and it is shown to minimize a  quantity directly aligned with it. The algorithm outperforms SVM and  the recently proposed AROW algorithm on a majority of $30$ NLP  datasets and binarized USPS optical character recognition datasets."
Towards Massive Multi-Way Classification: Structured Sparse Output Coding,"Multi-way classification with massive classes is a practical and challenging problem. In this paper, we propose structured sparse output coding, a principled way for massive multi-way classification, where a sparse output coding matrix is learned to maximize codeword separation and accuracy of each bit predictor. Moreover, we provide a concave-convex procedure based algorithm for the resultant optimization problem, which solves a series of l1 regularized convex optimization problems under linear constraints, using dual proximal gradient method. Experimental results demonstrate the effectiveness of our proposed approach."
Image Denoising and Inpainting with Deep Neural Networks,"We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method achieves state-of-the-art performance in the image denoising task. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning."
Low Rank Tensor Completion with Spatio-Temporal Consistency,"Video completion is a computer vision technique to recover the missing values in video sequences by filling the unknown regions with the known information.  In recent research, tensor completion, a generalization of matrix completion for higher order data, emerges as a new solution to estimate the missing information in video with the assumption that the video frames are homogenous and correlated.  However, each video clip often stores the heterogeneous episodes and the correlations among all video frames are not high. Thus, the regular tenor completion methods are not suitable to recover the video missing values in practical applications.  To solve this problem, we propose a novel spatially-temporally consistent tensor completion method for recovering the video missing data. Instead of minimizing the average of the trace norms of all matrices unfolded along each mode in a tensor data, we introduce a new smoothness regularization along video time direction to utilize the temporal information between consecutive video frames. Meanwhile, we also minimize the trace norm of each individual video frame to employ the spatial correlations among pixels. Different to previous tensor completion approaches, our new method can keep the spatio-temporal consistency in video and do not assume the global correlation in video frames. Thus, the proposed method can be applied to the general and practical video completion applications. Our method shows promising results in all evaluations on 3D biomedical image sequence and video benchmark data sets.  "
Max-Margin Structured Output Regression for Spatio-Temporal Action Localization,"Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because one needs to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efficient Max-Path search method, thus makes it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method significantly outperforms the state-of-the-art methods."
Matching Objects across the Textured-Smooth Continuum,"The problem of 3D object recognition is of immense practical importance and potential, with the last decade witnessing a number of breakthroughs in the state of the art. Most of the past work focused on the matching of textured objects using local appearance descriptors extracted around salient points in an image. The recently proposed bag of boundaries method was the first to address directly the problem of matching smooth objects using image based boundary features. However, no previous work has attempted to achieve a holistic treatment of the problem by jointly using textural and shape features which is what we describe in the present paper. Due to the complementarity of the two modalities (texture and shape), we combine the matching scores of textural and shape based representations by weighted summation. The optimal weighting is learnt in a data specific manner by optimizing discriminative performance on synthetically distorted data. For the textural description of an object we adopt a representation in the form of a histogram of SIFT based visual words. Similarly the apparent shape of an object is represented by a histogram of discretized features capturing local shape. Unlike previous work which uses an implicit, image based shape descriptor, we propose a more compact descriptor based on the local profile of boundary normals' directions. This descriptor is extracted at salient object boundary loci and at the corresponding characteristic scale, both detected automatically. On a large database of a diverse set of objects, the proposed method is shown to significantly outperform both purely textural and purely shape based approaches for matching across viewpoint variation. The advantage was particularly significant in the cases of large viewpoint changes between training and query data, when the correct rank-1 recognition rate was nearly twice that achieved using either of the modalities in isolation."
Estimating the Maximum Expected Value: An Analysis of (Nested) Cross Validation and the Maximum Sample Average,"We investigate the accuracy of the two most commonestimators for the maximum expected value ofa general set of random variables: a generalizationof the maximum sample average, and cross validation.No unbiased estimator exists and we show that it isnon-trivial to select a good estimator without knowledgeabout the distributions of the random variables.We investigate and bound the bias and variance of theaforementioned estimators and prove consistency.The variance of cross validation can besignificantly reduced, but not without riskinga large bias. The bias and variance ofdifferent variants of cross validation are veryproblem-dependent, and a wrong choice canlead to very inaccurate estimates."
Scalable Manifold Learning,"High computational costs of manifold learning make its application prohibitive for large point sets. A common strategy to overcome this problem is to sample a subset of points, called landmarks, on which the dimensionality reduction is performed and to reconstruct the embedding of all points using the Nystr?m method. In this paper, we address the two main challenges that arise in this setup. First, the selected subset of landmarks in non-Euclidean geometries must result in a low reconstruction error. Second, the nearest neighbor graph construction on sparsely sampled subsets must be robust and approximate the original data well. We propose an extension for sampling from determinantal distributions on non-Euclidean spaces by opearting on the geodesic distance on the manifold. Since current determinantal sampling algorithms have the same complexity as manifold learning, we propose an efficient approximation running in $\mO(ndk)$. We achieve excellent results with the proposed algorithm for manifold sampling by restricting the probability update to local neighborhoods. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Mahalanobis distance increases the robustness of dimensionality reduction on sparsely sampled manifolds. "
Non-parametric Approximate Dynamic Programming via the Kernel	Method,"This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful, dimension-independent approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our non-parametric procedure is competitive with parametric ADP approaches."
Multiclass Active Learning with Hierarchical-Structured Embedded Variance,"  We consider the problem of multiclass active learning where the relationship of the labels are represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to exploit the hierarchical structure of the label tree as well as the characteristics of the training data to select the most informative data for human labeling.  This goal can be achieved by a novel embedding-based approach called hierarchical-structured embedded variance, which learns an embedding of the labels that both preserves the structure of the label tree and reflects the characteristics of the training data. We show that the proposed approach is a generalization of entropy-based and cost-based uncertainty measure. We also demonstrate that notable improvement on the performance can be achieved with the proposed approach on synthetic and benchmark datasets."
Learning global properties of scene images from conditional correlational structure,"Scene images with similar spatial layout properties often display characteristic statistical regularities on a global scale. In order to develop an efficient code for these global properties that reflects their inherent regularities, we train a hierarchical probabilistic model to infer conditional correlational information from scene images. Fitting a model to a scene database yields a compact representation of global information that encodes salient visual structures with low dimensional latent variables. Using perceptual ratings and scene similarities based on spatial layouts of scene images, we demonstrate that the model representation is more consistent with perceptual similarities of scene images than the metrics based on the state-of-the-art visual features. "
Truncation-free Online Variational Inference for Bayesian Nonparametric Models," We present a truncation-free online variational inference algorithm for Bayesian nonparametric models. Unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly. Our experiments for Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms."
Consistency Analysis of  Empirical MEE Algorithm,"In this paper we study the consistency of the empirical minimum error entropy (MEE) algorithm for regression learning.Two types of consistency are studied. The error entropy consistency,which requires the error entropy of the learned functionapproximates the minimum error entropy, is shown to be always trueif the bandwidth parameter tends to 0 at an appropriate rate. Theregression consistency, which requires the learned functionapproximates the regression function, however, is a complicatedissue. We prove that the error entropy consistency implies theregression consistency for homoskedastic models where the noise isindependent of the input variable. But for heteroskedastic models, acounter-example is used to show the two types of consistency do notcoincide. A surprising result is that the regression consistency isalways true, provided that the bandwidth parameter tends to infinityat certain rates. This result, however, contradicts the motivationof MEE principle because the minimum error entropy is believed to benot approximated well with this choice of bandwidth parameter."
Multi-Granularity Image Categorization via Probabilistic Decoding,"Modern image data sets organize classes in a hierarchical taxonomy structure, such as a tree or DAG. Usually formulated as a multi-way classification, classical image categorization approaches can only predict leaf labels in such hierarchy. In this paper, based on the error correcting output coding formulation for multi-way classification, we propose a probabilistic decoding approach for both single-granularity, where only leaf level labels are allowed, and multi-granularity image categorization, which could generate internal labels from the taxonomy hierarchy if it is uncertain on leaf level labels. Experimental results demonstrate the effectiveness of our proposed image categorization approach on both single-granularity scenario and multi-granularity case."
3D Gaze Concurrences from Head-mounted Cameras,"A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanating from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the fixed relationship between the primary gaze ray and the head-mounted camera pose. The resulting gaze model enables us to build a social saliency field in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent mode-seeking in the social saliency field. Our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth."
ROST: Realtime Online Spatiotemporal Topics for Navigation Summaries and Surprise Detection," We describe a novel online topic modeling framework to compute a low dimension descriptor of visual observations made by a mobile robot, which is sensitive to the structural and thematic changes in the environment. Our approach is designed to run in realtime, and is suitable for long term execution on a robotic platform. Using this image descriptor we build online anytime summaries consisting of surprising observations experienced by a robot thus far. The observations in the summary are chosen such that they cover the set of all observations in topic space, while minimizing the cover radius. Like almost any summarization method, the technique is meant to produce data for human consumption. Thus, we assess our approach on 307 human subjects and compare it to the classic bag-of-words description based summaries, and find it superior."
Multi-Label Learning With Millions of Categories,"Our objective is to build an algorithm for classifying a data point into a set of labels when the output space contains millions of categories. This is a relatively novel setting in supervised learning and brings forth interesting challenges such as efficient training and prediction, learning from only positively labeled data with missing and incorrect labels and handling label correlations. We propose a random forest based solution for jointly tackling these issues. We develop a novel extension of random forests for multi-label classification which can learn from positive data alone and can scale to large data sets. We generate real valued beliefs indicating the state of labels and adapt our classifier to train on these belief vectors so as to compensate for missing and noisy labels. In addition, we modify the random forest cost function to avoid overfitting in high dimensional feature spaces and learn short, balanced trees. Finally, we write highly efficient  training routines which let us train on problems with forty million training points, over a million dimensional sparse feature vector and over a million categories. Extensive experiments reveal that our proposed solution is not only significantly better than other multi-label classification algorithms but also more than 10% better than the state-of-the-art in our application domain."
Which Ranking Measure shall We Use --- Some Suggestions from a Theoretical Perspective,"Ranking has been extensively studied in information retrieval, machine learning and statistics. A central problem in ranking is how to design or choose a ranking measure for the evaluation of ranking functions. In this paper we study, from a theoretical perspective, a class of ranking measures including NDCG, NDCG@k and Precision@k. Weanalyze, under some theoretical assumptions, the behavior of these ranking measures as the number of objects to rank getting large. Our theoretical results provide several suggestions for choosing ranking measures when there is a large set of objects to rank: 1) When employing NDCG as the ranking measure, it would be better to choose its cut-offversion NDCG@k and let $k$ grows with the number of objects; 2) If the users prefer a not-too-small $k$, it would be better to use a $r^{-\alpha}$ ($\alpha \in (0,1)$) discount instead of the $\frac{1}{\log(1+r)}$ in NDCG@k. We also conduct experiments on real data and find that our theory works well although the assumptions for the theorems may not hold in the real dataset."
Efficient Pool-Based Active Learning of Halfspaces,"We study pool-based active learning of halfspaces, in which a learner receives a pool of unlabeled examples, and iteratively queries a teacher for the labels of examples from the pool, in order to identify all the labels of pool examples. We revisit the idea of greedily selecting examples to label, and use it to derive an efficient algorithm, called ALuMA, that approximates the optimal label complexity for a given pool in $\reals^d$. We show that ALuMA obtains an $O(d^2 \log(d))$ approximation factor if the examples in the pool are numbers with a finite accuracy. We further prove a result for general hypothesis classes, showing that a slight change to the greedy approach leads to an improved target-dependent guarantee on the label complexity. In particular, we conclude a better guarantee for ALuMA if the target hypothesis has a large margin.  We further compare our approach to other common active learning strategies, and provide a theoretical and empirical evaluation of the advantages and disadvantages of the approach. "
Context-Sensitive Decision Forests for Object Detection,"In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods."
Estimating Nonstationary Inputs from a Single Spike Train Based on a Neuron Model with Adaptation,"Because every spike of a neuron is determined by input signals, a train of spikes may contain information about the dynamics of unobserved neurons. A state-space method based on the leaky integrate-and-fire (LIF) model, describing neuronal transformation from input signals to a spike train has been proposed for tracking input parameters represented by their mean and fluctuation. In the present paper, we propose to make the estimation more realistic by adopting an LIF model augmented with an adaptive moving threshold. Moreover, because the direct state-space method is computationally infeasible for a data set comprising thousands of spikes, we further develop a practical method for transforming instantaneous firing characteristics back to input parameters. The instantaneous firing characteristics, represented by the firing rate and non-Poisson irregularity, can be estimated using a computationally feasible algorithm. We applied our proposed methods to synthetic data and experimental data to clarify that they perform well."
Generalized sequential tree-reweighted message passing,"This paper addresses the problem of approximate MAP-MRF inference in general graphical models. Following [23], we consider a family of linear programming relaxations of the problem where each relaxation is specified by a set of nested pairs of factors for which the  marginalization constraint needs to be enforced. We develop a generalization of the TRW-S algorithm [6] for this problem, where we use a decomposition into junction chains, monotonic w.r.t. some ordering on the nodes. This generalizes the monotonic chains in [6] in a natural way. We also show how to deal with nested factors in an efficient way. Experiments show an improvement over min-sum diffusion, MPLP and subgradient ascent algorithms on a number of computer vision and natural language processing problems."
Unsupervised Object Matching via Probabilistic Latent Variable Models,"We propose a probabilistic latent variable model for unsupervised object matching, which is the task of finding correspondences between objects in different domains. With existing object matching methods, the numbers of objects in different domains must be the same, and the methods find one-to-one matching in two domains. The proposed model can handle multiple domains with different numbers of objects, and can find many-to-many matching. The proposed model assumes that there is a set of latent vectors that is shared by all domains, and each object is generated using one of the latent vectors and a domain-specific linear projection. By inferring a latent vector to be used for generating each object, we can match objects in an unsupervised manner. We demonstrate the effectiveness of the proposed model with experiments using synthetic, handwritten digit, music, and text data sets."
Online Discrimination of Nonlinear Dynamics with Switching Differential Equations,"How to recognise whether an observed person walks or runs? We consider a dynamic environment where observations (e.g. the posture of a person) are caused by different dynamic processes (walking or running) which are active one at a time and which may transition from one to another at any time. For this setup, switching dynamic models have been suggested previously, mostly, for linear and nonlinear dynamics in discrete time. Motivated by basic principles of computations in the brain (dynamic, internal models) we suggest a model for switching nonlinear differential equations. The switching process in the model is implemented by a Hopfield network and we use parametric dynamic movement primitives to represent arbitrary rhythmic motions. The model generates observed dynamics by linearly interpolating the primitives weighted by the switching variables and it is constructed such that standard filtering algorithms can be applied. In two experiments with synthetic planar motion and a human motion capture data set we show that inference with the unscented Kalman filter can successfully discriminate several dynamic processes online. "
Discriminative Low-Rank Representation Graph for Semi-supervised Learning,"The recently proposed low-rank representation (LRR) method is effective in ex-ploring subspace structures. However, LRR graph has no explicit connection tothe classification task. In this paper, we propose a discriminative low-rank rep-resentation (DisLRR) graph for semi-supervised learning. Our DisLRR graphcould not only seek the lowest rank representations among all the elements in thedictionary, but also incorporate discriminative information from labeled and un-labeled samples. The convergence of our algorithm is theoretically proved. Thenwe present a semi-supervised learning method by incorporating DisLRR graphand Gaussian harmonic function (GHF). Experimental results on a toy data set,the PIE, Extended YaleB and ORL databases demonstrate that our DisLRR graphoutperforms other related graphs, especially when the data are heavily corrupted."
Information-Theoretic Limits on Model Selection for Gaussian Markov Random Fields in the High-Dimensional Setting,"This paper focuses on the information-theoretic limitations of model selection for Gaussian Markov random fields in the high-dimensional setting, where the graph size $p$ and the number of edges $k$ are allowed to scale with the sample size $n$. We provide an a rigorous analysis of this problem for generic graphs in an ensemble. Our result establishes a necessary condition on the sample size $n(p,k)$ for any procedure, regardless of its computational complexity, to consistently recover the underlying graph. Moreover, our analysis implies a connection between that graphical model selection limits and eigenvalues of concentration matrices. The key way out of the difficulty is found via investigating the orthogonal systems from concentration matrices, making it possible to calculate the symmetric Kullback-Leibler divergence between generic graphs and obtain the final simple result. Our method of analyzing generic graphs using orthogonal systems would be of use to other model selection problems."
Fast Probabilistic Optimization from Noisy Gradients,"Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, we construct a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from *noisy* evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions."
Hierarchical Visual Feature Learning for Computer Aided Diagnosis Using 3D Medical Images on Large-Scale Evaluation,"Computer aided diagnosis (CAD) of cancerous anatomical structures via 3D medical images has emerged as an intensively studied research area. In this paper, we present a principled three-tiered image feature learning approach, to capture task specific and data-driven class discriminative statistics from annotated image database, apart from often hand-crafted, heuristic approaches. It integrates voxel-level, instance-level and database-level feature learning, aggregation and parsing. We demonstrate its effectiveness in unified lung nodule, lung vascular structure and colon polyp feature computation and detection by classification in CAD applications. The other advantage includes that it only requires fast segmentation-less (e.g., simple thresholding on voxel-level labeling probabilities) image processing in training and runtime, which also alleviates the classification bias on certain (over or under) segmentation algorithms. After instance-level aggregation, features can also be flexibly tuned for classifying positive and negative cancers/tumors, or discriminating different subcategories of nodule/polyp, e.g., according to the clinically-relevant size or shape morphologies, within our designed gating classifier. Our hierarchical feature learning framework enables to achieve significantly superior performances than previous state-of-the-art CAD systems, extensively validated on highly representative multi-site clinical datasets using $879$ and $770$ CT volumes, for lung and colon CAD tasks respectively. "
On Consistent Classification with Imbalanced Classes,"We consider the problem of imbalanced classes in binary classification, where one class is rare compared to the other. This problem arises frequently in practice and has been widely studied. However very little is understood in terms of the theoretical properties of the problem or of the algorithms proposed: what performance measures are appropriate, how these affect the learning process, and whether the algorithms are statistically consistent with respect to the desired performance measures. In this paper, we initiate a formal study of these issues, focusing on the balanced 0-1 error that evaluates errors on the majority and minority classes separately and effectively balances the two. The underlying balanced 0-1 loss bears similarity to cost-sensitive losses; however a critical difference between the two is that the balanced loss depends on the underlying distribution, while cost-sensitive losses are defined independent of the distribution. We establish statistical consistency of two types of algorithms with respect to the balanced 0-1 error: plug-in rules that use an empirically determined threshold, and certain types of empirically balanced risk minimization algorithms. Our experiments support our theoretical results, showing that both these approaches perform as well as (or better than) under-/over-sampling methods that are currently viewed as the state of the art."
Passivity-based Monitoring of POMDPs,"Maintaining exact belief states in POMDPs can be a difficult task since the size of the belief state grows exponentially with the number of state variables. Boyen and Koller described an approximation method which exploits locality in the process by maintaining smaller belief states over clusters of correlated variables. While this is a useful method, it does not fully account for the causal relations between the variables. We study a particular type of causal relation, called passivity, which captures the notion that a variable changes only if any of the variables that directly influence it change or if it is the target of an action. We show that passivity can be exploited in conjunction with locality to accelerate the monitoring task. The idea is to maintain separate beliefs over subsets of correlated variables, and to update only those beliefs whose variables we suspect to have changed. We present an algorithm, called Passivity-based Parallel Monitoring (PPM), that implements this idea. We show empirically that PPM outperforms two state-of-the-art solutions, while maintaining competitive accuracy. Our experiments indicate that the relative computational gains grow significantly with the size of the process."
Group-wise FMRI Activation Detection Based on DICCCOL ,"Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and its less sensitivity to variabilities in individual subject?s brains. However, current group-wise fMRI activation detection methods rely on the co-registration of individual brain?s fMRI images into the same atlas space, which has difficulty in dealing with the remarkable anatomical variation of different brains. As a consequence, the resulted misalignments between different brains could significantly degrade the required inter-subject correspondences, thus reducing the sensitivity and specificity of group-wise fMRI activation detection. This paper presents a novel approach to detect group-wise fMRI activation on the recently developed and validated Dense Individualized and Common Connectivity-based Cortical Landmarks (DICCCOL), which is a dense map of cortical landmarks that possess intrinsic structural and anatomical correspondences across individuals and populations. The basic idea in this paper is that fMRI activation detection is first performed on each corresponding DICCCOL landmark in the individual brain?s own space using the general linear model (GLM), and then the activation significances of the same landmark from a group of subjects are statistically integrated and assessed at the group level. Finally, the consistently activated landmarks are determined and declared as the detected brain locations in response to external stimuli. Our experimental results demonstrated that the proposed approach can locate activation substantially more precisely than the traditional group-wise activation detection methods. "
Multiclass Learning Approaches: A Theoretical Comparison with Implications,"We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error."
Stochastic Gradient Descent with Only One Projection,"Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $O(1/\sqrt{T})$ convergence rate for general convex optimization, and an $O(\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function."
On the Reconstruction of Piecewise Constant Dependences,"An approach to the restoration of dependences (regressions) is proposed that is based on solving problems of supervised classification. The main task is finding the optimal partitioning of the range of values of dependent variable on a finite number of intervals. It is necessary to find optimal number of change-points and their positions. This task is formulated as search and application of  piece-wise constant function. When restoring piecewise constant functions, the problem of local discrete optimization using a model of logic supervised classification in leave ?one-out mode is solved. The value of the dependent value is calculated in two steps. At first, the problem of classification of feature vector is solved. Further, the dependent variables is calculated as half of the sum of change-points values of the corresponding class"
Fast Max-kernel Search,"The wide applicability of kernels make the problem of max-kernel searchubiquitous and more general than the usual similarity search. We focus onsolving this problem efficiently. We begin by characterizing the inherenthardness of the max-kernel search problem with a novel notion of {\emdirectional concentration}. Following that, we present a method to use an $O(n\log n)$ scheme to index any set of objects (points in $\Real^\dims$ or abstractobjects) \textit{directly in the kernel space} without any explicitrepresentation of the points in this kernel space. A provably $O(\log n)$branch-and-bound algorithm is presented using this index for exact max-kernelsearch. Empirical results for a variety of data sets as well as abstract objectsdemonstrate up to 4 orders of magnitude speedup in some cases. Extensions forapproximate max-kernel search are also presented."
Sparse Approximation via Penalty Decomposition Methods,"In this paper we consider general sparse approximation problems, that is, general $l_0$ minimization problems with the $l_0$-``norm'' of a vector being a part of constraints or objective function. We assume that the $l_0$ part is the only nonconvex part in these problems. We then propose penalty decomposition (PD) methods for solving them in which a sequence of penalty subproblems are solved by a block coordinate descent (BCD) method. Under some suitable assumptions, we show that any accumulation point of the sequence generated by the PD method is a local minimizer of the problems. Moreover, we establish that any accumulation point of the sequence generated by the BCD method is a local minimizer of the penalty subproblem. Finally, we test the performance of our PD methods by applying them to sparse logistic regression and compressed sensing problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality.    "
Sparse Principal Components Analysis via Decomposition Methods,"In this paper we first propose new formulations for sequentially finding sparse PCs by using $l_0$-``norm'' of the loading vector as part of the constraints. By using these formulations, a sparse PC can be obtained with the desired cardinality of the loading vectors. Then we propose formulations that also consider the orthogonality of the loading vectors. We further generalize the proposed formulations to simultaneously find the first $r$ sparse PCs. To solve the proposed formulations, we apply decomposition methods. Under some suitable assumptions,  we show that any accumulation point of the sequence generated by the proposed decomposition methods satisfies the first-order optimality conditions of the corresponding problems. Finally, we compare the sparse PCA approaches proposed in this paper with several existing methods using gene expression data. The computational results demonstrate that the sparse PCs obtained by our approaches substantially outperform those obtained by the other methods in terms of solution quality. "
"Neuronal spike generation mechanism as an oversampling, noise-shaping A-to-D converter","We explore the hypothesis that the neuronal spike generation mechanism is an analog-to-digital converter, which rectifies low-pass filtered summed synaptic currents and encodes them into spike trains linearly decodable in post-synaptic neurons. To digitally encode an analog current waveform, the sampling rate of the spike generation mechanism must exceed its Nyquist rate. Such oversampling is consistent with the experimental observation that the precision of the spike-generation mechanism is an order of magnitude greater than the cut-off frequency of dendritic low-pass filtering. To achieve additional reduction in the error of analog-to-digital conversion, electrical engineers rely on noise-shaping. If noise-shaping were used in neurons, it would introduce correlations in spike timing to reduce low-frequency (up to Nyquist) transmission error at the cost of high-frequency one (from Nyquist to sampling rate). Using experimental data from three different classes of neurons, we demonstrate that biological neurons utilize noise-shaping. We also argue that rectification by the spike-generation mechanism may improve energy efficiency and carry out de-noising. Finally, the zoo of ion channels in neurons may be viewed as a set of predictors, various subsets of which are activated depending on the statistics of the input current. "
Assessing Blinding in Clinical Trials,"The interaction between the patient's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants' feedback."
Loss-Regularized CRF for Preference Aggregation,"We develop a flexible Conditional Random Field framework for supervised preference aggregation, which combines preferences from multiple experts over items to form a distribution over rankings. We explore methods of optimizing the parameters of this distribution given the expert preferences, some of which incorporate the loss metric used at test time, and propose a new loss-based training objective. Unlike existing aggregation methods the resulting model can incorporate most existing test metrics, and permits efficient optimization. Experiments on benchmark tasks demonstrates significant gains over existing rank aggregation methods."
Tree-structured Kernel Dimension Reduction,"Tree-structured approaches are to iteratively split data until some termination criterion is satisfied.  Most of the approaches can deal with either supervised learning, or unsupervised learning, but not both.  In this paper, we propose (residual) tree-structured Kernel Dimension Reduction (rtKDR/tKDR) approaches to address the issue. Specifically, we alternatively use kernel dimension reduction (KDR) to estimate a linear projection direction of (residual) response variables in each non-terminal node for maximizing conditional independence between explanatory variables and (residual) response variables under Reproducing Kernel Hilbert Spaces, and split the (residual) data based on the median of projection indices that the (residual) data project into the direction. When the explanatory variables are continuous, discrete and response ones, rtKDR/tKDR can deal with not only supervised learning but also unsupervised learning.  Furthermore, rtKDR has the potential of discovering the intrinsic dimensions from high-dimensional nonlinear data sets. Experiments in several benchmark datasets indicate that rtKDR/tKDR attain better space partition and prediction performances compared with several recently published space partition algorithms. "
A Recalibration Procedure which maximizes the AUC: A Use-Case for Bi-Normal Assumptions,"Area under the receiver operating characteristic curve (AUC) is a popular measure for evaluating the quality of binary classification rules. Commonly used score-based classifiers label an outcome as a positive if the score is greater than a certain threshold. We show that this may not be optimal in terms of maximizing the AUC. Under certain assumptions the optimal thresholding rule is derived using the Neyman-Pearson lemma. Specifically, we show that a thresholding rule that isquadratic in the score dominates the commonly used linear thresholding rule. Our work provides a real data use-case where the recalibration significantly improvesthe AUC."
Scalable nonconvex inexact proximal splitting,"We study large-scale, nonsmooth, nonconconvex optimization problems. In particular, we focus on nonconvex problems with \emph{composite} objectives. This class of problems includes the extensively studied convex, composite objective problems as a special case. To tackle composite nonconvex problems, we introduce a powerful new framework based on asymptotically \emph{nonvanishing} errors, avoiding the common convenient assumption of eventually vanishing errors. Within our framework we derive both batch and incremental nonconvex proximal splitting algorithms. To our knowledge, our framework is first to develop and analyze incremental \emph{nonconvex} proximal-splitting algorithms, even if we disregard the ability to handle nonvanishing errors. We illustrate our theoretical framework by showing how it applies to difficult large-scale, nonsmooth, and nonconvex problems."
Learning to Discover Social Circles in Ego Networks,"Our personal social networks are big and cluttered, and currently there is no good way to organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g. `circles' on Google+, and `lists' on Facebook and Twitter), however they are laborious to construct and must be updated whenever a user's network grows. We define a novel machine learning task of identifying users' social circles. We pose the problem as a node clustering problem on a user's ego-network, a network of connections between her friends. We develop a model for detecting circles that combines network structure as well as user profile information. For each circle we learn its members and the circle-specific user profile similarity metric. Modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles. Experiments show that our model accurately identifies circles on a diverse set of data from Facebook, Google+, and Twitter for all of which we obtain hand-labeled ground-truth data."
Statistical modeling of indoor scenes and objects,"We develop a Bayesian generative model for understanding indoor scenes.  While state-of-the-art methods approximate objects in these environments with gross 3D geometry (e.g., bounding boxes), we propose using geometric representations with a finer granularity. For example, we model a table as a set of four legs and a top.  Such models enhance recognition and reconstruction, and enable more refined use of appearance for scene understanding. In particular, we introduce a new likelihood function that rewards 3D object hypotheses whose 2D projection are more uniform in color distribution. Such a model would be confused if used on a bounding box for a concave object like a table. We present results showing the positive effect of each of these innovations.  The performance of our method is comparable to, and often exceeds, that of state-of-the-art methods on the scene surface orientation task, as well as object recognition, as evaluated on the two bench mark data sets used in this domain. "
Understanding Trees via Margins,"Building off recent work such as [Fruend, et.al.,2007], we seek to further understanding of  the behavior of multidimensional trees in high dimensional data. These trees are used in nearest-neighbor search, vector quantization, classification and other tasks. Usual analysis of trees investigate the capability of trees to adapt to some notion of intrinsic dimensions. In this paper, we provide an alternate avenue to mitigate high dimension effects -- margins. To this end, (1) we quantify the contribution of margins to the performance of tree, and (2) we formalize an intuitive notion of robustness and present its dependence on the margins.  We also provide empirical evidence showing that large margin splits can result in good quality indexing in high dimensions while being fairly robust to perturbations. "
Probabilistic Topic Coding for Superset Label Learning,"In the superset label learning problem, each training instance provides a set of candidate labels of which one is the true label of the instance.  Most approaches learn a discriminative classifier that tries to minimize an upper bound of the unobserved 0/1 loss. In this work, we propose a probabilistic model, Probabilistic Topic Coding (PTC), for the superset label learning problem. The PTC model is derived from logistic stick breaking process. It first maps the data to ``topics'', and then assigns to each topic a label drawn from a multinomial distribution.  The layer of topics can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art.  The discovered underlying structures also provide improved explanations of the classification predictions."
Weighted Online Learning,"We consider an unconventional online learning problem which we  call Weighted Online Learning (WOL), where each training example has associated with it a (non-uniform) non-negative weight. WOL problems occur in many real life applications including banking business, medical diagnosis and visual tracking, where different samples are of differing value to the learning process. We propose several algorithms for WOL and show these algorithms have similar regret bounds and convergence rates to Pegasos. Applications in bank credit estimation, medical diagnosis and visual tracking show a significant improvement over state-of-the-art methods using traditional online learning."
Majorization for CRFs and Latent Likelihoods,"The partition function plays a key role in probabilistic modeling including conditional random fields, graphical models, and maximum likelihood estimation. To optimize partition functions, this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Suchbounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems, low-rank versions of the bound are provided and outperformLBFGS as well as first-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods."
Efficient deep learning through novel sparsity constraints and heterogeneous maps,"Deep convolutional neural networks (CNNs) have been successfully applied to a range of tasks spanning object recognition to sparse basis transformation. However, these very high dimensional networks are computationally expensive and have not realized their potential for real time applications. How can we minimize run time and yet retain state of the art results? To answer this question, we introduce two key ideas not yet fully explored for CNNs: (i) True Sparsity and (ii) Heterogeneous Feature Mappings. True sparsity comes to us from biological networks where neurons have a binary activation scheme. Although in machine learning a penalty norm is usually used to impose network sparsity, we achieve sparsity by modifying the activation function for a discrete response and introduce a 'burn-in' optimization scheme. Heterogeneous feature mappings are inspired by mixed selectivity -- a recent neuroscience term for diverse neuronal behavior. These two ideas lead to deep sparse networks that are more computationally feasible for high dimensional classification and extend naturally to unsupervised basis extraction. We achieve very competitive results on benchmark datasets with a speed-up factor over conventional methods. "
QuickBoost - Quickly Training Boosted Decision Trees,"Boosting is one of the most popular and effective learning techniques in use today. While exhibiting fast classification speed at test time, the training of an ensemble or cascade of boosted weak classifiers is slow, making it impractical for applications with real-time training requirements. In this paper, we propose a principled approach to overcome this drawback. We compute and prove a bound on the error of a weak classifier given its error on a subset of the training data;the bound may be used to prune unpromising weak classifiers early on. We propose a fast training algorithm that exploits this bound, yielding up to a 10-fold speedup at no cost in the final performance of the classifier."
Inverse of Lorentzian Mixture for Supervised Learning of Prototypes and Weights,"This paper presents a novel distance-based classifier based on the multiplicative inverse of Lorentzian mixture, which can be regarded as a natural extension of the conventional nearest neighbor rule. We show that prototypes and weights can be trained simultaneously by General Loss Minimization, which is a generalized version of supervised learning framework used in Generalized Learning Vector Quantization. Experimental results for UCI machine learning repository reveal that the proposed method achieves higher classification accuracy than Support Vector Machine with a much fewer prototypes than support vectors."
Efficient high dimensional maximum entropy modeling via symmetric partition functions,"  The application of the maximum entropy principle to sequence  modeling has been popularized by methods such as Conditional Random  Fields (CRFs).  However, these approaches are generally limited to  modeling paths in discrete spaces of low dimensionality.  We  consider the problem of modeling distributions over paths in  continuous spaces of high dimensionality---a problem for which  inference is generally intractable.  Our main contribution is to  show that maximum entropy modeling of high-dimensional, continuous  paths is tractable as long as the constrained features   possess a certain kind of low dimensional structure.  In this case, we show that the associated {\em partition function} is  symmetric and that this symmetry can be exploited to compute the  partition function efficiently in a compressed form.  Empirical  results are given showing an application of our method to maximum  entropy modeling of high dimensional human motion capture data."
Contour Detection using Sparse Code Gradients,"Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most existing approaches including the state-of-the-art Global Pb (gPb) operator.  In this work, we show that contour detection accuracy can be vastly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding.  We use K-SVD and Orthogonal Matching Pursuit for efficient dictionary learning and encoding, and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear SVM. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and find contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours).  Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth images and surface normals lead to promising contour detection using depth and depth+color, as verified on the NYU Depth Dataset.  Our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation."
Analyzing 3D Objects in Cluttered Images,"We present an approach to detecting and analyzing the 3D configuration of objects in real-world images with heavy occlusion and clutter. We focus on the application of finding and analyzing cars. We do so with a two-stage model; the first stage reasons about 2D shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then refined by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset."
"Nonconvex Penalization, Levy Processes and Concave Conjugates","In this paper we study sparsity-inducing nonconvex penalty functions using Levy processes. We define such a  penalty as the Laplace exponent of a subordinator. Accordingly, we propose a novel approach for the construction of sparsity-inducing nonconvex penalties. Particularly,  we show that the nonconvex logarithmic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionaly, we explore the concave conjugate of nonconvex penalties. We find that the LOG and EXP penalties are the concave conjugates of the negatives of Kullback-Leiber (KL) distance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance."
Computing the M Most Probable Modes of a Graphical Model,"We introduce the M-modes problem for graphical models: predicting the M label configurations of highest probability that are at the same time local maxima of the probability landscape. M-modes have multiple possible applications: because they are intrinsically diverse, they provide a principled alternative to non-maximum suppression techniques for structured prediction, they can act as codebook vectors for quantizing the configuration space, or they can form component centers for mixture model approximation.We present two complementary algorithms for solving the M-modes problem on junction chains. When the underlying graphical model is a simple chain, their complexity is polynomial in all relevant quantities. On general junction chains algorithms do not come with worst-case guarantees, but show promising performance in our experiments. Besides the M-modes problem, our techniques are also applicable to a more general class of optimization problems."
Fast Multiple Kernel Learning With Multiplicative Weight Updates,"In this work, we propose a fast algorithm for multiple kernel learning (MKL). Our proposed approach builds on the original QCQP formulation of Lanckriet et al. It uses a multiplicative weight update based approximation for the underlying SDP, exploiting a careful reformulation of the MKL problem as well as a novel fast matrix exponentiation routine for QCQP constraints that might be of independent interest. Our method avoids the use of commercial nonlinear solvers, and scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with an uniform unweighted combination of kernels."
Local Importance Weight for Transductive Classification,"This paper proposes a weighting scheme of training data named local importance weighting for transductive classification when training and test distributions are different. Most of the previous efforts to resolve the distribution difference rarely focus on matching joint distributions of input and output, but we show that it is important at least in classification tasks to adjust the difference of joint distributions. To match up the joint distribution of training data with that of test data. the proposed scheme estimates the joint distributions of both training and test data. Since the class labels of test data are unknown, an EM algorithm is used to estimate them and the parameters of the transductive classifier simultaneously. Through a series of experiments, we show that the transductive classifier with the proposed scheme outperforms that with the existing weighting method. "
A Polylog Pivot Steps Simplex Algorithm for Classification,"We present a simplex algorithm for linear programming in a linear classification formulation. The paramount complexity parameter in linear classification problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear. This is in contrast to general linear programming, for which no sub-polynomial pivot rule is known. "
Order Preserving Hashing for Approximate Nearest Neighbor Search,"In this paper, we study hash-based indexing techniques for approximate nearest neighbor (ANN) search. The principle of searching ANNs is that the points corresponding to the hash codes near that of the query are more similar to the query than those corresponding to the farther hash codes. Motivated by this point, we propose a novel hashing approach, called order preserving hashing, which learns the hash functions by maximizing the alignment between the similarity orders computed from the original space and the hamming space. We also impose the constraint that the points are as uniformly as possible distributed in the buckets. To this end, we formulate the problem of mapping the NN points for a query point into different hash codes as a classifier learning problem in which a nearest neighbor classifier is used based on the hamming distance over the hash codes, and find the hash functions from the aggregated classifier pooled over all the training points. To make the optimization feasible, we develop several techniques, including Sigmoid relaxation, stochastic gradient decent, and active set, to efficiently learn hash functions. Experimental results demonstrate the superiority of our approach over existing state-of-the-art techniques."
Shifting Weights: Adapting Object Detectors from Image to Video,"Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video."
Nonparametric Bayesian Microphone Array Processing,"Sound source localization and separation from a mixture of sounds are essential functions for computational auditory scene analysis.The main challenges are designing a unified framework for joint optimization and estimating the sound sources under auditory uncertainties such as reverberation or unknown number of sounds.Since sound source localization and separation are mutually dependent, their simultaneous estimation is required for better and more robust performance. A unified model is presented for sound source localization and separation that is based on a nonparametric Bayesian model.Experiments using simulated and recorded audio mixtures show that a method based on this model achieves state-of-the-art sound source separation quality and has more robust performance on the source number estimation under reverberant environments."
Fast multiple-part based object detection using KD-Ferns,"Part-based models are currently considered state-of-the-art for object detection due to their ability to represent large appearance variations. Furthermore, using a large number of parts with diverse appearances can improve classification accuracy. The computational cost of the detection stage in existing methods has been a limitation for adopting such models in real-time applications. In addition, since the computation time grows linearly with the number of parts, most methods are limited to using several parts only. Our first contribution is the ``KD-Ferns'' algorithm for approximate nearest neighbor search which allows to compare each image location to only a subset of the model parts. This allows scaling the number of parts in the model. Our second contribution is a new algorithm for object detection which is an efficient variant of the ``Feature Synthesis'' (FS) method[1]. The FS uses multiple object parts for detection and is among state-of-the-art methods on human detection benchmarks but suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses learned classifiers with hundreds of parts in a coarse-to-fine strategy, achieving significantly faster object detection compared to existing part based methods. The AFS uses the ``KD-Ferns'' algorithm, and spatially sparse part locations, to reduce the required computation. We evaluate the AFS on the INRIA and Caltech pedestrian detection benchmarks. In this evaluation, the AFS is among the state-of-the-art methods in detection accuracy and is also the fastest method for close range pedestrians, reaching nearly 10 frames per-second on $640\times 480$ images. The AFS is to our best knowledge the first part-based object detection method capable of running in real-time."
Matching Human Actions in Videos,"Matching human motion in images and videos is one of the most fundamental problems in computer vision. In this paper, we attempt to extend the traditional appearance-similarity-based feature matching to action-similarity-based matching. The matching method we proposed is independent of appearances, camera views, and robust to the pace and speed of the specific human action. Our action similarity measure is based on a novel covariance descriptor for encoding the point motion trajectories of people. We encode each point trajectory into an optical flow covariance matrix, which is symmetry and positive definite (SPD). Using the metric of SPD matrix space, we can identify the similarity correspondence of any pairs of trajectories. We also introduce a subspace alignment algorithm, which allows us to perform dense matching between two trajectory set in a low-dimensional subspace. The resulting matching provides us a set of dense point-point correspondences between two people in videos, who are conducting the same action in the sampled video clips. The action-similarity-based human matching can provide a new view/appearance independent approach for video registration, human tracking, and action recognition."
Linked Tensor/Tucker Decomposition and its Application for Multi-block Group Tensor Analysis,"In this paper we propose a new algorithm for flexible group tensor analysis model called the linked tensor/Tucker decomposition (LTD).The LTD can decompose given multiple tensors into common factor matrices, individual factor matrices, and individual core tensors, simultaneously.The LTD provides a novel application called the linked multiway principal component analysis (LMPCA) by addition of orthogonal constraints.This method is derived by the alternating least squares (ALS) algorithm.Furthermore, we conducted experiments of this model for face reconstruction and denoising and EEG classification to demonstrate its advantages."
On Detecting Multiple Simultaneous Change-points in High Dimensional Time Series,"This paper studies the detection of multiple simultaneous (systematic) change points for high-dimensional nonstantionary time series data. The analytic framework used is based on the standard and adaptive fused group lasso method, where the mixed $L_{2,1}$ norms in the penalty are either uniform or re-weighted by data-dependent weights. This paper shows that, under appropriate conditions, this approach is $L_2$ consistent and, by adopting the data-dependent weights, could correctly select the change points with probability approaching unity. It quantifies the conditions on the interplay among the averaged (over different dimensions) minimum magnitude of structural changes, the number of change points and the number of observations for consistently discovering the change points. The performance of this approach is illustrated via an analysis of a large panel of U.S. economic and financial time series data over the past $50$ years. "
Semantic GIST: Probabilistic Modelling of Scenes using Scenelet,"In this paper, we propose a probabilistic modeling framework for scenes to encode semantic information of images into a compact Semantic Gist representation. The representation is based on a key concept called {\em scenelets}, which serves as building blocks for scenes. We learn these scenelets using a topic model to group correlated objects such that the learned set of scenelets maximally retain the semantic saliency of images in terms of KL-divergence. Our model also integrates information from individual discriminative object detectors and global image features by coding them as priors. Empirical results demonstrate the power of our model. We first show that using a small set of scenelet classifiers, we can predict the existence of a large set of objects without running individual object detectors.Furthermore, we can even predict the presence of objects without running large sets of object detectors by MAP estimation using our model.We also show that the framework can improve the performance of individual detectors by incorporating the contextual object and scenelet information. Experiments on challenging datasets including PASCAL and SUN09  demonstrate that our model outperforms other state-of-the-art ones."
Learning Modewise Independent Components from Tensor Data via Multilinear Mixing Model,"Independent component analysis (ICA) is a popular unsupervised learning method. This paper extends it to multilinear modewise ICA (MMICA) for tensors and explores two architectures in learning and recognition. MMICA models tensor data as mixtures generated from modewise source matrices that encode statistically independent information. It operates on much lower dimension than ICA and its compact representations require much fewer parameters to estimate. We embed ICA into the multilinear principal component analysis framework to solve for each source matrix alternatively with a few iterations. Then we obtain mixing tensors through regularized inverses of the source matrices. Simulations on synthetic data demonstrate that MMICA can estimate hidden sources from structured tensor data. Moreover, in face recognition experiments, it outperforms competing solutions, while being particularly effective with Architecture II due to sparser and more structured bases."
Identification of Recurrent Patterns in the Activation of Brain Networks,"Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks."
Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning,"Learning from experience how to manipulate an environment in a goal-directedmanner is one of the central challenges in research on autonomous agents. Inthe case of object manipulation, efficient learning and planning should exploit theunderlying relational structure of manipulation problems and combine geometricstate descriptions with abstract symbolic representations. When appropriate sym-bols are not predefined they need to be learned from geometric data. In this paperwe present an approach for learning symbolic relational abstractions of geometricfeatures such that these symbols enable to learn abstract transition models and touse them for goal-directed planning of motor primitive sequences. This is framedas an optimization problem, where a loss function evaluates how predictive thelearned symbols are for the effects of motor primitives as well as for reward. Theapproach is embedded in a full-fledged symbolic relational model-based reinforce-ment learning setting, where both the symbols as well as the abstract transitionand reward models are learned from experience. We quantitatively compare theapproach to simpler baselines in an object manipulation task and demonstrate iton a real-world robot."
Imbalanced Random Subspace Ensemble Methods for computer-aided nodule detection,"Many lung computer-aided detection (CAD) methods have been proposed for pulmonary nodules. Because high sensitivity is essential in the candidate identification stage, there are countless false positives produced by the initial suspect nodule generation process, giving more work to radiologists. Moreover, there are more false positives than real nodules detected. In order to eliminate or reduce the false positives, we address this issue as a binary classification problem. However, given the imbalance between false positives and true positives and the different misclassification costs, we propose two new strategies of random subspace ensemble methods. Experimental results show the effectiveness of the proposed methods in terms of sensitivity and specificity."
Density-Difference Estimation,"We address the problem of estimating the difference betweentwo probability densities.A naive approach is a two-step procedure of first estimating two densities separatelyand then computing their difference.However, such a two-step procedure does not necessarily work wellbecause the first step is performed without regard to the second stepand thus a small error incurred in the first stage can cause a big error in the second stage.In this paper, we propose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities.We derive a non-parametric finite-sample error boundfor the proposed single-shot density-difference estimatorand show that it achieves the optimal convergence rate.We experimentally demonstrate the usefulness of the proposed methodin class-prior estimation and change-point detection."
Online Learned Discriminative Hidden Structural Part Model for Visual Tracking,"We present a discriminative hidden structural part-based approach for visual trackingwithout any prior assumptions about the target and scenarios. Unlike otherweak-constrained or manual labeling part generation strategy in the previous partbasedtrackers, the state (e.g. position, width and height) of each part is consideredas the latent variable in our model and is inferred automatically online withthe dual objective functions. The two objective functions respectively encode theappearance variations of the target and separate the target from the backgroundwith the max-margin. Specifically, the constraints between parts are integratedin graph model through the dynamically constructed pair-wise Markov RandomField (MRF). The part-based Support Vector Machine (SVM) and Reverse JumpMarkov Chain Monte Carlo (RJMCMC) algorithm is adopted to complete thelatent variable inference task. The experimental results on various challengingdatabase demonstrate the learned part-based tracker outperforms other state-ofthe-art trackers (both bounding-box-based as well as part-based)."
Variational Inference for Crowdsourcing,"Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al, while our MF method is closely related to a commonly used EM algorithm. In both case, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state of art algorithms based on more complicated modeling assumptions."
VLSI Implementation of a Coupled MRF Model Using Pulse-coupled Phase Oscillators,"This paper proposes efficient pixel-parallel image processing using a pulse-coupled phase oscillator model and its VLSI implementation. A processing unit which corresponds to a pixel of an image transmits spike pulses to other units, and updates its own analog state value at timing when spikes come from other units. From a VLSI implementation point of view, this mechanism is suitable to very low power operation because analog buffers are unnecessary for data transmission. On the basis of this model, we have designed and fabricated a VLSI image processor chip that performs a coupled Markov random field (MRF) model for image region segmentation. Very low power VLSI design has been achieved by combination of an analog oscillator and digital coupling function generator circuits with time-domain computation. The performance of this oscillator-based image processor chip can be superior to the existing digital processors.  Experiments using the fabricated chip show successful image region segmentation in 1D and 2D images. "
MCMC for continuous-time discrete-state systems,"We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process, while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler, and show its advantages."
Learning about Canonical Views from Internet Image Collections,"Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or ?canonical? view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views?We start by manually finding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to find the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories."
Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data,"We propose an efficient, generalized, nonparametric, statistical Kolmogorov-Smirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efficient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods."
A Nonparametric Bayesian Classifier under a Mixture Loss Function,"Many classification problems can be conveniently formulated in terms of Bayesian mixture prior models. The mixture prior structure lends itself especially well for adapting to varying degrees of sparsity. Typically, parametric assumptions are made about the components of the mixture priors. In the following, we propose a parametric and a nonparametric classification procedures using a mixture prior Bayesian approach for a risk function that combines misclassification loss and an $L_2$ penalty. While the parametric procedure is closer to traditional approaches,in simulations, we show that the nonparametric classifier typically outperforms it when the parametric prior is misspecified; the two procedures have comparable performance even when the shape of the parametric prior is specified correctly.  We illustrate the properties of the two classifiers on a publicly available gene expression dataset."
TRaNce: Trace Norm for Ranking,"Learning to rank objects, in order of their perceived importance, finds diverse applications in several domains. Typically, the goal of ranking is to learn a real valued function that induces an ordering over the entire object space. In this paper, we formulate query specific ranking as a matrix completion problem: given the relevance values for a (partial) collection of objects over a set of queries, expressed in terms of a matrix, our task is to predict the relevance of the remaining objects by filling in the missing entries of the matrix. Specifically, we develop a matrix factorization based framework, TRaNce, for learning to rank. To our knowledge, TRaNce is the first technique to investigate the applicability of the trace-norm for ranking.  We provide a rigorous theoretical analysis encompassing generalization bounds (including stability and consistency) for the proposed methodology. We also provide experimental evidence to corroborate the efficacy of our technique."
Displacement Determination by Motion Compensation,Motion determination from an image sequence has been an important and challenging problem in computer vision and remote sensing applications. A fully constrained nonlinear system of equations combing the Displacement Vector Invariant (DVI) equation for displacement determination from an image sequence are proposed without approximation and imposing any additional constraint and assumption. An adaptive framework for solving the nonlinear system of equations has been developed. This work is to seek motion fields that are consistent with the physical observation because the observation may not be consistent with the physical motion in a featureless image sequence. The estimated displacement field is based on a single minimized target function that leads to optimized motion-compensated predictions and interpolations in a wide class of applications of the motion-compensated compression without any penalty parameters. Experimental tests on synthetic and natural image sequences are presented. Applications of motion-compensated interpolation are also demonstrated.
Localizing 3D cuboids in single-view images,"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners."
Active Metric Learning For Ground Level To Aerial Image Matching,"Image-based geolocation is a challenging problem that has recently captured the interest of computer vision and machine learning researchers.  In this work we focus on the specific problem of matching ground level images to 45 degree aerial images. Matching ground level and aerial images is a very hard problem due to wide disparities in viewpoint and imaging conditions, the combination of which leads to low level feature matching failure.  To overcome this problem, we propose an active metric learning framework that allows a human user to solve the problem collaboratively with the machine.  Our approach selects pairs of regions of interest based on an information gain criterion and asks the human user to establish correspondences between them.  Those pairs are subsequently used to update a metric to improve the correspondences. This process continues until the system finds the correct location of the ground level image.  We introduce a new Ground Level to Aerial Image Dataset (GLAID) to assess strengths and weaknesses of our proposed framework. Our experiments show that our system allows user to find the correct location with significantly reduced effort."
Learning to Align from Scratch,"  Unsupervised joint alignment of images has been demonstrated to  improve performance on recognition tasks such as face verification.  Such alignment reduces undesired variability due to factors such as  pose, while only requiring weak supervision in the form of poorly  aligned examples.  However, prior work on unsupervised alignment of  complex, real world images has required the careful selection of  feature representation based on hand-crafted image descriptors, in  order to achieve an appropriate, smooth optimization landscape.  In this paper, we instead propose a novel combination of  unsupervised joint alignment with unsupervised feature learning.  Specifically, we incorporate deep learning into the {\em congealing}  alignment framework.  Through deep learning, we obtain features that  can represent the image at differing resolutions based on network  depth, and that are tuned to the statistics of the specific data  being aligned.  In addition, we modify the learning algorithm for  the restricted Boltzmann machine by incorporating a group sparsity  penalty, leading to a topographic organization on the learned  filters and improving subsequent alignment results.  We apply our method to the Labeled Faces in the Wild database  (LFW). Using the aligned images produced by our proposed  unsupervised algorithm, we achieve a significantly higher accuracy  in face verification than obtained using the original face images,  prior work in unsupervised alignment, and prior work in supervised  alignment.  We also match the accuracy for the best available, but  unpublished method."
Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints,"Recent spiking network models of Bayesian inference and unsupervised learningfrequently assume either inputs to arrive in a special format or employ complex computations inneuronal activation functions and synaptic plasticity rules. Here we show in arigorous mathematical treatment how homeostatic processes,which have previously received little attention in this context, can overcome common theoretical limitationsand facilitate the neural implementation and performance of existing models.In particular, we show that homeostatic plasticity can be understood as the enforcement of a 'balancing'posterior constraint during probabilistic inference and learning with Expectation Maximization.We link homeostatic dynamics to the theory of variational inference, and show that, as a side effect, nontrivial terms which typically appear during probabilistic inference in a largeclass of models drop out.We demonstrate the feasibility of our approach in a spiking Winner-Take-All architecture of Bayesian inference and learning,and discuss general properties of the resulting network dynamics. Finally, we sketch how the mathematical framework can be extended to richer, recurrent network architectures.Altogether, our theory provides a novel perspective on the interplay of homeostaticprocesses and synaptic plasticity in cortical microcircuits, pointing to an essential computational roleof homeostasis during probabilistic inference and learning in spiking networks."
Discriminative Hidden Kalman Filters,"In on-line classification, a real-time decision needs to be made based on signals observed so far. Generative models such as switching Kalman filters are good at describing signals but not classifying them. Discriminative approaches such as conditional random fields offer better classification performance. Nevertheless, they often require pre-defined features. We propose a discriminative hidden Kalman filter that jointly learns features and the classifier based on a novel discriminative training criterion. The variational Bayesian algorithm is employed for optimization. An extension to handle multi-class problems is also presented. "
Regularized Nonnegative Matrix Factorization using Minimum Mean Square Error Estimates under Gaussian Mixture Prior Models with online Learning for the Uncertainties,"We propose a new method to enforce priors on the solution of the nonnegative matrix factorization (NMF). The proposed algorithm can be used for denoising or single-channel source separation (SCSS) applications. The NMF solution is guided to follow the Minimum Mean Square Error (MMSE) estimate under Gaussian mixture prior models (GMM) for the source signal. In SCSS applications, the spectra of the observed mixed signal are decomposed as a weighted linear combination of trained basis vectors for each source using NMF. In this work, the NMF decomposition weight matrices are treated as a distorted image by a distortion operator, which is learned directly from the observed signals. The MMSE estimate of the weights matrix under GMM prior and log-normal distribution for the distribution is then found to improve the NMF decomposition results. The MMSE estimate is embedded within the NMF cost function as a regularized NMF, and their corresponding update rules are driven in this paper. Experimental results show that, the proposed regularized NMF algorithm improves the source separation performance compared with using NMF only."
Transaction-Based Link Strength Prediction Using Matrix Factorizations,"The revolution of online social networks and methods of analyzing them have attracted interest in many research fields. Predicting whether a friendship holds in a social network between two individuals or not, link prediction, has been a heavily researched topic in the last decade. In this paper we research a related problem, link strength prediction, which aims to assign ratings or strengths to friendship links. A basic approach would be matrix factorization applied to only friendship ratings. However, the existence of extensive transactions among users may be used for better predictions. We propose a new type of multiple-matrix factorization model for incorporating a transaction matrix. We derive gradient descent update equations for learning latent factors that predict values in the target rating matrix. To evaluate the model, we introduce data from Cloob which is a popular Iranian social network as well as synthetic data."
Workflows for Computer Vision: Open Publication and Reproducibility of Experiments,"The inability to reproduce computational research is a rapidly growing area of concern in computer vision. In this paper, we incorporate a structured, end-to-end analysis methodology, based on workflows, to easily and automatically allow for standardized replication and testing of state-of-the-art models, inter-operability of heterogeneous codebases, and incorporation of novel algorithms. We demonstrate the utility of our approach by introducing a novel computer vision dataset and conducting an in-depth, comparative analysis of state-of-the-art methods on the new Atomic Pair Actions dataset using workflows. This allows us to re-use pre-existing workflows as well as incorporating new algorithms developed in heterogeneous codebases. The entire framework, including the workflows and the dataset, is then exported as web objects which can be executed via the web, or downloaded and imported into a compatible workflow system, by any user to re-create the full analysis or to change/extend the workflows as desired. In addition, we make the full dataset (the videos, their associated tracks with ground truth, and metadata) and all exported workflows widely available to the research community both as openly accessible web objects."
Sparse Manifold Alignment,"Previous approaches to manifold alignment are based on solving a (generalized) eigenvector problem. We propose a least squares formulation of a class of manifold alignment approaches, which has the potential of scaling better to real-world data sets. Furthermore, the least-squares formulation enables various regularization techniques to be readily incorporated to improve model sparsity and generalization ability. In particular, it enables using the $l_1$ norm regularization  framework to make previous manifold alignment algorithms more robust. The new approach can prune domain-dependent features automatically helping to improve transfer learning. This extension significantly broadens the scope of manifold alignment techniques  and leads to faster algorithms. We present detailed experiments to illustrate the approach using the domains of cross-lingual information retrieval and social network analysis."
Foveated Search Models That Learn Eye Movements,"This paper presents foveated search models that learn where to fixate inorder to improve perceptual performance on a simple visual search task. In particular, we combine models of eye movements during search with a perceptual learning model and a machine learning method for a task where the observer has to search for a target and then give a yes/no decision about its presence. We report simulation results for various eye movement models and learning methods. Foveated eye movements include maximum a posteriori (MAP), ideal searcher,random and systematic exploration models. We evaluate Bayesian and on-linelearning algorithms. Simulation results show that for the simple tasks the computationally tractable MAP model can approximate the ideal searcher irrespectiveof the learning algorithm. We also compare model performance (constrained toa human visibility map) to that of a human observer on the same task. Both human improvements of perceptual performance and convergence of eye movements towards the target location across sessions suggest that humans are not using either a random or systematic saccadic exploration or an on-line learning algorithm. Yet, humans are less efficient than the MAP eye movement model with Bayesianlearning."
Cluster-Based Active Learning to Address the Class Imbalance and Cold Start Problems,"Active learning (AL) has been used to improve the performance for supervised learning (SL) functions by selecting the training instances to be labeled.  However, there are two open problems which can actually make AL worse than passive learning:  class imbalance and cold start.  First, AL tends to ignore the minority label in the training instances making it unlikely that the function will ever predict the minority label.  Second, the quality for the instances selected depend on those previously selected.  This makes AL very sensitive to the order instances are selected for training.  To address this latter problem, cluster-based AL has been proposed, but they assume that instances in the same cluster have the same label, which is often untrue.  Further, cluster-based AL does not currently address class imbalance.  Therefore, we propose a novel new cluster-based AL powered by the Boundary of Use (BoU) framework which focuses on finding clusters which contain similar instances with multiple labels.  These BoU clusters are designed to capture the area around the decision boundary containing instances which are most informative for AL.  Our experiments, using 21 UCI datasets and four real-world datasets, show that cluster-based AL powered by the BoU framework improves test accuracy using three different SL systems.  Our approach is also more adept at addressing the two open problems than cluster-based AL."
A Framework for Enhancing Repair Mechanisms for Supervised Learning,"Repair mechanisms such as feature selection and noise correction have been used to improve the performance for supervised learning (SL) functions.  Furthermore, active learning has been used to identify cost-effective instances and could also be considered as a repair mechanism?removing less useful instances from training.  All these mechanisms have been enhanced with clustering to allow for more specific and localized repairs?hereby known as cluster-based repair?rather than performing repair on all the training data (i.e., universal repair).  However, traditional clustering?grouping similar instances with the same label?can make it difficult to know (for certain) whether a cluster still needs to be repaired or left alone.  Subsequently, repairs applied to these clusters can result in unnecessary repairs and overfitting which ultimately reduce function performance.  Therefore, we propose a novel framework called the Boundary of Use (BoU) framework which repairs clusters where there is a mixture of instances correctly and incorrectly labeled by the function and leaves correct clusters?no need for repair?and incorrect clusters?not repairable?alone.  Our experiments, using 21 UCI datasets, show that the cluster-based repair powered by the BoU framework reduces unnecessary repairs and overfitting and outperform traditional cluster-based repair and universal repair approaches on nine (repair mechanism ? SL system) configurations."
Topology Constraints in Graphical Models,"Graphical models are a very useful tool to describe and understand natural phenomena, from gene expression to climate change and social interactions. The topological structure of these graphs/networks is a fundamental part of the analysis, and in many cases the main goal of the study. However, little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data.In this work we propose extensions to the basic joint regression model for network estimation, which explicitly incorporate graph-topological constraints into the corresponding optimization approach. The first proposed extension includes an eigenvector centrality constraint, thereby promoting this important prior topological property. The second developed extension promotes the formation of certain motifs, triangle-shaped ones in particular, which are known to exist for example in genetic regulatory networks. The presentation of the underlying formulations, which serve as examples of the introduction of topological constraints in network estimation, is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge."
Scalable Heterogeneous Transfer Ranking,"Learning to rank aims to automatically learn a ranking function with point-wise, pair-wise, or list-wise cost functions and has been demonstrated to achieve superior performance in many information retrieval applications. It is known that the ranking problem usually requires significantly more labeling efforts per task. However, in practical application we can often obtain abundant labeled documents in popular languages but very few or even no labeled documents in less popular languages.  In this paper, we propose to study the problem of heterogeneous transfer ranking, a transfer learning problem with heterogeneous features in order to utilize the rich labeled data in popular languages to help the ranking task in less popular languages. We develop a large-margin algorithm, namely LM-HTR, to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We analyze the theoretical bound of the prediction loss and develop fast algorithms via stochastic gradient descent so that our model can be scalable to large-scale applications. Experiment results on four application datasets demonstrate the advantages of our algorithms over other state-of-the-art methods."
Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation,"Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available.Existing approaches strive to make the distribution of instances in one domain match that of the other domain.  Often, they are oblivious to individual differences of these samples and only tune to their statistical properties collectively as a population. Instead of this macroscopic view, we aim to be microscopic. In particular, we believe not all instances are created equally in terms of adaptability. Thus, it is beneficial to identify the most helpful instances for  adaptation. Mindful of this thought, we propose a landmark-based approach that broadens the notion of matching distributions  by examining it at the instance level. We define landmarks as a subset of labeled data instances in the source that are distributed most similarly to the target, thus presumably more amenable to being adapted. We then leverage the discovered landmarks to bridge the source to the target.  Specifically, we incorporate the landmarks in a cohort of auxiliary adaptation tasks that are provably easier to solve. The key intuition is that domain-invariant feature spaces for those auxiliary tasks form the basis to compose invariant features for the original problem. We show how this composition can be optimized discriminatively with the aid of landmarks,  requiring no labeled data from the target.  The proposed method is validated on standard benchmark datasets for object recognition. Empirical results show the proposed method outperforms the state-of-the-art significantly."
Stratified Tree Search: A Novel Suboptimal Heuristic Search Algorithm,"Traditional heuristic search algorithms use the ranking of states a heuristic function provides to guide the search. In this paper---with the object of improving suboptimality and runtime of search algorithms when only weak heuristics are available---, we present Stratified Tree Search (STS), a novel suboptimal heuristic search algorithm that uses a heuristic function to make a partition of the state space to guide search. We call the partition used by STS a type system. STS assumes that nodes of the same type will lead to solutions of the same cost. Thus, STS expands only one node of each type in every level of search. We empirically evaluated STS in heuristic search domains. Our results show that STS can find solutions of lower suboptimality in less time than standard heuristic search algorithms for finding suboptimal solutions."
A Polynomial Mechanism for Resource Allocation under Price Rigidities,"Computational issues on trading mechanisms for resource allocation have been studies intensively in the AI literature. The literature has focused almost entirely on economic models without price rigidities. However, it is ubiquitous in the real world that prices of items are not completely flexible but restricted to some admissible interval price rigidities for some economic or political reasons. This paper studies the computational issues ofdynamic mechanisms for selling multiple indivisible items under price rigidities. We propose a polynomial algorithm that can be used to find over-demanded sets of items, and then introduce a dynamic mechanism with rationing to discover constrained Walrasian equilibria under price rigidities in polynomial time."
Multiview Spectral Clustering via Pareto Optimization,"Traditionally, the input of spectral clustering is limited to single-view data. However, many real-world datasets come with multiple heterogeneous feature sets, which provide multiple views of the same data. Such datasets include scientific data (fMRI scans of different individuals), social data (different types of connections between people), web data (multi-type data), and so on. How to optimally combine knowledge from multiple views to help spectral clustering find a better partition remains a developing area. Previous work formulates the problem as a single objective function to optimize, typically by combining the views under a compatibility assumption and requiring the users to decide the importance of each view a priori. In this work, we propose a multi-objective formulation and show how to solve it using Pareto optimization. The Pareto frontier captures all possible good cuts without requiring the users to set the ``correct'' parameter. The effectiveness of our approach is justified by both theoretical analysis and empirical results on benchmark datasets."
A New Formulation for Deep Neural Net Optimization,"Deep neural nets are very difficult to train, even with parallel computers, because of the ill-conditioned nature of their objective function, which involves a deeply nested mapping from inputs to outputs. We propose a new formulation to optimize deep nets that directly addresses the ill-conditioning problem, based on an idea of auxiliary variables. This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. We solve the constrained problem with a quadratic-penalty approach using alternating optimization over the weights and the auxiliary coordinates. This procedure decouples into many independent subproblems and allows a trivial parallelization. In experiments using autoencoders of varying depth with image and speech data, we show our algorithm far outperforms all leading methods for deep net optimization."
"Combinatorial Multi-Armed Bandit: General Framework, Results and Applications","We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where multiple arms with unknown distributions organized in some unknown combinatorial structure to form super arms, each of which is a unit of play in each round. After a super arm is played, the outcome of related individual arms is revealed, and a reward for the super arm is given. The reward function only needs to satisfy a couple of mild assumptions and is otherwise unknown. Instead of knowing the specifics of the problem instance, we assume the availability of a computation oracle that takes the means of the distributions of arms and outputs a super arm that generates an $\alpha$-approximation of the optimal expected reward. The objective of a CMAB algorithm is to minimize {\em $\alpha$-approximation regret}, which is the difference in total expected reward between the $\alpha$ fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide two algorithms, CUCB and $\varepsilon_n$-greedy, and show that they achieve low regret of $O(\log n)$, where $n$ is the number of rounds played. We then demonstrate applications of our CMAB framework with several problem instances, namely probabilistic maximum coverage (PMC), social influence maximization, and matching bandit. The results on the first two problems are new, while the result on the last one matches the regret of a previous work that designed specifically for this problem."
High Dimensional Transelliptical Graphical Models,"We advocate the use of a new distribution family--the transelliptical--for robust inference of high dimensional graphical models. The transelliptical family is an extension of the nonparanormal family proposed by Liu et al. (2009). Just as the nonparanormal extends the normal by transforming the variables using univariate functions, the transelliptical extends the elliptical family in the same way. We propose a nonparametric rank-based regularization estimator which achieves the parametric rates of convergence for both graph recovery and parameter estimation. Such a result suggests that the extra robustness and flexibility obtained by the semiparametric transelliptical modeling incurs almost no efficiency loss. Thorough numerical experiments are provided to back up our theory."
Sparse Principal Component Analysis with missing observations,"In this paper, we study the problem of sparse Principal Component Analysis(PCA) in the high-dimensional setting with missing observations. Our goal isto estimate the first principal component when we only have access to partial ob-servations. Existing estimation techniques are usually derived for fully observeddata sets and require a prior knowledge of the sparsity of the first principal compo-nent in order to achieve good statistical guarantees. Our contributions is threefold.First, we establish the first information-theoretic lower bound for the sparse PCAproblem with missing observations. Second, we propose a simple procedure thatdoes not require any prior knowledge on the sparsity of the unknown first principalcomponent or any imputation of the missing observations, adapts to the unknownsparsity of the first principal component and achieves the optimal rate of estima-tion up to a logarithmic factor. Third, if the covariance matrix of interest admits asparse first principal component and is in addition approximately low-rank, thenwe can derive a completely data-driven procedure computationally tractable inhigh-dimension, adaptive to the unknown sparsity of the first principal componentand statistically optimal (up to a logarithmic factor)."
Tumor Gene Expression Purification Using Infinite Mixture Topic Models,"There is significant interest in using gene expression measurements to aid in the personalization of medical treatment.  The presence of significant normal tissue contamination in tumor samples makes it difficult to use tumor expression measurements to predict clinical variables and treatment response.  We propose a probabilistic method, TMMPure, to infer the expression profile of the cancerous tissue using a modified topic model that contains a hierarchical Dirichlet process prior on the cancer profiles. We demonstrate that TMMpure is able to infer the expression profile of cancerous tissue and improves the power of predictive models for clinical variables using expression profiles. "
Manifold Alignment Preserving Global Geometry,"This paper proposes a novel algorithm for manifold alignment preserving global geometry. This approach constructs mapping functions that project data instances from different input domains to a new lower-dimensional space, simultaneously matching the instances in correspondence and preserving global distances between instances within the original domains. In contrast to previous approaches, which are largely based on preserving local geometry, the proposed approach is suited to applications where the global manifold geometry needs to be respected. We evaluate the effectiveness of our algorithm for transfer learning in two real-world cross-lingual information retrieval tasks."
Regularized Off-Policy TD-Learning,"We present a novel $l_1$ regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization.A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm."
Multi-criteria Anomaly Detection using Pareto Depth Analysis,"We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria."
Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes,"To learn reliable rules that can generalize to novel situations, the brain must be capable of imposing some form of regularization. Here we suggest, through theoretical and computational arguments, that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system. The functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise. Noise on the inputs is shown to impose regularization, and when synchronization upstream induces time-varying correlations across noise variables, the degree of regularization can be calibrated over time. The resulting qualitative behavior matches experimental data from visual cortex."
Calibrated Elastic Regularization in Matrix Completion,"This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a unified analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. "
Guided Network Discovery in fMRI Data via Constrained Tensor Analysis ,We investigate the problem of network discovery which involves simplifying spatio-temporal data into nodes and edges. Such problems naturally exist in fMRI scans of human subjects which consist of activations of thousands of voxels over time with the aim to simplify them into the underlying cognitive network being used. We propose supervised and semi-supervised variations of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least squares solver that is easily implementable. For real fMRI data our approach can reproduce well known results in neurology regarding the default mode network in resting state healthy and Alzheimer affected individuals. We show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive scores both by itself and with additional clinical information.
Modeling Visual Clutter Using Parametric Proto-object Segmentation,"Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects defined by superpixel similarity in intensity, color, and texture, features. We introduce a novel parametric method of merging superpixels using mixtures of Weibull distributions of edge weights, then take the normalized number of proto-objects following partitioning as our estimate of clutter. We validated the model using clutter ratings of 90 images (SUN Dataset) obtained from humans, and showed that our method not only predicted clutter extremely well (r=0.76, p<0.001), but also outperformed other clutter prediction methods."
Parameter Learning for Submodular Quadratic Pseudo-Boolean Functions,"Submodular quadratic pseudo-Boolean functions play an important role in many inference tasks, particularly in computer vision. Their importance in recent years has grown due to their modeling capability and the development of very fast inference techniques based on max-flow/min-cut algorithms. However, learning the parameters of these models remains a challenge. In this work, we present a simple and efficient algorithm for learning the parameters of a pseudo-Boolean function from training data. Our method directly adjusts edge capacities in the max-flow/min-cut graph so as to minimize the discrepancy between the minimum cut in the graph and the cut induced by the ground truth assignment. We show how our algorithm relates to the subgradient method for structured support vector machines and perform extensive experiments to evaluate variants of our approach."
A System for Predicting Action Content On-Line and in Real Time before Action Onset in Humans ? an Intracranial Study,"The ability to predict action content from neural signals in real time before action onset has been long sought in the neuroscientific study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a ?matching-pennies? game against either the experimenter or a computer. In each trial, subjects were given a 5s countdown, after which they had to raise their left or right hand immediately as the ?go? signal appeared on a computer screen. They won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The working hypothesis of this experiment was that neural precursors of the subjects? decisions precede action onset and potentially also the awareness of the decision to move, and that these signals could be detected in intracranial local field potentials (LFP).We found that low-frequency LFP signals from a combination of 10 channels, especially bilateral anterior cingulate cortex and supplementary motor area, were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5s before the go signal with 68?3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 channels simultaneously, and tested it on retrospective data from 6 patients. On average, we could predict the correct hand choice in 80% of the trials, which rose to 90% correct if we let the system drop about 1/3 of the trials on which it was less confident. Our system demonstrates ? for the first time ? the feasibility of accurately predicting a binary action in real time for patients with intracranial recordings, well before the action occurs."
Fast Exact MAP Inference by Passing Incomplete Messages,"We propose a novel approach to faster exact MAP inference that builds on max-product (min-sum) message passing on clique trees and exploits the branch-and-bound idea. The high level procedure is to propagate incomplete messages over the clique tree while maintaining local upper bounds at sepsets. Our algorithm is guaranteed to converge and find the global optimal solution. Empirically we show that our method consistently demonstrates large savings over state-of-the-art methods on several different models with both synthetic and real world data. Our approach also suggests an interesting connection between exact and approximate MAP inference: If we can find tighter relaxations (over sub-graphs), we can make use of the lower bounds to perform exact inference even faster."
"Constrain, Train, Validate and Explain: A Classifier for Mission-Critical Applications","Classifiers used in mission-critical applications, where misclassification errors incurhigh costs, should be robust to training-set artifacts, such as insufficient ormisrepresentative coverage, as well as high levels of missing values. As such,they are required to support intensive designer-control, and a range of validationprocedures that go beyond cross-validation. For such applications, we advocatethe use of a family of classifiers that employ a factored model of the posteriorclass probabilities. These classifiers are simple, interpretable, allow their designersto enforce a variety of domain-specific constraints, and can handle missingdata both during training and at prediction time. Such classifiers are also capableof explaining their decisions in terms of the basic measured quantities."
Bayesian Estimation for Partially Observed MRFs,Bayesian estimation in Markov random fields is very hard due to the intractability of the partition function. The introduction of hidden units makes the situation even worse due to the presence of potentially very many modes in the posterior distribution. For the first time we propose a comprehensive procedure to approximate the evidence of partially observed MRFs based on the Laplace approximation. We also introduce a number of approximate MCMC-based methods for comparison but find that the Laplace approximation significantly outperforms these.
Time-Dependent Subset Mining via iHMM,"When we analyze a time series data with large cardinality of item sets (or, high-dimensional feature vectors), it is often the case that a pile of non-informativeitems disturbs mining of hidden dynamic patterns behind the data. In such cases, we need to find and extract a tiny portion of informative items that are related(informative) to the change of hidden patterns. In this paper, we address the problem of modeling time series data that are characterized by the large cardinalityof item sets and sparseness of related items among them, where the relatedness of items are time-dependent. We propose an extension of the infinite Hidden Markov Model so that only related items of each hidden state be automatically selected. We can improve estimation of hidden states by excluding unrelated (non-informative) items and focusing on the latent structure analysis of extracted related items. Combined with the nonparametric Bayes approach, the proposed model simultaneously selects related items and finds hidden states without knowing thenumber of states beforehand. The proposed model is experimentally verified and we show that both relatedness and time dependency are important for mining related items of time series data."
Multi-Domain Manifold Learning for Interaction Prediction,"Despite of the high dimensionality in many machine learning problems, data distributions in these high dimensional spaces usually span on certain low dimensional manifolds. In the last few years, extensive research efforts have been devoted to the utilization of manifold property on high dimensional data, e.g. dimension reduction methods preserving local structures of the manifolds. Motivated by the successes of these studies, we extend the manifold learning problem from single domain to multiple domain, especially on learning cross-domain interactive pairs. While many real-world applications (e.g. automatic image annotation) can be modelled as multi-domain interaction prediction, existing solutions to manifold learning fail to fully exploit the manifold property of the distributions. In this paper, we propose a general framework to bridge the gap, taking both manifold structures and known interaction/non-interaction information into account. To overcome the challenges of domain scaling and information inconsistency, we formulate the problem with Semidefinite Programming(SDP), including new constraints to improve the robustness of the learning procedure. A variety of optimization techniques are also designed to enhance the scalability of the problem solver. Effectiveness of the method is evaluated by experiments with two different tasks, including drug prediction and image annotation."
Model Selection in Markov Reward Processes,"Algorithms for solving Markov reward processes almost always start with the assumption that the state space is known. In this work we address the problem of how to use data to choose from a set of candidate discrete state spaces, where these spaces are constructed by a problem dependent domain expert. We discuss the difference between our proposed framework and the classical maximum likelihood framework, and give an example for such a criterion to fail. We propose alternative criterion and prove that it is consistent if the models are identifiable in an appropriate sense."
Kernelized Bayesian Matrix Factorization,"We extend kernelized matrix factorization with a full-Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (a) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas full-Bayesian treatments are not computationally feasible in the earlier approaches. (b) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our proposed method outperforms alternatives in predicting drug-protein interactions on two data sets."
Fast Convolutional Sparse Coding,"Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many signals (e.g. visual and acoustic) however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimisation problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and augmented Lagrange multipliers (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence. "
The Smoothness of The Stationary Distribution of Linear Predictive State Representations,"In this paper we consider linear predictive state representations (PSR), a modeling framework that has been proposed as an alternative to finite state partially observable Markov decision processes (POMDP), which is also capable of representing a strictly larger class of systems. Our main result is the proof of smoothness of the stationary distribution of a linear PSR with respect to the parameters of any finite memory policy. This property is important in cases when it is difficult to model the system accurately, while estimating expectations of different quantities from data is easy. Such result suggests that these estimates are robust with respect to slight changes in a policy. This result can also be seen as the first step in developing a perturbation theory for linear PSRs.In addition, we propose a new representation for the reward process, termed (linear) predictive-state reward process (PRP), which naturally extends a well known (linear) predictive state representations to capture a possibly continuous reward signal. Linear PRPs are, by design, suited for planning approaches that optimize average reward criterion. We then show that for a certain class of policies the average reward in linear PRPs is a smooth function of policy parameters. The result suggests that in fact the same should hold for any finite memory policies, remaining a subject of future work."
Deep Representations and Codes for Image Auto-Annotation,"The task of assigning a set of relevant tags to an image is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and fix an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of full sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we outperform or compete with existing annotation approaches that use over a dozen distinct image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efficient storage and fast comparisons. In our experiments, using deeper architectures always outperform shallow ones."
On High Dimensional Positive-definite Covariance Matrix Estimation,"We propose a novel approach called PLaCE (Positive-definite Large Covariance Estimation) for estimating high dimensional positive-definite covariance matrices. Our method can be viewed as an extension of the generalized thresholded operator (GTO, Rothman et. al 2010) with a positive-definite guarantee. Computationally, we derive an efficient algorithm named ISP (Iterative Soft-thresholding and Projection) based on the augmented Lagrangian method. Theoretically, we analyze the oracle properties of the PLaCE in the sparse covariance estimation under the different matrix norms. Empirically, we conduct the numerical experiments on both simulated and real data sets to illustrate the usefulness of the proposed method."
Enhanced statistical rankings via targeted data collection,"We study the dependence of the statistical ranking problem on the available pairwise data and propose a framework for which additional data may be collected to increase the informativeness of the ranking. Given a graph where vertices represent alternatives and pairwise comparison data is given on the edges, the \emph{statistical ranking problem} is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. For each edge $ij$, the pairwise comparison data consists of $w_{ij}$ comparisons between the alternatives $i$ and $j$ and  mean preference, written $y_{ij}$. Our goal, given an existing pairwise comparison dataset, $(w,y)$, is to augment this dataset, denoted $(\tilde w,\tilde y)$, so that  the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximally increases the informativeness of the ranking. Since there is a tradeoff between the amount of pairwise data to be ranked and the informativeness of the ranking, we constrain the total number of additional pairwise comparisons $\|\tilde{w}- w\|_{1}$. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding edge weights such that the $\tilde{w}$-weighted graph Laplacian has large second eigenvalue. This reduction of the data collection problem to spectral graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. "
Collaborative Filtering with Hybrid Restricted Boltzmann Machines,"We propose an novel framework for collaborative filtering based on RestrictedBoltzmann Machines (RBM), which extends previous RBM-based approaches inseveral important directions. First, we work with numbers as opposed to usingcategorical variables, which allows us to take the natural order between user-itemratings into account, e.g., we consider predicting a rating of 4 when the actual rat-ing is 5 to be better than predicting a rating of 2. More importantly, while previousRBM research modeled the correlation between item ratings only, we model bothuser-user and item-item correlations in a unified framework. Finally, we explorethe potential of combining the original training data with data generated by theRBM model itself in a bootstrapping fashion. The evaluation on the MovieLensdataset shows that our extended RBM model yields results that rival the predictionquality of the best previously-proposed collaborative filtering algorithms."
Local Learning with Local Patch Dissimilarity for Image Classification,"A new dissimilarity measure for images based on patches, called Local Patch Dissimilarity (LPD), was recently introduced in [1]. It was inspired from rank distance which is a distance measure for strings.There are many other patch-based techniques used in image processing. Patches contain contextual information and have advantages in terms of generalization. But most patch-based algorithms are heavy to compute with our current machines [2]. Despite LPD is also heavy computational, it has very promising results in image processing and works very well in image classification. In this paper we turn to unconventional learning methods to avoid the problem of the higher computational time on large sets of images. We conduct several experiments on large datasets using methods such as k-NN with filtering and local learning. All methods are based on LPD. The obtained results come to support the previous work of [1].  "
Identifying Stationary Behavior in Externally Driven Environments,"  In real-world settings, environments and sensory data are typically  subject to change over time. Sensor readings are a mixture of  changes due to control on actors of a system and resulting changes  of its internal state but might also contain changes in the  properties of the dynamical system itself. Such changes are caused  by faults, wear, adjusted system configuration, or other permanent  changes. The task of separating these two sources is hard because  the resulting effects in measurable signals might live on similar  time scales, moreover the irrelevant signal components can have  significant internal structure. We present a new approach based on  Stationary Subspace Analysis to detect changes in such complex  scenarios reliably. We demonstrate its capabilities on synthetic  data. Finally, we consider a real-world application, a gas turbine  simulation where we detect changes of internal parameters under  heavily varying external conditions caused by transient operation."
Fast variational inference for stochastic differential equations,We introduce a Gaussian variational mean field approximation for inference in continuous time stochastic differential equations. This approach allows us to express the variational free energy as a functional of the marginal moments of the approximating Gaussian process. A restriction of moments to piecewise polynomial functions over time makes the complexity of approximate inference for stochastic differential equation models comparable to that of  discrete time hidden Markov models. We demonstrate the algorithm on state and parameter estimation for nonlinear problems with up to forty state variables. 
Maximal Deviations of Incomplete U-statistics ?with Applications to Empirical Risk Sampling,"It is the goal of this paper to extend the Empirical Risk Minimization paradigm, from a practical perspective, to the situation where a natural estimate of the risk is of the form of a $K$-sample $U$-statistics, as it is the case in the $K$-partite ranking problem for instance. Indeed, the numerical computation of the empirical risk is hardly feasible if not infeasible, even for moderate samples sizes. Precisely, it involves averaging $O(n^{d_1+\ldots+d_K})$ terms, when considering a $U$-statistic of degrees $(d_1,\;\ldots,\; d_K)$ based on samples of sizes proportional to $n$. We propose here to consider a drastically simpler Monte-Carlo version of the empirical risk based on $O(n)$ terms solely, which can be viewed as an \textit{incomplete generalized $U$-statistic}, and prove that, remarkably, the approximation stage does not damage the ERM procedure and yields a learning rate of order $O_{\mathbb{P}}(1/\sqrt{n})$.Beyond a preliminary theoretical analysis guaranteeing the validity of this approach, numerical experiments are displayed for illustrative purpose."
Memory Constraint Online Multitask Classification,"We investigate online kernel algorithms which simultaneously processmultiple classification tasks while a fixed constraint is imposed onthe size of their active sets. We focus in particular on the design ofalgorithms that can efficiently deal with problems where the number oftasks is extremely high and the task data are large scale.  Two newprojection-based algorithms are introduced to efficiently tackle thoseissues while presenting different trade offs on how the availablememory is managed with respect to the prior information about thelearning tasks.  Theoretically sound budget algorithms are devised bycoupling the Randomized Budget Perceptron and the Forgetron algorithmswith the multitask kernel.  We show how the two seemingly contrastingproperties of learning from multiple tasks and keeping a constantmemory footprint can be balanced, and how the sharing of the availablespace among different tasks is automatically taken care of. We proposeand discuss new insights on the multitask kernel. Experiments showthat online kernel multitask algorithms running on a budget canefficiently tackle real world learning problems involving multipletasks."
Phase vs. amplitude ? learning from subjective image quality assessment,"In frequency-based representation of images, phase and amplitude convey complementary information. Phase has been regarded as dominating the image appearance, however power (amplitude) spectra have recently been found useful for image classification. In the primary visual cortex (V1), simple cells are sensitive to and thereby encode the phase of visual stimuli, while complex cells, being majority in V1, show phase-invariance and encode the energy (magnitude) of simple cells? spikes. In this paper, we attempt to quantitatively exploit the relative importance of phase and amplitude to visual perception by learning from subjective image quality assessment. We designed an image quality metric based on the weighted combination of the amplitude and phase errors, and determined the weights so as to maximize the prediction accuracy of the metric over subjectively- rated databases, where a joint optimization over multiple databases strengthened the reliability of weights. The results confirm that: 1) both the phase and the amplitude are necessary for image quality assessment; 2) the amplitude becomes more important at the finest image scale while the phase dominates at the coarser scale. Moreover, the multiplicative combination of the amplitude and phase errors plausibly interprets the visual perception on negative images."
Identification of Consistent Brain Networks via Maximization of Predictability of Functional Connectivity from Structural Connectivity,"Recent studies have suggested that structural brain connectivity is strongly correlated with functional connectivity. However, it is still largely unknown what brain networks best possibly exhibit such close structural/functional connectivity relationship and how this close relationship can guide the identification of brain networks. This paper presents a novel framework to infer brain networks that are consistent across multiple neuroimaging modalities and across individuals. Our basic premise is that the predictability of functional connectivity from structural connectivity within each brain network should be maximized, which is formulated by and solved via a novel feedback-regulated multi-view spectral clustering algorithm. We applied and tested the proposed algorithm on the multimodal structural and functional brain networks of 50 healthy subjects, and obtained promising results. Our validation experiments demonstrated that the derived brain networks are in agreement with current neuroscience knowledge and offer novel insights into the close relationship between brain structure and function."
Music Generation with Weighted Finite-state Transducers,"We approach the task of musical style imitation by probabilistically modeling the melody and harmony of music pieces in the framework of weighted finite-state transducers (WFSTs), which have been used successfully for probabilistic models in speech and language processing. We divide the generation process into different steps, each performed by inference though transducers. We present a method to imitate local and global structure in the melody of music pieces, and a method for four-part harmonization that models vertical and horizontal structure in the generated harmonization. The weights of our transducers are learned with maximum likelihood estimation from a corpus of music pieces. We compare the predictive power of our models against that of existing approaches."
Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs,"Given $\alpha,\epsilon$, we study the time complexity  required to improperly learn a halfspace with misclassification  error rate of at most $(1+\alpha)\,L^*_\gamma + \epsilon$, where  $L^*_\gamma$ is the optimal $\gamma$-margin error rate. For $\alpha  = 1/\gamma$, polynomial time and sample complexity is achievable  using the hinge-loss. For $\alpha = 0$, \cite{ShalevShSr11} showed  that $\poly(1/\gamma)$ time is impossible, while learning is  possible in time $\exp(\tilde{O}(1/\gamma))$.  An immediate  question, which this paper tackles, is what is achievable if $\alpha  \in (0,1/\gamma)$.  We derive positive results interpolating between  the polynomial time for $\alpha = 1/\gamma$ and the exponential  time for $\alpha=0$. In particular, we show that there are cases in  which $\alpha = o(1/\gamma)$ but the problem is still solvable in  polynomial time. Our results naturally extend to the adversarial  online learning model and to the PAC learning with malicious noise  model."
Regularized Mapping to Latent Structures and Its Application to Web Search,"The task of matching data from two heterogeneous domains naturally arises in various areas, for example, in real-world web search. However, to our knowledge, there is no principled approach to learning a matching model. In this paper, we propose a framework for matching heterogeneous objects, which renders a rich family of matching models when different regularizations are enforced. With orthonormal constraints on the mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with $\ell_1$+$\ell_2$ type of regularization,  we obtain a new model called \emph{Regularized Mapping to Latent Structures} (RMLS).  RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization.  As another contribution, we give a generalizationanalysis of this matching framework, and apply it to both PLS and RMLS.  We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods,  while RMLS significantly speeds up the learning process."
Learning with Marginalized Corrupted Features,"An important goal of machine learning is to develop predictors that are robust to noise in the observations. In this paper, we consider a particular type of observation noise in which features are ``blanked out'' with some probability. Such blank-out noise occurs, \emph{e.g.} when sensors measuring features (temporarily) break down or when particular words related to a topic are not observed in a document. A simple way to train predictors that are robust to such noise is to extend the training data with training examples in which some of the variables are blanked out at random, but such an approach is computationally costly. This paper presents a new approach, called \emph{marginalized corrupted features} (MCF), that trains robust predictors by minimizing the expected value of the loss function under the blank-out noise model. Experimental evaluation of our approach reveals that the resulting predictors are not only more robust to sensors breaking down, but that they also perform substantially better on data with high-dimensional, heavy-tailed features, such as bag-of-words text documents. "
Structured sparse coding via group gating,"Multiplicative feature learning models, like the Gated Boltzmann Machine,  are a recent extension of sparse coding for modeling relations and image transformations.  A potential shortcoming of these methods is that they do not allow any given feature to be transformed in multiple different ways, because features are matched in pairs.  We propose a ``group gating'' model that addresses this issue by allowing the re-use of filters.  In the model, features form groups, and multiplicative interactions are allowed between all features within each group. We demonstrate that the group gating extension can lead to improved performance in transformation extraction tasks.  We also show that learning on natural videos leads to filter-groups of similar frequency and orientation and of varying phase, as well as to ``pinwheel'' structures in the case of overlapping groups, providing a  new interpretation of this effect known previously from subspace energy models."
Directly Optimizing 0-1 Loss for Large-Scale Nonlinear Transductive Multiclass Classification,"We take a new look at graph-based transductive classifiers of min-cut type (MCCs), and we offer a new derivation for them as regularized risk minimizers for non-parametric, discrete-valued function spaces. MCCs directly optimize the expected 0\u20131 loss without need for a convex surrogate loss and they can be naturally formulated in the multi-class situation without need for one-vs-one or one-vs-rest constructions. Nevertheless, they can be trained efficiently (exactly in the two-class case, approximately with factor 2 guarantee in the multi-class case) using discrete energy-minimization techniques. This allows scaling them to large datasets, as we show in experiments on standard computer vision datasets"
Analog readout for optical reservoir computers,"Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers."
Robust Out-of-sample Extension for the Local and Global Consistency Algorithm,"The contribution of this paper is twofold. First, we reformulate the Local and Global Consistency (LGC) algorithm and show how it can be computed in linear time with the number of unlabeled examples combining an approximated eigendecomposition with Woodbury's formula. Last, we provide a robust out-of-sample extension for the LGC algorithm using this novel formulation. We provide a naive approximation to turn the proposed out-of-sample extension feasible for large-scale problems. Experiments on a number of benchmark data sets show the effectiveness of the proposed approach."
Visual Interestingness in Image Sequences,"Interestingness -- The power of attracting or holding one's attention(http://www.thefreedictionary.com/Interestingness, 2012/02/22).Visual attention is guided by experience with similar situations. Such experience can make us surprise and excite when unexpected visual events occur, or make us wait for an interesting upcoming event. Consider for example the image sequences in the figure on the left. The spider in front of the camera or the snow on the lens are examples of events that deviate from the context since they violate the expectations, whereas the egg in the stork's nest is an expected but exciting event that attracts huge attention. In this work we investigate what humans consider as interesting in image sequences and to what extent and why current state-of-the-art computer vision methods can automatically detect such interesting events."
Minimizing Sparse High-Order Energies by Submodular Vertex-Cover,"Inference on high-order graphical models has become increasingly important in recent years. We consider energies with simple 'sparse'high-order potentials. Previous work in this area uses either specialized message-passing or transforms each high-order potential to the pairwise case.We take a fundamentally different approach, transforming theentire original problem into a comparatively small instance of a submodularvertex-cover problem. These vertex-cover instancescan then be attacked by standard pairwise methods,where they run much faster (4--15 times) and are often more effectivethan on the original problem.We evaluate ourapproach on synthetic data, and we show that our algorithmcan be useful in a fast hierarchical clustering and model estimation framework."
Learning in Real-Time in Repeated Games,"Despite much progress, state-of-the-art learning algorithms for repeated games still often require thousands of moves to learn effectively, even in two-agent, two-action games.  Our goal is to find algorithms that learn effective strategies in tens of moves against many kinds of associates.  Toward this end, we describe a new method designed to increase the learning speed and proficiency of expert learning algorithms.  We show that this method improves four expert learning algorithms, and produces an algorithm that quickly learns effective strategies in many two-agent, two-action repeated matrix games played against many associates."
On the Relation of Loss-Based Error Bounds to Discriminative Training Criteria,"General Bayes decision theory involves a loss function which is linked to the error rate of the corresponding classification task. Many pattern recognition tasks like automatic speech recognition, part-of-speech tagging, machine translation, and other string (word sequence) recognition tasks use the symbol (word, character, tag, etc.) error rate as evaluation measure based on a non-trivial loss function. Opposed to string recognition tasks, most pattern classification tasks assume classes having no symbol level and are evaluated using the classification error performance measure based on the simple 0-1 loss function (cost 0/1 for correct/false classification). We follow a principled approach which derives discriminative training criteria from bounds on the loss-based error. The derived criteria have a sound property ---- in case of infinite training data the corresponding loss-based error is minimized. Theoretical error optimal bounds and criteria are a novelty for loss-based string recognition tasks. Furthermore, the minimum phoneme error (MPE) criterion, which is the state-of-the-art discriminative training criterion in ASR, is shown to be an approximation to one of the proposed criteria. This result connects MPE to the corresponding loss-based error and gives a better and so far unknown theoretical justification for the practical effectiveness of MPE. The theoretical results are complemented by experiments comparing the novel and state-of-the-art discriminative training criteria on large scale ASR tasks."
State Abstraction in Reinforcement Learning by Eliminating Useless Dimensions,"Q-learning and other linear dynamic learning algorithms are subject to Bellman?s curse of dimensionality for any realistic learning problem. This paper introduces a framework for satisficing state abstraction ? one that reduces state dimensionality, improving convergence and reducing computational and memory resources ? by eliminating useless state dimensions. Statistical parameters that are dependent on the state and Q-values identify the relevance of a given state space to a task space and allow state elements that contribute least to task learning to be discarded. Empirical results of applying state abstraction to a canonical single-agent path planning task and to a more difficult multi-agent foraging problem demonstrate utility of the proposed methods in improving learning convergence and performance in resource-constrained learning problems."
Near-optimal Batch Mode Active Learning and Stochastic Optimization,"Active learning can lead to dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing,  surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance.In this paper, we consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some case, surprisingly, the use of batches only increases the cost by a constant factor independent of the batch size, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on active learning tasks, as well as adaptive influence maximization in social networks."
Learning Grouped Parameters in Undirected Graphical Models,"In large-scale applications of undirected graphical models, similar kinds of relations can occur frequently and give rise to repeated occurrences of similar parameters. Therefore, it is often beneficial to group the parameters for more effective learning. In cases when the grouping is unknown, allowing the parameter learner to automatically identify these groups can lead to a more accurate estimate and a better understanding of dependence among the variables. In this paper, we provide such a method to learn groups during parameter learning. Specifically, we place a Dirichlet process prior on the parameters in the graphical models, which can avoid the model selection problem, and is useful when we do not know the number of groups in advance. We solve the posterior inference problem with a Gibbs sampling algorithm integrated with classical parameter learning methods for undirected graphical models. Our parameter learning method does not only estimate parameters for undirected graphical models, but can also identify the possible latent groups among the parameters. We evaluate our grouping-aware parameter learning method and the classical grouping-blind parameter learning method on different undirected graphical models, and our method significantly outperforms the grouping-blind method when there indeed exist groups. When there are no groups among the parameters in the ground truth, our method does not lose much estimation accuracy. "
Extending Multi-Atlas Label Fusion to Groupwise Segmentation,"Groupwise segmentation that simultaneously segments a set of images and ensures the segmentations for all images are consistent with each other usually work better than segmenting each image independently. However, existing groupwise segmentation techniques are all developed based on simple and less powerful segmentation techniques, which limits the performance of groupwise segmentation when compared with other leading non-groupwise segmentation techniques. To address this problem, we develop a novel statistical model to extend the multi-atlas label fusion technique, which has shown to be very competitive for challenging biomedical image segmentation problems, for groupwise segmentation. Experiments on hippocampus segmentation in magnetic resonance images show the effectiveness of the new technique."
Learning mid-level representations by learning to relate viewpoints,"The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks such as stereopsis and motion analysis.  We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to observed transformations.  We show how this allows us to learn pose-invariant features of objects by observing how the objects change.  We also describe a dataset of objects undergoing 3-D transformations, that we use to evaluate the model."
Perfect Dimensionality Recovery by Variational Bayesian PCA,"The variational Bayesian (VB) approach isone of the best tractable approximations to the Bayesian estimation,and it was demonstrated to perform well in many applications.However, its good performance was not fully understood theoretically.For example, VB sometimes produces a sparse solution,which is regarded as a practical advantage of VB,but such sparsity is hardly observed in the rigorous Bayesian estimation.In this paper, we focus on probabilistic PCA andgive more theoretical insight into the empirical success of VB.More specifically, for the situation where the noise variance is unknown,we derive a sufficient condition for perfect recovery of the true PCAdimensionalityin the large-scale limitwhen the size of an observed matrixgoes to infinity with its column-row ratio fixed.In our analysis, we obtain bounds for a noise variance estimatorand simple closed-form solutions for other parameters,which themselves are actually very useful for better implementation of VB-PCA."
Mirror Descent Meets Fixed Share (and feels no regret),"Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters."
Downsampling a neighborhood graph,"Given a large neighborhood graph $G$, we would like to downsample it to some smaller graph $G'$ with much fewer vertices. This downsampling procedure should ``keep the geometry in the graph invariant''. To this end we define the notion of geometry-preserving downsampling. We then introduce a simple downsampling procedure that is based on a uniform subsample of vertices that is connected based on shortest path distances in the original graph. We prove that this procedure is geometry-preserving if it is applied to random geometric graphs like $k$-nearest neighbor graphs or $\eps$-graphs. We also show that some other popular downsampling algorithms are not geometry-preserving. "
Indexed Optimization: Learning Ramp-Loss SVM in Sublinear Time,"Multidimensional indexing has been frequently used for sublinear-time nearest neighbor search in various applications. In this paper, we demonstrate how this technique can be integrated into learning problem with sublinear sparsity like ramp-loss SVM. We propose an outlier-free convex-relaxation for ramp-loss SVM and an indexed optimization algorithm which solves large-scale problem in sublinear-time even when data cannot fit into memory. We compare our algorithm with state-of-the-art linear hinge-loss solver and ramp-loss solver in both sufficient and limited memory conditions, where our algorithm not only learns several times faster but achieves more accurate result on noisy and large-scale datasets."
Risk Scores for Progression to Alzheimer?s Disease with Gaussian Processes,"Accurately identifying which mild cognitive impairment patients will go on todevelop Alzheimer?s Disease will be critical to finding the right populations foreffective treatment. Most previous work in this area has centered around usingimage and other biomarker data in multivariate classification techniques such assupport vector machines. However these techniques give categorical class decisions.Here we train a Gaussian processes classifier to perform classification ofAlzheimer?s disease and mild cognitive impairment patient subjects, combiningimaging with other biomarker data by multikernel learning in a fully Bayesianframework. This offers advantages such as automatic setting of model parametersvia type II maximum likelihood and probabilistic class predictions that are treatedas a risk score for conversion to Alzheimer?s disease when applied to a populationof mild cognitive impairment patients. These scores correlate well with actualchances of conversion, and predict conversion with good accuracy when used as abinary classifier."
Sparse Additive Matrix Factorization for Robust PCA,"Principal component analysis (PCA) can be regarded as approximating adata matrix witha low-rank one by imposing sparsity on its singular values,and its robust variant further captures sparse noise.In this paper, we extend such sparse matrix learning methods,and propose a novel framework called sparse additive matrix factorization(SAMF).SAMF systematically inducesvarious types of sparsityby the so-called model-induced regularization in the Bayesian framework.We  propose an iterative algorithm calledthe mean update (MU) for the variational Bayesian approximation to SAMF, which gives the global optimal solution for a large subset of parameters in each step.We demonstrate the usefulness of our  methodon artificial dataand the foreground/background video separation."
Novelty Detection with Extreme Function Theory,"We introduce extreme function theory as a novel method by which probabilistic novelty detection may be performed over functions, where the functions are represented by time-series of (potentially multivariate) discrete observations. We set the method within the framework of Gaussian processes (GP), which offers a convenient means of constructing a distribution over functions. Whereas conventional novelty detection methods aim to identify individually extreme data points, w.r.t. a model of normality constructed using examples of ?normal? data points, the proposed method aims to identify extreme functions, w.r.t. a model of normality constructed using examples of ?normal? functions, where those functions are represented by time-series of observations."
Design of Nonlinear Phase Oscillators with Custom Limit Cycle Shape and Convergence Behavior,In this contribution we present a general way to design low-dimensional nonlinear phase oscillators with arbitrary limit cycle shape and global asymptotic stability. We show examples for which the solution of the nonlinear oscillator with an arbitrary limit cycle shape can be obtained in closed form. The elegance of the oscillator allows for easy extension of the basic system to incorporate other interesting properties such as custom convergence behavior. Numerical simulation is used to show the properties of the proposed oscillator. We also demonstrate two example applications of the introduced oscillator: 1) We show how to couple a number of these oscillators to create a multidimensional Central Pattern Generator. We use this Central Pattern Generator to encode human kinematics as a dynamical system.  2) We present how an adaptive landscape shaping rule can be written for this oscillator to reduce the tracking error when controlling periodic movements of a mechanical system with a simple and suboptimal P-controller.
Convex Shape Priors for Isometry-Invariant Variational Image Segmentation,"Convex relaxations of variational approaches to image segmentation constitute an active field of research. Convex state-of-the-art functionals penalize segmentation boundaries in terms of length or curvature, whereas variational approaches that take into account more specific shape knowledge suffer from non-convexity, thus requiring careful initializations to converge. Moreover, invariant shape comparison is only implicitly achieved by additionally optimizing over transformation parameters, aggravating issues of non-convexity.This paper combines for the first time convex state-of-the-art variational segmentation in terms of continuous cuts with a variational shape prior, based on shape representation by metric structures that enables fully invariant shape comparison and matching, while preserving convexity of the overall variational approach. Thus standard convex programming techniques can be applied to variational segmentation enhanced by invariant shape priors."
Boundary Preserving Distance Metric for Superpixelization ,"Superpixel is a continuous pixel region sharing similar characteristics, such as color, texture, etc. In recent years, it has been widely accepted as an alternative to physically-defined grid pixels to aid and enhance further image analysis in various applications. In this paper, a generalized superpixelization algorithm with boundary preserving distance metric is proposed. It outperforms state-of-the-art superpixelization algorithms in three aspects: 1. It can be generalized to both color and highly-textured images; 2. Generated superpixels are more compact with less number of superpixels; 3. Less computational complexity and memoryconsumption."
Graph Estimation From Multi-attribute Data,"Many real world network problems often concern multivariate nodal  attributes such as image, textual, and multi-view feature vectors on  nodes, rather than simple univariate nodal attributes. The existing  graph estimation methods built on Gaussian graphical models and  covariance selection algorithms can not handle such data, neither can  the theories developed around such methods be directly  applied. In this paper, we propose a new principled framework for  estimating multi-attribute networks. Instead of estimating the  partial correlation as in current literature, our method estimates  the {\it partial canonical correlations} that naturally accommodate  complex nodal features.  Computationally, we provide an efficient  algorithm which utilizes the multi-attribue  structure. Theoretically, we provide sufficient conditions which  guarantee consistent graph recovery. Empirically, we apply our  method on a genomic dataset to illustrate its usefulness. "
Inverse Reinforcement Learning: An Alternative Definition,"This paper discusses the foundations of the definition of the Inverse Reinforcement Learning (IRL) problem. Given a finite-Markov Decision Process (MDP) without reward function and an expert policy, IRL is usually stated as the problem of finding a reward function for which the expert policy is optimal. This problem is clearly ill-posed as the zero-reward function is always solution. Thus, in order to give a more mature definition of the IRL problem, we introduce the notions of set-policy and optimality for a set-policy. These notions allow us to create a particular partition of the space of reward functions for which each part corresponds to a unique set-policy and inversely. Moreover, each set-policy is optimal for each reward of its corresponding part of the partition and only them. Thanks to this partition, we give a new and well-posed definition of IRL. Based on this framework, we introduce a new algorithm."
Recovering Block-structured Activations Using Compressive Measurements ,"We consider the problem of detection and localization of a small block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. However, contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that in our problem the magnitude of the weakest signals one can reliably localize is strongly influenced by both structure and the ability to choose measurements adaptively. We derive tight upper and lower bounds for the detection and estimation problems, under both adaptive and non-adaptive measurement schemes. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation."
Random function priors for exchangeable graphs and arrays,"A fundamental problem in the analysis of relational data---graphs, matrices or higher-dimensional arrays---is to extract a summary of the common structure underlying relations between individual entities. A successful approach is latent variable modeling, which summarizes this structure as an embedding into a suitable latent space. Results in probability theory, due to Aldous, Hoover and Kallenberg, show that relational data satisfying an exchangeability property can be represented in terms of a random measurable function. In a Bayesian model, this function constitutes the natural model parameter, and we discuss how available latent variable models can be classified according to how they implicitly approximate this parameter. We obtain a flexible yet simple model for relational data by representing the  parameter function as a Gaussian process. Efficient inference draws on the large available arsenal of Gaussian process algorithms; sparse approximations prove particularly useful. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases."
Multi-Source Sensing with Group Reliablity Ranking,"Prediction and decision-making tasks are often negatively impacted by the low-quality of the data collected from multiple sensing sources. In order to address these issues, we study the joint ranking of the reliability of data sources and infer their true values from their observations. Data sources are very often highly interdependent. Clearly, this interdependence can play an important role in the reliability estimation process as well. This is achieved by latent grouping of the dependent sources, and analyzing the underlying reliability at this level. Different groups usually contain conflicting observations from one other. In order to resolve such conflicts and improve the data quality, we assume that the observations from each group are not equal, and can be ranked based on their reliability. For this purpose, we impose a prior for the group reliability in a Markov chain structure, and rank the group reliability levels of the observations in a Bayesian inference framework. The highly ranked groups in the chain provide more information about data objects, and thus play a more important role in the true value inference. Finally, we demonstrate the effectiveness of the proposed approach on two real data sets."
Inverse Reinforcement Learning through Structured Classification,"This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called featureexpectation of the expert as the parameterization of the score function of a multiclass classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving a single time the direct RL problem. Moreover, up to the use of some heuristic, it may work with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator."
Classification with Asymmetric Label Noise,"We consider the problem of binary classification when the training labels are noisy. Previous theoretical work on this problem assumes that the two classes are separable, or that the label noise is independent of the class label. We present a general framework that allows for overlapping class-conditional distributions (so that the true labels are not deterministic) and that accommodates asymmetric label noise. This problem is motivated by several applications including nuclear particle classification, where background radiation and other environmental factors make it impossible to obtain pure training samples. We first identify a necessary and sufficient condition that makes the unknown noise proportions identifiable. Under this assumption, we describe estimation of the Type I and Type II errors, and use these estimates to design a classification rule that is consistent with respect to the minmax criterion. Our approach depends critically on recent results for mixture proportion estimation, which is the problem of estimating the percentage of one distribution that is present in another."
Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search,"Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sampled-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration."
Mining the brain with a theory of visual attention,"We propose a new target objective for BCI systems in which a parametric model of early visual perception follows the EEG decoder stage. This approach enables the supervised extraction of EEG components that jointly predict behavioral responses. We analyze the pre-stimulus EEG activity from a letter-recognition task using two EEG decoders running in stereo, and detect distinct components of the EEG that predict separable attentional parameters on a single-trial level."
Dimensionality Dependent PAC-Bayes Margin Bound,"Margin is one of the most important concepts in machine learning. Previous margin bounds, both for SVM and for boosting, are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or infinite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors fixed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of finite dimension; and the two bounds tend to be equivalent as the dimension goes to infinity. In addition, we show that the VC bound for linear classifiers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classifiers."
Local stability and robustness of sparse dictionary learning in the presence of noise,"A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries and noisy signals, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations."
Exact Cheeger-Cuts through Discrete Newton Method,"The Cheeger-cut is one of the most popular formulations for {?it balanced clustering} that has been actively discussed in machine learning. In this paper, we propose an efficient algorithm for finding an exact Cheeger-cut. We formulate the Cheeger-cut as a parametric submodular-minimization problem and apply the discrete Newton method, whose convergence to an optimal solution is guaranteed. We derive the optimality conditions for the sub-problem solved at each iteration and develop an efficient algorithm for this problem, which is calculated as submodular minimization using the maximum-flow method. We also analyze the computational complexity of the proposed algorithm, and consider an extension to multiple clusters although this is no longer solvable as a convergent procedure. The performance of the proposed method is investigated through empirical experiments."
Temporal Abstraction in Reinforcement Learning based on Holonic Concept Clustering and Attentional System,"This paper proposes a new method which extracts bottleneck states automatically based on abstraction concepts for reinforcement learning agents in offline/online manner -incremental-. Generally, the existing mechanisms for creating temporally extended actions need to burdensome of calculations and eventually prone to error. Our approach is built on lines of researches from cognitive science and behavior analysis. We utilized attentional mechanisms as an effective tool to extract bottlenecks. Holonic concept clustering and attentional-system are the motivation and the core of the proposed method. The experimentations confirmed that the proposed method is able to identify bottlenecks with more precision and thereupon the speed of learning and the ability of knowledge transferring are improved significantly. Also, the time complexity of the proposed method is considerably less in comparison to other similar methods."
Biased and Unbiased Natural Actor-Critics,"We show that NAC-LSTD and eNAC, two popular discounted-reward natural actor-critics, follow biased estimates of the natural policy gradient. We derive the first unbiased discounted-reward natural actor-critics using batch and iterative approaches to gradient estimation and prove their convergence to globally optimal policies for discrete problems and locally optimal policies for continuous problems. We discuss what the bias does to the system and suggest that in some situations it may be desirable."
Bayesian Nonparametric Image  Super-resolution,"Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data.  We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior.  However, this algorithm is not feasible for large-scale data.  To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries  in a fraction of the time needed by the Gibbs sampler. "
Learning Robust Low-Rank Representations,"In this paper we present a comprehensive framework for learning robustlow-rank representations by combining and extending recent ideas forlearning fast sparse coding regressors with structured non-convexoptimization techniques. This approach connects robust principalcomponent analysis (RPCA) with dictionary learning techniques andallows its approximation via trainable encoders. We propose anefficient feed-forward architecture derived from an optimizationalgorithm designed to exactly solve robust low dimensionalprojections. This architecture, in combination with different trainingobjective functions, allows the regressors to be used as onlineapproximants of the exact offline RPCA problem or as RPCA-based neuralnetworks. Simple modifications of these encoders can handlechallenging extensions, such as the inclusion of geometric data transformations.We present several examples with real data from image, audio, and videoprocessing. When used to approximate RPCA, our basic implementationshows several orders of magnitude speedup compared to the exactsolvers with almost no performance degradation. We show the strengthof the inclusion of learning to the RPCA approach on a music source separationapplication, where the encoders outperform the exact RPCA algorithms,which are already reported to produce state-of-the-art results on a benchmarkdatabase. Our preliminary implementation on an iPad showsfaster-than-real-time performancewith minimal latency."
Break and Conquer: Efficient Correlation Clustering for Image Segmentation,"We present a probabilistic model for image segmentation and an efficient approach to find the best segmentation. The image is first grouped into superpixels and a local information is extracted for each pair of spatially adjacent superpixels. The global optimization problem is then cast as correlation clustering which is knownto be NP hard. This study demonstrates that in many cases, finding the exact global solution is still feasible by exploiting the characteristics of the image segmentationproblem that make it possible to break the problem into subproblems. Each sub-problem corresponds to an automatically detected image part. The reducedcomputational complexity of the proposed optimization algorithm and the improved image segmentation performance are demonstrated on manually annotatedimages."
Hamming Distance Metric Learning,"Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs.  We develop a new loss-augmented inference algorithm that is quadratic in the code length.  We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes."
Beyond Distance Metric: Learning Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community. For example, our method achieves 89.3\% accuracy on the ``Labeled Face in the Wild'' (LFW) dataset, which outperforms the best reported results under the same setting in recent years."
Symmetry Detection by Distributed Synchrony of Spiking VLSI Neurons,"The detection of geometrical symmetries in visual scenes plays a key role in both animal perception and machine vision. Such types of sophisticated pattern detection can be obtained via spike-to-spike synchrony in recurrent networks of Integrate & Fire (I&F) neurons. To determine the network properties and the conditions required for synchronization we apply a formal contraction theory analysis using weakly coupled oscillator models and show how, under these conditions, the stability of the synchronous state can be guaranteed. These conditions can be experimentally verified through the measurement of the Phase Response Curve (PRC). To demonstrate the reliability of the method and its robustness to noise and parameter variability we used it to implement a bilateral symmetry detection network in analog/digital neuromorphic hardware and applied the system to real-time symmetry detection in response to real-world sensory data, provided by an event-based silicon retina. The silicon neurons quickly synchronize when a symmetric object is aligned with respect to the scene vertical axis. The presence of a symmetric input stimulus is reported by a separate read-out network of I&F neurons used as coincidence detectors. Our results demonstrate how this theory can be used to successfully configure low-power neuromorphic system for robust real-time pattern detection, making a central use of precise spike timing to detect user-specified symmetries present in visual scenes."
Sampling based approximation schemes for Normalized Cuts,"The Normalized Cuts (NCut) objective seeks to partition a graph into roughly balanced clusters, and forms the cornerstone of a wide variety of applications in computer vision and machine learning. Finding the optimal normalized cut is NP-hard, and only few results are known in terms of approximation guarantees for various solution strategies. Relaxations like spectral clustering perform quite well in practice but are difficult to analyze in terms of approximation ratios. This paper provides a sampling based approximation scheme for the Normalized Cuts problem.  We derive a polynomial-time algorithm which yields an approximation ratio of $(1+\epsilon)$ with constant probability."
Nonparametric Bayesian Clustering via Infinite Warped Mixture Models,"We introduce a flexible class of mixture models for clustering and density estimation. Our model allows clustering of non-linearly-separable data, produces a potentially low-dimensional latent representation, automatically infers the number of clusters, and produces a density estimate. Our approach makes use of two tools from Bayesian nonparametrics: a Dirichlet process mixture model to allow an unbounded number of clusters, and a Gaussian process warping function to allow each cluster to have a complex shape. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, and performs much better than infinite Gaussian mixture models at discovering meaningful clusters."
Clustering by Nonnegative Matrix Factorization Using Graph Random Walk,"Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity."
Delay Compensation with Dynamical Synapses,"Time delay is pervasive in neural information processing. To achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system. In the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner. The state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents. The parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network. We also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude."
A Gaussian Latent Variable Model for Ranking,"We describe a Gaussian latent variable model for ranking. The model learns a linear scoring function to maximize the probability that randomly chosen pairs of examples are correctly ranked. We show how to perform inference in this model and derive the Expectation-Maximization (EM) algorithm that monotonically increases the likelihood of correct ranking. We also highlight the intuitive form of the EM algorithm: at each iteration, the weight vector is re-estimated by a simple least-squares update. Finally, we explore two extensions of the model, based on kernels and boosting, to learn nonlinear ranking functions. The model?s effectiveness is demonstrated on problems in AUC maximization and information retrieval."
An Anytime Algorithm for Exact Bayesian Network Structure Learning,"Learning a Bayesian network structure that optimizes a scoring function for a given dataset is NP-hard. In recent years, several exact algorithms have been developed for learning optimal Bayesian network structures. Most of these algorithms only find a solution at the end of the search, so they fail to find any solution if stopped early for some reason, e.g., out of time or memory. We present a novel anytime algorithm that also guarantees to find an optimal Bayesian network structure upon completion. We use a sparse representation for storing search information which often reduces the memory requirements by several orders of magnitude. The algorithm improves the runtime to find optimal network structures up to 100 times compared to existing state of the art methods. It is also shown to have excellent anytime behavior and often finds better network structures faster than existing local search techniques and other anytime exact algorithms."
Real-time online denoising and speaker identification by learning low-rank non-negative sparse models,"In this paper we present a new framework for real time speech denoising under non-stationary noise. We first propose a regularized version of nonnnegative matrix factorization (NMF) for modeling time-frequency representations of speech signals in which the spectral frames are decomposed as sparse linear combinations of atoms of an undercomplete dictionary. This regularization minimizes an upper-bound of the nuclear norm of the reconstruction. The proposed model outperforms standard NMF in speech denoising experiments and reduces the sensitivity of the obtained results with respect to the size of the dictionary. The main contribution consists of combining this model with recent developments in fast regressors for approximating sparse codes, to produce efficient feed-forward architectures that approximate the output of the exact algorithms with low latency and a fraction of the complexity. Incorporating dictionary update and elements of discriminative learning makes the proposed architecture full-featured low-rank non-negative sparse models, significantly outperforming exact NMF algorithms. We present several experiments in speech denoising and speaker identification in the presence of non-stationary noise that show successful results and the potential of the framework."
An Efficient Feature Selection Algorithm,"Many computer vision and medical imaging problems are faced with learning classifiers from large datasets, with millions of observations and features. In this paper we propose a novel algorithm for variable selection and learning on such datasets, coming from the field of penalized likelihood optimization. We pose the learning problem as a constrained penalized likelihood optimization and introduce a suboptimal algorithm that gradually removes variables based on a criterion and a schedule. The approach is generic, allowing the use of any differentiable prior on the coefficients. Experiments on real and synthetic data show that the proposed method outperforms Logitboost and L1 penalized methods for both variable selection and prediction while being computationally faster. "
A dynamic excitatory-inhibitory network in a VLSI chip for spiking information reregistration,"Inhibitory synapse is an important component both in physiology and artificial neural network, which has been widely investigated and used. A typical inhibitorysynapse in very large scale integrated (VLSI) circuit is simplified from related research and applied in a VLSI chip for spike train reregistration. The spike trainreregistration network is derived from a neural network model for sensory map realignment for network adaptation. In this paper, we introduce the design of spiketrain registration in CMOS circuit and analyze the performance of the inhibitory network in it, which shows representative characters for the firing rate of inhibitedneuron and information transmission in circuit compared to math model."
Cost-Sensitive Trees of Classifiers,"Recently, machine learning algorithms have started to successfully enter large-scale real-world industrial applications. In these settings, test-time CPU usage needs to be budgeted and accounted for. Addressing the trade-off between classifier accuracy and test-time cost in a principled fashion has become a major challenge for machine learning. This test-time cost consists of classifier evaluation time and feature extraction time, with the latter varying dramatically from feature to feature. In this paper, we propose a meta-learning algorithm that learns a tree of classifiers. Test-inputs traverse the tree along different paths and features are only extracted for subsets of inputs where they are beneficial. Experimental results on a real-world data set demonstrates that our algorithm significantly improves over the current state-of-the-art in test-time cost-sensitive learning. "
Random Projections for Support Vector Machines,"Let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be a data matrix of rank $\rho$, representing $n$ points in $\mathbb{R}^d$. The linear support vector machine constructs a hyperplane separator that maximizes the  1-norm soft margin. We develop a new \emph{oblivious} dimension reduction technique which is precomputed and can be applied to any input matrix \math{\mathbf{X}}. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \math{\epsilon}-\emph{relative error}, ensuring comparable generalization as in the original space. We present extensive experiments in support of the theory."
ImageNet Classification with Deep Convolutional Neural Networks,"We trained a large, deep convolutional neural network to classifythe 1.3 million high-resolution images in the LSVRC-2010 ImageNettraining set into the 1000 different classes. On the test data, weachieved top-1 and top-5 error rates of 39.7\% and 18.9\% which isconsiderably better than the previous state-of-the-art results. Theneural network, which has 60 million parameters and 500,000 neurons,consists of five convolutional layers, some of which are followedby max-pooling layers, and two globally connected layers with a final1000-way softmax. To make training faster, we used non-saturatingneurons and a very efficient GPU implementation of convolutional nets.To reduce overfitting in the globally connected layers we employeda new regularization method that proved to be very effective. "
Recognizing Activities by Attribute Dynamics ,"The problem of modeling the dynamic structure of the attributes of human activities is considered. Video is first represented in a semantic feature space, where each feature encodes the probability of occurrence of an action attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining a binary observation variable with a hidden Gauss-Markov state process. In this way, it combines the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by the popular dynamic texture method for LDS learning, is proposed. A similarity measure between BDSs, which generalizes the Binet-Cauchy LDS kernel, is then introduced and used to design activity classifiers. These are shown to outperform similar classifiers derived from the kernel-LDS and state-of-the-art approaches to dynamics-based or attribute-based action recognition."
Compressive Sensing MRI with Wavelet Tree Sparsity,"In Compressive Sensing Magnetic Resonance Imaging (CS-MRI), one canreconstruct a MR image with good quality from only a small number ofmeasurements. This can significantly reduce MR scanning time.According to structured sparsity theory, the measurements can be further reduced to$\mathcal{O}(K+\log n)$ for tree-sparse data instead of$\mathcal{O}(K+K\log n)$ for standard $K$-sparse data with length $n$.However, few of existing algorithms has utilized thisfor CS-MRI, while most of them use Total Variation andwavelet sparse regularization. On the other side, some algorithmshave been proposed for tree sparsity regularization, but few of them has validated  the benefit of tree structure in CS-MRI. In this paper, we propose a fastconvex optimization algorithm to improve CS-MRI.  Waveletsparsity, gradient sparsity and tree sparsity are all considered in our model for real MR images.The original complex problem is decomposed to three simpler subproblemsthen each of the subproblems can be efficiently solved with an iterative scheme.Numerous experiments have been conducted and show that the proposedalgorithm outperforms the state-of-the-art CS-MRIalgorithms, and gain better reconstructions results on real MR images than generaltree based solvers or algorithms."
Graph Estimation with Joint Additive Models,"In recent years, there has been considerable interest in estimating conditional independence graphs in the high-dimensional setting in which the number of features exceeds the number of observations.  Most prior work in this area has assumed that the observations are drawn from a multivariate Gaussian distribution, or that conditional dependence relations among variables are linear, which as we will see are roughly equivalent. Unfortunately, if this assumption is violated, then the resulting conditional independence estimates can be inaccurate. Here we present a semi-parametric method, Sparse Conditional Estimation with Joint Additive Models (SpaCE JAM),  which allows for arbitrary additive conditional relationships among the features.  We present an efficient algorithm for its computation, and prove that our estimator is consistent.  We also  extend our method to estimation of  directed graphs.  SpaCE JAM enjoys superior performance to existing methods when there are non-linear relationships among the features, and is comparable to methods that assume multivariate normality when the features are linearly related."
A Study of Image Statistics through Higher-Order Interactions and Higher-Order Statistics,"  Nature image statistics is essential to the understanding of the  biological vision system, the Bayesian prior for image analysis,  texture analysis and synthesis, object detection, and scene  classification. The prior arts have mainly focused on the pairwise  interactions (e.g., correlation and 2-D joint histogram), and the  first and second order statistics (e.g. power spectrum and PCA). In  this paper, we use the multivariate (normalized) disjoint  information as a tool to capture the higher-order interactions  between two or more image subbands or transform coefficients. The  multivariate disjoint information satisfies the properties of  identity, non-negativity, symmetry, degeneracy, independence, upper  bounds, generalized triangle inequality, and simplex inequality. It  captures the higher-order interactions between an arbitrary number  of variables simultaneously, without simplification to the pairwise  interactions. In addition, the higher-order statistics, such as  bispectrum, are computed on the intensity images, wavelet  coefficients, and the disjoint information map. Preliminary results  based on the higher-order interactions and statistics are  demonstrated to differentiate the nature and man-made scenes."
Rates for Inductive Learning of Compositional Models,"Compositional Models are widely used in Computer Vision as they exhibit strong expressive power by generating a combinatorial number of configurations with a small number of components. However, the literature is still missing a theoretical understanding of why compositional models are better than flat representations, despite empirical evidence that compositional models need fewer training examples. In this paper we try to give the first theoretical answers in this direction. We focus on AND/OR Graph (AOG) models used in recent literature for representing objects, scenes and events, and bring the following contributions. First, we analyze the capacity of the space of AND/OR graphs, obtaining PAC (Probable and Approximately Correct) bounds for the number of training examples necessary to guarantee with a given certainty that the model learned has a given accuracy. We analyze both supervised and unsupervised learning approaches. Second, we observe that part localization leads to a reduction in the number of training examples required. Finally, we perform experiments for unsupervised learning of  AND/OR Graphs and part templates for objects and compare the theoretical bounds with the practical learning rates."
Evaluation of clustering stability using leave-one-out approach,"Currently, there are many clustering algorithms for the case of a known numberof clusters. Typically, clustering is a result of optimisation a quality criterionor iterative process. How to assess the quality of clustering obtained by somemethod? Is the data clustering corresponding to the objective reality or just astopping criterion of the method is made and obtained some partition? In thispaper, a practical approach and the general criterion based on an assessment ofthe stability of clustering are proposed. For the well-known clustering methods,efficient algorithms for computing the stability criterion according to the trainingset are obtained. We give illustrative examples."
Efficient Dimensionality Reduction for  Canonical Correlation Analysis,"We present the first sub-cubic time algorithm for Canonical Correlation Analysis. Given a pair of tall-and-thin matrices, our algorithm first employs a randomized dimensionality reduction transform to reduce the size of the input matrices, and then applies a standard SVD-based method to compute the canonical correlations. We prove that our algorithm computes an accurate approximation to the canonical correlations with high probability, and with asymptotic running times much better than the standard algorithm. We also show that our algorithm beats the standard algorithm in practice by 30-40% even on fairly small matrices."
Composite Discriminant Factor Analysis,"We propose a linear dimensionality reduction method, Composite Discriminant Factor (CDF) analysis, that searches for a discriminative but compact feature subspace that can be used as input to classifiers that suffer from problems such as multi-collinearity or the curse of dimensionality.  The subspace selected by CDF maximizes the performance of the entire classification pipeline, and is chosen from a set of candidate subspaces that are each discriminative by various local measures, such as covariance between input features and output labels or the margin between positive and negative samples.  Our method is based on Partial Least Squares (PLS) analysis, and can be viewed as a generalization of the PLS1 algorithm, designed to increase discrimination in classification tasks.  While our experiments focus on improvements to object detection, a computer vision task that often involves high dimensional features and benefits from fast linear approaches, we also demonstrate our approach on machine learning datasets from the UCI Machine Learning repository."
Linear and Nonlinear Predictive Coding using Biologically Plausible Neuronal Circuits,"Predictive coding has previously been proposed as a useful model of redundancy reduction in sensory systems and served as a foundation for a normative theory of sensory processing. However, its discussion has been restricted to an abstract mathematical algorithm without a mechanistic implementation. In this paper, we demonstrate that simplified linear neurons in biologically plausible circuits can implement optimal predictive coding. Further, the addition of a neuron-like rectilinear nonlinearity into a feedback inhibitory neuronal circuit approximates the response of predictive filters optimized for various stimulus statistics, without the need to vary any parameters. This nonlinear network, therefore, provides a mechanism for dynamic gain control, which can operate in sensory neurons on time scales much faster than adaptation requiring synaptic plasticity."
A kernel independence test for time series, Testing for independence between random variables when samples are not independent identically distributed is a complex issue. Here we provide a solution to this problem when random variables a sampled from stationary time-series of arbitrary objects. To achieve this goal we define a test based on the statistics of the cross-spectral density operator induced by positive definite kernels. The performance of our test is compared to using i.i.d assumptions and show the interest of this approach for testing dependency in complex dynamic systems.
Flexible Shift-Invariant Locality and Globality Preserving Projections,"In machine learning, the dimension reduction methods have commonly been used as a principled way to understand the high-dimensional data. To solve the out-of-sample problem, local preserving projection (LPP) was proposed and applied to many applications. However, LPP suffers two crucial deficiencies: 1) the LPP loses shift invariance property which is an important property of embedding methods; 2) the rigid linear embedding is used as constraint, which often inhibits the optimal manifold structures finding. To overcome these two important problems, we propose a novel flexible shift invariant locality and globality preserving projection method, which utilizes a newly defined graph Laplacian to make the projection shift invariant. Meanwhile, the relaxed embedding is introduced to allow the finding of more optimal manifold structures. We derive the new optimization algorithm to solve the proposed objective with rigorously proved global convergence. Extensive experiments have been performed on the machine learning benchmark data sets. In all empirical results, our method shows promising results."
On Multilabel Classification and Ranking with Partial Feedback,"We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show $O(T^{1/2}\log T)$ regret bounds, which improve in several ways on the existing results.We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance."
FAsT-Match: Fast Affine Template Matching,"FAsT-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. The smoother the image, the sparser the set of affine transformations we evaluate. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth these lead to a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We report quantitative results on a large data set of 9,500 images. To the best of our knowledge, this is the first template matching algorithm, which can handle arbitrary 2D affine transformations."
Adaptive Universal Linear Filtering,"We consider the problem of online estimation of an arbitrary real-valued signal corrupted by zero-mean noise using  linear estimators. The estimator is required to iteratively predict the underlying signal  based on the current and several last noisy observations, and its performance is measured by the mean-square-error. We design and analyse an algorithm for this task which achieves logarithmic adaptive regret against the best linear filter in hindsight. This bound is asymptotically tight, and resolves the question of \cite{MWPaper}. Furthermore, the algorithm runs in linear time in terms of the number of filter coefficients. Previous constructions required at least quadratic time. "
Dictionary Training with Side Information,"Recently, learning with side information, which incorporates information contained in the training data but not available in the testing phase to guide the learning process, attracts great attention in machine learning field. In this work, we propose a discriminative dictionary learning method that involves side information. In particular, we introduce a new soft constraint derived from side information and combine it with the reconstruction error and the classification error to form a unified objective function. The optimal solution to the objective function is efficiently obtained using the K-SVD algorithm. Our algorithm learns the dictionary and an optimal linear classifier jointly. We apply the proposed method to two pattern recognition problems, namely low resolution expression recognition and face recognition, where it demonstrates the effectiveness of the proposed method in classification performance."
Discriminative Restricted Boltzmann Machines for Regression,"The paper explores a variant of a discriminative Restricted Boltzmann Machine (RBM) model that performs regression, by defining an RBM with Gaussian output variables and conditioning on the data. The resulting model is intractable in general, but we derive a practical mean-field approximation to the predictive distribution of the model. We investigate two learning algorithms for discriminative RBM regressors, viz. one based on contrastive mean-field learning and one based on contrastive divergences. Experimental evaluation of the new discriminative RBM for regression illustrates the potential of the discriminative RBM regressor, in particular, for learning problems in which the data is high-dimensional."
A spectral learning framework for graphical models,"This work draws on the previous works on spectral algorithm(\cite{Hsu:COLT09-long},\cite{DBLP:conf/alt/BaillyHD10},\cite{DBLP:conf/icml/SongSGS10},\cite{DBLP:conf/pkdd/BalleQC11}). We propose an extension of the\emph{Hidden Markov Models}, called \emph{Graphical Weighted Models  (GWM)}, whose purpose is to model distributions over labeledgraphs. We expose the spectral algorithm for GWM, which generalizesthe previous ones for sequences and trees. We show that this algorithmis \emph{consistant}, and we provide bounds for the statisticalconvergence for the parameters estimate and for the learned distribution."
High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning,"Joint sparsity regularization in multi-task learning has attracted much attentionin recent years. The traditional convex formulation employs the group Lasso relaxationto achieve joint sparsity across tasks. Although this approach leads to asimple convex formulation, we argue in this paper that the quadratic regularizerinduced by the group Lasso formulation is suboptimal. To remedy this problem,we view jointly sparse multi-task learning as a specialized random effects model,and derive a convex relaxation approach that involves two steps. The first steplearns the covariance matrix of the coefficients using a convex formulation whichwe refer to as sparse covariance coding; the second step solves a ridge regressionproblem with a sparse quadratic regularizer based on the covariance matrix obtainedin the first step. It is shown that this approach produces an asymptoticallyoptimal quadratic regularizer in the multitask learning setting if the number oftasks approaches infinity. Experimental results demonstrate that the convex formulationobtained via the proposed model significantly outperforms group Lasso."
The Mondrian hidden Markov model,"This paper discusses a novel extension of hidden Markov models that can capture clusters embedded in transitions between the hidden states. In our model, the state-transition matrices are viewed as representation of relational data reflecting a network structure between the hidden states. We specifically present a nonparametric Bayesian approach to the proposed state-space model whose network structure is represented by a Mondrian Process-based relational model. We describe an inference scheme, and also show an application of the proposed model to music signal analysis."
Early Active Learning via Robust Representation and Structured Sparsity,"Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. The robust sparse representation loss function is utilized to reduce the effect of outliers and the structural sparsity regularization is adopted to find the most representative samples during the sparse representations. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost. We rigorously prove the global convergence of our solutions. Empirical results on multiple benchmark data sets show the promising results of our method."
Robust Rank-$k$ Matrix Completion,"Many applications can be formulated as reconstructing a matrix $M$ from noisy observations of a small, random subset of its entries. Much recent work has focused on the assumption that the data matrix has low rank and used its nuclear norm to approximate the rank. However, recent research casts doubts about this category of approach since such yielded solution could be indeed not low rank and unstable for practical applications. In this paper, we explicitly seek a matrix of \emph{exact} rank. Moreover, our method is robust to outlying or corrupted observations. We optimize the objective function in an alternative and asymptotic convergent manner, based on a combination of ancillary variables and augmented Lagrangian methods (ALM). We perform extensive experiments on three real world data sets and all empirical results demonstrate the effectiveness of our method."
Bayesian Games for Adversarial Regression Problems,"We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary's objective; instead, any knowledge of the learner about parameters of the adversary's goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression."
Online Optimization in Markov Decision Processes with Dynamic Uncertainty,"Markov Decision Processes (MDPs) are used to model many important and practical problems involving optimization under uncertainty. In online decision making this uncertainty may lead to an algorithm making irreversible sub-optimal decisions. Fortunately, in many natural scenarios some short-term `lookahead' is available into future rewards. Thus, the information available to the online algorithm changes from time step to time step as more information is revealed, leading to what we call \textit{dynamic} uncertainty. To study this class of problems we define a framework of adaptive optimization with dynamic uncertainty and provide online algorithms, based on discounting of future rewards, to handle time varying uncertainty in future reward structures and values. We derive theoretical bounds and establish that discounting is, in some sense, a method to effectively de-randomize against possible futures. We analyze the performance of our algorithms in terms of both regret and competitive ratio, under varying dynamics of uncertainty. In particular, we define a notion of \textit{effective uncertainty} that captures the accuracy of information, when it becomes available and how usable it is, and show how this plays a crucial role in our analyses."
Probabilistically Sampled Forests,"Random subspaces are the key idea in random forests.  While the optimal size s of random subspaces can significantly improve classification accuracy, a suboptimal choice can be detrimental, especially in high-dimensional feature spaces, which typically contain only a few discriminative features among a majority of (nearly) uninformative ones.  As to overcome this problem, we propose an alternative to the subspace method: learning a forest by probabilistically sampling trees from the Boltzmann distribution at temperature T.  This approach includes sampling from the posterior distribution as a special case for T=1.  An increased temperature T>1 serves as a conceptually well-established measure for the additional randomness introduced into the learning process.  Moreover, this approach also suggests a novel relationship between the bootstrap and the random subspace method.  Apart from this, in our experiments on UCI data sets, we also found improvements on the practical side: classification accuracy does not tend to suffer as much from a suboptimal T compared to a suboptimal s in high-dimensional feature spaces."
"A Mechanism of Generating Joint plans for Self-interested Agents, and by the Agents","Generating joint plans for multiple self-interested agents is one of the most challenging problems in AI, since complications arise when each agent brings into a multi-agent system its personal abilities and utilities. Some fully centralized approaches (which require agents to fully reveal their private information) have been proposed for the plan synthesis problem in the literature. However, in the real world, private information exists widely, andit is unacceptable for a self--interested agent to reveal its private information. In this paper, we define a class ofmulti--agent planning problems, in which self--interested agents' values are private information, and the agents are ready to cooperate with each other in order to cost efficiently achieve their individual goals. We further propose a semi--distributed mechanism to deal with this kind of problems. In this mechanism, the involved agents will bargain with each other to reach an agreement, and do not need to reveal their private information. We show that this agreement is a possible joint plan which is Pareto optimal and entails minimal concessions."
"Natural Images, Gaussian Mixtures and Dead Leaves","Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components --- including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models."
Shape Priors for Weakly Labeled Segmentation,"In this paper we tackle the problem of weakly labeled image segmentation.  Towards this goal, we propose a novel generative model of segmentation based on transformed hierarchical Pitman-Yor processes, where we augment each object class with a shape prior.  Our model exploits weakly label data, as it does not require a training set composed of  pixel-wise annotations. Instead, it learns appearance models for each object using  as labels only bounding boxes around the object of interest as well as a  shape prior. We demonstrate the effectiveness of our approach on the PASCAL 2010 dataset and show that we outperformed a set of baselines, improving $8\%$ absolute error over the unsupervised version of our model as well as $9\%$ over the detector, which is theinput to our approach. Importantly, our approach performs similarly to fully-supervised approaches.  "
Sparse coding for multitask and transfer learning,"We present an extension of sparse coding to the problems of multitask and transfer learning. The central assumption of the method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Preliminary experiments indicate the advantage of the sparse multitask coding method over single task learning and a previous method based on orthogonal and dense representation of the tasks."
Dual-Space Analysis of the Sparse Linear Model,"Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients.  These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters.  Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation.  The underlying cost functions can be related via a dual-space framework from Wipf et al. (2011), which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space.  This perspective is useful because some analyses or extensions are more conducive to development in one space or the other.  Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit.  As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I.  In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II.  For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular L1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations."
Fast Forward Feature Selection for Support Vector Machines,"When creating a pattern recognition system, the set of used features has typically a strong influence on the performance of the selected classifier. Automatic feature selection algorithms aim to select the best features of a given dataset. Whereas filter approaches do not take the target algorithm into account, wrapper approaches evaluate the target algorithm for each considered feature subset. Therefore, filter approaches are typically faster but wrapper approaches may deliver a higher performance.In this paper, we present an approach for reducing the run-time of the common wrapper approach forward selection for Support Vector Machines (SVM). The number of required evaluations of the SVM classifier is reduced by using experience knowledge gained during past feature selection runs.We evaluated the approach on 22 real world datasets and compared the results with state-of-the-art wrapper and filter approaches as well as one embedded method according to performance and run-time. The results show that the presented approach reaches the accuracy of traditional wrapper approaches requiring significantly less evaluations of the target algorithm. Moreover, the presented approach achieves statistically significant better results than the filter approaches."
Partition Tree Weighting,"This paper introduces Partition Tree Weighting, a  low-complexity probabilistic sequence prediction algorithm for piecewise stationary sources. It works by performing Bayesian model averaging over a large class of possible partitions of the data into stationary segments. Our prior is designed to be both efficiently computable and well suited towards data compression applications. We provide a competitive analysis of the redundancy (i.e. cumulative log-loss regret) of ourmethod, and show empirically that PTW consistently performs well relative to competing methods. We conclude by showing how to use PTW to improve the performance of a recently introduced universal data compression algorithm."
Active Comparison of Prediction Models,"We address the problem of comparing the risks of two given predictive models - for instance, a baseline model and a challenger - as confidently as possible on a fixed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values."
Low-Rank Affine Subspace Clustering,"We consider the problem of clustering high-dimensional data lying approximately in a union of affine subspaces. State-of-the-art methods solve this problem in two stages: learning an affinity matrix and applying spectral clustering. However, each stage is solved using a different optimization criteria. In this paper, we propose a unified approach in which we solve jointly for an affinity matrix and the segmentation of the data. We pose this problem as a rank minimization problem, which we solve using an alternating direction method, where the optimal solution at each iteration can be computed in closed form. Experiments on synthetic and real data demonstrate the superiority of our approach over state-of-the-art methods."
Incremental Online Boosting using Product of Experts,"We develop a novel incremental online algorithm for boosting based on a Product of Experts. The Product of Experts formulation allows for the first time, to devise rules to add experts to the ensemble thus incrementally adapt the model complexity of the ensemble along with learning in an online fashion. In this paper, we illustrate the efficacy of the online PoEBoost by comparing it with the previous approaches to online boosting on benchmark datasets."
Maximal Margin Learning Vector Quantisation,Kernel Generalised Learning Vector Quantisation (KGLVQ) was proposed to extend Generalised Learning Vector Quantisation into the kernel feature space to deal with complex class boundaries and thus yield promising performance for complex classification tasks in pattern recognition. However KGLVQ does not follow the maximal margin principle which is crucial for kernel-based learning methods. In this paper we propose a maximal margin approach to Kernel Generalised Learning Vector Quantisation algorithm which inherits the merits of KGLVQ and follows the maximal margin principle to favour the generalisation capability. Experiments performed on the well-known data sets available in UCI repository show promising classification results for the proposed method.
On the Consistency of AUC Optimization,"AUC has been widely used as an evaluation criterion in diverse learning tasks. Many learning approaches have been developed to optimize AUC; however, owing to the non-convexity and discontinuousness of AUC, almost all approaches work with surrogate loss functions. Thus, the consistency of AUC is a crucial issue. In this paper, we theoretically study the asymptotic consistency of learning approaches based on surrogate loss functions and provide a sufficient condition. Based on this result, we prove that the exponential loss and logistic loss are consistent with AUC, whereas the hinge loss is inconsistent. We then derive the {$q$-norm hinge loss} and {general hinge loss} that are consistent with AUC. We also derive the consistent bounds for the exponential loss and logistic loss. Additionally, we obtain the consistent bounds for many surrogate loss functions under non-noisy setting. Finally, we find an equivalence between the exponential surrogate loss of AUC and the exponential surrogate loss of accuracy, leading to a direct consequence that AdaBoost and RankBoost are asymptotically equivalent."
Stereopsis via deep learning,"Estimation of binocular disparity in vision systems is typically based on a matching pipeline and rectification. Estimation of disparity in the brain, in contrast, is widely assumed to be based on the comparison of local phaseinformation from binocular receptive fields. The classic binocular energy model shows that this requires thepresence of local quadrature pairs within the eye which show phase- or position-shifts across the eyes.  While numerous theoretical accounts of stereopsis have been based on these observations, there has been little work on how energy models and depth inference may emerge through learning from the statistics of image pairs.  Here, we describe a probabilistic, deep learning approach to modeling disparity and a methodology for generating binocular training data to estimate model parameters.  We show that within-eye quadrature filters occur as a result of fitting the model to data, and we demonstrate how a three-layer network can learn to infer depth entirely from training data. We also show how training energy models can provide depth cues that are useful for recognition. We also show that pooling over more than two filters leads to richer dependencies between the learned filters. "
Learning in deep architectures with folding transformations,"Statistical Learning Theory establishes the consistency of a learning process as an upper bound that holds with certain probability.  We propose a folding transformation paradigm for deep architectures for supervised layer-wise learning that assures the reduction of the complexity of the task to be learned without compromising the learning consistency regardless of the additional VC-dimension due to the depth of the architecture.   We introduce concepts of internal decision making, mapping and shatter complexity that aid in the analysis of a contribution of an individual hidden transformation to the solution of a task in a deep architecture, and apply these methods to investigate the capabilities of the proposed folding transformations. We also provide possible implementation using a neural network and carry out a small test of the architecture's performance on a classification task."
Online Regret Bounds for Undiscounted Continuous Reinforcement Learning,"We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside a technical condition to ensure existence of an optimal policy, the only assumptions made are Hoelder continuity of rewards and transition probabilities. "
Bayesian Sparse Partial Least Squares,"Born in the bosom of the chemometrics discipline, partial least squares (PLS) is a class of methods that makes use of a set of latent or unobserved variables to model the relation between (typically) two sets of input and output variables, respectively. Several flavors, consisting on how the latent variables or components are computed, have been developed over the last years. In this paper, we propose a Bayesian formulation of PLS along with some extensions. In a nutshell, we provide sparsity at the input space level and an automatic estimation of the optimal number of latent components. We follow the variational approach to infer the parameter distributions. We have successfully tested the proposed methods on electrocorticogram data associated with several motor outputs in monkeys."
Hybrid Hierarchical Growing Neural Gas for Clustering of 3D Environmental Map,"This paper proposes a method of clustering of 3D environmental map. Our method is based on Growing Cell Structure and Growing Neural Gas. These methods are one of self-organizing neural network based on unsupervised learning First, we propose 3D map building method using Kinect that can get camera image and 3D distance information simultaneously. Next, we propose a method of clustering method based on hybrid hierarchical growing neural gas to cluster 3D environmental map. Finally, we show experimental results of the proposed method and discuss the effectiveness of the proposed method."
Networks of rate neurons with no propagation of chaos: when mean-field theory fails,"We show how to compute analytically the correlation between pairs of neurons in a linear stochastic network described by rate equations. We prove that correlation depends on the eigenvalues of the connectivity matrix and that in special cases it isn?t negligible (no propagation of chaos), preventing us from applying mean-field theories to describe the activity of the network."
Generalization Bounds for Domain Adaptation,"In this paper, we provide a new framework to study the generalization bound of thelearning process for domain adaptation. Without loss of generality, we considertwo kinds of representative domain adaptation settings: one is domain adaptationwith multiple sources and the other is domain adaptation combining sourceand target data. In particular, we introduce two quantities that capture the inherentcharacteristics of domains. For either kind of domain adaptation, basedon the two quantities, we then develop the specific Hoeffding-type deviation inequalityand symmetrization inequality to achieve the corresponding generalizationbound based on the uniform entropy number. By using the resultant generalizationbound, we analyze the asymptotic convergence and the rate of convergenceof the learning process for such kind of domain adaptation. Meanwhile, we discussthe factors that affect the asymptotic behavior of the learning process. Thenumerical experiments support our results."
Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning,"One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called \textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency."
Bayesian Natural Actor Critic via Automatic Relevance Determination,"We present an algorithm which computes the posterior distribution of the natural policy gradient. The posterior mean of the gradient is used to update the parameters of the policy. The covariance matrix of the gradient can be used to evaluate the accuracy. Our algorithm can automatically prune out the irrelevant features in the value function approximation, resulting in a sparse model. This mechanism is achieved through automatic relevance determination which is similar with that in the relevance vector machine.  Finally, we demonstrate the performance of the algorithm experimentally and show that our work naturally combines the value basis function selection and the Bayesian natural policy gradient together."
Mixed Norm Regularization for Multi Class Learning,"Multiclass problems are everywhere, given an input the goal is topredict one of few possible classes. Most previous work reducedlearning to minimization the empirical loss over some training set,and an additional regulariztion term, prompting simple models, or someother prior knowledge. A common learning regulariztion promotessparsity, that is, small models or small number of features, asperformed in group LASSO. Yet, these assumption do not always hold, insome problems, for each class, there is a small set of features thatrepresents it well, yet the union of these sets is not small. Wepropose to use other regularizations that promptes this type ofsparsity, analyze the generalization property of such formulations,and show empirically that indeed, that not only perform well, but alsopromote such sparsity structure."
Non-accidental configuration representation in the human ventral visual pathway,"Non-accidental configurations have been proposed to help resolve ambiguities in the inputs to the visual system. Moreover, recent theoretical accounts propose that such configurations could be formally ordered by the degree of their 'non-accidentalness'. In this study, we wanted to explore the possibility that human visual system indeed assigns a special status to non-accidental configurations. In a fast blocked fMRI design, participants were presented with slowly moving stimuli composed of two lines in one of twelve configurations while performing a perceptual similarity task. Using multi-voxel pattern analysis we were able to reliably discriminate between most of the configurations. Moreover, our fMRI data were well modeled by a simple V1 filter bank model. Further analyses failed to reveal an special status of the configurations used. Our experiment sets a baseline for future experiments on non-accidental configuration representation in the visual system."
Boosting with Side Information,"In many problems in machine learning and computer vision, there exists side information, i.e., information contained in the training data and not available in the testing phase. This motivates the recent development of a new learning approach known as learning with side information that is aimed to incorporate side information for improved training of learning algorithms. In this work, we describe a new training method of boosting classifiers that uses side information.  In particular, we propose a novel feature space imputation method to construct extra weak classifiers from the available information that simulate the performance of better weak classifiers obtained from features in side information. We apply our method to two problems, namely handwritten digit recognition and facial expression recognition from low resolution images, where it demonstrates its effectiveness in classification performance."
Modeling Human-Object Interactions for Action Recognition in Real-World Videos,"This paper deals with the interesting problem of recognizing human actions in real-world videos. Such videos usually present large variation in background and camera motion, which makes the performance of low-level appearance and motion features unsatisfactory, particularly in the case of video classes sharing similar objects and background (e.g. ``snatch'' and ``clean-jerk'' weightlifting actions). In this paper, we tackle the problem through representation of action classes as human and object interactions (HOI). HOI is modeled as the spatio-temporal relationship between human and object tracks along with their appearance descriptions in a video. However, such a representation requires accurate detection of human and object tracks. This is a difficult task in its own right when dealt separately. We address the issue by extracting candidate tracks from a video and modeling the choice of correct tracks as latent variables in a latent SVM framework. This formulation enables the task of HOI modeling and action recognition without accurate initialization of human and object tracks. We demonstrate promising action classification results on the challenging Olympic Sports [1] and TRECVID11-MED [2] datasets, where our method outperforms state-of-the-art approaches."
Learning curves for multi-task Gaussian process regression,"  We study the average case performance of multi-task Gaussian process (GP)  regression as captured in the learning curve, i.e.\ the average Bayes error  for a chosen task versus the total number of examples $n$ for all  tasks. For GP covariances that are the product of an  input-dependent covariance function and a free-form inter-task  covariance matrix, we  show that accurate approximations for the learning curve can be  obtainedfor an arbitrary number of tasks $T$.  We use  these to study the asymptotic learning behaviour for large  $n$. Surprisingly, multi-task learning can be asymptotically essentially  useless: examples from other tasks only help when the  degree of inter-task correlation, $\rho$, is near its maximal value  $\rho=1$. This effect is most extreme for learning of smooth target  functions as described by e.g.\ squared exponential kernels. We also  demonstrate that when learning {\em many} tasks, the learning curves  separate into an initial phase, where the Bayes error on each task  is reduced down to a plateau value by ``collective learning''   even though most tasks have not seen examples,  and a final decay that occurs only once the number of examples is  proportional to the number of tasks."
New Relaxations of Graph Cuts for Clustering,"In recent clustering research, the graph cut methods, such as normalized cut and ratio cut, have been well studied and applied to solve many unsupervised learning applications. The original graph cut is an NP-hard problem. Traditional approaches used spectral relaxation to solve the graph cut problem. The main disadvantage of this approach is that the obtained spectral solutions %are not the final clustering results and the post-processing step has to be applied. Thus, the final results could severely deviate from the true solution.To solve this problem, in this paper, we propose a new relaxation mechanism for graph cut methods. Instead of minimizing the squared distances of clustering results, we use the $\ell_1$-norm distance. Meanwhile, considering the normalized consistency, we also use the $\ell_1$-norm for the normalized terms in the new graph cut relaxations. Due to the sparse result from the $\ell_1$-norm minimization, the solution of our new relaxed graph cut methods get discrete values with many zeros, which is close to the ideal solution. However, the new objectives are difficult to be optimized, because the minimization problem involves the ratio of non-smooth terms. The existing sparse learning optimization algorithms cannot be applied to solve this problem. In this paper, we propose a new optimization algorithm to solve this difficult non-smooth ratio minimization problem. The extensive experiments have been performed on three two-way clustering and eight multi-way clustering data sets. All empirical results show that our new relaxation methods consistently enhance the normalized cut and ratio cut clustering results."
Transformed Poisson-Dirichlet Processes for Differential Topic Modeling,"We want to compare topics from a number of different document collections:some of these topics capture shared content, others capture the different andunique aspects that the collections may contain. We propose the transformedPoisson-Dirichlet process (TPDP), which is defined to be a class of hierarchicalPoisson-Dirichlet processes (HPDP) with transformed base measures, to build differential topic model among different groups of data. The main challenge of using the TPDP is the non-conjugacy between the prior and likelihood. We propose an efficient sampling algorithm by introducing auxiliary variables, which effectively resolve this problem. Experiment results show a dramatic reduced test perplexity compared to existing approximating methods on a variety of text and image collections. The model also gives an insightful analysis of the Democrat versus Republican blogs leading up to the 2008 USA election."
Kernel Hyperalignment,"We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We conducted experiments using real-world, multi-subject fMRI data."
Universal restrictions on Natural Language determiners from a PAC-learnability perspective,"A classical conjecture in the generative linguistic literature is that universal restrictions on determiners in Natural Language (e.g.~{\em monotonicity}, {\em invariance}, and {\em conservativity}) serve the purpose of simplifying the language acquisition task. This paper formalizes this informal conjecture within the PAC-learnability framework.  "
Using Topic Models to Improve Object Detection in Context,"This paper describes a generative model of images, the objects they contain, their appearances, positions, sizes and locations, and the application of this model to the improvement of object detection. Our generative model has at its heart a topic model of object labels which enforces co-occurrence preferences. We demonstrate how this allows the model to take output from an object detector and enhance it with notions of context, and leverage this additional information to improve detection performance. Our proposed model exhibits key properties lacked by competing approaches: firstly, we are able to naturally model the repeat occurrence of objects in images; and secondly, our model is fully generative and thus easily interpreted and extended. We demonstrate the benefits of our model on the recently introduced SUN09 dataset, as well as the PASCAL 2007 challenge."
Dictionary-based Spatially-Variant Image Deblurring with Dual-Exposure Stereo,"Camera shake induces image blurring, which was assumed to be spatially  invariant in most traditional image restoration methods. State-of-the-art image deblurring methods utilize combination of projected views to handle camera rotation or approximate motion blur. Besides rotational motion, translation also induces non-uniform blurring because of non-uniform depth of the scene. Due to the complicated camera motion blur, a pair of images acquired with different settings could provide additional information to faciliate image deblurring. However, traditional methods consider image pairs which share almost the same viewpoint and mainly focus on uniform blur. In this paper, we use a stereo pair of images which contain one noisy image and one blurred image. We use sparse representation and kernel estimation techniques to decompose non-uniform motion blur and camera rotation blur. The proposed novel framework could be efficiently solved by convex optimization or compressive sensing algorithms, and it can be easily extended by increasing the size of the dictionary for sparse representation. The proposed algorithm can be used to recover the stereo pair with spatially varying blur. Experimental results show the proposed deblurring algorithm can effectively reduce the motion blur and provide better results than traditional methods."
Multiple Choice Learning: Learning to Produce Multiple Structured Outputs,"The paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade. Given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one. The standard approach for handling this scenario is to learn a single model and then produce M-best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we formulate this multiple {\em choice} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem. We present a max-margin formulation that minimizes an upper-bound on this loss-function. Experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this scenario and leads to substantial improvements in prediction accuracy."
Dynamical models and tracking regret in online convex programming,"This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that capture a comparator's deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network."
Learning to Rank: from Batch to Online Learning,"Learning to rank represents a family of important machine learning algorithms for information retrieval applications. Traditional methods typically learn a ranking model from training data in a {\it batch} learning mode, which suffer from two major limitations: (i) they can {\it hardly} track fast-changing search intention of online users in a timely fashion; and (ii) their {\it scalability} is usually poor, especially for web-scale applications. To overcome these limitations, in this paper, we propose a novel framework of online learning to rank, which sequentially updates ranking models in an online learning fashion which thus can adapt the fast-changing search intention of online users, which is of particular importance for many online applications. Specifically, we propose two algorithms for online learning to rank, and theoretically analyze their IR measure bounds. We conduct extensive empirical studies on the LETOR testbed, in which our results show that our algorithms significantly outperform some existing online ranking technique, and achieve fairly comparable results with the state-of-the-art batch learning to rank algorithms, but enjoy significant advantage in scalability. Our framework enjoys a wide range of applications for many web-scale online services, such as personalized search, recommendation, and online advertising, etc."
Planning with Representation Adaptation in Continuous POMDPs,"Partially observable Markov decision processes (POMDP) provide an elegant mathematical formalism to describe decision problems in stochastic and incompletely perceivable domains. Discrete POMDPs can be solved in reasonable time. However, most applications are continuous and then the belief space is infinite-dimensional.In this paper the general concept of Planning with RepresentationAdaptation in POMDPs (PRAin) is proposed. It comprises two novel ideas to make continuous planning feasible: First, learning a problem specific,efficient space representation during value iteration to generalize results in the continuous space. Second, asserting consistency for the generalized results byself-correction. In order to constraint the POMDP models as little as possible, the concept is implemented with a decision tree learning algorithm and based on Monte Carlo approximation. In an experimental comparison PRAin converges to higher values in significantly shorter time than previous approaches. Further, thenegative influence of higher dimensions is effectively reduced to a minimum."
A Method for Feature Induction in a Class of Higher-Order Random Fields,"Recently, there has been much interest, particularly in computer vision, in developing models which capture higher-order interactions between discrete random variables. Such interactions depend on the joint configuration of at least three variables, and may be described as `higher-order features' of a model, which may be data-dependent or structural, and can involve patterns over several variables, the counts of particular output values, subsets of output values in a solution, and the partition of variables by output value.  In computer vision these features have proved useful for tasks such as scene segmentation, image denoising and visual category discovery, where they can be used naturally to represent class co-occurrences, object histograms and power-law distributions of classes and segment sizes. Typically, particular higher-order features must be built into a model in advance, with the selection of the appropriate features for a given task being made manually.  We propose a feature induction method within a general class of random fields which we call `label-type networks', in which the automatic selection of higher order features of the kinds described can occur.  Our framework makes use of variational methods for both learning and inference, allowing us to draw on recent mean-field filtering techniques to construct dense models and provide the efficiency necessary to explore large feature spaces.  We demonstrate our approach on semantic segmentation using the PASCAL dataset."
Robust Manifold Learning via $l_1$ Minimization,"Previous research has shown that many existing manifold learning methods are sensitive to outliers in the data.  Although some robust extensions have been proposed, the techniques used by them are specifically tailored for a particular manifold learning method but cannot be applied readily to other methods.  In this paper, we propose a unified framework for robust manifold learning based on the $l_1$ norm.  By reformulating the optimization problems of several representative manifold learning methods as Rayleigh quotient maximization problems, we find that they can all be seen as minimizing the $l_2$ distance between a normalized similarity matrix and its reconstruction based on low-dimensional embedding.  Due to the sensitivity of the $l_2$ distance measure to outliers in the data, we replace it with an $l_1$ distance measure which is more robust.  To solve a convex relaxation of the optimization problem, instead of using off-the-shelf solvers, we devise an efficient algorithm based on the augmented Lagrange multipliers method by exploiting the special structure of the problem.  For experimental validation, we use the COIL image and UCI database and compare four representative manifold learning methods with their robust variants formulated based on the unified framework proposed."
The Incremental Risk Functional: Basics of a Novel Incremental Learning Approach,"Incremental learning gets increasing attention in research and practice as it has the advantages of continuous adaptation to nonstationary conditions with a low computation and memory demand at the same time. Several approaches have been proposed recently for specific approximators, but only few work has been done to pose the general problem of incremental learning. Hence, we introduce the \emph{incremental risk functional} describing the optimization problem to be solved for incremental learning, regardless of the specific task or approximator. Exemplarily, we apply this approach to regression estimation through linear-in-parameter approximators. We show that the resulting learning algorithm converges and changes the global functional behavior only as little as necessary with every learning step, thus resulting in a stable incremental learning approach."
Perceptual switching for a multistable motion stimulus: modelling and bifurcation analysis ,"Perceptual switches are known to occur during prolongedpresentations of barber pole stimuli viewed through a squareaperture due to competition between 1D motion cues aligned withthe grating's motion direction and 2D motion cues aligned withaperture edges. We study the temporal dynamics of this phenomenonwith a neural fields, population-level representation of activityin MT, a cortical area dedicated to motion estimation. Numericaltools from bifurcation analysis are used to investigate themodel's behaviour in the presence of different types of input;this general approach could be applied to a range of neuralfields models that are typically studied only in terms of theirspontaneous activity. The model reproduces known multistablebehaviour in terms of the predominant interpretations (percepts)of the barber pole stimulus. We describe key differences in thecharacteristic behaviour for two contrast regimes and we predicta change in the underlying distribution of percept switchingtimes for the different regimes."
Turing Test Imaging based on Non-Artificial Evidence Criterion,"We in this paper present a Turing test image (TTI) generated method. Using our produced image, we can tell computers and humans apart and protectwebsite from attack. To this end, we propose a robust TTI generated framework based on novelNon-Artificial Evidence Criterion (NAEC). Extensive user study is conduced to prove our TTI is user-friendly. Experiments demonstrate that our proposed Turing test image can resist attack from state-of-the-art object detection and imageclassification system."
Measuring Correlations Using Information Dimension,"We propose to measure the correlation between two continuous random variables by the mutual information dimension (MID), and present an efficient parameter-free estimation method of MID with linearithmical complexity with respect to the number of data points. Our method gives an effective solution, supported by sound dimension theory, to the problem of detecting interesting relationships of variables in massive data, which is now becoming a heavily studied topic not only in machine learning but also all over science. Different from the classical Pearson correlation coefficient, MID is zero if and only if two random variables are statistically independent and is translation and scaling invariant. We experimentally show superior performance of MID for various types of relationships in detection of them in the presence of noise."
MAP Inference in Chains using Column Generation,"Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning."
"Scoring Bayesian Networks with Informative, Causal and Associative Priors","A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. Currently however, there are limited practical ways of assigning priors to each possible network. In this paper, we present a method for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between a pair of variables X and Y. This type of knowledge naturally arises from prior experimental and observational datasets, among others. We show that incorporating such prior knowledge may not only improve the learning of the direction of the causal relations in the network, but also the learning of the network skeleton. This is particularly the case when sample size is low and thus prior knowledge increases in importance. Our approach is based on converting possibly-incoherent beliefs about marginals to joint distributions of priors by use of optimization theory."
Multi-Step Regression Learning for Compositional Distributional Semantics,"This paper presents a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it against two existing experiments, and find it to perform better than comparable methods in one, and on par with other best-performing models in the other. We argue in our analysis that the nature of this learning method renders it suitable for solving more subtle problems compositional distributional models might face, and suggest future work in this area."
Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ) for Multi-voxel Pattern Analysis in fMRI,"We present an efficient algorithm for simultaneously training elastic-net-regularized generalized linear models across many related problems, which may arise from bootstrapping, cross-validation and nonparametric permutation testing.  Our approach leverages the redundancies across problems to obtain approximately 10x computational improvements relative to solving the problems sequentially by the glmnet algorithm. We demonstrate our fast simultaneous training of generalized linear models (FaSTGLZ) algorithm,  for multivariate analysis of fMRI and run otherwise computationally intensive bootstrapping and permutation test analyses that are typically necessary for obtaining statistically rigorous classification results and  meaningful interpretation.  We also use our algorithmic framework to propose a new algorithm for stabilizing feature selection in sparse regression models."
Estimating Unknown Sparsity in Compressed Sensing,"Within the framework of compressed sensing, many theoretical guarantees for signal reconstruction require that the number of linear of measurements $n$ exceed the sparsity $\|x\|_0$ of the unknown signal $x\in\R^p$. However, when the sparsity parameter $\|x\|_0$ is unknown, the choice of $n$ remains problematic. In this paper, we consider the problem of directly estimating $\|x\|_0$ from a small number of linear measurements---without making any prior assumptions about the sparsity of $x$. Although we show that estimation of $\|x\|_0$ is generally intractable in this framework, we consider an alternative measure of sparsity $s(x):=\|x\|_1^2\big/\|x\|_2^2$, which is a sharp lower bound on $\|x\|_0$, and is more amenable to estimation. When $x$ is a non-negative signal, we propose a computationally inexpensive estimator $\hat{s}(x)$ for $s(x)$, and derive concentration bounds that imply $\hat{s}(x)/s(x)\to 1$ almost surely as $(n,p)\to\infty$. Remarkably, the quality of estimation is \emph{dimension-free}, which ensures that $\hat{s}(x)$ is well-suited to the high-dimensional regime where $n\ll p$. These results also extend naturally to the problems of using linear measurements to estimate the rank of a positive semidefinite matrix, or the sparsity of a non-negative matrix. Finally, we show that if no structural assumption (such as non-negativity) is made on the signal $x$, then the quantity $s(x)$ cannot generally be estimated when $n\ll p$."
Modelling Shapes and Segmentations by Fields of Expert Mixtures,"We introduce a Gibbs Random Field (GRF) for modelling segmentations, i.e., the shape and the spatial relations of segments. The latent variables of the model form a field of expert mixtures (i.e.~naive Bayes models). The resulting probability distribution for segmentations (obtained by marginalising over the expert field) is a GRF with factors of arity higher than two.We show how to formulate the corresponding learning and inference tasks for the resulting model and propose algorithms for their approximative solution. The resulting approach is experimentally analysed on the example of two prototypical segmentation tasks."
PRISMA: PRoximal Iterative SMoothing Algorithm,"Motivated by learning problems including max-norm regularized matrix completion and clustering, robust PCA and basis pursuit, we propose a novel optimization algorithm for minimizing a convex objective which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz part, and a simple non-smooth non-Lipschitz part. Our algorithm combines the methodology of forward-backward splitting, smoothing, and accelerated proximal methods."
Bayesian Nonparametric Modeling of Suicide Attempts,"The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the U.S. population. In the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts."
Using GSMDPs in Multi-Robot Decision-Making Problems,"Markov Decision Processes (MDPs) provide an extensive theoretical background for problems of decision-making under uncertainty. In order to maintain computational tractability, however, problems domains are typically discretized both in states and actions as well as in time. Assuming synchronous state transitions and actions at fixed rates may result in models which are not strictly Markovian, or where agents are forced to idle, losing their ability to react to sudden changes in the environment. In this work, we explore the application of Generalized Semi-Markov Decision Processes (GSMDPs) to a realistic multiagent scenario. A case study will be presented in the domain of cooperative robotics, where real-time reactivity must be preserved, and therefore synchronous discrete-time approaches are sub-optimal. By allowing asynchronous events to be modeled over continuous time, the GSMDP approach will be shown to provide greater solution quality, and reduced communication usage, than its discrete-time counterparts, while still being approximately solvable by existing methods."
Time Series Analysis - An Online Learning Approach,"In this paper, we address the problem of predicting a time series, under minimal assumptions on the noise terms. The focus of our work is on the ARMA model, which is a standard model for time series prediction. Using regret minimization techniques, we develop an effective online learning algorithm for the prediction problem, \emph{without} assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we perform an empirical study to verify the effectiveness of our algorithm."
A Spectral Learning Approach to Range-Only SLAM,"We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our  approach does not need to linearize a transition or measurement model; such linearizations can cause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly for the highly non-Gaussian posteriors encountered in range-only SLAM. We provide a theoretical analysis of our method, including finite-sample error bounds.  Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost."
Maximally informative models and diffeomorphic modes,"Motivated by recent experiments in transcriptional regulation and sensory neuroscience, we consider the following general problem in statistical inference. A system of interest, when exposed to a stimulus $S$, adopts a deterministic response $R = \theta(S)$, of which a noisy measurement $M \sim \pi(M|R)$ is made. Given a large number of measurements and corresponding stimuli, we wish to identify the correct response function $\theta$. However, we do not know the conditional distribution $\pi(M|R)$ a priori. The standard regression approach is to maximize the likelihood of a model response function $\theta$ assuming a specific noise function $\pi$. However, an incorrect $\pi$ will typically lead to systematic error in the inferred $\theta$.  In the large data limit, we argue that one should instead maximize likelihood over both $\theta$ and $\pi$, and we show that this is equivalent to maximizing the mutual information $I[R;M]$ over $\theta$ alone. Moreover, when any candidate response model fully explains the data, maximizing mutual information becomes equivalent to simultaneously maximizing all dependence measures which obey the Data Processing Inequality. Even so, one typically finds not a single optimal $\theta$, but rather an extended subspace of optimal $\theta$. This is because experiments of the type considered cannot distinguish between different $\theta$ within certain equivalence classes. We present a formula for ``diffeomorphic modes'', directions in parameter space which lie within these equivalence classes and cannot, as a result, be constrained by data. We then derive the diffeomorphic modes of response models having either a general linear form or a specific linear-nonlinear form, and find that the number of diffeomorphic modes is often far less then the number of response model parameters. In such cases, maximizing mutual information will, in the large data limit, determine nearly all model parameters without any systematic error."
Using Support Vector Machines for Solid State Materials Characterization,"Spectroscopic techniques like X-Ray diffraction and Nuclear Magnetic Resonance (NMR) are used in conjunction with empirical and ab-initio calculations to perform structural characterization for materials. Overall, these experimental and computational methods are generally expensive and time consuming, requiring much human input and interpretation, particularly with regards to novel materials. In this work, NMR spectral data for Titanium Oxide polymorphs are simulated and first principle calculations of NMR measurable quantities performed for these materials via batch processes. An array of machine learning kernels in the form of support vector machines (SVM) are then used to learn the complex mapping between structural details and simulated input NMR spectra. The SVM array when presented with input spectra for new but related polymorphs outputs structural details rapidly and accurately. This approach has the potential to reduce the time to discover structural details for new materials, by providing a viable method for solving the inverse problem, with minimal human intervention. A similar approach may be applied to other experimental characterization techniques and ab initio calculations, for example scanning tunneling microscopy images and density of states calculations, in order to build an expert system for solid state materials characterization."
Analysis on Co-Training with Insufficient Views,"Co-training is a famous semi-supervised learning paradigm which exploits unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sufficient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sufficient to correctly predict the label. First, we show that co-training might suffer from two limitations, i.e., label noise and sampling bias, when neither view is sufficient. We then define the diversity between the two views with respect to the confidence of prediction and prove that if the two views have large diversity, co-training suffers little from above two limitations and could succeed in improving the learning performance by exploiting unlabeled data even with insufficient views."
Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling,"Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches."
Object Recognition with Boosted Collections of Parts,"We describe an approach for incrementally learning parts for object recognition.  We use category-independent object region proposals to pool learned parts, which removes the need for a structurally constrained spatial model and enables more flexible and extensible learning. Our base model uses an incremental learning approach to refine statistical templates of parts. Boosting is used to learn a strong classifier that scores collections of part responses into an object category. Our system achieves competitive accuracy with the best-existing systems, outperforming on more deformable categories, and is easy to extend to new features or categories."
Neurally Plausible Reinforcement Learning of Working Memory Tasks,"A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity. It is however not well known how these neurons acquire their task-relevant tuning. Here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error. We propose that the action selection stage feeds back attentional signals to earlier processing levels. These feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. A globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to (1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and (2) learn to optimally integrate probabilistic evidence for perceptual decision making."
Robust PCA from incomplete observations using l-0-surrogates,"Many applications in data analysis rely on the decomposition of a data matrix into a low-rank and a sparse component.Existing methods that tackle this task use the nuclear norm and l-1-cost functions as convex relaxations of the rank constraint and the sparsity measure, respectively, or employ thresholding techniques.We propose a method that allows for reconstructing a matrix of upper-bounded rank from incomplete and corrupted observations. It does not require any a priori information about the number of outliers. The core of our algorithm is an intrinsic Conjugate Gradient method on the set of orthogonal projection matrices, the so-called Grassmannian. Non-convex sparsity measures are used for outlier detection, which leads to improved performance in terms of robustly recovering the low-rank matrix.In particular, our approach can cope with more outliers and with an underlying matrix of higher rank than other state-of-the-art methods."
Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions,"Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff."
Fully Bayesian inference for neural models with negative-binomial spiking,"  Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses.  The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability.  Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals.  This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efficient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models.  We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains."
Maximum Composite Likelihood Estimators for Gaussian Process Classification,"  Type-II Maximum Likelihood point estimation of covariance function  parameters is the inference framework of choice for the majority of  serious applications of Gaussian Process based discriminant analysis  and object classification. These estimators are typically derived  from approximations to the non-analytic marginal likelihood based on  Variational bounds or Expectation Propagation. However, the  frequentist properties of such estimators, such as the     unbiasedness, consistency and efficiency cannot  generally be determined analytically or indeed guaranteed. This  paper suggests that estimators with good frequentist properties for  binary Gaussian Process classifiers can be obtained by the use of  Composite Likelihoods for the estimation of the parameters of the  Gaussian Process. Furthermore, because of the structure of the  Composite Likelihood, the computational scaling achievable is vastly  superior to the typical ${\cal O}(n^3)$ scaling for Gaussian  Processes; the explicit computation of determinants, inverses and  derivatives of covariance matrices of arbitrary $n \times n$  dimension is obviated because exact analytical expressions can be  exploited in an independent and distributed manner."
Transductive Kernel Map Learning,"Transductive inference techniques are nowadays becoming standard  in machine learning due to their relative success in solving many real-world applications. Among them, kernel-based methods are particularly interesting but their success remains highly dependent on the choice of kernels. The latter are usually handcrafted or designed  in order to capture better similarity in training data. In this paper, we  introduce a novel transductive learning algorithmfor kernel design and classification. Our approach is basedon the minimization of an energy function mixing  i) a reconstructionterm that factorizes a matrix of  input data as a product of a learned dictionary  and a learned kernel map ii) a fidelity term thatensures consistent label predictions with those provided in a ground-truth and iii) a smoothness term which  guaranteessimilar labels for neighboring data and allows us to iteratively diffuse  kernel maps and labels from  labeled to unlabeled data. Solving this minimization problem makes it possible to learn both a decision criterion and a kernel map  that guarantee linearseparability in a high dimensional space and good generalizationperformance. Experiments conducted on object class segmentation, show improvements with respect to baseline as well as  related work on the challenging VOC database. "
Probabilistic Active Learning,"In the literature of active learning, most existing studies assume that the labels in the training dataset are deterministic, which is not quite realistic in many real-world applications. In many applications, however, the labels usually come in a probabilistic manner. Motivated by this observation, we propose a new frameworkwhere each label is enriched with a probability. Based on this framework, we propose an active learning algorithm. In addition, we show a theoretical bound on the label complexity of the proposed algorithm. Finally, we conducted comprehensive experiments in order to verify the effectiveness of our proposed algorithm."
Slice Normalized Dynamic Markov Logic Networks,"Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the domain of time points typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional randomfield for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues.It supports efficient online inference, and can directly model influencesbetween variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks."
Transductive Learning for Multi-Task Copula Processes,"We tackle the problem of multi-task learning with copula process. Multivariable prediction in spatial and spatial-temporal processes such as  natural resource estimation and pollution monitoring have been typically addressed using techniques based on Gaussian processes and co-Kriging. While the Gaussian prior assumption is convenient from analytical and computational perspectives, nature is dominated by non-Gaussian likelihoods. Copula processes are an elegant and flexible solution to handle various non-Gaussian likelihoods by capturing the dependence structure of random variables with cumulative distribution functions rather than their marginals.  We show how multi-task learning for copula processes can be used to improve multivariable prediction for problems where the simple Gaussianity prior assumption does not hold. Then, we present a transductive approximation for multi-task learning and derive analytical expressions for the copula process model. The approach is evaluated and compared to other techniques in one artificial dataset and two publicly available datasets for natural resource estimation and concrete slump prediction. "
EigenGP: KL-expansion based Gaussian process learning,"Gaussian processes (GPs) provide a nonparametric representation of functions. Given N training points, the exact GP inference incurs high computational cost. In this paper, we propose a sparse Gaussian process model, EigenGP, based on Karhunen-Lo`eve (KL) expansions of a GP prior. We use the Nystr?om approximation to obtain eigenfunctions of the covariance function and use an empirical Bayesian approach to select these eigenfunctions. To handle nonlinear likelihoods, we develop an efficient expectation propagation inference algorithm, and couple it with expectation maximization for evidence maximization. By selecting eigenfunctions of Gaussian kernels that are associated with data clusters, EigenGP is also suitable for semi-supervised learning. Our experimental results demonstrate improved predictive performance of EigenGP over alternative state-of-the-art sparse GP and semisupervised learning methods for regression, classification, and semisupervised classification."
A Scalable Approach to Column-Based Low-Rank Matrix Approximation,"In this paper we address the column-based low-rank matrix approximation problem in a novel parallel approach. Our approach is based on the divide-and-combine idea that it performs column selection on each submatrices in parallel, and then combines results from them. Like many existing methods, our method enjoys a relative-error bound by the theoretical analysis. Importantly, our method is scalable on large-scale matrix compared with the traditional methods. In addition, our method is deterministic in data partition and free from any matrix coherence assumption. Finally, the experiments on both simulated and real data shows that our approach is both efficient and effective. "
The Variational Garrote,"In this paper, we present a new model for sparse regression using L0 regularization. The model introduces a sparseness mechanism in the likelihood, instead of in the prior, as is done in the spike and slab model. The posterior probability is computed in the variational approximation. The variational parameters appear in the approximate model in a way that is similar to Breiman's Garrote model.  We refer to this method as the variational Garrote (VG).  We show that the combination of the variational approximation and L0 regularization has the effect of making the problem effectively of maximal rank even when the number of samples is small compared to the number of variables.  The VG is compared numerically with the Lasso method and with ridge regression.  Numerical results on synthetic data show that the VG yields more accurate predictions and more accurately reconstructs the true model than the other methods. It is shown that the VG finds correct solutions when the Lasso solution is inconsistent due to large input correlations.  The naive implementation of the VG scales cubic with the number of features.  By introducing Lagrange multipliers we obtain a dual formulation of the problem that scales cubic in the number of samples, but close to linear in the number of features."
Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem,"Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have serious limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.We propose a simple framework for adaptive quality control in crowdsourced multiple-choice tasks. In it, we identify a novel problem we call the bandit survey problem. This framework is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations."
An Alternative Kernel Method for the Two-Sample Problem,"We present an alternative kernel method for the two-sample problem that is based on Friedman's approach of using any binary classification learning machine to score the data.  When the learning machine is chosen to be a support vector machine, we show that this approach is a generalization of the permutation $t$-test.  Previous work has yielded a normal rate of convergence bound using Stein's Method in the simple setting of univariate data and a linear kernel with simulations, suggesting that this proof technique may be extended to address a more general setting.  Despite a lack of tuning of the SVM parameters, this method is shown to be competitive with the Maximum Mean Discrepancy (MMD) test."
Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization,"We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\cite{kRegExpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice. "
The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes,"Stochastic differential equations (SDE) are a natural tool for modelling systemsthat are inherently noisy or contain uncertainties that can be modelled as stochasticprocesses. Crucial to the process of using SDE to build mathematical modelsis the ability to estimate parameters of those models from observed data. Overthe past few decades, significant progress has been made on this problem, butwe are still far from having a definitive solution. We describe a novel methodof approximating a diffusion process that we show to be useful in Markov chainMonte-Carlo (MCMC) inference algorithms. We take the ?white? noise that drivesa diffusion process and decompose it into two terms. The first is a ?colourednoise? term that can be deterministically controlled by a set of auxilliary variables.The second term is small and enables us to form a linear Gaussian ?small noise?approximation. The decomposition allows us to take a diffusion process of interestand cast it in a form that is amenable to sampling by MCMC methods. We explainwhy many state-of-the-art inference methods fail on highly nonlinear inferenceproblems. We demonstrate experimentally that our method performs well in suchsituations. Our results show that this method is a promising new tool for use ininference and parameter estimation problems."
Incorporating Hidden Knowledge through a Latent Variable,"A teacher provides a student prior knowledge such that the student learns more efficiently and makes fewer mistakes. Although prior knowledge plays significantly important role in human learning, it has not yet been thoroughly investigated in machine learning. Among different sources of knowledge, hidden knowledge, the information that is only available during training but not testing, has a wide variety of applications and great potential to bring huge benefits. But it remains challenging to capture the hidden knowledge and to incorporate them into the learning process. In this work, we take the first step to exploit and encode the useful information within the hidden knowledge into the learning task through a discrete latent variable. Experiments on gesture recognition demonstrated that hidden knowledge could be successfully captured and incorporated to learn more discriminative and generalizable hidden state models for classification."
Learning Low-rank Nonparametric Kernel Matrices From the Point of View of Matrix Completion,"Many existing nonparametric kernel learning methods suffer from high computational cost, which limits theirapplications to large scale real-world problems. In this paper, we propose a novel nonparametric kernel learning method based on the matrix completion technique, which emphasizes the low rank property of the learned kernel matrices. Given some pairwise constraints, we formulate the nonparametric kernel learning problem into a rank minimization problem with  constraints that enforce the learned similarities between the known data pairs equal the values of the known pairwise constraints. The resulting optimization problem can be relaxed to a convex optimization problem of minimizing nuclear norm with graph Laplacian regularization. We then develop a singular value thresholding like algorithm to solve the constrained convex problem using the similar techniques as in the matrix completion problems. Preliminary experimental results show that our algorithm performs better than or comparably to the existing best method BCDNPKL of [10] in terms of clustering accuracy and scalability."
Learning as MAP Inference in Discrete Graphical Models,"We present a new formulation for attacking binary classification problems. Instead of relying on convex losses and regularisers such as in SVMs, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but \emph{discrete} formulation, where estimation amounts to finding a MAP configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees. Due to the discrete nature of the formulation, it also allows for \emph{direct} regularisation through cardinality-based penalties, such as the $\ell_0$ pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner. We also outline a number of open problems arising from the formulation."
Parallel and Asynchronous Reinforcement Learning from Customer Interaction Sequences,"In this paper, we explore applications in which a company interacts with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur asynchronously and in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for reinforcement learning in this setting, using an asynchronous variant of temporal-difference learning to learn efficiently from partial interaction sequences. We applied our asynchronous TD algorithm to two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records. Our TD algorithm achieved good performance in both scenarios. It significantly outperformed comparable approaches that learn only from complete interaction sequences, as well as bandit-based approaches that do not exploit the sequential nature of the problem."
A mechanistic model of early sensory processing based on subtracting sparse representations,"Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics."
Learning Canonical Correlations of Paired Tensor Sets via Tensor-to-Vector Projection,"Canonical correlation analysis (CCA) is a useful technique for measuring relationship between two sets of vectorial data. We propose a multilinear CCA (MCCA) method for paired tensorial data sets. Different from existing multilinear variations of CCA, MCCA extracts uncorrelated features under two architectures while maximizing paired correlations. One architecture imposes set-wise zero-correlation constraint while the other requires cross-correlation between different pairs to be zero. This is achieved through a pair of tensor-to-vector projections, estimated via a successive and iterative approach. We evaluate MCCA on matching facial images of different poses against competing solutions to show its superiority. We also study fusion of two architectures and observe small performance improvement, implying some complementary information captured."
Model Selection by Measuring Validated Information,"Which model best generalizes the available data?This study addresses the task of model selection.We introduce a general validation principle which is applicable to non-i.i.d. scenarios.Validated Model Information aims at selecting models whose predictions are optimally informative.The selection is performed by measuring quantifying predictive information, that is structure extracted from genuine regularities.This is in contrast to descriptive information, which includes accidental fluctuations due to noise.VMI exhibits stability with respect to resampling and Turing-universality.It is non-computable, yet effectively approximated by coding.Firstly, we evaluate the principle in a controlled setting with synthetic data.Then, we demonstrate its applicability with real biological data and external verification.Particular emphasis is given to the task of cluster model selection."
Multi-Stage Multi-Task Feature Learning,"Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex regularization problem, which is usually suboptimal, due to its looseness for approximating an $\ell_0$-type regularizer. In this paper, we propose a non-convex regularized formulation for multi-task sparse feature learning; we propose to solve the non-convex optimization problem by employing a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms."
Neural Network Models for Multilabel Learning,"Multilabel learning is an extension of standard binary classification where the goal is to predict a set of labels (which we call tags) for a given input example. There have been many models proposed for this problem, such as subspace learning, kernel methods, and nearest neighbour schemes. Recently, the probabilistic classifier chain method was proposed, which learns a series of probabilistic models that attempt to capture tag correlations. In this paper, we show how this model may be interpreted as a neural network with connections amongst output nodes. We argue that using an explicit hidden layer instead brings several advantages, such as tractable test-time inference, and removing the need for fixing a tag ordering. Further, the hidden units capture nonlinear latent structure that both improves classification performance and allows for interpretability. Compared to previous neural network models for multilabel learning, we discuss several design decisions that have a significant impact on training the network. Empirical results show that the model outperforms several existing methods."
Recursive Gaussian Process Regression,"For large data sets, performing Gaussian process regression is computationally demanding or even intractable. If data can be processed sequentially, the recursive regression method proposed in this paper allows incorporating new data with constant computation time. For this purpose two operations are performed alternating on a fixed set of so-called basis vectors used for estimating the latent function: First, inference of the latent function at the new inputs. Second, utilize the new data for updating the estimate of the latent function at the basis vectors. By means of numerical simulations it is shown that the proposed approach significantly reduces the computation time compared to existing on-line and/or sparse Gaussian process regression approaches."
A Coarse-to-Fine Approach to Flexible Activity Discovery and Data Segmentation,"The growing number of mobile sensors allow collection of activity data at differentlevels of complexity and at fine-grained temporal resolution. However,accurately translating unlabeled sensor streams into meaningful activity classesremains non-trivial. The primitives that comprise an activity are usually foundin multiple activity classes; activities exhibit high-order temporal dependenciesamong primitives; and these higher-order dependencies vary from activity to activityand are blended together in the data. This paper presents a novel unsupervisedcoarse-to-fine activity discovery framework that handles the temporal heterogeneitypresent in sequences of sensor data and automatically segments activitieseven when the dependency order is not known and not fixed across activities.Our framework designs a Mixed Memory Latent Dirichlet Allocation (MM-LDA)model that iteratively discovers sequential transition patterns, segments the sensordata accordingly, and groups contiguous activity samples using a locality metric.We demonstrate the effectiveness of the approach by empirical experimentationon real-world datasets."
Removing Localized Corruption from Natural Images,"  Traditional approaches to removing image corruption such as blur or  noise combine a natural image prior with a reconstruction term. The  latter relies on a good generative model of the corruption --- which  may not exist for many distortions encountered in the real world. In  this paper we explore approaches for learning a direct mapping from  the corrupt input image to the clean image, obviating the need for  any kind of generative model. We evaluate the approaches on several  types of synthetic corruption, finding that neural-network based  models perform the best.  Our techniques can be used for many types of localized corruption. We  demonstrate this using photographs of real-world scenes taken behind a pane  of glass with water droplets, akin to a rainy window.  Our model removes most  of the raindrops without significant blur, the first such demonstration of  this application."
Moving Object Detection and Pixel-Level Localization From Semantic Priors and Topological Constraints,"We describe an approach to incorporate scene topology and semantics into pixel-level object detection and localization. Our method requires {\em video} to determine occlusion regions, and thence local depth ordering, and any visual recognition scheme that provides a score at local image regions, for instance detection probability.  We set up a cost functional that incorporates occlusion cues induced by moving objects, label consistency and recognition priors, and solve it using modern discrete optimization schemes. We show that our approach improves localization accuracy of existing recognition approaches, or equivalently provides recognition labels to pixel-level localization and segmentation."
Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,"This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets."
What Can Pictures Tell Us About Web Pages? Improving Document Search using Images,"Traditional Web search engines do not utilize the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page. In this paper we study whether the {\em content} of the pictures appearing in a Web page can be used to enrich the semantic description of an HTML document and consequently boost the performance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using visual information extracted from the images contained in the pages. The reranker is query-independent and learned from labeled examples during an offline stage. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual information. We demonstrate our approach on the TREC 2009 Million Query Track, one of the premiere benchmarks in large-scale Web page retrieval. We show that the exploitation of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark. "
Structured Robust Subspace Learning: Efficient Recovery of Corrupted Low-rank Matrices,"In this paper, a Structured Robust Subspace Learning (SRSL) method is proposed for efficient low-rank recovery. Its main idea is a novel rank-minimization heuristic that imposes the group sparsity under orthonormal subspaces, which enables minimizing the rank by efficient sparse coding algorithms. Theoretical bounds of the group sparsity minimization under orthogonal subspace are given to validate this novel rank-minimization heuristic. In addition, a modified Nystrom method is introduced to further accelerate SRSL such that its sampling-based version (SRSL+) has linear complexity of the matrix size. Extensive experimental results demonstrate that SRSL and SRSL+ provide the state-of-the-art efficiency without compromising the recovery accuracy."
Bayesian estimation of discrete entropy with mixtures of Pitman-Yor process priors,"  We consider the problem of estimating Shannon's entropy H in the  under-sampled regime, where the number of possible symbols may be  unknown or countably infinite.  Pitman-Yor processes (a  generalization of Dirichlet processes) provide tractable prior  distributions over the space of countably infinite discrete  distributions, and have found major applications in Bayesian  non-parametric statistics and machine learning. Here we show that  they also provide natural priors for Bayesian entropy estimation,  due to the remarkable fact that the moments of the induced posterior  distribution over H can be computed analytically. We derive  formulas for the posterior mean (Bayes' least squares estimate) and  variance under such priors.  Moreover, we show that a fixed  Dirichlet or Pitman-Yor process prior implies a narrow prior on H,  meaning the prior strongly determines the entropy estimate in the  under-sampled regime. We derive a family of continuous mixing  measures such that the resulting mixture of Pitman-Yor processes  produces an approximately flat (improper) prior over H.  We  explore the theoretical properties of the resulting estimator, and  show that it performs well on data sampled from both exponential and  power-law tailed distributions."
Low-rank Panoramas for Street View Videos,"In this paper, we address how to automatically generate panoramas for a street view from a long video sequence. We formulate the problem as one of robust recovery of a low-rank matrix from highly incomplete, corrupted, and deformed measurements (the video frames). We leverage powerful high-dimensional convex optimization tools from compressive sensing of sparse signals and low-rank matrices to solve this problem. In particular, we show how the new method can effectively remove severe occlusions or corruptions (caused by trees, cars, or reflections, etc.), and obtain street panoramas that have very clean global appearance and very accurate global geometry. We also show how our method can automatically and robustly establish pixel-wise accurate registration among all the video frames. We demonstrate the effectiveness of our method by conducting extensive experimental comparison with other state-of-the-art video stitching systems."
Cost-Sensitive Online Active Learning for Online anomaly detection,"Online anomaly detection is an important problem in data mining, which enjoys many real-world applications in a variety of domains. In literature, most of existing studies attempt to solve anomaly detection by formulating it as either a classical batch supervised classification task or an online unsupervised learning task. Both of approaches have their limitations in solving a real-world anomaly detection task. In this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL) for online anomaly detection, which goes beyond conventional approaches to solve anomaly detection in a natural, effective and scalable way. We propose two CSOAL algorithms under the proposed framework, and theoretically analyze their performance in terms of cost-sensitive measures, including (i) the lower bound for the weighted sum of sensitivity and specificity achieved by the first algorithm, and (ii) the upper bound for the weighted cost made by the second algorithm. We extensively examine the empirical performance of the proposed algorithms on several challenging anomaly detection tasks, in which encouraging results validate the efficacy of our proposed technique in solving anomaly detection with online active learning."
Fast Hierarchical Topic Modeling via Nonnegative Matrix Factorization,"Hierarchical clustering is one of the cluster analysis tasks which aims at building a hierarchy of topics. Most state-of-the-art algorithms proposed in literature for hierarchical clustering are based on sampling. Their computational costs largely depend on the number of words in the given corpus, which is usually extremely. In this paper, we come up with an efficient hierarchical clustering method using non-negative matrix factorization (NMF), which is a dimension reduction technique that approximates a given matrix by a product of two low rank matrices. To maintain the hierarchy of topics, we design special constraints for low rank matrices, resulting in a novel optimization problem which we propose to solve using coordinate descent algorithms. Experiments on both artificial and real-world data sets demonstrate the effectiveness of our approach."
Online Convex-Concave Optimization,"We study online convex optimization (OCO) and online convex-concave optimization(OCC) under a unified framework of variational inequalities (VI). The OCCyields projection-free online learning algorithms, breaking the bottlenecks in OCOwhere the projection onto the constraints set is challenging. We define a new typeof regret based on VI named VI-regret, which includes standard regret in OCO asa special case. We show the prox method has a variation-based VI-regret boundand online alternating direction method has a sublinear regret bound in OCC. Wealso give two examples to show the projection-free OCC."
On the Numerical Stability and Value Function Stability of Value-Directed Compression for POMDPs,"Value-directed compression (VDC) improves the tractability of a large scale POMDP by finding a basis to project its high-dimensional belief space into a low-dimensional approximation, where the problem can be solved with less computations. Our empirical findings indicate lossless VDC may sometimes produce larger compression errors than lossy VDC truncated to the same compression level due to the trade-off between residual threshold and numerical stability.  This paper analyses the numerical stability and residual error of the lossless and lossy VDC algorithms according to their column selection heuristics, and proposes a slight modification of lossless VDC that has a more tractable condition number. In addition, we show that the factorability of a problem is not the main determiner of the learnability of compressed problems. We discuss a built-indeficiency of VDC that can possibly magnify a distortion in the value function caused by compression errors to an infinitely great degree, which in the worst case will make the quality of the policy optimised based on the compressed POMDP arbitrarily low. This work contributes to the fundamental underlying theory of VDC, with supporting empirical evidence using benchmark POMDP problems."
Large-Scale Bandit Problems and KWIK Learning,"We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model  known as ``Knows What It Knows'' or KWIK learning. We give matching impossibility results showing that the KWIK-learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time."
Learning the Architecture of Sum-Product Networks Using Clustering on Variables,"The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture significantly improves its performance compared to using a previously-proposed static architecture."
Spike triggered covariance for strongly correlated Gaussian stimuli,"Characterizing feature selectivity is an important problem because it can shed light on how neurons process their inputs. The spike triggered covariance method (STCM) is a very commonly used method to extract the relevant set of stimulus features to which a neuron responds. One of the main advantages of STCM is that it can determine the dimensionality of the cell's relevant subspace. The method has been previously thought to be applicable when stimuli are drawn from a Gaussian ensemble, with or without stimulus correlations.  Here we use random matrix theory to show that when STCM is used with strongly correlated Gaussian stimuli, the null distribution of eigenvalues has a large outstanding mode. As a result, STCM can either yield an extra feature, which often corresponds to the strongest eigenvalue, or fail to yield any significant dimensions. We present a simple correction scheme that removes this artifact and illustrate its effectiveness by analyzing model neurons and recordings from retinal ganglion cells probed with correlated Gaussian stimuli whose second-order statistics was matched to natural stimuli. Our results can serve as guidelines for design of reverse correlation experiments that can help illuminate how neurons are optimized to code natural stimuli."
A Sparsity Nonnegative Matrix Factorization Technique for Correspondences Problems,"Graph matching is an essential problem in computer vision and pattern recognition. In this paper, we present a robust graph matching method based on nonnegative matrix factorization with sparsity constraints. We show that our sparsity NMF based solution is sparse and thus naturally imposes the discrete mapping constraints strongly in the optimization process. Promising experimental results on both synthetic point matching and real world image feature matching tasks show the effectiveness of our graph matching method."
Cooperating with a Markovian Ad Hoc Teammate,"This paper focuses on learning in the presence of a Markovian teammate in Ad hoc teams. A Markovian teammate's policy is a function of a set of discrete feature values derived from the joint history of interaction, where the feature values transition in a Markovian fashion on each time step. We introduce a novel algorithm ``Learning to Cooperate with a Markovian teammate'', or LCM, that converges to optimal cooperation with any Markovian teammate that satisfies certain assumptions, and achieves safety with any arbitrary teammate, in tractable sample complexity. The novel aspect of LCM is the manner in which it satisfies the above two goals via efficient exploration and exploitation. The main contribution of this paper is a full specification and a detailed analysis of LCM's theoretical properties. "
Pointwise Tracking the Optimal Regression Function,"This paper examines the possibility of a `reject option' in the context of least squares regression. It is shown that using rejection it is theoretically possible to learn `selective' regressors that can $\epsilon$-pointwise trackthe best regressor in hindsight from the same hypothesis class, while rejecting only a bounded portion of the domain.Moreover, the rejected volume vanishes with the training set size, under certain conditions. We then develop efficient and exact implementation of these selective regressors for the case of linear regression. Empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error."
"Phase-Invariance, Sparsity and Image Category Discrimination","Approaches to image object recognition and categorization have tended to favour Gaussian scale space over Gabor-like representations. This is largely due to the success of descriptors based on Histograms of Gradients (HoG) to create efficient ndexing schemes. In this paper, we look at the alternative epresentation of complex-valued directional filters, widely used in face and texture recognition, and as models for peak rate of firing for phase-invariant, complex cells in primary visual area V1. We ask the question: can such Gabor-like models  be equipped with focus-of-attention operators and effective region descriptors, as for Gaussian scale-space ? We present some conclusions to this research: first, optimizing the tiling of Fourier space to encourage sparse coding turns out to be important to achieving good performance in interest-point localization and scale estimation. We suggest a simple spatial pooler, and find that a Winner-Take-All (WTA) approach to encourage sparse descriptors, gives better performance than without this step. We use   Area-Under-Curve (AUC) measures applied to Receiver Operator   Characteristic (ROC) curves of descriptor distances within and  between image classes. Results are assessed on standard image atabases used for categorization performance in computer vision. The significance of this work is that systems based on complex direction-selective filters can, with suitable adjustments, achieve the scalability of keypoint-based approaches, and potentially yield performance that is state-of-the-art.  This would potentially remove the need for parallel feature stacks in general-purpose vision systems designed to handle a wide variety of object classes and vision tasks."
Reconstructing ecological networks with hierarchical Bayesian regression and Mondrian processes,"Ecological systems consist of complex sets of interactions among species and their environment, the understanding of which has implications for predicting environmental response to perturbations such as invading species and climate change.  However, the revelation of these interactions is not straightforward, nor are the interactions necessarily stable across space. Machine learning can enable the recovery of such complex, spatially varying interactions from relatively easily obtained species abundance data. Here, we describe a novel Bayesian regression and Mondrian process model (BRAMP) for reconstructing species interaction networks from observed field data. BRAMP enables robust inference of species interactions considering autocorrelation in species abundances and allowing for variation in the interactions across space. We evaluate the model on spatially explicit simulated data, produced using a trophic niche model combined with stochastic population dynamics. We compare the model's performance against L1-penalized sparse regression (LASSO) and non-linear Bayesian networks with the BDe scoring scheme. Finally, we apply BRAMP to real ecological data."
Bayesian nonparametric models for bipartite graphs,"We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, an Indian Buffet-like generative process for network growth, and a simple and efficient Gibbs sampler for posterior simulation. Our model is shown to be well fitted to several real-world social networks."
Reducing statistical time-series problems to  binary classification,"We  show how binary classification methods developed to work on i.i.d.\ data can be used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solvging these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data."
Learning a manipulation task of cascaded blocks with passive joints from inexpert demonstrations using sample based stochastic policy,"Reinforcement learning (RL) methods have been successfully applied tothe learning of the robot controland becomes to allow a robot to learn various kinds of realistic tasksby designing the policy functionwith a prior knowledge about the control task.We present a novel representation of a stochastic policybased on a non-parametric model to obtain the knowledge directly from the demonstrations by human teacher.In this method, the action is selected from stored samples,and the importance of each sample is trainedby an RL method as a policy parameter.Since samples are generated from demonstrations,our method is expected to allow the robot to extractuseful knowledge included in human instructions.We applied our method to the manipulation task of a cascaded rigid linksand experimental results show that a good controller can be obtainedby our method."
Estimating noise correlations under signal drift,"Large data in biological and information sciences often show nonstationary trends andare beyond the scope of the conventional methods under assumption of stationary.Especially, if infinitely many possible trends can occur unpredictably, it is difficult to tackle them with a single algorithm without previous knowledge.However, it is possible to estimate interesting statistical parameters from the data with unpredictable drifts for some semiparametric statistical models.In this paper, we consider a semiparametric, mixture of Gaussian models where the trend distribution is not restricted at all.We propose estimators of noise correlations for multivariate time series with brain signals in mind and demonstrate thatit works robustly against any unpredictable drift in signals (means) while the conventional correlogram shows spurious correlations contaminated by the temporal drift."
Classification Calibration Dimension for General Multiclass Losses,"We study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of \emph{classification calibration dimension} of a multiclass loss matrix, which measures the smallest `size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al.\ (2010) for analyzing the difficulty of designing `low-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems."
Online Information-Geometric Change Detection with Exponential Families,This paper studies online change detection with exponential families. We formulate a generic statistical framework for sequential abrupt change detection and introduce generalized likelihood ratio test statistics with arbitrary estimators. We show intrinsic links of these statistics with maximum likelihood estimates and interpret this within the context of information geometry. It provides a unifying view of change detection for many common statistical models and corresponding distances. We also derive a computationally efficient scheme for change detection based on exact generalized likelihood ratios with maximum likelihood estimators. This scheme is applied to synthetic and real-world datasets of various natures.
Collaborative Gaussian Processes for Preference Learning,"We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms."
Approximating Concavely Parameterized Optimization Problems,"We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\varepsilon >0$ by a set of size $O(1/\sqrt{\varepsilon})$. A lower bound of size $\Omega (1/\sqrt{\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\sqrt{\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion."
Lasso Screening Rules via Dual Polytope Projection,"Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. Bytransforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose a fast and efficient screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. We have evaluated our screening rule using both synthetic and real data sets. Results show that our rule is more effective to identifyinactive predictors than existing state-of-art screening rules. We also extend our screening rule to identify inactive groups in group Lasso."
Gradient-based kernel method for feature extraction and variable selection,"We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter finds a subset of predictors. First, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method.  In comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets.  Second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by Chen et al. (2010).  Experimental results show that the proposed methods successfully find effective features and variables without parametric models."
Rational impatience in perceptual decision-making: a Bayesian account of discrepancy between two-alternative forced choice and Go/NoGo Behavior,"Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes."
MEAN-FIELD ANNEALING BASED COMMITTEE MACHINES  Estimation of Weather-Related Outages in Power Distribution Systems,"The reliability of electrical delivery is an important concern for utility companies. Weather related outages have a significant impact on it. There are many regression based models to estimate outages from weather factors in overhead distribution system. This paper proposes the use of committee machines composed of multiple neural networks to estimate outages. A major challenge for using a committee machine is to properly combine predictions from multiple networks, since the performance of individual networks is input dependent due to mapping misrepresentation. This paper presents a new method in which the individual network predictions are combined dynamically. The error minimization is performed using the mean field annealing theory. Results obtained for the study area in Kansas are compared with observed outages to evaluate the performance of the model for estimating these outages. The results are also compared with previously studied regression and neural network models to determine an appropriate model to represent effects of wind and lightning on outages."
Hypergraph-based Gaussian Process Models with Qualitative and Quantitative Input Variables,"Most existing Gaussian process models assume that all the input variables are quantitative, which makes them fall short in many applications that involve both qualitative and quantitative inputs. The fundamental challenge for enabling GP models on these applications is the design of a proper correlation function. We develop such a correlation function based on hypergraph and the associated Laplacian matrix. Compared with existing models, the proposed model requires much fewer free parameters and shows better performance on benchmark examples."
Topographic Analysis of Correlated Components,"Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants area assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the data where the components can have linear or higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations that are sensitive to linear or higher order correlations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data."
Toward automated semantic leaf categorization by image analysis,"Understanding the diversity of the plant community is a crucial issue for the development of many botanical industries as well as for the conservation of the ecosystem biodiversity. Traditionally, botanists have proposed detailed key descriptions (generally qualitative) or concepts about the morphology of plants and particularly of leaves that allow them to construct relationships between different plants and between them and their species. However, extracting these concepts is complicated, painstaking and can only be carried out by experts. One way to accelerate and broaden the use of these key descriptions is to automatically extract them directly from images. In this paper, we focus mainly on one of the most commonly used key leaf descriptions which is the foliage arrangement (simple, compound, pinnate, palmate). To do so, we analyse the spatial distribution of the leaf contour points and mainly its maxima (concave and convex) and inflexion points. For each category, we define a particular geometric feature that describes its point distribution. We test the proposed method on real world leaf images (Pl@ntLeaves scans). The experiments have demonstrated the robustness of our algorithm for a high number of leaf species (70 species) and even in the presence of some distortions (such as rotation, some leaf damage, partial leaf overlapping). In addition to its accuracy, the proposed approach satisfies real-time requirements with a low computational cost."
3D Scene Grammar for Parsing RGB-D Pointclouds,"We pose 3D scene-understanding as a problem of parsing in a grammar.  A grammar helps us capture the compositional structure of real-word objects, e.g., a chair is composed of a seat, a back-rest and some legs. Having multiple rules for an object helps us capture structural variations in objects, e.g., a  chair can optionally also have arm-rests. Finally, having rules to capture composition at different levels helps us formulate the  entire scene-processing pipeline as a single problem of finding most likely parse-tree---small segments combine to form parts of objects, parts to objects and objects to a scene. We attach a generative probability model to our grammar by having a feature-dependent probability function for every rule. Our model can be trained very efficiently (within seconds), and it scales only linearly in with the number of rules in the grammar.   We show that we obtain good parse trees on 84 real point-clouds obtained from RGB-D cameras."
On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks,"In this paper, we argue for representing networks as a bag of {\it triangular motifs},particularly for important network problems that current model-based approaches handle poorlydue to computational bottlenecks incurred by using edge representations.Such approaches require both 1-edges and 0-edges(missing edges) to be provided as input, and as a consequence, approximate inference algorithms for thesemodels usually require $\Omega(N^2)$ time per iteration, precluding their application to larger real-world networks.In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality.A triangular motif is a vertex triple containing 2 or 3 edges, andthe number of such motifs is $\Theta(\sum_{i}D_{i}^{2})$ (where $D_i$ is the degree of vertex $i$),which is much smaller than $N^2$ for low-maximum-degree networks. Using this representation, we develop a novelmixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networkswith high maximum degree, the triangular motifs can be naturally subsampled in a {\it node-centric}fashion, allowing for much faster inference at a small cost in accuracy.Empirically, we demonstrate that our approach, when compared to that of an edge-based model,has faster runtime and improved accuracy for mixed-membership community detection.We conclude with a large-scale demonstration on an $N\approx 280,000$-node network, which isinfeasible for network models with $\Omega(N^2)$ inference cost."
"Feature Selection using L_{p,\infty } norm","In this paper, we present a feature selection method using  L_{p,\infty} norm as a regularization term. Compared with standard  L_{p,\infty} feature selection,  L_{p,\infty } feature selection gives more flexible to approach the number of non-zero features/variables, which is the desired goal in feature/variable selection tasks.   We use proximal gradient method to solve  L_{p,\infty } norm problem. An efficient algorithm is proposed to solve the associated proximal operator with rigorous analysis.    Extensive experiments on both multi-class and multi-label feature selection tasks demonstrate the effectiveness of our methods. The experiment results also suggest smaller $p$ values(say p=0.25) give relatively better results. "
Classi?er Calibration: A Bayesian Non-Parametric Approach,"A set of probabilistic predictions is well calibrated if the events that are predicted to occur with probability p do in fact occur about p fraction of the time. Well calibrated predictions are particularly important when machine learning models are used in decision analysis. This paper presents two new non-parametric methods for calibrating outputs of binary classi?cation models: a method based on the Bayes optimal selection and a method based on the Bayesian model averaging. The advantage of these methods is that they are independent of the algorithm used to learn a predictive model, and they can be applied in a post-processing step, after the model is learned. This makes them applicableto a wide variety of machine learning models and methods. These calibration methods, as well as other methods, are tested on a variety of datasets in terms of both discrimination and calibration performance. The results show the methods either outperform or are comparable in performance to the state-of-the-art calibration methods."
Local Features for Robust Manifold Modeling,"Despite the promise of low-dimensional manifold models for various vision and machine learning tasks, their utility has been hamstrung in practice by two fundamental challenges: practical image manifolds are plagued by a large number of nuisance variables and are non-isometric to the parameter space. In this paper, we develop a new manifold modeling, learning, and processing framework that directly addresses these challenges. At the heart of the framework are two key ideas: the use of the Earth Mover's Distance (EMD) to enable isometry of image manifolds to the underlying parameter space, and the use of local image features (such as SIFT features) to enable robust learning even to nuisance articulations.We analytically describe the performance of our approach and propose a fast kernel-based method for approximate EMD calculation to ensure computational efficiency. A powerful application of our approach is the automatic organization of large, unstructured collections of photographs gathered from the internet. "
A non-parametric Bayesian prior for causal inference of auditory streaming,"Perceptual grouping of sequential auditory cues has traditionally been modeled using a mechanistic approach. The problem however is essentially one of source inference ? a problem that has recently been tackled using statistical Bayesian models in visual and auditory-visual modalities. Usually the models are restricted to performing inference over just one or two possible sources, but human perceptual systems have to deal with much more complex scenarios. We have developed a Bayesian inference model that allows an unlimited number of signal sources to be considered: it is general enough to allow any discrete sequential cues, from any modality. The model uses a non-parametric prior, so increased complexity of the signal does not necessitate more parameters. The model not only determines the most likely number of sources, but also specifies the source that each signal is associated with. The model gives an excellent fit to data from an auditory stream segregation experiment in which the pitch and presentation rate of pure tones determined the perceived number of sources."
Low Rank Data Recovery Using Schatten p-Norm,"In this paper, we present Schatten $p$-Norm model for low-rank data recovery. Besides playing the role of data recovery, Schatten p-Norm model is more attractive due to its suppression on the shrinkage of singular values at smaller p.The proposed Schatten p-Norm model can be transformed into solving the proximal operator, and an efficient algorithm based on ALM method is proposed.  Another iterative algorithm is also presented to solve this problem with rigorous convergence analysis.Extensive experiment results on 6 occluded datasets indicate the relatively better data recovery results at smaller p values."
Privacy preserving data publishing for Bayesian network structure learning,"In order to prevent the leak of personal electronic health record, the federal Health Insurance Portability and Accountability Act (HIPAA) has set a national standard to protect privacy of this kind of information. In this paper, we propose a privacy preserving data publishing method for Bayesian network structure learning. We come up with a new method that using simulated annealing to find an optimal noise adding to the original dataset so that the resulting dataset will be different from the original one. By publishing this noised dataset, other research institutes are able to use their own Bayesian network structure learning algorithms to build a network whose accuracy is comparable to the accuracy of network built with the original dataset."
Parametric Learning of Generalized Decision Trees,"Decision trees are efficient models for representing piecewise-defined functions. We present gradient based learning algorithms to estimate the parameters for two general classes of decision trees. Decision trees can be distinguished based on their type of split functions. The first class divides the input space into disjunct regions by making hard splits. The second class, which we call generalized decision trees, relaxes the constraints of the split functions to enable a soft partitioning. This richer class is well suited to represent smooth functions, but has an increased computational complexity. The first contribution is a gradient based learning algo- rithm for generalized decision trees, that can learn in on-line settings with limited resources. The second contribution is a learning algorithm for decision trees with hard splits. We derive this learning scheme by combining the learning capabilities of the first algorithm with a continuation method. This is especially useful for post optimizing trees that are constructed with heuristic methods."
Complex Activity Recognition using Granger Constrained DBN (GCDBN) in Sports and Surveillance Video,"Common approaches for modeling interactions of multiple interacting and co-occurring objects in complex activities can theoretically model any number of co-occurring agents or events. However, these can be intractable for complex representations, require manual structure definitions, and/or generatively learn their structures. Our approach involves automatically constraining the nodes and links of a Dynamic Bayesian Network (DBN) in an informative and discriminative manner, resulting in sparse models that are both tractable and improve classification. This is accomplished by explicitly constraining the temporal links based on a direct measure of temporal dependence using Granger Cause statistics, resulting in the Granger Constrained DBN (GCDBN). The experiments show how the GCDBN outperforms other state-of-the-art models in complex activity classification on both handball and surveillance video data."
Robust exponential binary pattern storage in Little-Hopfield networks,"The Little-Hopfield network is an auto-associative computational model of neural memory storage and retrieval.  This model is known to robustly store collections of randomly generated binary patterns as stable-points of the network dynamics.  However, the number of binary memories so storable scales linearly in the number of neurons, and it has been a long-standing open problem whether robust exponential storage of binary patterns was possible in such a network memory model.  In this note, we design elementary families of Little-Hopfield networks that solve this problem affirmitavely."
Shadow Densities for Speeding Up Kernel Methods ,"This paper presents an approach to improve the training and evaluation of kernel manifold learning algorithms relying on spectral decomposition. The approach, called the shadow method, exploits research regarding the spectral decomposi-tion of kernel operators applied to probability distributions. It is used to define the shadow of a kernel density estimate. The shadow density estimate (ShDE)in turn defines shadow KPCA (ShKPCA). For large, redundant datasets ShKPCA improves training and evaluation time of KPCA by an order of magnitude or more, each. A single parameter $\ell$ controls the computational gains. The shadow method is justified through bounds on the density estimate error, on the spectral decomposition error, and on the spectral operator error, all in terms of $\ell$. For low $\ell$ there are large improvements but the method is lossy. Increasing approaches baseline performance and leads to lower speed improvements. Experimentally $\ell$ =  4 works well across a broad spectrum of datasets. Modifications to  improve $\ell$ low performance are given, but with reduced computational gains during training."
Linear Time Solver for Primal SVM,"Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, and designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with proved linear convergence for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easy parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state of the art solvers such as SVMperf, Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms."
Parallel Execution of Self-Organizing Maps,"Self-organizing maps have been noted as useful tools for augmenting scientificdata visualizations, particularly in the case where visualization of multidimensionaldata is required. However, a chief disadvantage associated with the selforganizingmap in this capacity is its large runtime complexity, which may resultin impractical execution times in real-world use cases. Because of the propertiesassociated with neural networks, we tested the feasibility of applying parallel executionto the self-organizing map in an attempt to reduce execution time. Thoughwe had predicted that the parallelization of self-organizing maps would result innear-linear speedup, we have discovered evidence against this hypothesis."
Gaussian Process Model Predictive Control: A Comparison with Conventional Techniques,"Gaussian processes are gaining increased popularity in the area of system identification for control. In this paper a new Model Predictive Control (MPC) algorithm based on a Gaussian process internal model is defined. The framework can be used for modelling and control of arbitrary nonlinear state-space systems, an extension of previous work which only considered modelling of ARMAX models. The performance of the algorithm is then compared with standard MPC and an adaptive Linear Quadratic Regulator (LQR), which use linearised models of the real system, on a benchmark industrial process control problem."
Generic Active Appearance Models Revisited: The Active Orientation Models Paradigm ,"The proposed Active Orientation Models (AOMs) are generative models of facial shape and appearance. Their main differences with the well-known paradigm of Active Appearance Models (AAMs) are (i) they use a different statistical model of appearance (ii) they are accompanied by a robust algorithm for model fitting and parameter estimation and (iii) and, most importantly, they generalize well to unseen faces and variations. Their main similarity is computational complexity. The project-out version of AOMs is as computationally efficient as the standard project-out inverse compositional algorithm which is admittedly the fastest algorithm for fitting AAMs. We show that not only does the AOM generalize well to unseen identities, but also it outperforms state-of-the-art algorithms for the same task by a large margin. Finally, we prove our claims by providing Matlab code for reproducing our experiments."
Compressible Motion Fields,"Traditional video compression methods obtain a compactrepresentation for image frames by computing coarse motion fieldsdefined on patches of pixels called blocks. This piecewiseconstant flow approximation reduces the size of the motion fieldbut introduces block artifacts in the decoded (warped) imageframe. In this paper, we address the problem of estimating densemotion fields that, while accurately predicting one frame from agiven reference frame by warping it with the field, are also\emph{compressible}. We introduce a representation for motionfields based on wavelet bases, and approximate thecompressibility of their coefficients with a piecewise smoothsurrogate function that is relatively easy to optimize. We thenshow how to quantize and encode such coefficients with adaptiveprecision. We demonstrate the effectiveness of our approach bycomparing its performance with a state-of-the-art videocompression algorithm. Experimental results reveal that ourmethod significantly outperforms classical block-based motioncompensation adopted in most modern video compression algorithms."
Efficient and optimal Little-Hopfield auto-associative memory storage using minimum probability flow,"We present an algorithm to store binary memories in a Hopfield neural network using minimum probability flow, a recent technique to fit parameters in energy-based probabilistic models.  In the case of memories without noise, our algorithm provably achieves optimal pattern storage (which we show is at least one pattern per neuron) and outperforms classical methods both in speed and memory recovery.  Moreover, when trained on noisy or corrupted versions of a fixed set of binary patterns, our algorithm finds networks which correctly store the originals.  We also demonstrate this finding visually with the unsupervised storage and clean-up of large binary fingerprint images from significantly corrupted samples."
Smooth Convolutional Stacked Autoencoder for Feature Learning in ECoG Based Brain Computer Interface,"Recent years have seen great interest in ECoG based Brain Computer Interface (BCI) systems. The performance of the BCI systems largely depends on the low-level features used in the decoding algorithm. The currently widely used features are based on frequency decompositions and designed manually, which are only justified empirically. In this work, we describe an automatic feature learning framework for ECoG based BCI decoding algorithms, using a multi-layer deep learning structure we termed as {\em smooth convolutional stacked auto-encoders (SCSA)}. One advantage of SCSA is that it is an open architecture that facilities incorporating various domain-specific constraints, e.g., smoothness of the extracted features over time.  Based on SCSA and data sets of ECoG signals, we demonstrate significant improvement in performance over the current state of the art methods based on manual features. Furthermore, the automatically extracted features from SCSA also shed light on the optimality of those features obtained empirically."
Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL,"Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.  We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging."
Active Learning on Low-Rank Matrices,"Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. With various criteria, we can actively choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many large points as possible. We evaluate our methods on simulated data and show their applicability on movie ratings prediction as well as discovering drug-target interactions."
How safe is your room? 3D scene parsing using intuitive mechanics,"This paper proposes a new perspective for 3D scene parsing through reasoningabout object stability and safety using intuitive mechanics. Our approach utilizesa simple observation that, by human design, objects in static scenes should bestable and safe with respect to various disturbances, such as gravity, earthquakeand daily human activities. Given a 2.5D depth map captured by Kinect cameraor a 3D point cloud for the full scene by SLAM, firstly, our method performsvolumetric reasoning to recover the solid 3D volumes from defective point cloud.Secondly we introduce two new representations for intuitive mechanical reason-ing: i) A disconnectivity graph (DG) to represent the potential energy landscapeswith local minima (stable points), local maxima (unstable equilibrium) and energybarriers between them; and ii) A disturbance field (DF) to represent the possiblephysical work applied to each position in the scene. These allow us to evaluate theunsafeness as the expected risk, such as falling by gravity, knocked off by passing-by human etc. If a 3D entity (voxel in point cloud, shape primitive, or object) is atrisk, our algorithm applies two possible operators: a) attaching or connecting it toother objects to form a larger stable object; b) supporting it by hidden parts in theinvisible volume. By minimizing an energy function defined on both unsafenessand scene prior, our algorithm achieves the following objectives. i) Grouping theunstable primitives hierarchically to form stable objects in a parse graph; ii) Infer-ring hidden parts in the occluded areas; and iii) computing some cognitive maps,such as what area can a cup move freely on the scene, what objects are risky inthe room."
"Texture, Structure and Visual Matching","We propose a formal definition of ``visual texture'' and characterize it by the approximate sufficient statistics of the underlying process. These are inferred from data and used for compression, extrapolation, inpainting and segmentation. The formalization highlights relations between textures and other early vision operations, such as co-variant feature selection and correspondence. We show that co-variant frames (``structures'') are the complement of textures in an image. Unlike prior work on texture/structure partitioning, however, we show that such a decomposition requires multiple images: to attribute structures in the image to properties of the scene, a proper sampling condition has to be satisfied, which requires multiple realizations."
Spectral Learning of General Weighted Automata via Constrained Matrix Completion,"Many tasks in text and speech processing and computational biology involve functions from variable-length strings to real numbers. A wide class of such functions can be computed by weighted automata. Spectral methods based on singular value decompositions of Hankel matrices have been recently proposed for learning probability distributions over strings that can be computed by weighted automata. In this paper we show how this method can be applied to the problem of learning a general weighted automata from a sample of string-label pairs generated by an arbitrary distribution. The main obstruction to this approach is that in general some entries of the Hankel matrix that needs to be decomposed may be missing. We propose a solution based on solving a constrained matrix completion problem. Combining these two ingredients, a whole new family of algorithms for learning general weighted automata is obtained. Generalization bounds for a particular algorithm in this class are given. The proofs rely on a stability analysis of matrix completion and spectrallearning."
HGLMMF: Generalizing Matrix Factorization with Hierarchical Generalized Linear Model,"Matrix factorization (MF) has become the dominant method of collaborative filtering. Recently, various MF methods have been proposed and tried to jointly model multiple relations. However, such methods are vulnerable to the changes of data or sub-models. Moreover, data often follows the Pareto rule, which may lead to a poor result due to the global bias caused by such imbalanced data. To overcome these defects, we designed a generalized MF method based on hierarchical generalized linear models (HGLMMF) that augments knowledge with learned extra information from other related models. More specifically, HGLMMF uses one portion of the augmented knowledge to construct augmented covariates to better capture fixed effects while the other portion is used to model the cluster-specific random effects to adjust the global bias problem. We also demonstrate that a number of state-of-the-art MF models can be viewed as special cases of HGLMMF."
"Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L_p$ Loss","In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general $L_p$ norm.  We generalize the Cramer-Rao lower bound and show how the $L_p$ loss can be written as a functional of the Fisher Information in the asymptotic limit, by proving the moment convergence of certain functions of Poisson random variables.  In this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing $L_p$ loss in the limit as $p$ goes to zero.  "
Which classifiers are worth learning?,"Despite advances in model selection techniques,  choosing among different statistical models to select the ``correct'' one for the problem at hand remains a fairly subjective and even personal decision.  This short note suggests that so long as one is only interested inpolynomial-time algorithms then fairly mild assumptions about the learning problem ---namely, that the inputs are noisy---can lead to a simple understanding of what concepts are learnable even in principle.  In the case of uniformly distributed inputs and iid noise, there is a simple characterization of learnable concepts as well as a  simple universal learning algorithm that runs in polynomial time. The main technical observation involves the theory of {\em noise stable} functions (considered earlier in context of Fourier learning)."
Monitoring Cardiac Stress Using Acoustic Heart Signals,"Cardiovascular complications arising from non-cardiac surgery exceed 1 million patients worldwide each year. Due to the increasing size of the elderly population it is predicted that during the coming decades, perioperative complications will increase by 100%. Current non-invasive cardiac monitoring techniques are based mostly on ECG signals. This kind of monitoring reflects cardiac electrical activity, but not its mechanical activity.It is known that acoustic heart sounds carry significant information about the cardiac state. In this work we present a novel monitoring that is based on the mechanical activity of the heart and is manifested by the sounds emitted from the heart. Two physiological features are extracted, which reflect cardiac morphology change from its baseline behavior. We use laparoscopic surgery as a model for a procedure which involves externally induced cardiac stress. The framework is applied to heart sounds recorded during laparoscopic surgeries of 25 patients. We demonstrate that suggested features change during cardiac stress, and are more significant for patients with cardiac problems. Furthermore, we show that other ECG morphology features are less sensitive during cardiac stress."
Active Learning for Multiclass Cost-sensitive Classification Using Probabilistic Models,"Multiclass cost-sensitive active learning is a relatively new problem. In this paper, we derive the maximum expected cost and cost-weighted minimum margin strategy for multiclass cost-sensitive active learning. These two strategies can be seem as the extended version of classical cost-insensitive active learning strategies. The experimental results demonstrate that the derived strategies are promising for costsensitive active learning. In particular, the cost-sensitive strategies outperform cost-insensitive ones. The experimental results reveal how the hardness of data affects the performance of active learning strategies. Thus, in practical active learning applications, data analysis before strategy selection can be important."
Algorithms for Learning Markov Field Policies,"We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications.The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects."
Schizophrenia Detection and Classification by Advanced Analysis of EEG Recordings using a Single Electrode Approach ,"Electroencephalographic (EEG) analysis has emerged as a powerful tool for brain state interpretation and diagnosis. However, it has not emerged as a powerful tool in diagnosis of mental disorders. This may be explained by the low spatial resolution or depth sensitivity of EEG.  This paper concerns diagnosis of Schizophrenia using EEG, which currently suffers from few cardinal problems: it heavily depends on assumptions, conditions and prior knowledge of the patient. Additionally, the diagnostic experiments take hours, and accuracy of the analysis is low or unreliable.This article presents a novel approach for Schizophrenia detection showing great success in classification accuracy. The methodology is built for a single electrode recording attempting to make the data acquisition process feasible and quick for most patients."
Affine Independent Variational Inference,"We present a method for approximate inference for a broad class of non-conjugate probabilistic models. In particular, for the family of generalized linear model target densities we describe a rich class of variational approximating densities which can be best fit to the target by minimizing the Kullback-Leibler divergence.  Our approach is based on using the Fourier representation which we show results in efficient and scalable inference."
Simulation of Database-Valued Markov Chains Using SimSQL,"This paper describes the SimSQL system, which allows for SQL-based specification,simulation, and querying of database-valued Markov chains, i.e., chainswhose value at any time step comprises the contents of an entire database. This isof particular interest in statistical machine learning, because SimSQL can easilybe used to declaratively specify Markov Chain Monte Carlo simulations that areautomatically parallelized to run on a large compute cluster."
Multi-Task Averaging,"We present a multi-task learning approach to jointly estimate the means of multipleindependent data sets. We prove that the proposed multi-task averaging (MTA) algorithmresults in a convex combination of the single-task maximum likelihood estimates.We derive the optimal amount of regularization, and show that it can be effectivelyestimated. Simulations and real data experiments demonstrate that MTAoutperforms both maximum likelihood and James-Stein estimators, and that ourapproach to estimating the amount of regularization rivals cross-validation in performancebut is more computationally efficient."
Functional Mesh Learning for Pattern Analysis of Cognitive Processes,"Here we propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain, acquired via functional magnetic resonance imaging (fMRI). In the proposed learning machine, local meshes are formed around each voxel. The distance between voxels in the mesh is determined by using functional neighborhood concept. In order to define functional neighborhood, the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed. Then, the local mesh for each voxel is formed by including the functionally closest neighboring voxels in the mesh. The relationship between the voxels within a mesh is estimated by using a linear regression model. These relationship vectors, called Local Relational Features (LRF) are then used to train a statistical learning machine. The proposed method was tested on a recognition memory experiment, including data pertaining to encoding and retrieval of words belonging to ten different semantic categories. Two popular classifiers, namely K-Nearest Neighbor and Support Vector Machine, are trained in order to predict the semantic category of the item being retrieved, based on activation patterns during encoding. The proposed model exhibits classification performance superior to the state of the art methods available in the literature."
Tracking 3-D Rotations with the Quaternion Bingham Filter,"A deterministic method for sequential estimation of 3-D rotationsis presented.  The Bingham distribution is used to representuncertainty directly on the unit quaternion hypersphere.  Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions.  Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic.  We present two versions of the  QBF--suitable for tracking the state of first- and second-order rotating dynamical systems."
Robust Dictionary Learning by Source Decomposition,"It is now well established that sparse coding is well suited to many applications such as image restoration, denoising and classification. Especially, adaptive sparsemodels learned from data, or ?dictionary?, outperformfixed basis such as Discrete Cosine Basis and Fourier Basis. This paper extends this line of research to consider outlier elimination, proposing two methods to decompose the reconstructive residual into two components: a non-sparse component for small universal noises as well as a sparse one for the outliers respectively. In addition, further analysis reveals the connection between our model and the ?partial? dictionary learning approach, updating only part of the codewords (informative codewords DInfo)with remaining (noisy one DNoisy) fixed. We validate and evaluate our new approach on synthetic data as well as real applications and achieved satisfactory performance."
Matrix Completion with Ordering Relation Constraints,"We relax the equality constraints in the very general and well-known affine Schatten p-norm minimization problem into one-side inequality constraints. Owing to the imposed equality con-straints, existing methods can only achieve some degree of denoising, via the optimization of the objective energy function. By our proposed re-laxation, the decision variables in the objective function possess flexible nonlinearity while maintaining their ordering relation constraints. We show that, our new objective function is convex, and its global minimum can be obtained by a more general form of the Fixed-Point Con-tinuation framework with almost the same com-putational cost. Experiments show that, our algo-rithm has good performance over various widely used datasets."
On the Equivalence of the Lasso and the SVM,"We investigate the relation of two fundamental tools in machine learning, that is the support vector machine (SVM) for classification, and the Lasso technique used in regression. We show that the resulting optimization problems are equivalent, in the following sense: Given any instance of one of the two problems, we construct an instance of the other, having the same optimal solution. In consequence, the two large classes of existing optimization algorithms for both SVMs and Lasso can also be applied to the respective other problem instances.Also, the equivalence allows for many known theoretical insights for SVM and Lasso to be translated between the two settings. One such implication gives a simple kernelized version of the Lasso, analogous to the kernels used in the SVM setting. Another consequence is that the sparsity of a Lasso solution is equal to the number of support vectors for the corresponding SVM instance.Furthermore, we can directly relate sublinear time algorithms for the two problems, and give a new such algorithm variant for the Lasso."
Fused Multiple Graphical Lasso,"In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which  encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a blockwise coordinate descent method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach."
A polygon-based interpolation operator for super-resolution imaging ,"We outline the super-resolution reconstruction problem posed as a maximization of probability. We then introduce an interpolation method based on polygonal pixel overlap, express it as a linear operator, and use it to improve reconstruction.Polygon interpolation outperforms the simpler bilinear interpolation operator and, unlike Gaussian modeling of pixels, requires no parameter estimation. A free software implementation that reproduces the results shown is provided."
Discovering Common Functional Connectomics Signatures,"Based on the structural connectomes constructed from diffusion tensor imaging (DTI) data, we present a novel framework to discover functional connectomics signatures from resting-state fMRI (R-fMRI) data for the characterization of brain conditions. First, by applying a sliding time window approach, the brain states represented by functional connectomes were automatically divided into temporal quasi-stable segments. These quasi-stable functional connectome segments were then integrated and pooled from populations as input to an effective dictionary learning and sparse coding algorithm, in order to identify common functional connectomes (CFC) and signature patterns, as well as their dynamic transition patterns. The computational framework was validated by benchmark stimulation data, and highly accurate results were obtained. By applying the framework on the datasets of 44 post-traumatic stress disorder (PTSD) patients and 51 healthy controls, it was found that there are 16 CFC patterns reproducible across healthy controls/PTSD patients, and two additional CFCs with altered connectivity patterns exist solely in PTSD subjects. These two signature CFCs can successfully differentiate 85% of PTSD patients, suggesting their potential use as biomarkers."
Blind Image Deblurring by Spectral Properties of Convolution Operators,"In this paper, we study the problem of recovering a sharp version of a given blurry image when the blur kernel is unknown. Previous methods often introduce an image-independent regularizer (such as Gaussian or sparse priors) on the desired blur kernel. For the first time, this paper shows that the blurry image itself encodes rich information about the blur kernel. Such information can be found through analyzing and comparing how the spectrum of an image as a convolution operator changes before and after blurring. Our analysis leads to an effective convex regularizer on the blur kernel which depends only on the given blurry image. We show that the minimizer of this regularizer guarantees to give good approximation to the blur kernel if the original image is sharp enough. By combining this powerful regularizer with conventional image deblurring techniques, we show how we could significantly improve the deblurring results through simulations and experiments on real images, especially when the blur is large. In addition, our analysis and experiments help explaining why edges are good features for image deblurring."
Approximate l-fold cross-validation with Least Sqaures SVM and Kernel Ridge Regression,"Kernel methods have difficulties scaling to large modern data sets. The scalability issues are based on computational and memory requirements for working with a large matrix. These requirements have been addressed over the years by using low-rank kernel approximations or by improving the solvers' scalability. However, Least Squares Support Vector Machines (LS-SVM), a popular SVM variant, and Kernel Ridge Regression still have several scalability issues. In particular, the  $O(n^3)$ computational complexity for solving a single model, and the overall computational complexity associated with tuning hyperparameters are still major problems. We address these problems by introducing an $O(n\log n)$ approximate $l$-fold cross-validation method that uses a multi-level circulant matrix to approximate the kernel. In addition, we prove our algorithm's computational complexity and present empirical runtimes on data sets with approximately 1 million data points. We also validate our approximate method's effectiveness at selecting hyperparameters on real world and standard benchmark data sets. Lastly, we provide experimental results on using a multi-level circulant kernel approximation to solve LS-SVM problems with hyperparameters selected using our method."
Unsupervised Structure Discovery for Semantic Analysis of Audio,"Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines."
Identification of Spike-Processing Neural Circuits,"Reverse engineering of neural circuits requires the development of sound experimental and theoretical methods for determining the circuit connectivity and for estimating the processing of both spiking and continuous sensory signals. Here we present a new approach for identification of receptive fields in spiking neuron models that admit both continuous  signals and multidimensional spike trains as input stimuli. We consider circuit models of the sensory periphery in olfaction, audition and  vision as well as models of spike processing in higher brain centers, including models with lateral connectivity and feedback. We present algorithms for identifying temporal, spectrotemporal and spatiotemporal receptive fields directly from spike times produced by a neuron. The algorithms obviate the need to repeat experiments in order to compute the neuron's rate of response, rendering our methodology of interest to both experimental and theoretical neuroscientists."
Spherical Quantization based Binary Embedding,"This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods."
Sparse Correlation Estimation for Elliptical distributions,"We propose a semiparametric procedure---named RTK (Rank-based Thresholding via Kendall's tau)---to estimate the correlation matrix of high dimensional elliptical distributions. Unlike most existing methods that are based on Pearson's correlation, our approach exploits the nonparametric rank-based correlation estimator. Theoretically, our procedure achieves the optimal parametric rates of convergence for both parameter estimation and sparsity recovery in high dimensional settings; Empirically, our method always deliver a positive definite solution. Numerical results on both simulated and real datasets are also provided to support our theory."
Max-Margin Min-Entropy Hidden Conditional Random Fields,"We introduce the novel max-margin min-entropy hidden conditional random field (M$^3$E-HCRF), which encodes the conditional distribution over the latent variables and the single output variable given the input variables. The proposed M$^3$E-HCRF model provides a sparse and factorized representation of the conditional distribution. Given an observation, the M$^3$E-HCRF model infers the output by selecting the class label that minimizes the Renyi entropy of the unnormalized measure of the conditional distribution, which is equivalent to simultaneously (1) maximizing the conditional log-likelihood of the output given the inputs, and (2) minimizing the entropy of the conditional distribution of the hidden variables given the inputs and the output. The parameters of the proposed M$^3$E-HCRF model are learned by minimizing an $l_2$-regularized loss function, resulting in a non-convex optimization problem that can be solved by the non-convex bundle cutting plane algorithm. We evaluate our model's effectiveness on sequence labeling and structured learning using two public datasets, and demonstrate that our model achieves results comparable to the state of the art."
Laplacian Consistency,"Computing a faithful similarity/affinity metric is essential to many graph-based learning algorithms.In this paper, we propose a graph-based affinity learning method in an unsupervised scenario and show its application to shape retrieval, face clustering and web categorization.Our method, Laplacian Consistency  (LC), performs a dynamic diffusion process by propagating the similarity mass along the intrinsic manifold of data points.Convergence analysis is given and a closed-form solution is provided, making the LC process fast to calculate and easy to understand. Theoretical analysis shows our LC process only changes the eigenvalues gradually while keeping the eigenvector in the Laplacian spectral space. We also prove the superiority of our method from different points of views. Our method has nearly no parameter tuning and leads to significantly improved affinity maps, which help to greatly enhance the quality of various graph-based learning algorithms."
Learning to Classify From Multiple Experts,"Label is a critical component of the classification learning framework. However in many practical applications when labels are based on human  assessments, it is infeasible to assume one can obtain a perfect set of labels everybody agrees on. A solution that has been recently explored by the machine learning community is learning from multiple annotators: instead of collecting labels from a single expert/annotator, we collect labels from a number of annotators/experts. Since there may be substantial disagreements among labels of multiple annotators, some kind of a consensus model, that incorporates the characteristics (e.g. reliability) of each annotator, is sought.In this work, we study and develop a new approach for learning classification models from labels provided by multiple annotators. Our approach explicitly models and learns annotator-specific  model, reliability, and bias and incorporates them into the learning of the consensus model. Experimental results show that our approach outperforms commonly used multiple annotators baselines on multiple UCI-based datasets and a real-world medical data set."
Factoring nonnegative matrices with linear programs,"This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes."
Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.  In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores.  We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models.  Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports for a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training.  We have successfully used our system to train a deep network 100x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories.  We show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. "
A Generalized Kernel Approach to Structured Output Learning,"We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. We show that the existing KDE formulations are special cases of our framework. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on two structured output problems, and compare it to the state-of-the-art kernel-based structured output regression methods."
Optical FLow Estimation by Adaptive Data Fusion,"Many state-of-the-art optical flow estimation algorithms are based on the variational method, which optimizes regularity and data terms simultaneously. This study presents a novel approach that provides weights to various data terms adaptively against a single data term in the conventional variational framework. In this study, a new optical flow estimation model with weighted sum of multiple data terms is introduced, and its optimization procedure is proposed. Competitive experimental results on the Middlebury optical flow benchmark show that the proposed method outperforms conventional methods with the aid of complementary data terms. In particular, this study is of importance for cases that incorporate various data terms into a unified variational framework."
Learning from many experts: sparsity and model selection,"Experts classifying data are often imprecise. Recently, several models have been proposed to train classifiers using the noisy labels generated by these experts. Such models often have a large number of parameters, which can lead to overfitting. In order to avoid this and find better classifiers, we propose a new model which searches for sparse classifiers. We also develop a general method for model selection and apply it to optimize the tuning parameter in our model."
Posterior contraction of the population polytope in finite admixture models,"We study the posterior contraction behavior of the latent population structure that arises in admixture models as the amount of data increases. An admixture model  --- alternatively known as a topic model --- specifies $k$ populations (or topics), each of which is characterized by vector of frequencies for generating a set of discrete values of observations. The population polytope is defined as the convex hull of the $k$ frequency vectors. Given a prior distribution over the space of population polytopes, we establish rates at which the posterior distribution contracts to $G_0$, under the Hausdorff metric and a minimum matching Euclidean metric, as the amount of data tends to infinity. Rates are obtained for the overfitted setting, i.e., when the number of extreme points of $G_0$ is bounded above by $k$, and for the setting in which the number of extreme points of $G_0$ is known. Minimax lower bounds are also established. Our analysis combines posterior asymptotics techniques for the estimation of mixing measures in hierarchical models with arguments in convex geometry."
Teaching Classification Tasks to Humans,"Given a classification task, what is the best way to teach the resulting boundary to a human? While machine learning techniques can provide excellent techniques for finding the boundary, they tell us little about how we would teach a human the same task. We propose to investigate the problem of example selection and presentation in the context of teaching humans, and explore a variety of mechanisms in the interests of finding what may work best. In particular, we begin with the baseline of random presentation and then examine combinations of several mechanisms: the indication of an example?s relative difficulty, the use of the shaping heuristic from the psychology literature (moving from easier examples to harder ones), and a novel kernel-based ?coverage model? of the subject?s mastery of the task. From our experiments on 53 human subjects learning classification tasks via our teaching system, we found that we can achieve the greatest gains with a combination of shaping and the coverage model."
Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination ,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) isa fundamental component of a variety of studies in Neuroscience. The goal hereis to identify regions that exhibit changes as a function of the clinical conditionunder study. As the clinical questions of interest move towards identifying veryearly signs of diseases, the corresponding statistical differences at the group levelinvariably become weaker and increasingly hard to identify. Indeed, after a mul-tiple comparisons correction is adopted (to account for correlated statistical testsover all surface points), very few regions may survive. In contrast to hypothesistests on point-wise measurements, in this paper, we make the case for perform-ing statistical analysis on multi-scale shape descriptors that characterize the localtopological context of the signal around each surface vertex. Our descriptors arebased on recent results from harmonic analysis, that show how wavelet theoryextends to non-Euclidean settings (i.e., irregular weighted graphs). We providestrong evidence that these descriptors successfully pick up group-wise differences,where traditional methods either fail or yield unsatisfactory results. Other than thisprimary application, we show how the framework (i) allows performing corticalsurface smoothing in the native space (without a unit sphere mapping) and (ii)yields an efficient algorithm for perceptual shape segmentation of 3-D model thatcompares very favorably with the state of the art."
Generalized Ambiguity Decomposition for Convex Ensembles of Experts and Arbitrary Differentiable Loss Functions,"The squared error of a convex ensemble of regressors is related to the squared error of the individual regressors and the diversity of the ensemble as measured by the weighted sum of squared errors of each regressor's prediction from the ensemble's prediction. This relationship, also known as ambiguity decomposition, highlights the impact of diversity on ensemble's performance for least squares regression. In this paper, we present a generalization of ambiguity decomposition that can be applied to any convex ensemble of experts under a differentiable loss function. The proposed decomposition is applicable to both classification and regression, and provides a task-driven notion of diversity. It is shown that the diversity term in this decomposition is scaled by a factor dependent on the instance and the loss function in a classical supervised learning setting. This lends support to the intuition that not all instances are equally important from a diversity perspective. We then derive the decomposition for some common regression and classification loss functions, and demonstrate its accuracy on different UCI datasets."
Better Mixing via Deep Representations,"It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation.  To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples."
Pareto-Path Multi-Task Multiple Kernel Learning,"Traditional Multi-Task Multiple Kernel Learning (MT-MKL) methods routinely optimize the sum (thus, the average) of objective functions to simultaneously improve performances for all tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO) problem, which considers the concurrent optimization of all task objectives involved in the Multi-Task Learning problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel Support Vector Machine (SVM) MT-MKL framework, that considers an implicitly-defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of improving performances uniformly over tasks, when compared to the traditional MTL approach."
A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation,"A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lovasz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset."
Fused sparsity and robust estimation for linear models with unknown variance,"In this paper, we develop a novel approach to the problem of learning sparserepresentations in the context of fused sparsity and unknown noise level. We proposean algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes theaforementioned learning task by means of a second-order cone program. A special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers. We establish finite sample risk bounds and carry out an experimental evaluation on both synthetic and real data."
Biased perception leads to biased action: Validating a Bayesian model of interception," We tested whether and how biases in visual perception might influence motor actions. To do so, we designed an  interception task where subjects had to indicate the time when a moving object, whose trajectory was occluded from the  subjects, would reach a target-area. Subjects made their judgements based on a brief display of the objects initial  motion at a starting point. Based on the known illusion that slow contrast stimuli appear to move slower than high  contrast ones, we predict that if perception directly influences motion actions subjects would show delayed  interception times for low contrast objects. In order to provide a more quantitative prediction, we developed a Bayesian  model for the complete sensory-motor interception task. Using fit parameters for the prior and likelihood on visual  speed from a previous study we were able to predict not only the expected interception times but also the precise  characteristics of response variability. Psychophysical experiments confirm the model's predictions. Individual  differences in subjects timing response can be accounted for by individual differences in the perceptual priors on  visual speed. Taken together, our behavioral and model results show that biases in perception percolate downstream to  bias action response in a predictable manner. Furthermore, our work emphasizes that the Bayesian model of speed  perception is generalizable to new domains."
High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer Disease Progression Prediction,"Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.  "
Discriminative Learning of Infinite Latent Variable Models,"We propose probabilistic models to infer discriminative latent variables in Hamming space from observed data. Our models allow for a simultaneous inference of the dimension of the binary latent variables, and their entries values. Further, the latent variables are discriminative in the sense that objects in the same category or semantic concept have similar latent values, and objects in different categories have dis-similar latent values. The inferred latent variables can be directly used to perform a nearest neighbour search for the purpose of classification or retrieval. We formulate this discriminative infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the proposed method is able to find semantically similar neighbours due to the discriminative nature of the latent space. The coupling structure of the inferred latent space lends itself to an application of extending hash codes in a discriminative way."
Compressed Sparse Concept Coding for Large Scale Data Representation,"Data representation is a fundamental problem in various research fields.When representing data as vectors, the feature space is usually of very high dimensionality, which makes itdifficult for applying learning algorithms for analysis.One then hope to apply matrix factorization techniques,such as Singular Vector Decomposition (SVD) to learnthe low dimensional hidden concept space. Among various techniques, sparse coding receives considerableinterests in recent years because its sparse representation leads to an elegant interpretation.However, most of the existing sparse coding algorithms are computational expensive since theycompute the basis vectors and the representations iteratively. Moreover, all the existing methodsare linear and not be able to capture the non-linear structure of the data. To tackle these issues, wepropose a novel sparse coding method, called {\em Compressed Sparse Concept Coding} (CSCC), for largescale data representation in this paper. Our method is non-linear and scales linearly with the numberof samples. Extensive experimental results on real world applicationsdemonstrate the effectiveness and efficiency of the proposed approach."
Functional Brain Interactions during Free Viewing of Video Stream,"Natural stimulus fMRI (N-fMRI) such as free viewing of video streams provides an uncontrolled environment to study the human brain's perception and cognition engaged in natural scene comprehension. Hence, it is receiving increasing interest in neuroimaging and multimedia analysis in recent years. In these fields, researchers rely on consistent and discriminative functional interactions such as functional or effective connectivity to measure the human brain's responses. However, the computational cost increases significantly in model-driven methods such as the dynamic causal modeling (DCM) when the cortical regions of interests (ROIs) are dense (e.g., 358 in this paper). In this paper, we present a data-driven computational pipeline to explore consistent and discriminative functional interactions during free viewing of video. The underlying premise is that the functional interactions, characterizing the semantic content of video samples in multiple categories and derived from N-fMRI data of multiple subjects, are simultaneously selected by multiple feature selection methods to pose both consistency and discriminativity. Then the spatial distribution of the ROIs involved in the identified interactions and the distribution of the functional sub-networks associated with the ROIs are assessed. Meanwhile, structural connectivity derived from diffusion tensor imaging (DTI) and video classification is used to evaluate the consistency and discriminativity of the identified functional interactions, respectively. Our findings provide new insights into the functional mechanism of the human brain in perception and cognition of complex natural scenes."
Constructing Deep Neural Networks via the Extended Restricted Boltzmann Machines,"We exploit an uniform training algorithm for the extended restricted Boltzmann machines (ERBM) to initialize the parameters of a deep neural network (DNN). Due to the conservative energy-based generative model and the intractable samples from the model distribution, the restricted Boltzmann machines (RBM) is extended as the following two aspects. Firstly, using a new method that introduces the free-weighting matrices, a novel energy-based generative model is proposed to establish an excellent RBM. To adapt to different high-dimensional data distribution and to speed up the Contrastive Divergence (CD), secondly we use the normal and no-sampling methods instead of the uniform-sampling method in CD. Our experiments finally confirm that the free-weighting matrices have important positive effects on DNN in face and MNIST datasets and DNN with no-sampling method can reconstruct better image than one with normal and uniform-sampling methods in face datasets."
Dimension Independent Similarity Computation,"We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO)to compute all pairwise similarities between very high dimensional sparse vectors.All of our results are provably independent of dimension, meaningapart from the initial cost of trivially reading in the data, all subsequentoperations are independent of the dimension, thus the dimension can be very large.We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash.Our results are geared toward the MapReduce framework. We empirically validate ourtheorems at large scale using data from the social networking site Twitter."
Hippocampal CA3 Cells As Hidden Units of A Recurrent Neural Network,"Abstract Hippocampal cells are known for their spatial and temporal selectivity. However, it is unclear how such selectivity arises in different regions of the hippocampus and how it contributes to episodic memory. We simulate learning in a recurrent neural network (RNN) structurally similar to the neural circuit in area CA3. Our methods based on general temporal sequences can be extended to more specific inputs such as spatial and temporal correlated signals. Our simulation results provide a novel explanation of how multi-modal episodic memory is learned and suggest that the experimentally observed, sparse and selective tuning of CA3 cells facilitates the learning of temporal sequences as memory episodes."
Modeling Fashion,"We propose a method to try to model fashionable dresses in this paper. We first discover common visual patterns that appear in dress images using a human-in-the-loop, active clustering approach. A fashionable dress is expected to contain certain visual patterns which make it fashionable. An approach is proposed to jointly identify fashionable visual patterns and learn a discriminative fashion classifier. The results show that interesting fashionable patterns can be discovered on a newly collected dress dataset. Our model can also achieve high accuracy on distinguishing fashionable and unfashionable dresses. We test visual pattern centric dress retrieval, which is promising and interesting for visual shopping."
Symmetric Correspondence Topic Models for Multilingual Text Analysis,"Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models."
Efficient Sampling for Bipartite Matching Problems,"Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in real-world applications of these problems is intractable, making efficient approximation methods essential for learning and inference. In this paper we propose a novel {\it sequential matching} sampler based on the generalization of the Plackett-Luce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems - ranking and image correspondence - which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches."
Learning visual motion in recurrent neural networks,"We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model."
Tensor Analyzers,"Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its \emph{additive} nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact \emph{multiplicatively}. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe a fairly efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches and of images containing a variety of simple shapes that vary in size and color. Tensor Analyzers can also accurately recognize a face under significantly pose and illumination variations when given only one previous image of that face. We also show that Mixtures of Tensor Analyzers outperform Mixtures of Factor Analyzers at modeling natural image patches and artificial data produced using multiplicative interactions."
Online Learning for Auction Mechanism in Bandit Setting,"This paper is concerned with the online learning of the optimal auction mechanism for sponsored search in a bandit setting. We point out that this task corresponds to a new type of bandit problem, which we call the \textit{armed bandit problem with shared information} (AB-SI). In the AB-SI problem, the arm space (corresponding to the parameter space of the auction mechanism which can be discrete or continuous) is partitioned into a finite number of clusters (corresponding to the finite number of rankings of the ads), and the arms in the same cluster share the explored information (i.e., the click-through rates of the ads in the same ranked list) when any arm from the cluster is pulled. We propose an upper confidence bound algorithm called UCB-SI to tackle this new problem. We show that when the total number of arms is finite, the regret bound obtained by our proposed algorithm is tighter than the classical UCB algorithm. In the continuum armed bandit setting, our algorithm can handle a larger classes of reward function and achieve a reasonable regret bound of $O(T^{2/3}(d\ln T)^{1/3})$, where $d$ is the pseudo dimension for the real-valued reward function class."
Large-Margin Tensor Decomposition for Multi-Relational Learning,"We propose a novel large-margin framework for multi-relational learning via tensor decomposition. In this setting, the training data consists of multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a transformed linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using a weighted objective function. The objective combines multiple task-specific loss functions, to accommodate different types of relations. For the typical cases of real-valued functions and binary relations, we propose a combination of quadratic and smooth hinge losses and derive the associated parameter gradients. We solve the resulting optimization problem using memory efficient quasi-Newton methods. We evaluate our method on synthetic and real data, showing that it obtains significant accuracy improvement over related techniques even when training data is limited. Further, we show that our decomposition is able to transfer information across the various relations, thus better exploiting the multi-relational structure."
Understanding Indoor Scenes with Latent Interaction Template Models,"Visual scene understanding is a difficult problem, interleaving object detection, geometric reasoning and scene classification. In this paper, we present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the latent Interaction Template Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings, while also improving individual object detections."
Convex Tensor Decomposition via Structured Schatten Norm Regularization,"Conventionally, tensor decomposition has beenformulated as non-convex optimization problems, which despite theirempirical success, hindered the analysis of their performance. In thispaper, we propose structured Schatten norms for tensor decomposition based onconvex optimization. The proposed norms include two recently  proposedapproaches for convex tensor decomposition, which we call overlapped approach andlatent approach. Moreover, we mathematically analyze the performance of the latentapproach, which was empirically found to perform better than theother one in some settings. We show theoretically that this is indeedthe case. In particular, when the unknown true tensor is low-rank in a specific mode, the latent approach performs as good as knowing the mode with the smallest rank. We confirm through numerical simulations that our theoretical prediction can precisely predict the scaling behaviour of the mean squared error. "
Towards Sparse Representation on Cosine Distance,"Sparse code is a regularized least squares solution by $L_1$ or $L_0$ constraint, based on Euclidean distance between original and reconstructed signals with respect to a pre-defined dictionary.  The Euclidean distance, however, is not a good metric for many visual feature descriptors especially histogram features,~\eg~SIFT, HOG, LBP and Spatial Pyramid.  Instead, a cosine distance is a semantically meaningful metric for the visual features.  To leverage the benefit of cosine distance in sparse representation, we formulate a new sparse coding objective function based on approximate cosine distance by forcing a norm of reconstructed signal to be close to the norm of original signal.  We evaluate our new formulation on two datasets: Extended YaleB and AR dataset.  Our formulation shows consistent improvement over the traditional Euclidean distance based sparse coding formulation in our evaluations and achieve the state-of-the-art performance on the datasets."
Eliciting Predictions from a Connected Crowd of Traders,"We study an online trading community where traders can communicate with each other as well as perform trades. We discuss characteristics of social influence on trading decisions within this connected crowd. We discover traders are still heavily affected by social influence even when every trade is with their own money, and social influence often negatively affects their returns. Based on our observations, we implement three trading strategies to elicit the crowd?s prediction. In particular, we design a novel way of inferring predictions by modeling the crowd reasoning process under social influence. We find that even complex social dynamics can dramatically effect trades, it is still possible to infer knowledge from the crowd. Our novel algorithm achieves the best performance by modeling decision making processes under influence rather than the decisions from the crowd."
Generalized Classification-based Approximate Policy Iteration,"Classification-based approximate policy iteration allows us to benefit from the regularities of the optimal policy by explicitly controlling the complexity of the policy space. This leads to considerable improvements whenever the optimal policy is easy to represent. The conventional classification-based methods, however, do not benefit from the regularities of the value function as they often use a rollout-based estimate of the action-value function, which is rather data-inefficient and cannot generalize the estimate of the action-value function over states. In this paper, we introduce a general framework for classification-based approximate policy iteration, CAPI, that lets us benefit from the present regularities of both the policy and the value.Our theoretical analysis extends existing work by allowing the policy evaluation to be performed by any reinforcement learning algorithm, by handling nonparametric representations of policies, and by providing tighter convergence bounds on the estimation error of policy learning.A small illustration shows that this approach can be faster than purely value-based methods."
Q-Learning on a Multi-state Markov Decision Process,"In this paper, we consider a different setting for the agent in Markov Decision Process (MDP):The agent can occupy multiple states at the same time and take an action from a set of states.We refer to this problem as multi-state MDP.This multi-state MDP has exponentially large state and action spaces which might be computationally expensive.However, we can take the advantage of the nondeterminism of the agent in multi-state MDP and propose two nondeterministic Q-learning algorithms.We show that the convergence and optimality of the standard Q-learning algorithm still holds.Furthermore, in the experiments we also show that the nondeterministic algorithms converge faster and are more robust with different learning rates than the standard Q-learning algorithm."
Value Pursuit Iteration,"Value Pursuit Iteration (VPI) is an approximate value iteration algorithm that finds a close to optimal policy for  reinforcement learning and planning problems with large state spaces.VPI has two main features: First, it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features. The algorithm is almost insensitive to the number of irrelevant features. Second, after each iteration of VPI, the algorithm adds a set of functions based on the currently learned value function to the dictionary. This increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function.We theoretically study VPI and provide a finite-sample error upper bound for it."
Link Prediction in Biological Networks using Penalized Multi-Attribute ERGMs,"Reconstruction of genetic networks is an important, yet challenging problem in systems biology. Gene networks often include different interaction mechanisms, such as transcriptional regulatory and protein-protein interactions. Further, different data sources provide valuable information about the relationships among genes, which motivate methods that allow for data integration.We propose a novel multi-attribute exponential random graph model for supervised prediction of gene networks, coupled with a penalized estimation framework for improved prediction performance. The proposed framework facilitates the analysis of gene networks with multiple edge types, and provides a systematic method for incorporating multiple sources of biological data, as well as diverse attributes regarding the function and location of genes, and structure of observed networks. Results of numerical experiments indicate that the method enjoys superior performance compared to state-of-the-art reconstruction methods."
"Compressive neural representation of sparse, high-dimensional probabilities","This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a high-dimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain."
Information Theoretic Pairwise Clustering,In this paper we develop an information-theoretic approach for pairwise clustering. The Laplacian of the pairwise similarity matrix can be used to define a Markov random walk on the data points. This view forms a probabilistic interpretation of spectral clustering methods. We utilize this probabilistic model to define a novel clustering cost function that is based on maximizing the mutual information between consecutively visited clusters of states of the Markovian process defined by the graph Laplacian matrix. The algorithm complexity is linear on sparse graphs. The improved performance and the reduced computational complexity of the proposed algorithm are demonstrated on several standard datasets.
Shifted Subspace Tracking on Sparse Outliers,"In low-rank \& sparse matrix decomposition, the sparse part is often assumed to be generated by a random model. Analysis to its structure, which is of central interest in various problems, is rarely considered. One such example is tracking multiple object flows in video. We introduce ``shifted subspace tracking (SST)'' to both separate the object flows and recover their trajectories by exploring their shifted subspaces on the sparse outliers. SST can be summarized in two steps, i.e., background modeling and flow tracking. In step 1, we propose ``semi-soft GoDec'' to separate all the moving objects as the sparse outlier $S$ from the data matrix $X$. Its soft-thresholding of $S$ significantly speeds up GoDec and facilitates the parameter setting. In step 2, we treat the sparse $S$ in step 1 as the new $X$, and develop ``SST algorithm'' decomposing $X$ as $X=\sum\nolimits_{i=1}^k L(i)\circ\tau(i)+S+G$, wherein $L(i)$ denotes the subspace of the $i^{th}$ flow after transformation $\tau(i)$. The decomposition solves $k$ sub-problems of alternating minimization in sequel, each of which recovers a $L(i)$ and its $\tau(i)$ with randomized acceleration. Sparsity of $L(i)$ and smoothness between adjacent frames are explored to save computations. We justify the promising performance of SST on four surveillance video sequences."
CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem,"While compressive sensing (CS) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models. This limits its application and excludes many areas where CS ideas could make a difference. This paper presents a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose a novel solution using a lifting technique -- CPRL, which relaxes the NP-hard problem to a nonsmooth semidefinite program. Our analysis shows that CPRL inherits many desirable properties from CS, such as guarantees for exact recovery. We further provide scalable numerical solvers to accelerate its implementation. The source code of our algorithms will be provided to the public. "
Optimization of non-metric MRFs with QPBO via graph approximation,"Markov random field (MRF) has been used for many areas in computer vision. Many optimization methods were proposed to achieve good solutions on MRFs. Among them, graph cuts have gained widespread popularity. They achieve good approximated solutions when the energy function is metric using an $\alpha$-expansion scheme. However, if the energy function is non-metric, the conventional $\alpha$-expansion cannot solve the problem. In this case, the possible choice so far is the truncation, partial labeling with unlabeled nodes, and fusion move with heuristic proposals. In this paper, we propose a general way to handle non-metric MRFs with graph cuts using graph approximations. Extensive experiments support our claims and show that the proposed algorithm obtains better solutions than others both on synthetic and real problems."
Towards active event recognition,"Directing robot's sensors to anticipate events  like  goal-directed actions is complicated by intrinsic time constraints and spatially distributed sources of information.  The problem thus requires an integrated solution for tracking, exploration and recognition, which  traditionally have been seen as separate problems in active-vision.We propose a probabilistic generative framework  based on a mixture of Kalman filters  to use predictions in both recognition and sensor-control. This framework can efficiently use the observations of one element in a dynamic environment to provide information on other elements, and consequently enables guided exploration of the environment.Experiments on a humanoid robot  observing a human executing goal-oriented actions demonstrated improvement on recognition time and precision over baseline approaches."
Leveraging for Fitting Linear Models in Large-scale Data,"Recent empirical and theoretical work has focused on using the empirical statistical leverage scores of data matrices in order to develop improved algorithms for common matrix problems such as least-squares approximation and low-rank matrix approximation.  Existing work focuses on algorithmic issues such as worst-case running times or on the usefulness of this approach in downstream data applications.  Here, we examine the statistical properties of this leveraging paradigm in the context of fitting a linear model to data.  We derive the mean squared errors for two related leveraging-based estimates and for uniform sampling estimates.  Depending on the the mean, variance and skewness of the leverage scores, one procedure or another is preferred.  We also describe the empirical behavior of these procedures on several synthetic and real data sets."
Incremental Learning Hierarchical Functional Categories for Interacting Objects,"We present an algorithm which has been designed as an online learning module in a cognitive system to perform two fundamental and often coupled tasks: category learning and value function approximation. There are four important features in our algorithm. First, categories are incrementally constructed without using externally provided category labels. Second, categories are organized into hierarchies, making the algorithm scalable with respect to diversity of the inputs. Third, different from unsupervised hierarchical clustering algorithms, categorization in our algorithm can be influenced by externally provided feedbacks (in form of utility values or rewards). Finally, when there are multiple interacting objects from different domains (such as prey and weapon), multiple category hierarchies are learned simultaneously. We use systematically generated synthetic data to evaluate our algorithm in a function approximation task, where our algorithm is shown to learn significantly faster than standard machine learning algorithms used in cognitive systems."
Identity maps and their extensions on parameter spaces: Applications to anomaly detection in video processing,"It is now commonplace for analysts to model data on non-Euclidean spaces.   The non-linear structure of these parameter spaces can accurately reflect non-linear nature of complex data, and analysis is performed using algorithms which respect the geometry of the parameter space.  Several well-known linear algorithms have been generalized to analogous algorithms on parameter spaces such as k-means and PCA. These generalized algorithms have been shown to be effective ways to cluster and classify data on parameter spaces.  It is the aim of this paper to generalize an established (Euclidean based) Identity map extension (MSET) to well-known parameter spaces.  An identity map extension (IME) is a function that acts as the identity mapping when restricted to a special subset, and this property has made the MSET algorithm useful for anomaly detection.  We define a generalization of this map to several parameter spaces, we prove it has the IME property, and we evaluate its performance as an anomaly detector on a real dataset."
Scaled Gradients on Grassmann Manifolds for Matrix Completion,This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices while maintaining established global convegence and exact recovery guarantees. We also show the connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure. Our conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods.
Dual Semi-Supervised Co-Clustering Informed by Geometry,"Co-clustering algorithms, which group a data matrix based on the similarities of both rows (samples) and columns (features), often yield impressive performanceimprovement over traditional one-side clustering approaches. Efficient utilizing partial supervision in the form of row labels as well as column labels is still a challenge, especially when the number of labels is small. Moreover, since many real world data are sampled from a low dimensional manifold, effective co-clusteringalgorithms will depend upon the intrinsic structures of rows as well as columns of the data matrix. In this paper we propose dual semi-supervised co-clusteringinformed by geometry (DSCIG) to address these two issues. First, we provide a general framework for co-clustering that incorporates partial supervision informationand preserve local geometry. Second, we augment this framework with an additional step for similarity propagation that generate richer supervision information.To manage the different applications of DSCIG, we derive an alternative optimization procedure and show the convergence is guaranteed theoretically ."
Online Self-Supervised Segmentation of Dynamic Objects,"We address the problem of learning models to automatically segment dynamic objects in an urban environment from a moving camera without manual labelling, in an online, self-supervised manner. We use input images obtained from a single uncalibrated camera placed on top of a moving vehicle, extracting and matching pairs of sparse features that represent the optical flow information between frames. This optical flow information is initially divided into two classes, static or dynamic, where the static class represents features that comply to the constraints provided by the camera motion and the dynamic class represents the ones that do not. This initial classification is used to incrementally train a Gaussian Process (GP) classifier to segment dynamic objects in new images. The hyperparameters of the GP covariance function are optimized online during navigation, and the available self-supervised dataset is updated as new relevant data is added and redundant data is removed, resulting in a near-constant computing time even after long periods of navigation. The output is a vector containing the probability that each pixel in the image belongs to either the static or dynamic class (ranging from 0 to 1), along with the corresponding uncertainty estimate of the classification. Experiments conducted in an urban environment, with cars and pedestrians as dynamic objects and no prior knowledge or additional sensors, show promising results even when the vehicle is moving at considerable speeds (up to 50 km/h), a scenario that produces a large quantity of featureless regions and false matches that is very challenging for conventional approaches. "
Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging ,"Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model?s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject?s conversion to Alzheimer?s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here, our new model outperforms the state of the art (p-values << 10?3 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity)."
Stochastic blockmodeling of relational event dynamics,"For continuous-time network data, several approaches have recently been proposed for modeling dyadic event rates conditioned on the observed history of events and nodal or dyadic covariates.  In many cases, however, interaction propensities -- and even the underlying mechanisms of interaction -- vary systematically across subgroups whose identities are unobserved.  For static networks, such heterogeneity has been treated via methods such as stochastic blockmodeling, which operate by assuming latent groups of individuals with similar tendencies in their group-wise interactions.  Here, we combine these two approaches by positing a latent partition of the node set such that event dynamics within and between subsets evolve in potentially distinct ways.  We illustrate the use of our model family by application to several forms of dyadic interaction data, including email communication and Twitter direct messages.  Parameter estimates from the fitted models clearly reveal heterogeneity in the dynamics among groups of individuals.  We also find that the fitted models have better predictive accuracy than either baseline models or relational event models without latent structure.  Our approach illustrates the utility of latent structure methods based on detailed dynamics, which can succeed even in the absence of differences in marginal interaction rates across groups. "
Privacy Aware Learning,"We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.  In this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator."
Alternating Update Procedures for Unconstrained and Constrained Binary Matrix Factorization,"In general, binary matrix factorization (BMF) refers to the problem of finding a matrix product of two binary low rank matrices such that the difference between the matrix product  and a given binary matrix is minimized. In the current literature on BMF,  the matrix product is not required to be  binary. We call this  unconstrained BMF (UBMF) and similarly constrained BMF (CBMF) if the matrix product is required to be  binary. In this paper, we first introduce two specific variants of CBMF and discuss the relationship between BMF and UBMF.Then we propose alternating update procedures for both UBMF and CBMF. In every iteration of the proposed  procedure, we solve a specific binary quadratic programming (BQP) problem to update the involved matrix argument. Two different algorithms are presented to cope with the BQP subproblem in the procedure. In particular, we show that the BQP subproblem can be reformulated as a specific clustering problem. Based on the clustering reformulation, we also derive an effective 2-approximation algorithm for CBMF. By exploring the interrelation between UBMF and CBMF, we show that  we can  obtain good  approximation to rank-1  UBMF. The complexity of the proposed algorithms is  discussed. Numerical results show that the proposed algorithms for UBMF are able to find better solutions in less CPU time than several other algorithms in the literature, and the solution obtained from CBMF is very close to that of UBMF."
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of neuroscience research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships to the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and cannot deal with high-order, network-scale functional interactions. In this paper, we propose a structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. The structural connectivity constraints derived from diffusion tenor imaging (DTI) data will guide the selection of the weights which adjust the penalty levels of different coefficients corresponding to different ROIs. The robustness and accuracy of our models are evaluated and the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and healthy controls (NC). Our results indicate that the proposed model has significant potential to enable constructing high-order functional networks and their applications in clinical datasets."
Optimal Computational Trade-Off of Inexact Proximal Methods,"In this paper, we investigate the trade-off between convergence rate and computational cost when minimizing a composite functional with proximal-gradient methods, which are popular optimization tools in machine learning. We consider the case when the proximity operator is approximated via an iterative procedure, which leads to an algorithm with two nested loops. We show that the computationally optimal strategy to reach a desired accuracy in finite time is to set the number of inner iterations to a constant, which differs from the strategy indicated by a convergence rate analysis. In the process, we also present a new procedure called SIP that is both computationally and practically efficient. Our numerical experiments confirm the theoretical findings and suggest that SIP can be a very competitive alternative to the standard procedure."
Smooth and Monotone Covariance Regularization,"The dangers of using the sample covariance matrix obtained from scarce data in high-dimensional settings are well recognized. In particular, the inconsistency of its eigenvalue spectrum has grave implications for modeling risk within the Markowitz portfolios framework. A variety of approaches to improve the covariance estimates exploit knowledge of structure in the data, including low-rank models (principal component and factor analysis), banded models, sparse inverse covariances, and parametric models. We investigate a different nonparametric prior for random vectors indexed along a low-dimensional manifold: we assume that the covariance matrix is monotone and smooth with respect to this indexing. This fits a variety of problems including interest-rate risk modeling in econometrics, and sensor array noise modeling. We formulate the estimation problem in a convex-optimization framework as a semidefinite-programming problem, and develop efficient first-order methods to solve it. We apply our framework on a number of examples with limited, missing and asynchronous data, and show that it has the potential to provide more accurate covariance matrix estimates than existing methods, and exhibits a desirable eigenvalue-spectrum correction effect."
Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods,"We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{\dim}$ in convergence rate over traditional stochastic gradient methods, where $\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors."
The impact on mid?level vision of statistically optimal divisive normalization in V1,"The first two areas of the primate visual cortex (V1 and V2) provide a paradigmatic example of hierarchical computation in the brain. However both the interactions between the two areas, and the functional properties of V2, are not well understood. Here we present insights gained from statistical models of natural scenes. In particular, we study the impact of V1 output nonlinearities on the statistics seen by V2. We focus on divisive normalization, a canonical computation that has been found in many neural areas and modalities. We consider models of V1 complex cells with (and without) different forms of surround normalization derived from the Gaussian Scale Mixture (GSM) generative model of natural scenes, and a Mixture of GSMs that also accounts for image non?homogeneities. When surround normalization is used, followed by ordinary PCA to linearly combine V1 responses across space, then V2-like receptive fields emerge. To provide a more quantitative assessment, we compare the resulting 2?stage models on perceptual tasks of figure/ground judgment and object recognition; in both cases we find systematic advantages for using a V1 stage with statistically optimal surround normalization."
Near optimal policy decomposition for tasks with multiple goals explains human behavior,"The natural environment presents people with multiple potential goals that compete for action selection. Recent studies suggest that the brain generates several concurrent and partially prepared actions associated with alternative goals and use perceptual information to drive the goal competition, until a single action is selected. In the current study, we propose a near-optimal policy decomposition that the brain may use in visuomotor tasks with multiple competing goals. We show how human and animal strategies in the presence of competing goals can be expressed as a weighted mixture of multiple control policies, each of which produces a sequence of actions associated with a specific goal. We evaluate the performance of the proposed framework in a series of simulated reaching and saccade tasks with multiple targets in environments with and without presence of obstacles. The results show that the proposed model can qualitatively predict many aspects of human/animal strategies in goal-directed movements."
Hard and Easy Distributions of Bayesian Networks for Junction Tree Computation ,"The effort associated with Bayesian network computation is vital in many infer-ence and machine learning settings. In this paper, we introduce a novel algorithm,GPART, that generates synthetic Bayesian networks that reflect several input pa-rameters. Using the algorithm, we investigate how various parameters of Bayesiannetworks can affect junction tree characteristics and hence computation time. Wegeneralize previous approaches to randomly generating Bayesian network by (i)introducing a novel depth parameter as well as (ii) allowing state space size andnumber of parameters for a non-root node to be probability distributions. In ex-periments, we surprisingly find that increasing our novel depth parameter dramati-cally increases clique tree size and computation time. Using parameters computedfrom application networks as parameters in GPART, and comparing the resultingjunction trees, we better understand the similarities and differences between ap-plication and synthetic Bayesian networks."
Unidimensionality of sequential effects in human response times,Evidence is present showing that sequential effects occurring in 2-alternative forced-choice tasks are in fact a unidimensional phenomenon. Individual differences data from four different experiments was analyzed a multidimensional scaling analysis was performed on the distances between individual results. We found that the sequential effects described previously in the literature fit well in single dimension space. A dynamic belief model fit well to data from individual subjects and its parameters correlated strongly with sequential effects measures and distances within the one-dimensional space identified.
Sparse Prediction with the $k$-Support Norm,"  We derive a novel norm that corresponds to the tightest convex  relaxation of sparsity combined with an $\ell_2$ penalty. We show  that this new norm provides a tighter relaxation than the elastic  net, and is thus a good replacement for the Lasso or the elastic net  in sparse prediction problems.  But through studying our new norm,  we also bound the looseness of the elastic net, thus shedding new  light on it and providing justification for its use."
Robust Sparse Regression and Matching Pursuit,"In this paper we consider support recovery in sparse regression, when some number $n_1$ out of $n+n_1$ total covariate/response pairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interested in understanding how many outliers, $n_1$, we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables) provide guarantees on support recovery. Perhaps surprisingly, we also show that the natural brute force algorithm that searches over all subsets of $n$ covariate/response pairs, and all subsets of possible support coordinates in order to minimize regression error, is remarkably poor, unable to correctly identify the support with even $n_1 = O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in the basic setting we consider, where all authentic measurements and noise are independent and Gaussian. In this setting, we provide a simple algorithm that gives stronger performance guarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$ corrupted points, where $p$ is the dimension of the signal to be recovered."
Graphical Model Selection Using Junction Trees: Decompositions and Active Learning,"This paper proposes a framework for decomposing the undirected graphical model selection (UGMS) problem into multiple subproblems over clusters and separators of a junction tree.  Under certain conditions, we show that the junction tree framework significantly weakens the sufficient conditions on the number of observations required for high-dimensional consistent graph estimation.  When the conditions on the graphical model do not hold, we recover the standard conditions for high-dimensional consistency.  This motivates the use of our framework as a wrapper around algorithms for more accurate graph estimation.  Further, we show that the subproblems over the clusters and separators can be solved independently, which allows for using different regularization parameters or different UGMS algorithms to learn different parts of the graph.  Finally, the junction tree framework motivates active learning for UGMS that sequentially draws observations from the graphical model based on prior observations.  In the high-dimensional setting, we identify conditions under which the sufficient conditions on the number of scalar observations needed for an active algorithm is significantly less than that needed for a non-active algorithm.  Intuitively, the active learning algorithm draws more observations from parts of the graph that are difficult to learn and less measurements from parts of the graph that are easy to learn.  Our numerical results clearly identify the advantages of using the junction tree framework for both non-active and active learning for UGMS."
Active Learning of Multi-Index Function Models,"We consider the problem of actively learning \textit{multi-index} functions of the form $f(\vecx) = g(\matA\vecx)= \sum_{i=1}^k g_i(\veca_i^T\vecx)$ from point evaluations of $f$. We assume that the function $f$ is defined on an $\ell_2$-ball in $\Real^d$, $g$ is twice continuously differentiable almost everywhere, and $\matA \in \mathbb{R}^{k \times d}$ is a rank $k$ matrix, where $k \ll d$.  We propose a randomized, active sampling scheme for estimating such functions with uniform approximation guarantees. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive an estimator of the function $f$ along with sample complexity bounds. We also characterize the noise robustness of the scheme, and provide empirical evidence that the high-dimensional scaling of our sample complexity bounds are quite accurate."
Robust Structural Metric Learning,"Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degradesaccordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation,while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methodsin both high- and low-noise settings."
Learning Multiple Tasks using Shared Hypotheses,"In this work we consider a setting where we have a very large number  of related tasks with few examples from each individual task. Rather  than either learning each task individually (and having a large  generalization error) or learning all the tasks together using a  single hypothesis (and suffering a potentially large inherent  error), we consider learning a small pool of {\em shared    hypotheses}. Each task is then mapped to a single hypothesis in  the pool (hard association). We derive VC dimension generalization  bounds for our model, based on the number of tasks, shared  hypothesis and the VC dimension of the hypotheses  class. We conducted experiments with both synthetic problems and  sentiment of reviews, which strongly support our approach."
Differentially Private Learning with Kernels,"In this paper, we consider the problem of differentially private learning using kernel empirical risk minimization (ERM) where access to the training features is through a kernel function only. Existing work for this problem is either for the linear kernel or for translation invariant kernel, where (approximate) training features are available explicitly and furthermore their generalization error guarantees are dependent on the data dimensionality. Restricting access to data through kernel functions eliminates possibility of explicitly releasing the optima w^* to the kernel ERM. To alleviate this problem, we define three different models for differential private learning using kernel ERM. Our first model is an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. In the second model, learner sends back a differentially private version of the optimal parameter vector w^* but requires to see a small subset of unlabeled test set beforehand. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of w^*. For each of the model, we derive algorithms inspired by the technique for online database release by Gupta et al. 2011 and provide privacy as well as ``goodness'' guarantees. Furthermore, we show that our method can be applied to the setting of Rubinstein et al. 2009, Chaudhuri et al. 2011 also and obtain similar generalization error bounds with two distinctions: a) our bounds are independent of the data dimensionality, b) our sample complexity bounds have worse dependence on the required generalization error."
Semi-supervised Attribute Pattern Learning,"We focus on combining multiple views of object features to learn attribute patterns for semi-supervised classification. By formulating the problem as a semi-supervised classification with constrains: objects in the same class are similar to labeled data of the class upon each view of features and the quantized multi-modality attributes generated from the multiview features, an iterative attribute pattern learning method are proposed. In our approach, feature attributes are initialized by Euclidean Weighted Constrained-KMeans in each view, followed by attribute pattern discovery via Hamming Weighted Constrained-KMeans on the multi-modality attributes. These attribute patterns can further help to adjust the results of feature attributes in individual views by the feature co-occurrence. We therefore adopt a self-learning strategy to reconcile the disagreements between the classifications of the individual views of features and the multi-modality attributes with guaranteed convergence. On the other hand, we also achieve a variant of our method, where the supervision information is only imposed upon the multi-modality attributes for classification. The classifications on both synthetic and real-world datasets demonstrate that our attribute pattern learning method achieve better performance than the most informative view of features and concatenated features of all views by utilizing the complementary information from multiple views by feature co-occurrence."
On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization,"The ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge. This paper presents a new reinforcement-learning algorithm, called iKBSF, which extends the benefits of kernel-based learning to the on-line scenario. As a kernel-based method, the proposed algorithm is stable and has good convergence properties. However, unlike other similar algorithms,iKBSF's space complexity is independent of the number of sample transitions, and as a result it can process an arbitrary amount of data. We present theoretical results showing that iKBSF can approximate (to any level of accuracy) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator. In order to show the effectiveness of the proposed algorithm in practice, we apply iKBSF to the challenging three-pole balancing task, where the ability to process a large number of transitions is crucial for achieving a high success rate."
Multiclass Clustering using a Semidefinite Relaxation,"Spectral and other cut-based relaxations have been applied to graph clustering problems. In this paper, we propose a novel semidefinite relaxation for graph clustering known as Max-cut clustering. The clustering problem is formulated in terms of a discrete optimization problem and then relaxed to a SDP. To make the optimization scalable, we represent the SDP by a low-rank factorized approximation that reduces the number of variables, and then use a simple projected gradient method to solve it. To obtain the clustering, we propose a reweighted rounding scheme to get integral solutions. We also extend this formulation to a global approach to multi-class clustering and MAP inference in graphical models. Experimental results indicate that we outperform state-of-art several clustering methods. The algorithm is extended to perform MAP inference in graphical models and outperforms competing methods."
Convex Loss Minimization with Noisy Labels,"We study supervised binary classification in the presence of random classification noise. This setting can be thought of as a particular instance of learning from partial information: the learner, instead of seeing the actual labels, sees labels that have been flipped with some small probability. Using a simple unbiased estimator of the gradient of the loss, we derive online regret bounds for convex loss functions. These bounds immediately lead to efficient algorithms for learning from iid data with noisy labels via a simple online-to-batch conversion. We point out an interesting situation for hinge loss: a batch method using unbiased estimates leads to a non-convex problem whereas online learning using unbiased estimates is still efficient and comes with theoretical guarantees. We show that convexity of the batch problem can be retained if the loss function satisfies a simple symmetry condition. We illustrate the usefulness of our techniques on synthetic and real data."
Higher-order Nonparametric Models for Recognition by Analogy,"Nonparametric classification methods such as nearest neighbor offer the ability tolearn by association, leveraging large amounts of data, avoiding a training phase,and placing no assumptions on the structure of label space. However, such meth-ods often perform poorly due to the limited ability of typical distance functions tocapture complex relationships in the data. We propose a method for learning dis-tance functions using higher-order nonparametric models, resulting in better per-formance while still learning by association with no training phase. Our methodreplaces single example association with pair association, and can be interpretedas finding analogies among training and test examples. We test our method onRGB-D [1], a multi-view object data set, which lets us learn implicitly when dif-ferent 2D shapes describe a similar 3D structure. We show that our method isparticularly beneficial in the one-shot transfer regime, where only one exampleis available for a test category. Where traditional supervised learning methodsperform poorly, our method can use the relationships between objects in differentcategories to learn the structure of categories with impoverished training data."
A Performance Function for Multi-class Classification,"A performance function for multi-class classification is proposed in this paper. This performance function takes Nearest Subspace (NS) residual together with Collaborative Representation (CR) residual as variables. Strong underlying geometric explanations make those well-known residual measurements effective for multi-class classification problem. Nearest Subspace Classification (NSC) is a local measurement that considers distance between testing sample and each class, while Collaborative Representation based Classifier (CRC) is a global method measuring both intra-class and inter-class measurements. These two measurements are independent to each other. The first and the second order Taylor series of the performance function are analyzed, which characterizes this function well in some degree. A Second Order Performance Function (SOPF) is derived by involving a quadratic term of the first order terms and a product term. The SOPF contains two parameters with a positive factor constraint. A classifier based on SOPF is proposed, which improves a recent reported classifier called CROC(Collaborative Representation Optimized Classifier). The proposed algorithm is tested against human face and handwritten digits recognition and it achieves competitive classification result comparing to baseline methods. A large range of parameter configuration is acceptable once the positive factor constraint is satisfied."
Identifiability and Unmixing of Latent Parse Trees,"This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models."
Encoding Natural Images with Mean and Covariance,"Here we show how the mean and covariance of the statistical structure of natural images can be learned efficiently by using maximum likelihood estimation. In particular, the parameters of mean and covariance are learned with two sets of latent variables independently. Simulation results demonstrate that the model is ableto successfully capture details of the natural image distribution not represented by either covariance or mean alone. The joint approach is thus a step towards a more realistic natural image representation."
Bayesian nonparametric models for ranked data,"We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items.   Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process.  We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation.  We then develop a time-varying extension of our model, and apply our model to the New York Times lists of weekly bestselling books."
Object Detection from Multiple Overlapping Views,"We present a method for object detection in a multi view 3D model. We use highly overlapping views, geometric data, and semantic surface classification in order to boost existing 2D algorithms. Specifically, a 3D model is computed from the overlapping views, and the model is segmented into semantic labels using height information, color and planar qualities. 2D detector is run on all images and then detections are mapped into 3D via the model. The detections are clustered in 3D and represented by 3D boxes. Finally, the detections, visibility maps and semantic labels are combined using a Support Vector Machine to achieve a more robust object detector."
Learning Granger Graphical Models via Alternating Direction Method of Multipliers,"This paper presents a recent powerful algorithm, namely, the alternating direction method of multipliers (ADMM) for solving topology selection problems in Granger graphical models of autoregressive processes. The existence of a directed edge from node $j$ to node $i$ in the graph can be specified by the nonzero $(i,j)$ entry of the autoregressive coefficients. The problem of estimating the graph topology is formulated as a least-squares problem with an $\ell_1$-type regularization and can be regarded as a variant of Group Lasso problem. The value of the regularization parameter which controls the density of the estimated graph can be determined by minimizing Bayes information criterion score. We illustrate the idea and verify the performance of the ADMM algorithm on randomly generated data sets. This approach is finally applied on Google Flu Trends data set to learn a causal structure of flu activities from $51$ states in the USA."
Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions,"We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a $\order(\pdim/T)$ convergencerate for strongly convex objectives in $\pdim$ dimensions and $\order(\sqrt{\spindex( \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our algorithm is based on successively solving a series of $\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error ofour solution after $T$ iterations is at most$\order(\spindex(\log\pdim)/T)$, with natural extensions toapproximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem."
Sparse Matrix based Random Projection for Face Recognition,"Random projection (RP) is a powerful method in dimensionality reduction for itsdata independence and lower computation requirement. For the application ofRP, the construction of random matrix is critical due to its instability in performance.However, there is few directional work in this respect. Although a fewtheoretical work has proposed some matrices in terms of performance distortionand computation cost in the past decade, to the best of our knowledge, there is nocomprehensive theoretical or experimental work to compare their performance. Inthis paper, we attempt to evaluate current popular RP matrices by extensive experimentswith face recognition, and propose one kind of most sparsest RP matrices,which shows better performance than existing RP matrices with nearly the lowestcomputation complexity."
Graphical Gaussian Vector for Image Categorization,"This paper proposes a novel image representation called a Graphical Gaussian Vector, which is a counterpart of the codebook and local feature matching approaches. In our method, we model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. We consider the parameter of GMRF as a feature vector of the image. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Finally we define a new image feature by embedding the metric into the parameters, which can be directly applied to scalable linear classifiers. Our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. As the proposed method simply calculates the local auto-correlations of local features, it is able to achieve both high classification accuracy and high efficiency."
High theta and low alpha waves may be a pattern for BCI illiteracy in motor imagery ,"While brain computer interfaces (BCI) can be employed for patients and healthy humans, there are problems that need to be solved before the technique will become a useful tool. In most BCI systems, the significant number of target users are not able to use BCI systems with a control paradigm such as motor imagery (MI), P300, and steady state evoked potential (SSEP). Such target users are called the BCI illiterate users. Only a few studies, however, investigated such phenomena and they have not provided a clear understanding of the BCI illiteracy mechanism or solution to this problem. Recently, alpha power in default mode network (DMN) was proposed to predict a user?s potential performance in MI BCI, and the causal relationship of gamma band to sensory motor rhythm was reported. However, what differences exist between BCI-literate and BCI-illiterate groups are not fully understood; moreover, the theta band has not been thoroughly investigated for BCI illiteracy. In this study, we sought to demonstrate the neurophysiological differences between two groups (literate, illiterate) among 52 subjects using a default mode network during the eyes-opened state. As a result, we found that high theta and low alpha waves are a pattern for BCI illiteracy relative to BCI-literate persons. Using an un-paired student t-test, we found that the spatially significant areas between the two groups were found in the frontal and post-parietal areas for theta, roughly in the overall area for alpha, and in the post-central area for gamma. In addition, from the results of the relationship between band power and offline accuracy, we propose a simple performance predictor using four band powers. This gives a Pearson correlation coefficient of r=0.59, indicating that our proposed predictor explains 35% of the variance in subject performance."
Neuromorphic Sound localization based on Leaky Integrate-and-Fire Neurons,"We report on the neuromorphic sound localization circuit which consists of delay, coincidence detection, time division multiplexing, and integration neurons.  All elements are simple leaky integrate-and-fire neuron circuits with different parameters optimized to suppress the impacts of synaptic noises.  The detection range and resolution of the proposed neuromorphic circuit are 500 us and 5 us, respectively.  Our results show that, the proposed technique can localize a sound pulse with extremely narrow duration (~ 1 ms)."
Optimally fuzzy scale-free memory buffer,"Any system with the ability to learn patterns  from a time series of stimuli and predict the subsequent stimulus at each moment, should have  a buffer storing the stimuli from the recent past. In cases where the external environment generating the time series has a fixed scale, the buffer can be a simple shift register---a moving window of finite width extending into the past. However, such a traditional buffer is inappropriate for signals with scale-free long range correlations, which are found in many physical environments. We argue  for a scale-free fuzzy buffer that optimally sacrifices accuracy  in favor of  capacity to represent long time scales. Here we describe a neuro-cognitive model of internal time that satisfies these constraints."
Reinforcement Learning and Hierarchical State Representation for Modeling Incubation and Restructuring in Primate Insightful Problem Solving,The paper proposes a reinforcement learning model with hierarchical state representation for modeling the learning behavior observed in rhesus monkeys in a reverse-reward contingency task. The hierarchical state representation provides an ability to solve insightful problems by restructuring the internal belief representation of the environment through incubation in which the evidence of constructing an appropriate representation is accumulated. The experiment and simulation results show that the proposed model accounts for the three-stage learning patterns observed in experiments on rhesus monkeys. It also applies to behavioral results observed in rhesus monkeys on the same task during a transfer test when novel quantity combinations are presented.
Proper losses for learning from partial labels,"This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, establish a necessary and sufficient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. An interesting result is that the full knowledge of this matrix is not required, and losses can be constructed that are proper in a subset of the probability simplex."
Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation,"Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented."
Limiting memory improves online particle-?lter learners for word segmentation,"This paper shows that limiting the memory of an on-line particle-?lter algorithm for word segmentation improves its accuracy, yielding accuracies that are competitive with state-of-the-art batch algorithms. Our algorithm is derived by replacing the Chinese Restaurant Processes in a non-parametric Bayesian word segmentation model with distance-dependent Chinese Restaurant Processes (Blei and Frazier, 2011). This is easiest to do in a particle-?lter algorithm (B?rschinger and Johnson, 2011), and leads to a bounded-memory, on-line learning algorithm whose accuracy on standard evaluation data actually exceeds that of the corresponding algorithm without limited memory. We discuss the relevance of our results for ?over-learning? (speci?cally, under-segmentation) in Bayesian modelsand the ?less-is-more? effect (Newport, 1990)."
Selecting Diverse Features via Spectral Regularization,"We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse featuresthat can predict a given objective. Diversity is usefulfor several reasons such as interpretability, robustness to noise, etc.  We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized approximately by efficient greedy and local search algorithms, with provable guarantees.  We compare our algorithms to traditional greedy and $\ell_1$-regularizationschemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations."
Accelerating Graphical Models Inference via Iterative Algorithms,"In tree-structured graphical models, the max-product algorithm provides efficient and exact solutions to inference problems. However, as the number of states becomes large, the max-product algorithm becomes prohibitively slow. In this paper, we propose an iterative max-product algorithm to accelerate the inference. In addition, we apply iterative procedure to accelerate the k-best inference problem (in contrast to 1-best). We show the efficiency of these iterative based algorithms using both synthetic data and real world data."
Sample-based non-negative matrix tri-factorization,"We present a general dimensionality reduction algorithm applicable to non-negative bi-dimensional data sets composed of multiple samples. The new algorithm extends the standard non-negative matrix factorization to a constrained tri-factor decomposition. Our non-negative matrix tri-factorization has the unique feature of reducing dimensionality while identifying sample-independent factors in both the rows and columns of the input matrices, separately and simultaneously. This decomposition is motivated by neurophysiological data for which time-varying signals are typically recorded from several sources and in multiple samples. We derive the main algorithm, referred to as sNM3F, and present two possibly useful variants with orthogonality and time-shifts features. By applying the method to simulated muscle patterns we demonstrate that, compared to standard decompositions, our algorithm is effective in identifying reliably the underlying structure. Finally, we use it to decompose electromyographic signals recorded during arm reaching movements into separate spatial and temporal components."
Goal-Directed Grasp Imitation through Probabilistic Inference over Graphical Models,"The main contribution of this paper is a methodology for encoding of goal-directed grasp imitation using probabilistic techniques and vision based human and object tracking. We show how to formulate the problem and infer action goals using probabilistic graphical models in human and robot object grasping tasks. To deal with the high-dimensional state-spaces and mixed data types (discrete and continuous) involved in grasp imitation, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Real-life experimentation is enabled by employing a novel vision-based hand-object tracking system that allows for automatic extraction of grasping parameters from natural human demonstration, without any markers and specialized sensors. The most striking result is that the use of a model trained on simulated data provides basis for accurate goal-inference with partial and noisy observations of actual demonstrations. Goal-directed action selection is illustrated on a physical robot platform."
Sparse Online Topic Models,"Probabilistic online topic models have been developed for discovering latent semantic representations from massive data corpora. However, due to normalization constraints, probabilistic topic models can be ineffective in controlling the sparsity of discovered representations. In this paper, we present a sparse online topic model, which directly controls the sparsity of word and document codes by imposing sparsity-inducing regularization. The topical dictionary is learned by an online algorithm, which is efficient and guaranteed to converge. We extensively evaluate the basic sparse online topic model as well as its collapsed and supervised extensions on large-scale data sets. Our results demonstrate appealing performance."
Discriminative Sub-categorization,"The objective of this work is to learn sub-categories. Rather thancasting this simply as a problem of unsupervised clustering, we investigatea weakly supervised approach using both positive and negativesamples of the category.We make the following contributions: (i) we introduce a new model fordiscriminative sub-categorization which determines clustermembership for positive samples whilst simultaneously learning amax-margin classifier to separate each cluster from thenegative samples; (ii) we show that this model does not suffer fromthe degenerate cluster problem that afflicts several competing methods(e.g., Latent SVM and Max-Margin Clustering); (iii) we show that the method is able to discoverinterpretable sub-categories in various datasets.The model is evaluated experimentally over several UCI datasets, andits performance advantages over $k$-means and Latent SVM are demonstrated. We also stresstest the model and show its resilience in discovering sub-categoriesas the parameters are varied.  "
Solving Relational MDPs with Exogenous Events and Additive Rewards,"We formalize a simple but natural subclass of service domains for relational planning problems with object-centered independent exogenous events and additive rewards, capturing, for example, problems in inventory control and fire and rescue operations. Focusing on this subclass, we then present the first complete symbolic solution for stochastic planning problems in relational domains that is able to handle exogenous events and additive rewards, and is independent of domain size.  Our planning algorithm provides a lower bound approximation on the optimal solution given by the true value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation that can represent and manipulate real-valued functions over relational world states. A preliminary experimental evaluation demonstrates the validity and potential of our approach.  "
Input Variable Selection for Linear Regression Model using Nearest Correlation Spectral Clustering,"Linear regression models have been widely accepted in many scientific and engineering fields for the estimation or interpretation of phenomena.  When a linear regression model is built, appropriate input variables have to be selected to achieve high estimation performance.  This work proposes new methodologies for selecting input variables for linear regression models using nearest correlation spectral clustering (NCSC), which is a correlation-based clustering method.  In the present work, NCSC is used for variable group construction, and a few variable groups are selected by their contribution to estimates or by group Lasso; they are referred to as NCSC-based variable selection (NCSC-VS) and NCSC-group Lasso (NCSC-GL). The performances of the proposed NCSC-VS and NCSC-GL are examined through a case study of chemometrics data."
On Smoothness in Low-Rank Models,"We propose the Smooth Low-Rank Models (SLRM) to address problems in applications where the data matrix is not only low-rank, but also has a small total variation. Low-rank models are important in a number of problems such as matrix completion, denoising, and motion recovery. However, exact recovery of a low-rank matrix from a set of randomly sampled entries is not guaranteed when an entire column or row is not sampled. The problem can be alleviated if  prior information, such as smoothness in data, is available. This can be formulated as a nuclear-norm minimization problem, regularized by a Total Variation (TV) constraint.   Alternating Direction Method of Multiplier (ADMM) is used for solving the model. We studied the problems of matrix completion, denoising  and  motion capture data reconstruction.  Experiments on synthetic data, motion capture data and background modeling datasets demonstrated that SLRMs can significantly improve upon the original low-rank models and superior than other state-of-the-art models, especially when the set of sampled entries are insufficient or corruption is heavy. "
Path Integral Control by Reproducing Kernel Hilbert Space Embedding,"We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided."
A Linear Time Active Learning Algorithm for Link Classification,"We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V,E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\log|V|$."
How can probable mechanisms of object recognition in the visual cortex be determined by visual features?,"There are lines of evidence demonstrating that the fusiform face area (FFA) preforms processes related to specific object recognition, which requires expertise (e.g. facial recognition). It seems that our brain utilize different mechanisms for generic object recognizing (e.g. face vs. non-face) and specific recognition (e.g. two different faces). To address this issue, we first introduce a biologically inspired object recognition model and then examine it in two experiments. The model has a hierarchical structure that employs a biologically plausible approach for visual feature extraction. Our results suggest that the mechanisms underlying generic and specific object recognition are performed distinctly. The results are in agreement with evidence that indicates inferotemporal cortex (IT) is responsible for object recognition and FFA is involved in tasks that requires expertise. Furthermore, the results suggest that the important factor which makes these two mechanisms different may lie under the visual feature extraction. We attempt to propose a mechanism for these two different kinds of recognition. We also investigate the influence of extracted visual features on view-invariant object recognition and show that while the target objects and distractor are very similar to each other, a moderate degree of view invariance can be achieved without making association between views. It seems that by making use of some visual features which are common between near views of objects we can obtain moderate level view invariant object recognition."
Analysis of Differential Privacy Based on Importance Weighting,"This paper introduces and analyzes a novel data-publishing mechanism based on computing weights that make an existing dataset, for which there are no confidentiality issues, analogous to the dataset that must be kept private. The existing dataset may be genuine but public already, or it can be synthetic. The only necessary requirement is that it have similar schema as the private dataset. The weights are importance sampling weights, but they are regularized and have noise added. The weights allow statistical queries to be answered approximately while provably guaranteeing differential privacy. We derive expressions for the variance of the approximate answers. Experiments show that the new mechanism performs well even when the public dataset is quite different from the private dataset,and the privacy budget is small."
Bayesian Meta-classifier Learning from Biased Multiple Predictions,"We propose a probabilistic generative model for combining the predictions of multiple classifiers to form a meta-classifier that provides high classification accuracy.  The key feature of the model is the introduction of a latent variable that can identify whether the classifier in the ensemble is {\it related or unrelated} to each of the classes.  Our modeling is motivated by the idea that classifiers which provide incorrect predictions but are informative in terms of discriminating one class from others can also be effectively utilized for learning a meta-classifier.  The proposed meta-classifier learningscheme is particularly useful when the performance of each classifier is biased toward some specific class.  We perform empirical evaluations using both synthetic and real data. As a real case study of a combination of biased classifiers, we show its application to the high-level recognition of actual nursing activity by using accelerometers."
Bayesian Warped Gaussian Processes,"Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classification, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets."
Temporal Coding of Local Spectrogram Features for Robust Sound Recognition,"There is much evidence to suggest that the human auditory system uses localised time-frequency information for the robust recognition of sounds. Despite this, conventional systems typically rely on features extracted from short windowed frames over time, covering the whole frequency spectrum. Such approaches are not inherently robust to noise, as each frame will contain a mixture of the spectral information from noise and signal.Here, we propose a novel approach based on the temporal coding of Local Spectrogram Features (LSFs), which generate spikes that are used to train a Spiking Neural Network (SNN) with temporal learning. LSFs represent robust location information in the spectrogram surrounding keypoints, which are detected in a signal-driven manner, such that the effect of noise on the temporal coding is reduced. Our system models characteristic clusters of LSFs in an unsupervised way, using tonotopic learning based on Self Organising Maps (SOMs).Our experiments demonstrate the robust performance of our approach across a variety of noise conditions, such that it is able to outperform the conventional frame-based baseline methods."
Large Margin Metric Learning for Sparse Representation-Based Classification,"Sparse representation-based classification (SRC) has achieved great successes in many visual recognition tasks in recent years, such as face recognition and image classification. However, SRC is usually performed in the original feature space, which may not be discriminative enough for some classification problems. In this paper, we propose a large margin metric learning method to learn a discriminative distance metric to calculate the sparse reconstruction errors. The distance metric is learned by enforcing a margin between intraclass reconstruction error and interclass reconstruction error, for each training example. Experiments conducted on face recognition, gait recognition, and palmprint recognition show the efficacy of our proposed method. This approach has the potential to be used to enhance the SRC method in many applications."
Multiresolution analysis on the symmetric group,"There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of O(n^p) complexity with small p for sparse signals (in contrast to the O(n^q n!) complexity typical of FFTs). We discuss potential applications in ranking, sparse approximation, and multi-object tracking. "
PCA transform via Partial Rotation,"We present a relaxed version of high-dimensional rotation called \emph{partial rotation}. For two $d\times k$ matrices $S_1$ and $S_2$, each consisting of $k$ selected columns from two orthonormal bases in $\RR^d$, respectively, a partial rotation of degree $k$ is a $d\times d$ orthonormal matrix $R$ such that $R S_1=S_2$ and the null space of ${S_1}^T$ coincides with the null space of ${S_2}^T$ under $R$.  We show that such a rotation can be represented by a sequence of $k$ Givens rotations and provide an efficient algorithm to find the rotation in $O(k^2d)$ time. Since a partial rotation of degree $k$ is represented by a sequence of $k$ Givens rotation, it takes only $O(kd)$ time to rotate a $d$ dimensional vector by the rotation. This is faster than the standard rotation algorithm of time complexity $O(d^2)$ by orders of magnitude when $k$ is much smaller than $d$. Partial rotation is especially useful to principal component analysis (PCA) with $k$ principal components. By substituting the standard projection method of PCA with our partial rotation method, PCA transform can be done without any information loss. Our empirical results show that even a simple brute-force algorithm using PCA with partial rotation outperforms the state-of-art techniques in high-dimensional nearest neighbor search."
Sparse Optimal Control Signals for Natural Human Movements Using the Infinity Norm,"Optimal control models have been a successful tool in describing many aspects and characteristics of human movements. While such models have a sound theoretical foundation, their interpretation and neuronal implementation in the Central Nervous System (CNS) is not clear. We propose that the CNS not only utilizes control policies that are optimal with respect to a criterion, but also satisfy sparsity constraints. In recent years sparsity has played a pivotal role in theoretical neuroscience for information processing (such as vision). Typically, sparsity is imposed by introducing a cardinality constraint or penalty measured or approximate by the one-norm. In this work, to obtain sparse control signals, however, the $L_{\infty}$ norm is used as a penalty on the control signal. Even though such sparse control signals are discontinuous, the movements that result are continuous and smooth.  In addition, such sparse control signals are more biologically realistic and have a clear neuronal interpretation with a sequence of neuronal spikes. We show that moreover sparse optimal control signals quantitatively describe real human arm movements with high accuracy. "
Local Learning Algorithms for Multi-Task Learning,"Multi-task learning is to improve the performance of one task by utilizing information from other related tasks. Almost all existing multi-task learning methods belong to global learning approach. In this paper, different from existing methods, we propose local learning methods for multi-task classification and regression problems by extending some single-task local learning methods. For classification problems, we extend k-nearest-neighbor classifier by formulating the decision function on each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific. By defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one, a regularized objective function is proposed and we develop an efficient coordinate descent method to solve it. For regression problems, we extend kernel regression to multi-task setting in a similar way to the classification case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods."
A population search algorithm for clustered connectivity patterns,"The identification of time, frequency and spatial locations between which connectivity occurs within the brain is traditionally done via a brute force search of all available locations in a specified range. However, this is inefficient and slows down progress in the identification of connectivity patterns related to previously unexplored cognitive processes. Therefore, a novel, population based, search algorithm is proposed based upon the behaviour of foraging animals.The method is evaluated on both a simple grid search problem and on the identification of time-frequency locations of statistically significant phase synchronisation in the EEG. The method is shown to exceed brute force searches in terms of speed by several times while identifying a large proportion of available solutions."
A statistic for testing equality of distributions in metric space,"Deciding whether two sets of samples originate from the same probability law is essential in many fields of science and engineering such as economics, biology, medicine and neuroscience. Although this problem has been studied extensively when the underlying random variables are categorical or real valued, it remains largely unexplored when the random variables have more exotic domains such as time series, graphs, probability measures, and spike trains. A common aspect of these latter domains is that they can be assigned appropriate distance metrics, such as edit distance. In this paper, we exploit this natural characteristic to develop a computationally efficient, and parameter free statistic for testing equality of distributions. We compare the proposed approach with other state-of-the-art methods, and demonstrate that it performs equally well."
Structured Message Passing,"Almost all message-passing based approximate inference approaches proposed to date, e.g., belief propagation (BP), its generalizations and expectation propagation (EP), use tabular representation of messages and potentials. In this paper, we argue that this limits their accuracy in practice. To remedy this, we propose structured message passing (SMP), a unifying framework for taking advantage of structured representations. Within this framework, we investigate two structured approaches: sparse hash tables and algebraic decision diagrams for representing and manipulating messages, and propose a new message-passing algorithm that is an instance of SMP. The key idea in our new SMP algorithm is to artificially introduce determinism and context-specific independence in the messages which enables us to exploit the power and compactness of structured representations. We investigate our new algorithm both theoretically and experimentally. Our experimental results show the power and superiority of SMP over tabular message passing algorithms."
"A meta algorithm making centralized graph computation faster, distributed and at times better","In this paper, we present a meta algorithm that takes existing centralized algorithms for graph computation and makes them distributed and faster. In a nutshell, the meta algorithm creates a randomized partition of  the graph, with each partition being a small subgraph, and it then runs the centralized algorithm on each partition separately and stitches the resulting solutions to produce a global solution. We illustrate this meta algorithm with two popular problems: computation of Maximum A Posteriori (MAP) assignment in an arbitrary pairwise Markov Random Field (MRF), and modularity optimization for clustering and community detection.We show that the resulting distributed algorithms for these problems essentially run in linear time  and that they perform as well -- or even better -- than the original centralized algorithm as long as the graphs have geometric structure. More precisely, if the centralized algorithm is a constant factor approximation, the resulting distributed algorithm is also a constant factor approximation with constant slightly bigger; but if the centralized algorithm is a non-constant (e.g. logarithmic) factor approximation, then the resulting distributed algorithm becomes a constant factor approximation. For general graphs (not necessarily geometric), we  compute explicit bounds on the loss of  performance of the distributed algorithm with respect to the centralized algorithm."
Cross-Domain Information Role Classifier for Profiling Users in Online Q&A Forums,"This paper presents a novel application of text classification techniques for analyzing forum dynamics and profiling discussants in online Q&A discussions. Building on the existing Speech Act research, we identify dialogue features that capture true information seeking and providing roles of the discussants and their messages within threaded discussions. As message-level lexical information is not enough in capturing true information roles, we include additional thread-level information such as author turns and message positions in the thread. We generated and evaluated user information roles across four different Q&A forums including student group project forums and industry troubleshooting forums. The current result indicates that information role classifiers are robust across several different domains. We also found that the role-based user profiles are useful in predicting user performance or user expertise within the community."
Locally Optimized Hashing,"Fast nearest neighbor search is becoming more and more important to utilize massive data. Recent work shows that hash learning is effective for nearest neighbor search in terms of computational time and space. Existing hash learning methods try to convert near samples to near binary codes, and their hash functions are globally optimized on the data manifold. However, such hash functions often have low resolution of binary codes; each bucket, a set of samples with same binary code, may contain a large number of samples in these methods, which makes it infeasible to obtian the nearest neighbors of given query with high precision. As a result, existing methods require long binary codes for precise nearest neighbor search. In this paper, we propose Locally Optimized Hashing to overcome this drawback, which explicitly partitions each bucket by solving optimization problem based on that of Spectral Hashing with stronger constraints. Our method outperforms existing methods in image and document datasets in terms of quality of both the hash table and query, especially when the code length is short."
A Multiscale Composite Dirichlet Process for modelling rhythm tracks,"This paper introduces a novel non-parametric Bayesian model for hierarchically structured, pseudo-repetitive data, where the distribution of data on each scale of the hierarchy is generated by a Dirichlet Process.  The generality of this model makes it easily applicable to a wide range of data types, but it is particularly successful for composing melody or drum tracks.  In music, relative positional information is important, as patterns tend to be aligned to a metrum, a regular and hierarchical division of the sequence which generates an audible impression of a regular beat.  The model described in this paper is suitable for data with such properties, and can be trained using a Gibbs sampling algorithm.  The generative process is fast enough to compose melody or rhythm tracks in real time."
Variational Bayesian Matching,"Matching of samples refers to the problem of inferring unknown co-occurrence or alignment between observations in two data sets. Given two sets of equally many samples, the task is to find for each sample a representative sample in the other set, without prior knowledge on a distance measure between the sets. Recently a few alternative solutions have been suggested, based on maximization of joint likelihood or between-data dependency. In this work we present a variational Bayesian solution for the problem, learning a   Bayesian canonical correlation analysis model with a permutation parameter for re-ordering the samples in one of the sets. We approximate the posterior over the permutations, and demonstrate that the resulting matching algorithm clearly outperforms all of the earlier solutions."
Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model,"We consider linear regression in the high-dimensional regime in which the number of observations $n$ is smaller than the number of parameters $p$. A very successful approach in this setting uses $\ell_1$-penalized least squares (a.k.a. the Lasso) to search for a subset of $s_0< n$  parameters that best explain the data, while setting the other parameters to zero. A considerable amount of work has been devoted to characterizing the estimation and model selection problems within this approach. In this paper we consider instead the fundamental --but  far less understood-- question of statistical significance. Roughly speaking, when the Lasso estimates a specific parameter to be zero (or non-zero), \emph{how certain is this conclusion}? We study this problem under the random design model in which the rows of the design matrix are i.i.d. and drawn from an unknown high-dimensional Gaussian distribution. This situation arises --for instance-- in learning high-dimensional Gaussian graphical models. Leveraging on an asymptotic distributional characterization of regularized least squares estimators, we develop a procedure for computing p-values and hence assessing statistical significance for hypothesis testing. We characterize the power of this procedure, and evaluate it on synthetic and real data, comparing it with earlier proposals."
Statistics of edge co-occurences are sufficient to categorize natural images,"The analysis and interpretation of a visual scene to extract its category, such as whether it contains an animal, is typically assumed to involve higher-level associative brain areas.  Previous proposals have been based on a series of processing steps organized in a hierarchy that would successively interpret the scene at different levels of abstraction, from contour extraction, to low-level object recognition, to object categorization. We explore here an alternate hypothesis that second-order statistics of edges are sufficient to perform a rough yet robust (translation, scale and rotation invariant) scene categorization. The method is based on a realistic model of image analysis in the primary visual cortex that extends previous work from Geisler et al. (2001). Using a scale-space analysis coupled with a sparse coding algorithm, we achieved detailed and robust extraction of edges in different sets of natural images. This edge-based representation allows for a simple characterization of the ``association field'' by computing the second-order statistics of edge co-occurences. We show that the geometry of angles is sufficient to distinguish different sets of natural images taken in a variety of environments (natural, man-made, or containing an animal). This is quantitatively illustrated by using a na?ve classifier that allows to classify images solely on the basis of this geometry which performs at similar levels to hierarchical models. Such results call for the importance of the relative geometry of local image patches and its possible applications for image analysis, for instance to improve the efficiency of visual analysis systems. Most importantly, it challenges assumptions about the flow of computations in the visual system and emphasizes on the relative importance of associative connections, and in particular of intra-areal, lateral connections, in this process."
Thompson Sampling for Complex Online Problems,"We study stochastic multi-armed bandit settings with complex actionsover the basic arms, where the decision maker has to select a subsetof the basic arms or a partition of the basic arms at every round(rather than only selecting a single basic arm). The reward of thecomplex action is some function of the basic arms' rewards, and thefeedback observed may not necessarily be the reward per-arm. Forexample, when the complex action is a subset of the arms, we may onlyobserve the total reward or the maximum reward over the chosensubset. We use Thompson sampling to decide which complex action toselect. We prove a general theorem showing that a variant of Thompsonsampling with uniform exploration obtains logarithmic regret, and weshow how the regret depends explicitly on the information gain fromthe observations. As applications, we obtain several corollaries forspecific complex bandit problem setups with improved rates. Usingparticle filters for computing posterior distributions, we devise andsimulate Thompson-sampling algorithms for subset selection andjob-scheduling problems."
Learning pattern-based CRF for predicting the protein local structure,"We describe a pattern-based conditional random field. Such CRFs appear naturally in sequence labeling problems of bioinformatics and can be considered as relative to Hidden Markov Models. In this model, factors that participate in conditional probability are nontrivial only if their argument belong to a certain set of sequences. We describe an inference algorithm based on dynamic programming that is optimized to the structure of the sequence set. This algorithm becomes preferable as lengths of patterns become long. Then our model is applied to predicting $\phi,\psi$ angles of all-alpha proteins. Learning of parameters for this problem was done by structural SVM technique. The accuracy in prediction of dihedral angles $\phi$ and $\psi$ we achieved was 21.1 and 48.4 degrees respectively. The MDA score, defined as the percentage of residues that are found in correctly predicted eight-residue segments, attained 57.2\%."
Gossip-based On-Line Learning in Multi-Agent ?Systems with Local Decision Rules,"This paper is devoted to investigate binary classification in a distributed and on-line setting. The framework considered accounts for situations where both the training and test phases have to be performed by taking advantage of a network architecture by the means of local computations and exchange of limited information between neighbor nodes. An online learning gossip algorithm (OLGA) is introduced, together with a variant which implements a node selection procedure. Beyond a discussion of the practical advantages of the algorithm we promote, the paper proposes an analysis of the accuracy of the rules it produces, together with preliminary experimental results."
Bayesian Fusion of Image Modalities with Disjoint Attributes,"Large scale monitoring of spatial phenomenon often produces image data with multiple modalities. In the computer vision literature, fusion of images is usually formulated as a principled sensor inversion problem, with a wide variety of techniques applicable to the case where multiple observed images have the same underlying attributes. On the other hand, in many remote sensing applications the acquired modalities have no common attribute channels. For this case, we seek to use the detail of the high-resolution image modalities to enhance the low resolution modalities, considering that the data may be related through their content structure rather than the sampling process alone. This paper presents a Gaussian Process (GP) formulation to transfer spatial structure from the high-resolution image into the low-resolution modality without assuming a value mapping. Experimentation suggests the proposed approach is able to reconstruct local detail across large resolution differences, and we present fusion results from real aerial data with modalities from both a low flying unmanned robotic aircraft and a high altitude commercial hyperspectral imager."
Estimating Node Labels via Feature Propagation,"We propose a new method for estimating node labels from given instances (nodes) with a graph, particularly focusing on the estimation of the graph edge weights. We estimate edge weights through hyper-parameter optimization of a harmonic Gaussian field model for feature vectors, which we call feature vector propagation (FVP). FVP defines edge weights as a parameterized similarity function and optimizes edge hyper-parameters by cross-validation over feature vectors of all nodes. That is, the optimization is independent of labeled instances, leading to several important advantages, such as the robustness against sparsely labeled graphs and the applicability to multi-class problems. FVP can also capture the local structure of data by the objective function which shares the same form as the local reconstruction error in locally linear embedding. Experimental results demonstrated the effectiveness of FVP both in synthetic and real datasets."
How to sample if you must: On Optimal Functional Sampling,"Abstract. We examine a fundamental problem that models various active sampling setups, such as network tomography. We analyze sampling of a multivariate normal distribution with an unknown expectation that needs to be estimated: in our setup it is possible to sample the distribution from a given set of linear functionals, and the difficulty addressed is how to optimally select the combinations to achieve low estimation error. Although this problem is in the heart of the field of optimal design, no efficient solutions for the case with many functionals exist. We present some bounds and an efficient sub-optimal solution for this problem for more structured sets such as binary functionals that are induced by graph walks. "
Infinitesimal Annealing for Training Semi-Supervised Support Vector Machine,"The semi-supervised support vector machine(S^3VM) is a popular classification algorithm for finding the maximum-margin separating hyper-plane for both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good(local) solution of S^3VM is to gradually increase the effect of unlabeled data, `a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches. "
Semantic Kernel Forests from Multiple Taxonomies,"When learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient.  While an \emph{object taxonomy} specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that \emph{are} relevant.  In light of these issues, we propose a discriminative feature learning approach that leverages \emph{multiple} hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenic ties, while another could reflect their habitats).  For each taxonomy, we first learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes.  Then, using the resulting \emph{semantic kernel forest}, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class.  To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure.  We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements."
Data Representation with Rank Regularized PCA,"Trace-norm is often used in low-rank data representation models. In this paper, we point out some drawbacks of the trace norm based approach and propose a rank regularized formulation which can be solved very efficiently. We did extensive experiments on six datasets. Experiments show the advantage of the proposed approach."
Diversity and Capacity Control in Boosting,"Various explanations have been proposed for understanding the great success of boosting algorithms, particularly AdaBoost, mainly including the statistical view and the margin theory. However, there are still observations out of the explanations. In this paper, we investigate the learning capacity (also known as the hypothesis space complexity and structure risk) of AdaBoost, which has not been well investigated in existing explanations. Previously, the learning capacity of boosting was canonically measured by the VC-dimension of the convex hull of the linear combination of hypotheses, which shows that the capacity grows exponentially to the number of base hypotheses. This paper proves the connection between the learning capacity and the diversity among base hypotheses, which discloses that the learning capacity can be small if the diversity is large. We then reveal that AdaBoost can automatically maximize diversity while optimizing its loss function, and therefore, implicitly controls its learning capacity. The investigation on diversity of AdaBoost may provide a clue to complement the understanding of AdaBoost and boosting algorithms."
Gradient-Boosted Adaptive Codes for Classification and Embedding,"Discriminative classifiers pursue simultaneous embeddings of observations and classes in a shared space such that the embedding of each observation is more similar to the embedding of its associated class than to that of any other class. Boosting-based classifiers are often partitioned into two sets: those which assume a fixed embedding of the classes and those which concurrently learn to embed both the observations and classes. The classifier we introduce falls into the latter camp; it learns all dimensions of the observation and class embeddings concurrently. With L1 regularization applied to the class embedding, our method outperforms existing boosted classifiers on standard benchmarks. Using Euclidean distance to measure class-observation similarity, rather than the typical dot-product, and with additional regularization controlling the spread of each class' embedded representation, our method also produces meaningful embeddings of labeled data. We begin our presentation by recapitulating boosting as functional gradient descent and then examining a weakness in one frequently cited theorem concerning the convergence of gradient-based boosting."
Bandit Market Makers,"We propose a flexible framework for profit-seeking market-making, using a sequence of cost-function based automated market-makers with bandit learning algorithms. We do this by considering the magnitude to which a cost-function extends beyond the simplex as a bandit arm, and the minimum-expected profits consistent with a no-arbitrage condition as the rewards. This allows for the creation of market-makers that can adjust bid-asks spreads dynamically, maximising worst-case-expected profits. "
Clustered Bandits,"We consider a multi-armed bandit setting that is inspired by real-world applications in e-commerce.  In our setting, there are a few types of users, each with a specific response to the different arms. When a user enters the system, his type is unknown to the decision maker.  The decision maker can either treat each user separately ignoring the previously observed users, or can attempt to take advantage of knowing that only few types exist and cluster the users according to their response to the arms. We devise algorithms that combine the usual exploration-exploitation tradeoff with clustering of users and demonstrate the value of clustering.  In the process of developing algorithms for the clustered setting, we propose and analyze simple algorithms for the setup where a decision maker knows that a user belongs to one of few types, but does not know which one."
Discovering Latent Styles in Human Movements,"There are often latent styles underlying the dynamics of human movements; for instance, table tennis strokes can be executed with forehand push or backhand chop. Modeling latent styles in human movements is crucial for learning prior models in many applications. We propose a latent style dynamics model to capture the generative process of human movements from latent styles. As efficient inference is desired in practice, we introduce an approximate inference method based on proxy variables, which, despite its low complexity, also takes into account the uncertainty in latent style variables. On both synthetic data and human table tennis stroke data, our method successfully discovers interpretable latent styles and provides reliable modeling of human dynamics."
Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Hamiltonian Monte Carlo Sampling,"This paper proposes a novel framework for multigroup shape analysis relying on a hierarchical graphical statistical model on shapes within a population. Under the proposed hierarchical model, individual shapes are represented as pointsets, derived from their group shape model. Similarly, each group shape model is derived from a single population shape model. The hierarchical model follows the natural organization of population data and enables comparison of shape models between groups, via hypothesis testing, by proving a common frame of reference for the shape models. Unlike typical approaches for shape modeling, the proposed model is a generative statistical model that defines a joint distribution of objectboundary data and the model variables in the hierarchical model. Furthermore, it naturally enforces optimal correspondences during the process of model fitting and thereby subsumes the correspondence problem. The proposed optimization framework employs an expectation maximization (EM) algorithm that treats the individual and group shape variables as hidden random variables and integrates them out before estimating the parameters (population mean and variance and the group variances). The underpinning of the EM algorithm is the sampling of shapes from their posterior distribution, for which the paper exploits a highly efficient scheme based on Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted hierarchical model to perform hypothesis testing for comparison between pairs of groups using permutation testing. The paper validates the proposed framework on simulated data and demonstrates results on real data."
Adaptive Training for Online Transfer Learning ,"Training an effective prediction model using small amount of data is important topic in machine learning.For this purpose, one of the most widely studied frameworks is transfer learning.To achieve scalability and reduce memory, we focus on online transfer learning. Although most works on online learning intends to minimize cumulative errors throughout online training,we introduce a novel online algorithm for transfer learningto obtain the best prediction adaptivelyand improve generalization error rapidly rather than cumulative errors.We give a strong theoretical support on the predictive accuracy of our algorithm.Numerical experiments for several datasets demonstrates that our algorithm always have optimal accuracy in course of online training."
Learning Representations for Detecting and Recognizing Sequences of Animated Motion - A Neural Model,"The detection and categorization of animate motions is a crucial task underlying social interaction and perceptual decision-making. Neural representations of perceived animate objects are built in the primate cortical region STS which is a region of convergent input from intermediate level form and motion representations. Populations of STS cells exist which are selectively responsive to specific animated motion sequences, such as walkers. It is still unclear how and to which extent form and motion information contribute to the generation of such representations and what kind of mechanisms are involved in the learning processes. The paper develops a cortical model architecture for the unsupervised learning of animated motion sequence representations. We demonstrate how the model automatically selects significant motion patterns as well as meaningful static form prototypes characterized by a high degree of articulation. Such key poses are selectively reinforced during learning through a cross-talk between the motion and form processing streams. Next, we show how sequence selective representations are learned in STS by fusing static form and motion input from the segregated bottom-up driving input streams. Cells in STS, in turn, feed their activities recurrently to their input sites along top-down signal pathways. We show how such learned feedback connections enable making predictions about future input as anticipation generated by sequence-selective STS cells. Network simulations demonstrate the computational capacity of the proposed model by reproducing several experimental findings from neurosciences and by accounting for recent behavioral data."
The Design of a Vibro-tactile Watch for Long-term Use and Learning,"In this article, we present a mechanical tactile device that emulates a watch. This device has many practical applications but requires further study. We discuss its design, benefits, and flaws. We then discuss possible solutions to specific problems, as well as future uses of the device. We also discuss the implications this device may have on haptic research."
Spectral Differential Privacy,"Positive semidefinite matrices are important for a number of machine learning applications. We consider the problem of differentially private publication of positive semidefinite matrices computed from private information. Differential privacy is typically achieved by adding random noise.However, when the outputs form positive semidefinite matrices, element-wise additive randomization causes problems. First, when not a single element, but the entire matrix is released, the scale of noises to provide differential privacy can be too large. Second, such randomization not only destroys the positive semidefiniteness, but may be statistically denoised in some cases.For these problems, we introduce a new randomization mechanism which separately randomizes eigenvectors and eigenvalues so that the randomization does not completely destroy the spectral features. Furthermore, noting that low-rank approximation preserves useful information of matrices while discarding unnecessarily details, we incorporate low-rank approximation into randomization.We prove that the scale of perturbation required to guarantee differential privacy is inversely proportional to the rank of the output matrices in the proposed randomization mechanism. Thus, if a data analyst does not need the output matrix itself, but needs only a low-rank approximation, the scale of perturbation can be relatively smaller without sacrificing privacy. This is convenient for machine learning applications which work well even with lower-rank approximation.We experimentally demonstrate that low-rank approximation helps to implicitly control the accuracy-privacy trade-off with  a collaborative filtering example."
Multimodal Learning with Deep Boltzmann Machines,"We propose a Deep Boltzmann Machine for learning a generative model of multimodal data. We show how to use the model to extract a meaningful representation of multimodal data. We find that the learned representation is useful for classificationand information retreival tasks, and hence conforms to some notion of semantic similarity. The model defines a probability density over the space of multimodal inputs. By sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval fromboth unimodal and multimodal queries. We further demonstrate that our model can significantly outperform SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains."
Learning with Target Prior,"In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\y$ can be modeled with a prior model $p(\y)$ and the relations between data and target variables are estimated through $p(\y)$ and a set of uncorresponded data $\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\t$ that maximizes the log likelihood of $f_\t(\x)$ on a uncorresponded training set with regards to $p(\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video."
On Pre-training Shallow Networks with Support Vector Machine Primals,"We present a methodology to pre-train shallow neural networks with Support Vector Machine primals. We train a Support Vector Machine and extract the primal weights to embed them as pre-trained prior knowledge in a shallow neural network; we then proceed to apply backpropagation to leverage and fine tune this knowledge. This contrasts with previous work on pre-training, in which unsupervised pre-training has been used as feature extractors in deep learning. In our MNIST experimental results, we find that using even only $\frac{1}{60}$ of the original dataset for the Support Vector Machine primal pre-training yielded a consistently faster convergence in the network. We believe this paper opens up interesting opportunities for pre-training shallow networks using prior knowledge."
Slice sampling normalized kernel-weighted completely random measure mixture models,"A number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality.  However, the inference algorithms are often slow and unwieldy, and are in general highly specific to a given model formulation. In this paper, we describe a wide class of nonparametric processes, including several existingmodels, and present a slice sampler that allows efficient inference across this class of models.  "
Learn to rank Based on Topic Relation Models,"Most of learning to rank algorithms use word based features to train ranking models. Topic features are seldom considered. In this paper we aim at building a topic based ranking model which uses the relational topic model (RTM) to learn ranking function. The original RTM is a one-class model. To make RTM learn from data with different ranking labels, we extend RTM to two-class model and regression model. We employ variational inference algorithm to compute the posterior distribution of the latent variables and use the Expectation Maximization algorithm to estimate the topic vectors and parameters of ranking function. Our experiments on OHSUMED data set show that the proposed models have better accuracy than word based ranking model(SVM) and the original RTM model."
Scalable Inference of Overlapping Communities,"We develop a scalable algorithm for posterior inference of overlappingcommunities in large networks.  Our algorithm is based on stochasticvariational inference in the mixed-membership stochastic blockmodel.It naturally interleaves subsampling the network with estimating itscommunity structure.  We apply our algorithm on ten large, real-worldnetworks with up to 60,000 nodes. It converges several orders ofmagnitude faster than the state-of-the-art algorithm for MMSB, findshundreds of communities in large real-world networks, and detects thetrue communities in 280 benchmark networks with equal or betteraccuracy compared to other scalable algorithms."
Maximize Short-term Memory in Direct Model of Echo State Networks,"Echo state networks (ESN) are a kind of novel recurrent neural network (RNN) which has a large number of randomly connected neurons (called ?reservoir?) and an adaptable output. The short-term memory (STM) of ESN is the ability of storing information about recent inputs in the reservoir's transient response. It is indispensable for the time varying information processing. Previous work suggested that for i.i.d. input, the upper bound of memory capacity (MC) is N, where N is the number of neurons in the reservoir. In this paper, we show that this is not always the case. We transform the iterative mathematical model of ESN to direct one. In this model, we establish a direct relationship between memory capacity of ESN and its connectivity. We find that some reservoir topologies proposed by previous papers are the special solutions of our method. Furthermore, our experimental results show that the maximum MC in ESN can exceed the upper bound N even with i.i.d. input."
Learning from Data and Constraints: an Unified Probabilistic View of Clustering,"In this paper we introduce an unified view of clustering, i.e. learning from unlabeled data and constraints. Clustering is considered as a prediction problem for the states of latent variables, i.e. unknown labels. Unlabeled data and constraints are seen as two main sources to provide related information of latent variables. We present a probabilistic clustering model based on Hidden Markov Random Fields (HMRFs), which can embed different related information together to guide theclustering process. We also present a novel constrained clustering method, i.e. a simplified version of HMRF-based clustering model. Unlabeled data and a few pairwise constraints are combined to generate the neighborhood system between latent variables. One of the main limitations of existing constrained clusterings, which is the requirement of a large amount of constraints, can be significantly alleviated. Further, connections between HMRF-based clustering model and manyexisting clusterings are established to demonstrate the inclusiveness and flexibility of the proposed model. Experiments on synthetic and real data are also performed to demonstrate the benefits of the proposed constrained clustering method."
A systematic approach to extracting semantic information from functional MRI data,"This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure."
Learning optimal spike-based representations,"How do neural networks learn to represent information? Here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder. We define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss. The dynamical equations yield a network of integrate-and-fire neurons undergoing Hebbian plasticity. We show that, through learning, initially regular and highly correlated spike trains evolve towards Poisson-distributed and independent spike trains with much lower firing rates. The learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder. We show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs. In turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme. Our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code."
Collaborative Ranking With 17 Parameters,"The primary application of collaborate filtering (CF) is to recommend a small set of items to a user, which entails ranking. Most approaches, however, formulate the CF problem as rating prediction, overlooking the ranking perspective. In this work we present a method for collaborative ranking that leverages the strengths of the two main CF approaches, neighborhood- and model-based. Our novel method is highly efficient, with only seventeen parameters to optimize and a single hyperparameter to tune, and beats the state-of-the-art collaborative ranking methods. We also show that parameters learned on one dataset yield excellent results on a very different dataset, without any retraining."
Rational inference of relative preferences,"Statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions. However, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented. In this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects. Complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function. "
Multiresolution Value Function Approximation in Reinforcement Learning using the Wavelet Basis,"We present the wavelet basis, a linear value function approximation scheme that enables multiresolution value function approximation in continuous state spaces. We apply the wavelet basis to two standard reinforcement learning domains, and show that it performs as well as or better than existing commonly used basis functions when used as a fixed basis.We also briefly demonstrate how it can be used to add representational power to better represent spatially local detail."
Off-Policy Actor-Critic with Function Approximation,"We present a new off-policy learning algorithm with an actor-critic architecture that is convergent to a locally optimal solution. Off-policy learning---learning about a policy different from the one being followed---plays an important role in reinforcement learning (RL) due to exploration-exploitation tradeoff. Recent advances in off-policy Temporal-Difference (TD) learning, such as Greedy-GQ, have been hitherto  limited  to value-function based methods and have not been fully extended to policy gradient methods,  which can represent a larger class of policies and also can handle problems with large (or continuous) action space.  Among policy gradient methods, actor-critic methods substantially have been considered for large-scale applications due to their desirable algorithmic features---e.g., they use bootstrapping methods such as TD learning that can reduce variance, and generally are easy to use with function approximation. The critic in our algorithm is based on recent gradient-TD prediction (GTD) methods with linear function approximation and the actor updates the policy parameters via stochastic gradient-ascent of a performance measure. Recently, Degris et al. (2012) have presented an off-policy actor-critic algorithm (OPAC) with similar objectives. However, OPAC does not update the actor via gradient-ascent and, as we will establish, does not always converge. In this paper, we address this issue by proposing an algorithm, called GTD-AC, that shares several of OPACs desirable features: online operation, incremental updating, linear complexity both in terms of memory and per-time-step computation, and in addition it maintains the same number of tuning parameters. Most importantly, we establish a convergence guarantee."
Minimum Distortion Sketches for Learning,"The number of unique features can be prohibitively large in various classification problems, especially in document analysis. For such problems, storing all coefficients of a classifier model would require massive amount of memory on a single machine. Therefore, in this work, we propose a new sketching technique to approximately store the coefficients of a classifier model in constant memory budget. The proposed technique cuts down the space requirement by a factor of sketch depth (typically 3-7 in practice) in the best case, while matching the best known bounds of the existing sketching technique in the worst case. We also demonstrate the performance improvement on various large-scale classification problems."
Recognizing Human Activities from Incompletely Observed Videos,"In this paper, we present a novel method for handling the problem of recognizing human activities from incompletely observed videos. Compared with the similar problem of human activity prediction from unfinished activities [12], in an incompletely observed video an un-observed subsequence of frames may occur any time with any duration and yield a temporal gap in the video. In practice, incompletely observed videos may occur when the video signal drops off, when camera or objects of interest are occluded, or when videos are composited from multiple sources. In this paper, we formulate the problem of human activity recognition from incompletely observed videos in a probabilistic framework. In this framework, we take a set of training video samples (completely observed) of each activity class as the basis, and then use sparse coding to derive the likelihood that an incompletely observed test video belongs to a certain activity class. Furthermore, we propose to divide each activity into multiple temporal stages, apply sparse coding to derive the activity likelihood at each stage, and finally combine the likelihoods at each stage to achieve a global posterior for the activity. We evaluate the proposed method on both the widely used UT-Interaction human activity dataset and a new human activity dataset selected from the Year-1 corpus of the DARPA Mind's Eye program [4]. For the new DARPA dataset, both the activities and the videos show very large within-class temporal, spatial, and background variation. Our results demonstrate that the proposed method performs substantially better than several competing methods on both datasets."
Clustered Approximation for Gaussian Kernel Support Vector Machines,"The Gaussian kernel support vector machine (SVM) is one of the most widely used classification methods. However, scalability of this method is a big issue when facing millions of samples. Recently, many papers have suggested tackling this problem by using a low-rank approximation for the Gaussian kernel matrix. In this paper, we first show that the structure of the Gaussian kernel matrix changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter $\gamma$ in the Gaussian kernel. Based on this observation, we propose a Clustered Approximation (CA) framework for Gaussian kernel matrix approximation. For many non-linearly separable datasets, we find that the best parameter $\gamma$ for classification usually lies in the region where CA achieves lower approximation error than low-rank approximation when they both use the same amount of memory.Moreover, when we apply CA methods to scale and speed up the Gaussian kernel SVM training, the resulting algorithms outperform state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on a non-linearly separable dataset \covtype with half-a-million samples, our proposed method Clustered Block Approximation SVM (CBA-SVM) achieves 96.03\% testing accuracy with training time of less than 3 minutes on a single workstation with 4G RAM. In comparison, on this problem, \LIBSVM takes more than 13 hours to get 96.08\% accuracy, while the fast low-rank approximation based method, low-rank linearized SVM (LLSVM), takes 1 hour but only gets 85.05\% testing accuracy. "
Online computation of sparse representations of time varying stimuli using a biologically motivated neural network,"Natural stimuli are highly redundant, possessing significant spatial and temporal correlations. While sparse coding has been proposed as an efficient strategy employed by neural systems to encode sensory stimuli, the underlying mechanisms are still not well understood. Most previous approaches model the neural dynamics by the sparse representation dictionary itself and compute the representation coefficients offline. In reality, faced with the challenge of constantly changing stimuli, neurons must compute the sparse representations dynamically in an online fashion. Here, we describe a leaky linearized Bregman iteration (LLBI) algorithm which computes the time varying sparse representations using a biologically motivated network of leaky rectifying neurons. Compared to previous attempt of dynamic sparse coding, LLBI exploits the temporal correlation of stimuli and demonstrate better performance both in representation error and the smoothness of temporal evolution of sparse coefficients."
The topographic unsupervised learning of natural sounds in the auditory cortex,"The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efficient coding strategy and that the disorder in A1 reflects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reflected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner."
A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation,"In this paper, we consider the $\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables. We further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure. As an example, a recent state-of-the-art method, QUICrequires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem."
Importance Sampling Active Learning Algorithm,"This paper presents the importance sampling active learning (ISAL) algorithm, which can be very sample efficient by drawing instances to label from some appropriate distribution.  In particular,  ISAL  begins with a set of user-specified instance distributions; on each iteration, it identifies  a distribution that puts large weight on instances whose labels are  uncertain, then requests the label of an instance drawn from that distribution. We prove that ISAL can be more sample-efficient than passive learning, and that it  can achieve an exponential convergence rate to the Bayes classifier on noise-free data.  We also provide empirical studies that show  ISAL   is more efficient than many other active learning algorithms. "
Fast Manifold Learning with Unsupervised Nearest Neighbors,"In this paper we introduce a simple and extremely fast dimensionality reduction method for point-wise embedding of patterns in continuous latent spaces. The approach is an iterative method, which fits nearest neighbors into the framework of unsupervised regression. We introduce unsupervised nearest neighbors for continuous latent spaces. Latent points are iteratively embedded with a stochastic approach: distances in data space are employed as standard deviation for Gaussian sampling in latent space, neighborhood relations are preserved with a nearest neighbor regression-based data space reconstruction error. We extend the approach to handle missing data, and analyze the employment of kernel functions for computation of the data space reconstruction error. Experimental studies show that kernel unsupervised nearest neighbors is an an efficient method for embedding high-dimensional patterns on artificial test data, and real-world data from astronomy."
Conditional Distance Variance and Correlation,"Recently a new dependence measure, the distance correlation, has been proposed to measure the dependence between continuous random variables. A nice property of this measure is that it can be consistently estimated with the empirical average of the products of certain distances between the sample points. Here we generalize this quantity to measure the conditional dependence between random variables, and show that this can also be estimated with a statistic using a weighted empirical average of the products of distances between the sample points. We demonstrate the applicability of the estimators with numerical experiments on real and simulated data sets."
A Simple and Practical Algorithm for Differentially Private Data Release,"We present a new algorithm for differentially private data release, based on a simple combination of the Exponential Mechanism with the Multiplicative Weights update rule.  Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques."
Implementing Attention Focus in Model-based Reinforcement Learning,"We propose a new framework for learning the world dynamics of model-based reinforcement learning (RL) in high-dimensional, feature-rich environments. We model the world dynamics through predicting changes with the action effects, and differentiating the model features involved.  We present a factored transition function representation that supports efficient learning of the relevant features. We also introduce an online sparse coding learning technique to implement attention focus in learning the transition models. We provide theoretical analyses and empirical evaluation of our new RL algorithm, loreRL, in two benchmark domains."
"Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders","We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ is chosen uniformly at random from $\{+1, -1\}^n$, $\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers $A$ and $\Sigma$ up to an additive $\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \epsilon$. To accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search."
Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins,"While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice. However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent. In this work we propose to augment these algorithms with an $\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers."
Approximate Factored Real-time Dynamic Programming,"Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when there is information about the initial state. Traditional RTDP approaches work best on problems with sparse transition matrices where they can often efficiently achieve e-convergence without visiting all states; however, the advantages of traditional RTDP are often lost on problems with dense transition matrices where most states are reachable in one step (which is the case of a variety of control problems with exogenous events), as we demonstrate in this paper. One approach to overcome this caveat is to exploit the regularities in the domain dynamics, reward and value function throughout factored representation and calculations. In this paper, we propose a new  factored RTDP algorithm, called FactRTDP, and its approximate version, called aFactRTDP, which is the first straight forward factored version of enumerative RTDP, i.e., without performing generalized updates. Experiments show that these new algorithms can deal with dense transition matrices and have good online behavior when compared to the best probabilistic planning systems, without engineering optimizations, but by simply exploiting factored backups."
When Block Meets Group: Structured Sparse Modeling For Learning,"This paper proposes a novel framework, the {\it block/group sparse coding} ($\bg$), for dictionary learning.  Two important features distinguish $\bg$ from other existing methods in that all dictionary blocks are trained simultaneously with respect to each data group and instead of the inter-block coherence, the intra-block coherence is explicitly minimized as an important objective.  We provide both empirical and heuristic evident for this latter novel feature that can be regarded as the consequence of using the group structure for the data and the block structure for the dictionary.  The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-coordinates descent, and the details of the optimization algorithms are presented.  We evaluate the proposed method on several classification (supervised) and clustering (unsupervised) problems using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed $\bg$ algorithm."
Recurrent Gradient Temporal Difference Networks,"Temporal-difference (TD) networks (Sutton and Tanner, 2004) are a predictive representation of state in which each node is an answer to a question about future observations or questions. Unfortunately, existing algorithms for learning TD networks are known to diverge, even in very simple problems. In this paper we present the first sound learning rule for TD networks. Our approach is to develop a true gradient descent algorithm that takes account of all three roles performed by each node in the network: as state, as an answer, and as a target for other questions. Our algorithm combines gradient temporal-difference learning (Maei et al., 2009) with real-time recurrent learning (Williams and Zipser, 1994). We provide a generalisation of the Bellman equation that corresponds to the semantics of the TD network, and prove that our algorithm converges to a fixed point of this equation. We also demonstrate empirically that our learning algorithm converges to the correct solution in a benchmark problem for which prior learning rules diverge."
Split and Approximate in Monte Carlo,"We advance a general approach to increase the efficiency of Monte Carlo algorithms for estimating multi-dimensional integrals: split the integral into two nested parts; treat the outer integral by sampling while computing the inner integral to within a guaranteed approximation ratio. We give a unifying view of some previously presented specializations of this approach, provide analytical justifications and guidelines for algorithm design, and demonstrate the power of the approach in Bayesian structure discovery in Bayesian networks."
Algorithms for finding the source of an outbreak and other epidemic inference problems,"During the course of a disease outbreaka fundamental problem in public health planning is to use surveillance data(e.g., partial subset of people who became infected) to solve variousepidemic inference problems, e.g., what is the probability that a givenindividual was the source of the outbreak, or is infected, or the expectednumber of infections (conditional on the observed infections). We developa systematic approach to formulate these problems in terms of random generationof subgraphs with specific connectivity constraints, by means of an equivalencebetween disease transmission in complex networks and the edge percolationprocess. We then use a Markov chain approach for random sampling of suchsubgraphs. We develop efficient analytical bounds on the mixing time, i.e., thetime needed for the Markov chain to reach close to its stationary distribution.We study the empirical performance of our approach on different graphs, andfind that it is able to determine the node infection probabilities quiteaccurately."
Limits of Adaptation in Crowdsourcing,"Crowdsourcing systems, where numerous tasks are electronically distributed to an unidentified pool of workers through an open call, has emerged as an effective tool for human-powered solving of data intensive tasks such as image classification, video annotation, product categorization, and transcription. Since these low-paid workers can be unreliable, all crowdsourcers need to devise a way to cope with the errors and ensure a certain reliability in their answers. A common solution is to add redundancy by asking each question to multiple workers and combining their answers using some scheme such as majority voting. A fundamental question of interest for such systems is how much redundancy is necessary to achieve a certain accuracy in our answers? In this paper, we investigate the fundamental limit on the minimum number of queries necessary to achieve the target error probability. In particular, we want to identify how much we can gain by switching to an adaptive algorithm from an existing low-complexity and non-adaptive algorithms. To establish  this result, we provid a lower bound on the probability of error achieved by the optimal adaptive algorithm.  Compared to a known upper bound for a practical and non-adaptive algorithm, this shows that there is no significant gain in using adaptive algorithms. In terms of the budget required to achieve the target error probability, the gain of using an adaptive scheme is at most a constant factor. "
Distributed Reinforcement Learning for Policy Synchronization in Infinite-Horizon Dec-POMDPs,"In many multi-agent tasks, agents face uncertainty about the environment, the outcomes of their actions, and the behaviors of other agents. Dec-POMDPs offer a powerful modeling framework for sequential, cooperative, multiagent tasks under uncertainty. Solution techniques for infinite-horizon Dec-POMDPs have assumed prior knowledge of the model and have required centralized solvers.  We propose a method for learning infinite-horizon Dec-POMDP solutions in a distributed fashion.  We identify the issue of policy synchronization that distributed learners face and propose incorporating rewards into their learned model representations to both ameliorate this issue and to improve the quality of the agents' learned models. Most importantly, we show that even if rewards are not visible to agents during policy execution, exploiting the information contained in reward signals during learning is still beneficial."
Video-based Object Recognition by Sets of Sets,"We address the problem of automatic object recognition in videos, where users move their mobile camera around an unknown object of interest in order to capture more information in a random manner. Using videos that capture variations in an object's appearance due to camera motions (viewpoints and scales), cluttering and lighting conditions, can accumulate evidences and improve object recognition accuracies. Most previous works have taken a single image as input, or tackled a video by a collection i.e. sum of frame-based recognition scores. In this paper, we explore two novel representations and matching methods of videos for object recognition beyond frame-based recognition: 1) Video is first represented as a set of frames, in which each frame itself is a set of detected feature points (SURF in our experiments); 2) Each feature point in the initial frame is tracked in following frames, which forms one trajectory containing a set of similar feature points, therefore a video is a set of trajectories. Each representation forms sets of sets. We combine bag-of-words (for a set of data spatially distributed) and manifolds method (for a set of data with temporal smooth changes) to depict the two set-of-set representations. Also we propose how to match such representations for object recognition. The proposed representation and matching techniques are evaluated on our video data sets, which contain 830 videos of ten objects and four environment variations. The experiments on the challenging data set show that our proposed solution significantly outperforms the traditional frame-based methods."
Bayesian models for Large-scale Hierarchical Classification ,"A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods. "
Recovery of Sparse Probability Measures via Convex Programming,"We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. It's well-known that the classical $\ell_1$ regularizer fails to promote sparsity on the probability simplex since $\ell_1$ norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on $\ell_1$ norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm."
Efficient Learning in (Recurrent) Helmholtz Machines,"In this paper, we revisit the Helmholtz machine architecture as a generative model for both stationary data and sequences with long term time dependencies. We pro- pose an efficient method for calculating the gradient of the free energy by back- propagating its first and second derivatives through the model?s Gaussian genera- tive and recognition passes, as such providing an alternative to both the previous wake-sleep algorithm and REINFORCE. While not necessarily competitive with recent techniques from the deep learning community on stationary distributions, the resulting algorithm?s quick unbiased sampling procedure renders the method tractable on complicated sequential video tasks."
Decomposing information,"How can the information contained in a group of random variables be decomposed? Ideally, we would like to understand to what extent different subgroups provide the same, i.e. redundant, information, carry unique information or interact for the emergence of synergistic information.  So far, no convincing solution has been found, that captures our intuitions behind these concepts.Motivated by recent results due to Williams and Beer we discuss natural properties that such an information decomposition should have. We proof that some of these properties contradict each other, and we illustrate further puzzling aspects of shared information. We conclude that intuition and heuristic arguments might not suffice when thinking about information."
Multiple Operator-valued Kernel Learning,"Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces."
MAP Inference on Million Node Graphical Models: KL-divergence based Alternating Direction Method,"We consider the problem of maximum a posteriori (MAP) inference in graphical models with millions of nodes. We present a parallel primal MAP inference algorithm called KL-ADM based on two ideas: tree-decomposition of a graph, and the alternating direction method (ADM). However, unlike the standard ADM, we use an inexact ADM augmented with a Kullback-Leibler (KL) divergence based regularization. Theunusual modification leads to an efficient iterative algorithm while avoiding double-loops. We rigorously prove global convergence of the KL-ADM algorithm. The proposed algorithm is extensively evaluated on simulated datasets and compares favorably to existing approximate MAP inference algorithms. We also implement parallel KL-ADM using Open MPI and the experimental results on a drought detection problem withmore than 7 million variables demonstrate that the algorithm scales nicely in the multicore setting."
An efficient feature allocation for parallel stochastic optimizations with lazy updates,"  This paper proposes an efficient feature allocation algorithm to  accelerate parallelized stochastic optimization algorithms using  lazy updates for large-scale sparse data. Our key observation is  that a feature allocation governs efficiency of parallelized lazy  update algorithms. In fact, in the worst case, the total  computational cost of the parallelized algorithm is the same or even  worse than that of non-parallelized (single-core) algorithms. This  paper formulates the feature allocation problem as a specific form  of variable assignment problem whose optimal feature allocation  minimizes the computational cost for parallelized lazy  updates. Since the assignment problem itself requires large  computational cost, we propose two efficient algorithms using  randomization and a greedy search."
{Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning,"We consider the estimation of an i.i.d.\ vector $\xbf \in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possiblynonlinear) measurement channel. We present a method, calledadaptive generalized approximate message passing(Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\xbf$.The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriorsin the E-steps are computed via approximate message passing.The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yieldasymptotically consistent parameter estimates, which implies that the algorithmachieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic,general and computationally efficient methodapplicable to a large range of complex linear-nonlinearmodels with provable guarantees."
A Better Way to Pre-Train Deep Boltzmann Machines,"We describe how the pre-training algorithm for Deep Boltzmann Machines (DBMs) is related to the pre-training algorithm for Deep Belief Networks and we show that under certain conditions, the pre-training procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pre-training DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pre-training algorithm allows us to learn better generative models."
EEG single-trial detection in a rapid serial visual presentation paradigm task with supervised spatial filtering,"The detection of single-trial event related potentials (ERPs) in the electroencephalogram (EEG) is a difficult problem that requires efficient signal processing and machine learning techniques due to the poor spatial resolution and low signal-to-noise ratio of the EEG signal. Among the different steps that are typically used for the detection, spatial filtering is an important part. Spatial filtering allows enhancing the relevant information in the signal by combining the signal recorded across the different sensors. We propose a neural network with a convolutional layer dedicated to spatial filtering (CNN) for the detection of ERPs. The method is compared with a method based on the maximization of the signal-to-signal-plus-noise ratio (xDAWN) and common spatial pattern (CSP) that maximizes the discriminative activity to the common activity ratio for the creation of spatial filters. The two latter methods are combined with a neural network (MLP) for the classification. We have compared these methods with an MLP without spatial filtering as pre-processing. These techniques were evaluated on a rapid serial visual presentation (RSVP) task where eight participants had to detect faces from car images. The mean area under the ROC curve (AUC) is 0.843, 0.810, 0.753, and 0.820 for CNN, xDAWN+MLP, CSP+MLP, and an MLP without spatial filtering, respectively."
Towards a learning-theoretic analysis of spike-timing dependent plasticity,"This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli."
"On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution","We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition."
"Predicting Human Gaze Using Low-, Object- and Social- Saliency: A Dataset and Computational Models","Previous models to predict where people look in natural scenes focused on low-level image features. To bridge the semantic gap between the predictive power of computational saliency models and human behavior, we propose a new saliency architecture that incorporates information at three layers: low-level image features, object-level features, and social-level features. Object- and social-level information is frequently ignored, or only a few sample object categories are discussed where scaling to a large number of object categories is not feasible nor neutrally plausible. To address this problem, this work constructs a principled vocabulary of basic attributes to describe object- and social-level information thus not restricting to a limited number of object categories. We build a new dataset of 700 images with eye tracking data of 15 viewers and annotation data of 5551 segmented objects with fine contours and 12 social attributes (publicly available with the paper). Experimental results demonstrate the importance of the objectand social-level information in the prediction of visual attention."
Taxonomic Prediction with Tree-Structured Covariances,"Taxonomies are natural structures for representing the relationships between concepts, and are useful sources of prior information to learning algorithms.  The use of taxonomies may give a statistical improvement, in that training data present in nearby classes may be leveraged to effectively increase the sample size of all classes.  Taxonomies may improve performance by serving as a modified regularizer: the taxonomic structure may guide selection from the set of possible prediction functions by indicating that risky sets of functions are those that have very different values for nearby classes.In this work, we explore taxonomic prediction in the structured output setting using joint kernel maps following Cai and Hofmann (2004).  In particular, we relate taxonomic structured prediction to two key concepts, (i) tree structured covariance matrices, and (ii) non-parametric dependence measures.  We show that the joint kernel map for taxonomic prediction is tightly coupled to the concept of a tree-structured covariance matrix, and that Tikhonov regularization results in regularization by a special case of the Hilbert-Schmidt Independence Criterion (HSIC).  Using these concepts, we derive a family of highly computationally efficient algorithms for learning with arbitrary covariance matrices over output classes and evaluate its computational and empirical performance in a number of structured prediction settings."
Detecting Local Manifold Structure for Unsupervised Feature Selection,"Unsupervised feature selection is fundamental in statistical pattern recognition, and has drawn persistent attention in the past several decades. Recently, much work have shown that feature selection can be formulated as nonlinear dimensionality reduction with discrete constraints. This line of research emphasizes the manifold learning techniques, where the Laplacian eigenmap has been extensively studied. In this paper, we propose a new feature selection perspective from locally linear embedding (LLE), which is another popular manifold learning method. Our algorithm, called locally linear selection (LLS), can select the feature subset which optimally represents the underlying data manifold. We further develop a locally linear rotation-selection (LLRS) algorithm which extends LLS to identify the optimal coordinate subset from a new space. Experimental results on five real-world datasets show that our method can be more effective than Laplacian eigenmap based feature selection methods. "
Fitting community models to large sparse networks,"Many algorithms have been proposed for fitting network models with communities but most of them do not scale well to large networks, and often fail on sparse networks.   Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees.   We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs.  We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood.   "
Learning Manifolds with K-Means and K-Flats,"We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-?ats, and analyze their performance. We extend previous results  for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-?ats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-?ats, both the results and the mathematical tools are  new."
Multivariate discrete kernels and representations for sequence data,"String kernel-based machine learning methods have yielded great success in practical tasks of structured/sequential data analysis. They often exhibit state-of-the-art performance on tasks such as document topic elucidation, music genre classification, protein superfamily and fold prediction.However, typical string kernel methods rely on analysis of discrete 1D (univariate) string data (e.g., amino acid sequences, word sequences, codeword sequences, etc).This work introduces new {\em multivariate (2D)}  representations and {\em multivariate (2D) string kernel} methods for data in the form of sequences of feature vectors(as in music MFCC sequences, biological sequence profiles, or image sequences).On three music classification tasks as well as protein sequence classification proposed multivariate (2D) representations and kernels show significant 25-40\% improvements compared to traditional codebook learning and existing state-of-the-art sequence classification methods."
Convex Collective Matrix Factorization,"In many realistic applications, multiple interlinked sources of data are available and they cannot be easily represented in the form of a single matrix. Collective matrix factorization has recently been introduced to improve generalization performances by jointly factorizing multiple relations or matrices. In this paper, we extend the trace norm for matrix factorization to the collective matrix factorization case. This norm defined on the space of relations is used to regularize the empirical loss, leading to a convex formulation of the problem. Similarly to the trace norm on matrices, we show that the collective-matrix completion problem admits afast iterative singular-value thresholding algorithm.The collective trace norm is also characterized as a decomposition norm, usefulto find an optimal solution thanks to an unconstrained minimization procedure. Empirically we show that stochastic gradient descent suits well for solving theconvex collective factorization even for large scale problems. We also show thatthe proposed algorithm directly solving the convex problem is muchfaster than unconstrained gradient minimization optimizing in the space of low-rankmatrices."
A Polynomial-time Form of Robust Regression,"Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach compared to standard methods."
Learning Probability Measures with respect to  Optimal Transport Metrics,"We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures."
A Flexible Integer Linear Programming Formulation of Hierarchical Clustering,"In this paper we formulate hierarchical clustering as an integer linear programming (ILP) problem.  We present algorithmic results for our objective, showing that  a special case can be solved exactly in polynomial time (using an linear programming relaxation) and provide approximationschemes for the general case.  We show the flexibility of our approach by removing the transitivity constraint typically required of hierarchies, so we can learn hierarchies that contain overlapping clusterings.  Our experiments showed that our formulation is capable of outperforming standard agglomerative clustering algorithms in a variety of settings, including traditional hierarchical clustering as well as learning overlapping clusterings."
An Online Learning Algorithm for Multi-valued Function Learning,"In this paper we introduce a learning algorithm based on an infinite mixture of linear experts (IMLE) that is able to properly learn multi-valued functions. It consists of a generative model similar to the one found in Xu et al.~\cite{xu1995ame}, together with a set of priors that improve the algorithm versatility and performance, while providing some regularization of the parameters being learned. It is trained by a generalized Expectation-Maximization algorithm in an online, incremental fashion that can automatically grow the number of active components of the mixture as needed. Contrary to most state-of-the-art function approximation algorithms, IMLE can successfully learn multi-valued functions, and it equals or even outperforms popular online learning algorithms in single-valued prediction tasks."
Fast and Scalable Online CCA for Realtime Impact Analysis of Social Media Data,"The dynamics of temporal dependencies in time series of web graphs can be used to study the influence of single web sources on other web sources. Previous approaches to analysis of temporal dynamics in web graph data were either based on simple and manually tuned heuristics or not designed for online applications with massive amounts of data. Here we propose a simple but efficient and robust online learning approach to canonical correlation analysis. We show that the algorithm converges to the optimal canonical correlations and canonical variates. Using online CCA for canonical trend analysis we can a) assess the impact of a single node on all other nodes, b) explore the temporal dynamics of this impact and c) interpret the features (e.g. in BoW space) that gave rise to an information cascade. We provide preliminary results showing that we can efficiently estimate  canonical trends and thus assess the impact of single users in realtime on large data streams of web data obtained from the social network Twitter. Our results represent a first step beyond simple heuristics and towards an automatized content based realtime impact analysis for large scale data such as social networks activity."
Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting,"This paper proposes an efficient on-line learning algorithm to track the smoothing functions of additive models. The key idea is to combine the linear representation of additive models with a recursive least squares filter. In order to quickly track model changes and put more weight on recent data, the recursive least squares filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behavior is further enhanced by using an adaptive forgetting factor which is updated based on the gradient descent method, with the maximum admissible value of the learning rate provided by Lyapunov stability theory. The algorithm is applied to additive models tracking 6 years of electricity demand data from the French utility company EDF (Electricite de France). Compared to state-of-the-art methods, it achieves a superior performance in terms ofprediction accuracy."
Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High Dimensional Results,"Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. This paper develops efficient OMP-like algorithms to deal with precisely this setting. Our algorithms are as efficient as OMP, and improve on the best-known results for missing and noisy data in regression, both in the high-dimensional setting where we seek to recover a sparse vector from only a few measurements, and in the classical low-dimensional setting where we recover an unstructured regressor. In the high-dimensional setting, our support-recovery algorithm {\it requires no knowledge} of even the statistics of the noise. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise."
Statistical inference in compound functional models,"We consider a general nonparametric regression model called the compoundmodel. It includes, as special cases, sparse additive regression and nonparametric(or linear) regression with many covariates but possibly a small number of relevantcovariates. The compound model is characterized by three main parameters: thestructure parameter describing the macroscopic form of the compound function,the microscopic sparsity parameter indicating the maximal number of relevant covariatesin each component and the usual smoothness parameter corresponding tothe complexity of the members of the compound. We find non-asymptotic minimaxrate of convergence of estimators in such a model as a function of these threeparameters. We also show that this rate can be attained in an adaptive way."
Exponential Concentration for Mutual Information Estimation with Application to Forests,"We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph."
Augment-and-Conquer Negative Binomial Processes,"By developing augment-and-conquer methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters."
Transferring Expectations in Model-based Reinforcement Learning,"We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without pre-defined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. "
Minimization of Continuous Bethe Approximations: A Positive Variation,"We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions.  While existing message passing algorithms define fixed point iterations corresponding to stationary points of the Bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge. For continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as Gaussians with negative variance. Conversely, our approach leverages multiplier methods with well-understood convergence properties,and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random fields, showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation."
Synergy is the whole minus the union of the parts,"Quantifying cooperation among random variables in predicting a single target random variable is an important problem in biological systems with 10s to 1000s of co-dependent variables.  A common definition of synergy within these system is based on the intuition that synergy is the difference between the whole and the sum of the parts. We introduce *synergistic mutual information* as the sum minus the union of the parts. We compare both measures against a set of pedagogical examples. We conclude that in the presence of redundant information, subtracting the sum underestimates true synergy."
Object Classification with Attributes as Side Information,"This paper proposes a new approach to incorporate attributes as side information for object classification. Attributes are a list of semantically meaningful properties that are available only during training. Instead of predicting these attributes during testing with attribute classifiers, we utilize attributes as side information to improve learning the category classifier on the primary features in a learning with side information paradigm. We propose a novel approach for learning with side information based on the assumption that the posteriors of the label using side information and primary features are close. With the proposed approach, we develop the corresponding learning methods for logistic regress model with L2 regularization. Experiments demonstrate the effectiveness of our approach in classification performance."
Non-linear Metric Learning,"In this paper, we introduce two novel metric learning algorithms, ?2-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: ?2-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear ?2-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of ?2-LMNN, obtain best results in 19 out of 20 learning settings. "
Factorial LDA: Sparse Multi-Dimensional Text Models,"Multi-dimensional latent variable models can capture the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional latent variable model in which a document is influenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientific discipline, and focus (e.g. methods vs. applications.) Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors."
Joint Modelling of Structural and Functional Brain Networks,"Functional and structural magnetic resonance imaging (fMRI and dMRI) have become the most important noninvasive windows into the human brain. A major challenge in the analysis of brain networks is to establish the similarities and dissimilarities between functional and structural connectivity. We formulate a nonparametric Bayesian network model which allows for joint modelling and integration of multiple networks. We demonstrate the model?s ability to detect vertices that share structre across networks, both on artificial data as well as joint analysis of fMRI and dMRI data. Using two fMRI and dMRI scans per subject, we establish significant structures that are consistently shared across subjects and data splits. This allows us to unsupervisedly establish structure-function relationships in the brain and provides a general framework for multimodal integration."
Automating Collusion Detection in Sequential Games,"Collusion is the practice of two parties deliberately cooperating to the detriment of others.  While such behavior may be desirable in certain circumstances, in many it is considered dishonest and unfair.  If agents otherwise hold strictly to the established rules, though, collusion can be challenging to police.  In this paper, we introduce an automatic method for collusion detection in sequential games.  We achieve this through a novel object, called a collusion table, that aims to capture the effects of collusive behavior, i.e., advantage to the colluding parties, without committing to any particular pattern of behavior.  We demonstrate the effectiveness of this method in the domain of poker, a popular game where collusion is prohibited."
A Unified Framework for Probabilistic Component Analysis,"In this paper we attempt to unify many very popular and well-studied componentanalysis algorithms, such as Principal Component Analysis (PCA), LinearDiscriminant Analysis (LDA), Locality Preserving Projections (LPP) and SlowFeature Analysis (SFA) under a single, probabilistic framework. We firstly showthat the projection directions produced by all the above mentioned methods arealso produced by the Maximum Likelihood (ML) solution of a single joint probabilitydensity function (pdf), just by choosing the appropriate prior over the latentspace. Subsequently, we propose novel Expectation Maximization (EM) algorithmsutilising the proposed joint pdf. Experimental results show the usefulnessof the proposed EM framework in both simulated and real world data."
Modelling Reciprocating Relationships,"We present a Bayesian nonparametric model that discovers implicit socialstructure from interaction time-series data.Social groups are often formed implicitly, through actions among members ofgroups.Yet many models of social networks use explicitly declared relationships toinfer social structure.We consider a particular class of Hawkes processes, a doubly stochastic pointprocess, that is able to model reciprocity between groups of individuals.We then extend the Infinite Relational Model by using these reciprocatingHawkes processes to parameterise its edges, making events associated with edgesco-dependent through time.Our model outperforms general, unstructured Hawkes processes as well as structuredPoisson process-based models at predicting verbal and email turn-taking, andmilitary conflicts among nations."
Expectation Propagation in Gaussian Process Dynamical Systems,"Rich and complex time-series data, such as those generated from engineering sys-tems, financial markets, videos or neural recordings are now a common feature ofmodern data analysis. Explaining the phenomena underlying these diverse datasets requires flexible and accurate models. In this paper, we promote Gaussianprocess dynamical systems as a rich model class appropriate for such analysis. Inparticular, we present a message passing algorithm for approximate inference inGPDSs based on expectation propagation. By phrasing inference as a general mes-sage passing problem, we iterate forward-backward smoothing. We obtain moreaccurate posterior distributions over latent structures, resulting in improved pre-dictive performance compared to state-of-the-art GPDS smoothers, which are spe-cial cases of our general iterative message passing algorithm. Hence, we providea unifying approach within which to contextualize message passing in GPDSs."
A quasi-Newton proximal splitting method,"We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification."
Hessian-Free Optimization for Long Short-Term Memory,"The application of $2^{nd}$-order optimization techniques to overcome the inherent limitations of gradient-based learning in Recurrent Neural Networks have proven to be quite succesful. However, comparisons between Hessian-Free optimization and LSTM trained by stochastic gradient descent represent a false equivalence. The many non-linear units and gating cells in LSTM can greatly benefit from $2^{nd}$-order learning methods, without negating the powerful role played by the constant error carousel in providing . In this paper, we have presented HF-optimization for LSTM, in order to more accurately compare performance with standard RNNs which have been similarly trained. On the chosen test set, we find that LSTM outperforms RNNs by solving the task up to a full order of magnitude more quickly. Admittedly, this is a narrow comparison overall, and we intend to run experiments across a number of pathological test cases, as well as real world examples. "
Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems ,"We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes.More recently, an asymptotic regret bound of $\tilde{O}(\sqrt{T})$ was shown for $T \gg p$ where $p$ is the dimension of the state space.In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large.We present an adaptive control scheme that for $p \gg 1$ and $T \gg \polylog(p)$ achieves a regret bound of $\tilde{O}(p \sqrt{T})$.In particular, our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$.This is in comparison to previous work on the dense dynamics where the algorithm needs $\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy.We believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks."
Analysis of Algorithms for the Memory Hierarchy,"Batch gradient descent looks at every data point for everystep, which is wasteful for early steps where the current position isnowhere near optimal.There has been a lot of interest in warm-start approaches to gradientdescent techniques, but little analysis. In this paper, we formallyanalyze a method of warm-starting batch gradient descent using smallbatch sizes. We argue that this approach is fundamentally differentthan mini-batch, in that after an initial shuffle, it requires only sequential passes over thedata, improving performance on datasets stored on a disk drive. We also analyze sequential gradient descent."
Generalized quadratic models and moment-based neural dimensionality reduction,"A popular approach for investigating the neural code is to identify a low-dimensional subspace of stimuli that modulate a neuron's response. Here we describe a set of methods for neural characterization based on Generalized Quadratic Models (GQMs). These models contain a low-rank quadratic form Q that defines the neural subspace, a nonlinear transfer function f, and an exponential-family distribution function P. Special cases include the 2nd order Volterra model (with linear f and Gaussian P), the elliptical-LNP model (with arbitrary f and Poisson P), and the quadratic-logistic regression model (for logistic f and Bernoulli P).  Here we show that for ``canonical form'' GQMs, the first two response-weighted moments yield simplified maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes spike-triggered covariance analysis to analog and binary response data, and provides closed-form estimators under a variety of non-Gaussian stimulus distributions. In the linear-Gaussian case, we show that the corresponding estimator depends only on the first four moments of the raw stimulus distribution.  Finally, the GQM extends generalized linear models (GLMs) to allow multi-dimensional dependence on spike history.  We apply these methods to simulated and real neural data from retina and V1."
A Bias-Variance Analysis of Model-Based Estimation in Reinforcement Learning ,"This paper provides the first bias and variance characterization of the model-based value function estimator in finite-horizon reinforcement learning (RL) problems with discrete state spaces. The closed-formed formulas we derive to estimate the bias and variance rely on an approximation that is exact if the estimates of the transition model, reward model, and value function are normally distributed. These results can be used to characterize performance of RL systems in a wide range of application domains. We are particularly interested in applications concerning resource management domains, and therefore we include experiments demonstrating the use of our estimators to evaluate strategies for population management of animal species. We find the bias/variance estimates produced by our method to be more accurate than those produced by the well-known bootstrap or jackknife estimators. We also compare our results to the bias-variance analysis of Mannor et al. [2007], and show that even in their setting (infinite-horizon, discounted problems), our approach may be preferable."
Online Multi-Task Collaborative Filtering,"Traditional model based approaches for Collaborative Filtering are often based on batch learning algorithms, which assume all labeled data are given a priori before the learning tasks and the models often have to be re-trained when new data arrives in a recommendation task. Such techniques have several critical limitations, e.g., low efficiency and poor scalability for large-scale online applications. Recently, online collaborative filtering (OCF) has emerged as a promising technique to overcome the limitations, which sequentially learns the model over a sequence of data in an online learning fashion. Despite the advantage of high efficiency, the existing approach to OCF using a simple online gradient descent algorithm suffers from slow convergence. In this paper, we propose a new online collaborative filtering framework, that is, online multi-task collaborative filtering (OMTCF), which tackles the online collaborative filtering task by exploiting the idea of online multi-task learning. Unlike the existing OCF approach, OMTCF, by defining a user interaction matrix, effectively updates the models of multiple users simultaneously at each learning iteration, which is able to converge significantly faster. Encouraging results on real-world datasets show that the proposed technique is considerably more effective than the state-of-the-art OCF algorithm."
Multilabel Classification using Bayesian Compressed Sensing,"In this paper, we present a Bayesian framework for multilabel classification using compressed sensing. The key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key benefits of the model are that a) it can naturally handle datasets that have  missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model."
On Computational Feasibility of Mapping Kernels,"The mapping kernel framework has proven useful to design kernels for discrete structures.The resulting kernelsare positive definite, and can be efficiently computed. Recently, a certain class of string kernels,called partitionable kernels, was found to havetight relation to the computational feasibility of mapping kernels.Also, it turns outonly a small portion of the entire partitionable kernelshave been actually used in the literature, and most of them remain unused.In this paper, we shed light on the unexplored area of partitionable kernels, and show interesting and useful propertiesof a certain subclass of partitionable kernels,which is important from both the theoretical and practical points of view."
Scaling Constrained Continuous Markov Random Fields with Consensus Optimization,"We study scaling a class of probabilistic graphical models well-suited to constrained, continuous domains. We show how to solve the most-probable-explanation problem for these models with a consensus-optimization framework. We derive closed-form solutions for consensus-optimization subproblems induced by several types of common dependencies. We improve the performance of consensus optimization by deriving an algorithm that can additionally find closed-form solutions to subproblems in certain cases, depending on the current optimization iterate, not just the subproblem itself. We demonstrate superior performance of our approach over commercial interior-point methods, the current state-of-the-art for the problems we study. In fact, in our evaluation our method scales linearly with the size of the problem."
Query Complexity of Derivative-Free Optimization,"Derivative Free Optimization (DFO) is attractive when the objective function's derivatives are not available and evaluations are costly.   Moreover, if the function evaluations are noisy, then approximating gradients by finite differences is difficult.  This paper gives quantitative lower bounds on the performance of DFO with noisy function evaluations, exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients. This challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient.  However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions.  A distinctive feature of the algorithm is that it only uses Boolean-valued function comparisons, rather than evaluations.  This makes the algorithm useful in an even wider range of applications, including optimization based on paired comparisons from human subjects, for example.  Remarkably, we show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same."
Emergence of Object-Selective Features in Unsupervised Feature Learning," Recent work in unsupervised feature learning has focused on the goal  of discovering high-level features from unlabeled images.  Much  progress has been made in this direction, but in most cases it is  still standard to use a large amount of labeled data in order to  construct detectors sensitive to object classes or other complex  patterns in the data.  In this paper, we aim to test the hypothesis  that unsupervised feature learning methods, provided with only  unlabeled data, can learn high-level, invariant features that are  sensitive to commonly-occurring objects.  Though a handful of prior  results suggest that this is possible when each object class  accounts for a large fraction of the data (as in many labeled  datasets), it is unclear whether something similar can be  accomplished when dealing with completely unlabeled data.  A major  obstacle to this test, however, is scale: we cannot expect to  succeed with small datasets or with small numbers of learned  features.  Here, we propose a large-scale feature learning system  that enables us to carry out this experiment, learning 150,000  features from tens of millions of unlabeled images.  Based on two  scalable clustering algorithms (K-means and agglomerative  clustering), we find that our simple system can discover features  sensitive to a commonly occurring object class (human faces) and can  also combine these into detectors invariant to significant global  distortions like large translations and scale."
Annotation on the cheap,"We consider the task of producing a high-quality labeling of a new data set, given access to a human annotator who is to be used sparingly. Our approach involves the active learning of a classifier that is allowed to abstain on difficult inputs."
"Burn-in, bias, and the rationality of anchoring","Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as burn-in. Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks. Our theoretical and empirical results suggest that the anchoring bias is consistent with approximate Bayesian inference."
Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes,"Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning.  In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics."
Stochastic Optimization of the Variational Bound,"We present an algorithm for performing posterior inference by stochastically optimizing a variational lower bound.  Importantly, this algorithm circumvents symbolic evaluation of the variational lower bound and requires only an unnormalized likelihood function $p(x, y)$, making the benefits of variational inference accessible to casual practitioners.  We compare this algorithm with MCMC and demonstrate that it provides a fast, simple alternative.  We also demonstrate that this opens the door to a variety of variational posteriors which were previously unexplored."
A Neural Autoregressive Topic Model,"We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm."
On the convergence and optimality of optimistic approximate policy iteration,"A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. While the former is often considered to possess practical advantages over the latter, there is, in the interactive case, currently little understanding on its behavior in the proximity of an optimum; after certain amount of guaranteed improvement, the learning process can become trapped in sustained oscillation or chattering, or it can converge to a solution with rather unknown properties. In this paper, we provide insight on the convergence behavior of the general, optimistic form of the greedy methodology by reflecting it against the policy gradient approach. First, we consider an important effect that approximations, either in state estimation or in value function representation, have on policy evaluation, and discuss how this effect defines the natural choice of methodology for policy improvement. Second, we use a recently proposed explanation to the policy oscillation phenomenon and extend it to cover also the optimistic hard-greedy case and the associated policy chattering phenomenon. Third, we show for a substantial subset of soft-greedy approaches that, while having potential for avoiding oscillation and chattering, this subset can never converge in any state to any optimal policy, except for certain pathological cases. We link this failure to an underlying incorrect interpretation of the value function and illustrate it with a minimal artificial example. Finally, we show that as softness and the step size are decreased together toward zero, the general form of Gibbs/Boltzmann soft-greedy optimistic policy iteration using an advantage function becomes, in the limit, equivalent with the natural actor-critic algorithm."
Network floods reveal regulatory control flows and minimal networks in synthetic and bacterial datasets,"Biological networks tend to have high interconnectivity, complex topologies and multiple types of interactions. This renders difficult the identification of sub-networks that are involved in condition-specific responses. In addition, we generally lack scalable methods that can reveal the information flow in gene regulatory and biochemical pathways. Doing so will help us to identify key participants and paths under specific environmental and cellular context. This paper introduces the theory of network flooding, which aims to address the problem of network minimization and regulatory information flow in biological networks. Given a regulatory biological network, and a set of source (input) and sink (output) nodes, our task is to find (a) the minimal sub-network that encodes the regulatory program involving all input and output nodes and (b) the information flow from the source to the sink nodes of the network. To this direction, we describe a novel, scalable, network traversal algorithm, and we demonstrate its ability to achieve significant network size reduction in both synthetic and E. coli networks, without disrupting the core regulatory pathways. "
Efficient Inference and Learning of switching Kalman filters and their Application to Gesture Recognition,"Computational models for high dimensional time series  such as video sequences, spectral trajectories of a speechsignal or the kinematic measurements of skilled human activity hold considerable interest,particularly models that capture the inherent stochastic variability in the signal.  The hidden Markov Model (HMM) is widely used  for modeling such data.  More complex models such as switching linear dynamical systems (S-LDS) account better for the continuity of the observations, which an HMM assumes to be conditionally independent, but they lack efficient learning and inference procedures.  This paper makes three advances to address these limitations\begin{enumerate}\item A previously known inference technique by Barber \cite{barber2006,mesot2007switching} is extended to S-LDS learning.  This extension provides computationally tractable EM-based estimation of S-LDS parameters.\item Under the diagonal assumption on the observation noise, a dynamic programming algorithm is proposed to speed up the per-frame inference-complexity from cubic to linear in the observation dimension.\item A system identification algorithm is provided for initializing the parameters of an S-LDS, leading to effective S-LDS learning.\end{enumerate}The effectiveness of the new algorithms is demonstrated in gesture recognition from kinematic measurements in a robot-assisted minimally invasive surgery (RMIS) task: S-LDS models show significant improvement in recognition accuracy over comparable factor analyzed HMMs.The ability to perform automatic gesture recognition in RMIS has several applications, such as assessing dexterity or manipulative skills during surgical training or providing guidance or assistance during tele-operated surgery."
A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes,"Parametric policy search algorithms are one of the methods of choice for the optimisation of Markov Decision Processes, with Expectation Maximisation and natural gradient ascent being considered the current state of the art in the field. In this article we provide a unifying perspective of these two algorithms by showing that their step-directions in the parameter space are closely related to the search direction of an approximate Newton method. This analysis leads naturally to the consideration of this approximate Newton method as an alternative gradient-based method for Markov Decision Processes. We are able show that the algorithm has numerous desirable properties, absent in the naive application of Newton's method, that make it a viable alternative to either Expectation Maximisation or natural gradient ascent. Empirical results suggest that the algorithm has excellent convergence and robustness properties, performing strongly in comparison to both Expectation Maximisation and natural gradient ascent."
On the Sample Complexity of Ranking,"Learning to rank is a core machine learning problem. When the truescoring functions are hard to learn or training data is scarce, thesample complexity for predicting a ranking with small error is ofconsiderable interest. We present a lower bound for such a samplecomplexity for any algorithm that estimates a broad class of scoringfunction based on randomly sampled binary comparisons. Additionally,we demonstrate two simple algorithms that achieve the bound inexpectation.  While one algorithm predicts rankings with roughlyuniform quality across the ranking, the other predicts more accuratelynear the top of the ranking than the bottom. Results are presented onsynthetic examples and on an application to epitope (peptide) ranking."
Near-Optimal MAP Inference for Determinantal Point Processes,"  Determinantal point processes (DPPs) have recently been proposed as  computationally efficient probabilistic models of diverse sets for a  variety of applications, including document summarization, image  search, and pose estimation.  Many DPP inference operations,  including normalization and sampling, are tractable; however,  finding the most likely configuration (MAP), which is often required  in practice for decoding, is NP-hard, so we must resort to  approximate inference.  Because DPP probabilities are  log-submodular, greedy algorithms have been used in the past with  some empirical success; however, these methods only give  approximation guarantees in the special case of DPPs with monotone  kernels.  In this paper we propose a new algorithm for approximating  the MAP problem based on continuous techniques for submodular  function maximization.  Our method involves a novel continuous  relaxation of the log-probability function, which, in contrast to  the multilinear extension used for general submodular functions, can  be evaluated and differentiated exactly and efficiently.  We obtain  a practical algorithm with a 1/4-approximation guarantee for a  general class of non-monotone DPPs.  Our algorithm also extends to  MAP inference under complex polytope constraints, making it possible  to combine DPPs with Markov random fields, weighted matchings, and  other models.  We demonstrate that our approach outperforms greedy  methods on both synthetic and real-world data."
SARSA Training of Deep Networks in Complex Games,"We SARSA-lambda trained a program based on deep neural nets to produce a program to compete in the 2011 AI Challenge Sponsored by Google. This was an extremely complicated video game, involving controlling the motions of hundreds of ants in real time as they played in novel game boards of up to 200 by 200 for 1000 steps with imperfect information. This may be the most complex domain ever attempted by reinforcement learning. A number of engineering methods are described that allowed us to finish near the top 10% of 9000 hand-coded human submitted entrants. We are extending our methods to produce a strong player of No Limit Texas Holdem."
Active Batch Selection via Convex Relaxations with Guaranteed Performance Bounds,"Batch mode active learning (BMAL) effectively reduces human annotation effort in training a reliable classifier by selecting batches of promising and exemplar instances from large quantities of unlabeled data. In this paper, we propose two novel BMAL algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP hard optimization problem; we then propose two convex relaxations, one based on linear programming (LP) and the other based on semi-definite programming (SDP) to solve the batch selection problem. Finally, a deterministic performance bound is derived for the first relaxation and a probabilistic bound for the second. Our extensive empirical studies on the UCI datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques and also deliver high quality solutions."
Probabilistic Low-Rank Subspace Clustering,"In this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we first develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in clustering and identifying outliers.  "
How They Vote: Issue-Adjusted Models of Legislative Behavior,"We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues.  Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space."
Density Propagation and Improved Bounds on the Partition Function,"Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds."
Learning to rank for data-driven image segmentation,"We propose to learn a similarity between images to estimate how similar are their segmentation masks. Our similarity is expressed as a dot product in a linear subspace, and we learn the subspace which minimizes the number of triplets for which there is a mismatch between the orderings imposed by our similarity and the true segmentation.By adapting the supervised semantic indexing framework \cite{Bai:09}, we derive a stochastic gradient descent (SGD) algorithm to efficiently learn the similarity. We also explore different strategies to impose large margins. Experiments on two detection scenarios demonstrate that(i) the learned similarity presents an improved ranking-by-segmentation ability; (ii) the fraction of images which are correctly detected just by direct transfer of the nearest-neighbor bounding boxes also increases. This has implications for data-driven segmentation approaches where a set of good neighbors is key to the segmentation algorithm. While existing approaches use appearance-based similarities to select the neighbors, our similarity directly optimizes the selection of good neighbors for segmentation. "
Towards Sparse Coding on Riemannian Manifolds via K-Geodesic Clustering,"In this paper we study the problem of sparse coding and dictionary learning on Riemannian manifolds. The extension of these concepts from Euclidean spaces is not straightforward. We first present a model comprised of geodesic submanifolds to represent manifold valued data. We present an algorithm called K-geodesic clustering to learn the model from the data. Then, we argue that this lends itself naturally to be considered as a dictionary model for sparse coding on manifolds. Then we propose an intrinsic and extrinsic approach for manifold sparse coding. Our experiments on human activity data show that our model fits the data more accurately compared to other classical approaches such as K-means clustering. We demonstrate the discriminatory power of the sparse codes in an activity recognition experiment and obtain accuracies that compare well with recent approaches proposed in the literature."
A Probabilistic Model for Joint Active Learning and Model Selection,"In active learning the goal is to train an accurate model using as few activelylabeled samples as possible. Most active learning methods do not perform modelselection because only one model is trained on the actively labeled samples. Wepresent a framework for active learning where multiple models are trained withdifferent regularization parameters. In this framework the labeled data needed formodel selection from these models is part of the total budget of labeled samples.This framework exposes a natural trade-off between the focused active samplingthat usually is most effective for training models, and the unbiased sampling that isdesirable to reliably estimate model accuracy for model selection. We present analgorithm that adds actively labeled samples to either a training set or to a set usedfor model selection, with the goal being to increase the accuracy of the best modelwith as few total samples as possible. We demonstrate the algorithm on three datasets and show that actively sampling both the train and hold out sets yields moreaccurate models with fewer labels than actively sampling the train sets alone."
Decoding Finger Flexion from Electrocorticographic Signals with Knowledge Based Prior Model,"Decoding by incorporating domain knowledge about the target variable (body movements in BCI) has been shown to be able to significantly improve the performance \cite{WangSJ11}. In the existing model, training a prior model to capture domain knowledge relies on training samples about the target variable. However, in most real BCI applications, brain signals are only collected under thoughts without actual body movements. Even though training sampels for the target variable are available, the model trained on which tends to be biased and has difficulty to generalize. In this paper, we train prior model by explicitly incorporating the domain knowledge without resorting to the training data. The experiment demonstrates its competitive performance."
Stochastic gradient descent  confers resistance to label noise,"This paper explains why machine learning algorithms using thestochastic gradient descent (SGD) algorithm sometimes generalizebetter than algorithms using other optimization techniques.  Weillustrate our point with artificial data sources on which using SGDwith the SVM objective function generalizes much more accurately thanan algorithm which performs more intensive optimization, over a widevariety of choices of the regularization parameters.  We also reporton some similar effects on natural data."
Distributed large-scale natural graph factorization,"More and more natural graphs arise in numerous Web applications, such as socialnetworks, email, and instant messaging. Such large-scalegraphs today easily contain hundreds of millions of nodes and billions of edges,and are expected to grow even larger in the future. While severaltheoretical models have been proposed for such networks, their analysisis still difficult due to their scale and nature. Inthis paper we propose a distributed framework for large-scale graph factorizationthat allows for easier analysis and more compact representation oflarge natural graphs. "
Perceptron Learning of SAT,"Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science.  In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem.  In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space.  Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm.  Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates.  We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT.  Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task."
Bellman Error Based Feature Generation using Random Projections on Sparse Spaces,"We address the problem of automatic generation of features for value function approximation.Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error.  Empirical results demonstrate the strength of this method."
Scalable Matrix-valued Kernel Learning and  High-dimensional Nonlinear Causal Inference,"We propose a general matrix-valued multiple kernel learning framework for high-dimensional nonlinear multivariate regression problems. This framework allows a broad class of mixed norm regularizers, including those that induce sparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel Hilbert Spaces~\cite{MichelliPontil05} induced by a collection of separable kernels. We develop a highly scalable and eigendecomposition-free Block coordinate descent procedure that orchestrates two inexact solvers: a Conjugate Gradient (CG) based Sylvester equation solver for solving vector-valued Regularized Least Squares (RLS) problems, and a specialized Sparse approximate SDP solver~\cite{HazanSDP} for learning output kernels. We show how high-dimensional causal inference tasks can be naturally cast as sparse function estimation problems within our framework, leading to novel nonlinear extensions of Grouped Graphical Granger Causality techniques. The algorithmic developments and extensive empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds."
Multiclass Learning  with Simplex Coding,"In this paper we dicuss a novel  framework for multiclass learning, defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classesa relaxation approach commonly used in binary classification.In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized methods with training/tuning complexity which is {\em independent} to the number of classes.Tools from convex analysis are introduced that can be used beyond the scope of this paper. "
FastEx: Fast Clustering with Exponential Families," Clustering is a key component in data analysis toolbox. Despite its  importance, scalable algorithms often eschew rich statistical models  in favor of simpler descriptions such as $k$-means clustering. In  this paper we present a sampler, capable of estimating  mixtures of exponential families. At its heart lies a novel proposal distribution using random  projections to achieve high throughput in generating proposals, which is crucial  for clustering models with large numbers of clusters. "
Using Both Supervised and Latent Shared Topics for Multitask Learning,"Since its introduction, Latent Dirichlet Allocation (LDA) has been extended to include two different types ofdocument-level supervision: topic labels and category labels. We introduce a new framework, Doubly SupervisedLatent Dirichlet Allocation (DSLDA), that integrates both types of supervision. We demonstrate thatthis approach is particularly useful for multitask learning, in which both supervised and latent (unsupervised)topics are shared between multiple categories. Experimental results on document classification show thatboth types of supervision improve the performance of DSLDA and that sharing both latent and supervisedtopics allows for better multitask learning."
A tree-decomposed EM algorithm for covariance selection in noisy graphical models,"Gaussian graphical models (GGMs) are widely used in computer science, and have also enjoyed wide applicability in a number of scientific areas. We consider the problem of covariance selection, i.e. estimation of the (inverse) covariance matrix of the joint probability distribution of random variables on a high dimensional graph. To extend the applicability of GGMs, we consider the case where observations for variables are also subject to additional measurement noise. Unfortunately, the the estimation of model parameters in this setting becomes complicated by the fact that the structure of the underlying graph no longer provides direct information about the location of zeros in the inverse covariance matrix. We propose an efficient EM algorithm which uses the tree decomposition of the underlying graph in order to perform the parameter estimation through local operations. We also explore the effect of the treelike structure of the graph on computational performance of the algorithm as well as the accuracy of the estimates by applying it to a wide range of random graph models."
Learning Label Trees for Probabilistic Modelling of Implicit Feedback,"User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. In the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data."
Learning with Recursive Perceptual Representations,"Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks."
Link Prediction in Graphs with Autoregressive Features,"In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm."
Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images,"We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity.To segment {\em biological} neuron membranes, we use a special type of deep {\em artificial} neural network as a pixel classifier.The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it.The input layer maps each window pixel to a neuron. It is followed by a succession ofconvolutional and max-pooling layers which preserve 2D information and extract features withincreasing levels of abstraction. The output layer produces a calibrated probability for each class.The classifier is trained by plain gradientdescent on a $512 \times 512 \times 30$ stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \emph{rand error}, \emph{warping error} and \emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer."
An MDL/Bayesian Approach without Assuming either Discrete or Continuous,"In the minimum description length (MDL) and Bayesian criteria, we construct description length  of data $z^n=z_1\cdots z_n$ of length $n$ such that the length divided by $n$ almost converges to its entropy rate as $n\rightarrow \infty$, assuming $z_i$ is in a finite set $A$. In model selection, if we knew the true probability $P$ of $z^n\in A^n$, we would choose a model $F$ such that the posterior probability of $F$ given $z^n$ is maximized. But, in many situations, we use $Q:A^n\rightarrow [0,1]$ such that $\sum_{z^n\in A^n}Q(z^n)\leq 1$ rather than $P$ because only data $z^n$ are available. In this paper, we consider an extension such that each of the attributes in data can be either discrete or continuous. The main issue is what $Q$ is qualified to be an alternative to $P$ in the generalized situations. We propose the condition in terms of the Radon-Nikodym derivative of $P$ with respect to $Q$, and give the procedure of constructing $Q$ in the general setting. As a result, we obtain the MDL/Bayesian criteria in a general sense. Numerical experiments demonstrate that the novel algorithm works efficiently enough to deal with many practical estimations."
Noise Never Helps ? Revisited !,"Compensating changes between a subjects? training and feedback sessions in Brain Computer Interfacing is challenging but of great importance for a robust BCI operation. We contribute by noting that such individual changes can be reliably estimated using data from other subjects. Surprisingly it is the non-discriminative ?noise? signal subspace that can aid to construct features invariant to the change. This is in contrast to e.g. averaging the covariance matrices or construction of a common feature space between users. Notably, the prominent directions of change are very similar between subjects in the noise subspaces, whereas in the most discriminative directions they are not. Our noise harvesting method compares favourably to other state-of-the-art methods on toy data and EEG recordings from five subjects performing motor imagery. We show that not only a significant increase in performance can be achieved, but also that the changes observed in the non-discriminative noise subspace allow for a neurophysiologically meaningful interpretation."
Gradient Weights help Nonparametric Regressors,"In regression problems over $\real^d$, the unknown function $f$ often varies more in some coordinates than in others.We show that weighting each coordinate $i$ with the estimated norm of the $i$th derivative of $f$ is an efficient way to significantly improve the performance of distance-based regressors, e.g. kernel and $k$-NN regressors. We propose a simple estimator of these derivative norms and prove its consistency. Moreover, the proposed estimator is efficiently learned online. "
Monte Carlo Tree Search Using Goal-Directed and Constraint-Biased Action Abstractions,"Monte Carlo Tree Search (MCTS) is a family of methods for planning in large domains. It focuses on finding a good action for a particular state, making its complexity independent of the size of the state space. Effective application of MCTS requires both a good pruning heuristic and a roll-out policy. In this paper we leverage the human tendency to decompose tasks into 1) goal-directed action sequences and 2) sets of constraints to generate stochastic policies that facilitateefficient exploration in MCTS.We demonstrate the efficacy of our approach in the PacMan domain and highlight its advantages over traditional MCTS."
Tree Learning Strategies for Large-Scale Taxonomies,"Standard linear models for multi-class categorization have a decision-time complexity which is linear in the number of categories, whereas approximating them by a tree-based sequence of decisions can reducethe prediction time to the logarithm of the number of categories. In this paper, we review several heuristics to build the best hierarchical taxonomyand propose a novel tree-learning approach by formulating the problem asa sequence of max-cut problems where the categories are split intosubcategories in a top-down fashion. We provide an empirical comparison on five different tree-building approaches on multiple datasets,showing that the previous approaches to learn the tree can significantly failif one is interested in the predictive log-likelihoods or on theaverage classification accuracies of the classifiers. "
Behaviorally decoding search targets from gaze fixations,"Using a technique that we refer to as behavioral decoding, we demonstrate that the information available in the fixation behavior of subjects is often sufficient todecode the category of their search target?essentially reading a person?s mind by analyzing what they look at. One group of subjects searched for teddy bear targets among random category distractors, another group searched for butterflies among the same distractors. Two SVM-based classifiers trained to recognize teddy bears and butterflies were then used to classify the distractors that were preferentially fixated by subjects during search as either teddy bears or butterflies, based on their distance from the SVM decision boundary. Two methods of preferential fixationwere explored, the object first fixated during search and the object fixated the longest. Using the longest-fixation method, we found that the target of a person?s search could be decoded perfectly when one of the distractors were rated as being visually similar to the target category. Even with completely random distractors, the target category could still be decoded for 75-80% of the subjects. The much harder task of decoding the target on individual trials (from a single object fixation) resulted in much lower classification rates, although targets were stilldecoded above chance. These findings have implications for the visual similarity relationships underlying search guidance and distractor rejection, and demonstratethe feasibility in using these relationships to decode a person?s task or goal."
Memory-based Pipelined Hardware Architecture for Communication-free Neural Computation,"Communication has an important impact on the performance of neural simulation systems. In this paper, we propose a neurocomputing architecture in which synapses are computed in parallel, and communication between neurons is carried out simply by accessing memories. In the proposed architecture, a large set of memories produce a wide stream of data for which large-scale pipelining is can be obtained. We also describe a method for translating functional specifications of computations into fine-grained pipelined circuits. Furthermore, we present the design of a simulator for spiking neural networks (SNNs), in order to show that the proposed architecture can be used to build simulators supporting various neural models. Without using the low activation property of SNNs, the performance of our system is comparable to that of event-driven systems."
Dimensionality reduction for data visualisation using Taylor network,"It is well known that any continuous derivable function can be expanded to a Taylor series and an artificial neural network is used to approximate an unknown function, so a neural network is theoretically equivalent to a polynomial. In this paper we prose a structure of such a polynomial that can be trained quickly as an alternative to  traditional artificial neural network. We test it by applying on to data dimensionality reduction such as Sammon's mapping as well as its new extensions on both synthetic and real world data sets."
Online Sum-Product Computation,"We consider the problem of performing efficient sum-product computations in an online setting over a tree.  A natural application of our methods is to compute the marginal distribution at a vertex in a tree-structured Markov random field.  Belief propagation can be used to solve this problem.  However, belief propagation requires time linear in the size of the tree.  This is too slow in an online setting where we are continuously receiving new data and computing individual marginals.  With our method we aim to update the data and compute marginals in time that is no more than logarithmic in the size of the tree, and is often significantly less.  We accomplish this via a hierarchical covering structure that caches previous local sum-product computations.  Our contribution is three-fold: we i) give a linear time algorithm to find an optimal hierarchical cover of a tree; ii) give a sum-product-like algorithm to efficiently compute marginals with respect to this cover; and iii) apply ``i'' and ``ii'' to find an efficient algorithm with a regret bound for the online {\em allocation} problem in a multi-task setting."
Adaptive Methods for Online Learning with Kernels,"In online convex optimization, adaptive algorithms, which can utilize the second-order information of the lossfunction's (sub)gradient, have shown improvements over standard gradient methods. However, existing adaptive algorithms mainly consider linear models, and are often designed only for classification problems. In this paper,we first provide a general framework that unifies various existing adaptive algorithms. Based on this, new adaptive algorithms can be easily derived.  Next, we show that adaptive learning can be generalized to nonlinear models byusing the kernel trick in a computationally efficient manner. Regret guarantee of the proposed methods are analyzed, and experiments on various benchmark data sets demonstrate their outstanding performance."
Rounding Methods for Discrete Linear Classification,"Learning discrete linear functions, whose weights represent indivisible properties, is a notoriously difficult challenge.In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space,the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set.Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem.Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods.These methods are compared on both synthetic and real-world data."
Sparse Approximate Manifolds for Differential Geometric MCMC,"One of the enduring challenges in Markov chain Monte Carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost. The recent introduction of locally adaptive MCMC methods based on the natural underlying Riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration.In this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid MCMC algorithm that extends the applicability of Riemannian Manifold MCMC methods to statistical models that do not admit an analytically computable metric tensor. Secondly, we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made. We demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust student-t error model, for which the expected Fisher Information is analytically intractable."
Randomized Proximal Point Algorithm for Large Scale Multiple Kernel Learning,"We consider the problem of learning weights to combine kernels to learn a good predictor. We study the computational problem that arises. We propose a randomized version of the proximal point algorithm to avoid a linear dependence on the number of predictors in the convergence rate. We derive finite-time performance bounds for the new algorithm that show that under mild conditions it finds the optimum of our penalized empirical risk criterion in an efficient manner. Experiments with simulated and real data are used to illustrate the new algorithm, which is found to be computationally more efficient with performance competitive to state-of-the-art alternatives."
Decision under time constraints in spiking networks,A spiking network is proposed that implements decision/classification under time- constraints. The network is able to approximate the optimal decision making rule using first-to-fire neurons as well as estimate parameters from external reward signals. The parameter estimation rule is derived from a principled cost function and resembles Hebbian learning in the limit. The proposed architecture and the learned rule exhibit near-optimal tradeoffs of error rate and response time.
Kernel-based Distance Metric Learning in the Output Space,"In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2- or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches."
Learning with Multiple Models,"The standard approach to analyzing data in supervised or unsupervised learning is to assume that a certain specific model generated the data. The goal of the learning process is typically to recover the generating model, or a good approximation thereof. But in many cases, the data are generated by multiple models rather than a single one. In this paper we study the problem of learning when multiple models are considered, generalizing well known schemes such as clustering and multi-subspace approximation. The objective is to learn several models that explain the data best, and the loss for any given data point is the minimal loss among all considered models. We develop an efficient iterative optimization based procedure for the multiple model setup and provide sample complexity bounds."
Fast Variational Inference in the Conjugate Exponential Family ,We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our methodunifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. Weexploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equationshave been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.
Controlled Recognition Bounds for Visual Learning and Exploration,"We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of ?visual search? of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a ?passive? agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an ?omnipotent? agent, capable of infinite control authority, can achieve arbitrarily good performance(asymptotically)."
Efficient and Optimal Active Learning on Graphs,"We investigate the problem of active label prediction on a given graph. Towards this end, we propose an extremely efficient algorithm S^2 that adaptively selects a sequence of nodes that it would like to see the labels of. We present a theoretical analysis of the performance of S2 that is based on a novel refinement of standard measures of label complexity. We also show that this algorithm is information theoretically optimal in a certain regime and demonstrate its performance on real world data. "
Sparse Probit Factor Analysis for Learning Analytics,"Providing personalized instructions requires a significant amount of effort spent organizing educational material, and analyzing each student's strength and weakness. This, in turn, places an enormous burden on course instructors. Intelligent tutoring systems (ITS) using machine-learning techniques are a novel way to reduce the instructor's efforts. Specifically, ITS consist of two parts, i.e., learning analytics (LA) and scheduling. LA corresponds to the analysis of student response data, whereas scheduling corresponds to the automatic suggestion of learning materials to the student based on the database retrieved through LA. In this work, we propose a statistical approach towards LA based on sparse probit binary factor analysis, and propose two novel algorithms to analyze student response data obtained in a course or test. The first algorithm utilizes convex optimization techniques, whereas the second utilizes a Bayesian latent feature framework. We demonstrate for synthetic and real-world student data that the proposed framework enables us to recover a question--concept association map, as well as a profile of the concept understanding for each student. This proposed framework represents a first step towards an ITS that alleviates the course instructor's workload and  improves the efficacy of student learning."
Distributed Probabilistic Learning for Camera Networks with Missing Data,"Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points.  However, many problems in wide-area surveillance can benefit from distributed modeling, either because of physical or computational constraints.  Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data.  In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing.  In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors.  We demonstrate the utility of this approach on the problem of distributed affine structure from motion.  Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations."
A Statistical Model for Recreational Trails in Aerial Images,"We present a statistical model of aerial images of recreational trails, and a method to infer trail routes in such images. We learn a set of textons describing the images, and use them to divide the image into super-pixels represented by their texton. We then learn, for each texton, the frequency of generating on-trail and off-trail pixels, and the direction of trail through on-trail pixels. From these, we derive an image likelihood function. We combine that with a prior model of trail length and smoothness, yielding a posterior distribution for trails, given an image. We search for good values of this posterior using a novel stochastic variation of Dijkstra?s algorithm. Our experiments on trail images and groundtruth collected in the western continental USA, show substantial improvement over those of the previous best trail-finding method"
On the importance of initialization and momentum in deep learning,"In this work, we show that using carefully crafted (but fairly simple) random initializations, deep autoencoders and recurrent neural networks (RNNs) can be trained effectively using stochastic gradient descent (with suitably small learning rates), and that these results can be significantly improved through the use of aggressive momentum-based acceleration.   For deep autoencoders we show that using any one of a variety of recently proposed random initializations schemes, deep autoencoders can be trained to a level of performance exceeding that reported by Hinton and Salakhutdinov, and with the addition of Nesterov-type momentum, the results can be further improved to surpass those reported by Martens.  For RNNs we give a simple initialization scheme related to the one used for Echo State Networks and successfully train them on various datasets exhibiting pathological long range dependencies (spanning 50-200 timesteps, depending on the problem).Our results suggest that previous attempts to train deep and recurrent neural networks from random initializations failed mostly due to poor choices for such initializations, and that the curvature issues which are present in the training objectives of deep models can be addressed through the use of aggressive momentum-based acceleration, without the need for 2nd-order methods."
Probability-One Homotopy Maps for Tracking Constrained Clustering Solutions,"Modern machine learning problems typically have multiple criteria, but there iscurrently no systematic mathematical theory to guide the design of formulationsand exploration of alternatives. Homotopy methods are a promising approach tocharacterize solution spaces by smoothly tracking solutions from one formulation(typically an ?easy? problem) to another (typically a ?hard? problem). We presentnew results in constructing homotopy maps for constrained clustering problems,which combine quadratic loss functions with discrete evaluations of constraintviolations. Our maps help balance requirements of locality in clusters as well asthose of discrete must-link and must-not-link constraints. Our experimental resultsdemonstrate significant advantages in tracking solutions compared to state-of-theartconstrained clustering algorithms."
Recklessly Approximate Sparse Coding ,"Introduction of the so called K-means features caused significant discussion in the deep learning community. Despite their simplicity, these features have achieved state of the art performance on several benchmark image classification tasks, beating out many more sophisticated learning methods. In this paper we demonstrate that a variant of these features arises as a one-step approximation to non-negative sparse coding with a fixed dictionary. This result connects these features to a broader theoretical framework and provides an explanation for their success."
A Truncated Variational EM Approach for Spike-and-Slab Sparse Coding,"We study the recovery of sparse hidden dimensions based on sparse coding with `spike-and-slab' prior. As standard sparse coding, the used model assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse  prior, we study the application of a more flexible `spike-and-slab' prior which models the absence or presence of a source's contribution independently of its strenghts if it contributs. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: firstly, a novel truncated variational EM approach; and, secondly, a recently suggested approach based on standard factored variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of `spike-and-slab' priors for this domain. Furthermore, we find the truncated variational approach to improve on the standard factored approach in most of these tasks -- which may hint to biases introduced by assuming posterior independence in the factored variational approach. Likewise, we find the truncated variational approach to improve on the factored variational approach in applications to a standard denoising task.  While the performance of the factored approach saturates with increasing number of hidden dimensions, performance of the truncated approach improves. For higher noise levels, the truncated approach finally improves the state-of-the-art on this standard benchmark."
Querying Discriminative and Representative Samples for Batch Mode Active Learning,"Empirical risk minimization (ERM) provides a principal guideline for many machine learning algorithms. Under the ERM principle, we minimize an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, the training data should be i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where we select the most informative samples to label and these data may come from a distribution different with the source. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. Our active learning method queries the most informative samples which also preseve the source distribution as much as possible, thus identifying most uncertain and representative queries. Experiments on benchmark data sets and real world applications show our method outperforms the existing state-of-the-art batch mode active learning methods."
Divide and prosper --- fault tolerant scalable sketches,"We describe a family of algorithms that can be used to extend  sketches such as the CountMin sketch and SpaceSaving, to settings  where fault tolerance and scalability are crucial. We show how tools  from systems research, namely consistent and proportional hashing  can be used to increase accuracy and throughput linearly in the  number of processors while simultaneously decreasing the failure  probability exponentially. We provide both tight theoretical  guarantees and experimental results that corroborate our findings."
The Dual Regularization Path of Lasso,"The Lasso regularization path has been extensively studied in the literature. It is now well understood that the path is well defined, unique and continuous piecewise linear under certain conditions. However, when the covariates in the active set (covaraites with greatest absolute correlations with the residual) are dependent, the regularization path is not unique and existing path following algorithms such as LARS may break down. In this paper, we systematically analyze the dual regularization path of Lasso, i.e., the path of the dual optimal solution. In contrast to the primal regularization path, the dual regularization path is well defined, unique and continuous piecewise linear without the independence assumption. We then propose how to compute the sequence of breakpoints of the dual regularization path with or without dependent covariates in the active set. Under the independence assumption, when the sign restriction is violated, the covariate which violates the sign restriction should be dropped and a new direction of the current line segment is updated accordingly. However, when the independence assumption fails, we show it is not necessary to drop the covariate which violates the sign restriction and change the direction of the current line segment, overcoming one of the major issues in deriving the primal regularization Lasso path. We systematically study different possibilities and show how to find the correct solution. Once we get the dual regularization path, we obtain the primal regularization path by the KKT conditions. Our simulation studies validate the correctness of the path generated by the proposed algorithm."
Sparse Bayesian unsupervised learning,"This paper is about variable selection, clustering and estimation in an unsupervised high-dimensional setting. Our approach is based on fitting constrained Gaussian mixture models, where we learn the number of clusters $K$ and the set of relevant variables $S$ using a generalized Bayesian posterior with a sparsity inducing prior. We prove a sparsity oracle inequality which shows that this procedure selects the optimal parameters $K$ and $S$. This result is the first of its kind for sparse model-based clustering. Our procedure is implemented using a Metropolis-Hastings algorithm, based on a clustering-oriented greedy proposal, which makes the convergence to the posterior very fast."
Keyword-supervised topic models,"Supervised topic models are able to use document labels to find topics that are predictive of the labels. However, these models only predict labels through indirect topic allocations and do not account for the fact that different words can have different degrees of effect on the label of a document. In this paper, we present the keyword-supervised latent Dirichlet allocation (ksLDA) model as a supervised model that allows different words to have a more direct effect on the model of a document's label and for the effect of the words to be perturbed by their context. In this paper, we show that this new model performs better than supervised latent Dirichlet allocation (sLDA) on real-world classification and regression tasks and on limited-size training sets."
First-Order Models for POMDPs,"Interest in relational and first-order languages for probabilitymodels has grown rapidly in recent years, and with it the possibilityof extending such languages to handle decision processes---both fullyand partially observable.  We examine the problem of extending afirst-order, open-universe language to describe POMDPs and identifynon-trivial representational issues in describing an agent'scapability for observation and action---issues that were avoided inprevious work only by making strong and restrictive assumptions. Wepresent a solution based on ideas from modal logic, and show how tohandle cases like being able to act upon an object thathas been detected through one's observations."
Comparative Locally Linear Classifiers,"A common issue in recent successful locally classifiers is the relatively high computational complexity in testing because nearest neighbor search always be involved for localizing the data points using anchor points. In this paper, we introduce the idea of locally sensitive hashing (LSH) into the linear classifiers to speed up the localization process, and thus propose another large-margin based locally linear classifier. The features generated by the LSH process are called comparative features. We learn the comparative features by transforming the data points into a new feature space and performing threshold on them using the max-operator. The transformation matrix is actually the anchor point matrix, which is learned in an unsupervised manner by enforcing the corresponding coefficients are the elements on a hypercubic structure in the new feature space. The nearest neighbor search forlocalization is approximated by the max-operator. Compared to other local classifiers, the computational complexity of our classifier in testing is almost identical to linear support vector machines, and only 4-line simple MATLAB code is needed for the binary classification. Experimental results demonstrate that during testing our method is not only comparable or even better than other state-of-the-art local classifiers in terms of accuracy but also much faster in testing."
Learnable Pooling Regions for Image Classification,"From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition. Grouping of local features and their codes is part of most recent recognition pipelines and equips these methods with a certain degree of robustness to translations and deformation yet preserving the same spatial layout of the local image features.Despite the predominance of this approach, we have seen little progress to fully adapt  the pooling strategy to the task at hand. This paper proposes a learning method that allows for learning a task dependent pooling scheme -- which includes previously proposed pooling schemes as a particular instantiation of our method.In contrast to previous work we allow different pooling strategies for each code, which shows in particular beneficial for small codes. We propose a batch-based optimization strategy that allows our approach to scale up to sizable dictionary. In this manner we discover new pooling regions that have not been previously used in computer vision."
The Time-Marginal Coalescent Prior for Hierarchical Clustering,"We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clus-tering. The prior is constructed by marginalizing out the time information ofKingman?s coalescent, providing a prior over tree structures which we call theTime-Marginalized Coalescent (TMC). This allows for models which factorizethe tree structure and times, providing two benefits: more flexible priors may beconstructed and more efficient Gibbs type inference can be used. We demonstratethis on an example model and show we get competitive experimental results."
Detecting Activations over Graphs using Spanning Tree Wavelet Bases,"We consider the detection activations over graphs under Gaussian noise, where signals are supposed to be peice-wise constant over the graph.Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with proveable theoretical guarantees for general graph topologies.We first provide a universal necessary condition for asymptotic distinguishability of the null and alternative hypotheses.We introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph.We prove that for any spanning tree, we can hope to correctly detect signals in a low signal-to-noise regime using spanning tree wavelets.We propose a randomized test, in which we use a uniform spanning tree in the basis construction.Using electrical network theory, we show that the uniform spanning tree provides strong theoretical guarantees for arbitrary graphs that in many cases match our necessary condition.We prove that for edge transitive graphs, $k$-nearest neighbor graphs, and $\epsilon$-graphs we obtain nearly optimal performance with the uniform spanning tree wavelet detector."
Fusion with Diffusion for Robust Visual Tracking,"A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. The edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations.In this paper, a novel approach to integrate multiple similarity measures is pro-posed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the dif-fusion on the TPG is the same as the diffusion process on each of the original graphs, Moreover, it is not necessary to explicitly construct the TPG in our frame-work. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods."
Top-down particle filtering for Bayesian decision trees,"Decision trees are a fundamental tool in machine learning and statistics, and Bayesian variants, which introduce a prior distribution on the decision tree itself, have demonstrated the utility of full posterior inference.  Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, existing Bayesian decision tree algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection) iteratively through local Monte Carlo modifications to the structure of the tree.  We present a Sequential Monte Carlo (SMC) algorithm that works in a top-down manner, mimicking the behavior and speed of classic algorithms. Through empirical comparisons with existing methods, we demonstrate the potential of this new approach and conclude that it represents a better computation-accuracy tradeoff."
A nonparametric variable clustering model,"Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a clustering, of observed variables so that variables in a cluster are highly correlated. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. "
Robust Saliency Estimation using Visual Structures,"Emulation of human perception system can usually inspire computer vision and help understand the natural scenes that are usually recorded as raster images or videos. Saliency detection is one of such procedures that can extract useful cues about the key visual information in an observed scene. In this paper, we provide a robust saliency estimation method based on a geodesic analysis of visual structures in an image. In the proposed approach, manifold learning is first applied to compare local spatial variation against global contrast, and obtain a locally smoothed image projection. Geodesic analysis is then applied to find the plateaus in the hierarchical image structure that corresponds to the geodesic saliency map. With the estimated saliency map, visual structures are hierarchically cut out by simple adaptive thresholding. The experiment validated that our algorithm consistently outperformed a number of the state-of-art methods, yielding higher precision and better recall rates, when evaluated using one of the largest publicly available data sets. Further experiments on saliency cut also testified that the proposed method provide an efficient way for unsupervised object cut, which may have extensive application in image/video editing, object recognition and robotic vision."
Priors for Diversity in Generative Latent Variable Models,"Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization, for building density models of data, and for  providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions  on internal parameters.  For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the  underlying i.i.d.\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions, we are able to define a DPP on probability measures.  We show how to perform MAP inference  with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model."
24 Parallel Codes for Sparse PCA,"Given a multivariate data set, sparse principal component analysis aims to extract several linear combinations of the variables which together explain the variance in the data as much as possible, while controlling the number of nonzero loadings in these combinations. In this paper we consider 8 different optimization formulations for computing a single sparse loading vector; these are obtained by combining the following factors: we employ two norms for measuring  variance (L2, L1) and two sparsity-inducing norms (L0, L1), which are used in two different ways (constraint, penalty). Three of our formulations, notably the one with L0 constraint and L1 variance, have not been considered in the literature. We give a unifying reformulation which we propose to solve  via a natural alternating maximization method. Besides this, we provide one serial (single-core) and three parallel (multi-core, GPU, cluster) codes for each of the 8 problems. Parallelism in the methods is aimed at i) speeding up computations (our GPU code can be 100 times faster than an efficient serial code written in C), ii) obtaining solutions explaining more variance and iii) dealing with large-scale problems (our cluster code is able to solve a 357 GB problem in about a minute)."
A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function,"We propose a novel Bayesian approach to solve stochastic optimization problems that involve ?nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ?rst, doing inference over the function space and second, ?nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf?cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function."
Model Class Priors in Reinforcement Learning,"Many reinforcement learning problems have such a structure that estimation of the optimal policy may be significantly simpler than in the general case. While Bayesian approaches would naturally be suited to discovering and exploiting such a structure, previous work has concentrated priors which could model arbitrary environments. This paper proposes instead a hierarchical model where beliefs over different classes of priors are maintained. The model is fundamentally different from classical hierarchical Bayesian approaches, since the evidence depends on the policy followed so far. This obstacle can be removed by an appropriate decomposition of the posterior. Finally, we derive a number of online decision making algorithms, in conjunction with the class prior distribution, which maintain a distribution of value functions. Their performance is examined in a number of reinforcement learning problems and we show that it is at least as good as the performance of the model that knows the correct model class a priori."
Bias-corrected Q-learning to Control Max-operator Bias in Q-learning,We identify a class of stochastic control problems with random rewards which induce high levels of statistical error in the estimate of Q-factors.  This produces significant levels of max-operator bias in Q-learning algorithms which can induce the algorithm to diverge for millions of iterations.  We present a bias-corrected Q-learning with asymptotically unbiased resistance against max-operator bias and show that the algorithm asymptotically converges to the optimal policy as Q-learning does.  We demonstrate that bias-corrected Q-learning shows practical resistance against max-operator bias and performs well in select problems with highly random rewards where Q-learning and other provably convergent algorithms rooted on Q-learning suffer max-operator bias.
Structure inference in cryo-electron microscopy using Gaussian mixture models,"The reconstruction problem in cryo-electron microscopy is to infer an unknown 3D electron density from a set of its 2D projections, where the projection directions are also unknown. Most existing algorithms need to be initialized with a 3D density, and therefore produce biased results. Futhermore, they typically use non-probabilistic approaches that depend on many user-specified hyperparameters. In this paper we introduce a new reconstruction algorithm that places the entire problem in a probabilistic framework with a very small number of hyperparameters. Our algorithm does not require an initial 3D model. Most importantly, it makes use of a novel representation of 3D densities using Gaussian mixture models. Model parameters are estimated using an expectation maximization type algorithm. The algorithm is applied to synthetic and real datasets."
Shortest stochastic path with risk sensitive evaluation,"In an environment of uncertainty where decisions must be taken, how to make adecision considering the risk? The shortest stochastic path (SSP) problem modelsthe problem of reaching a goal with the least cost. However under uncertainty, abest decision may: minimize expected cost, minimize variance, minimize worstcase, maximize best case, etc. Markov Decision Processes (MDPs) defines optimaldecision in the shortest stochastic path problem as the decision that minimizesexpected cost, however MDPs does not care about the risk. An extension of MDPwhich has few works in Artificial Intelligence literature is Risk Sensitive MDP.RSMDPs considers the risk and integrates expected cost, variance, worst case andbest case in a simply way. We show theoretically the differences between MDPsand RSMDPs for modeling the SSP problem and show the results of each modelin an artificial scenario."
The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification," Linear models are often much faster to learn and test than non-linear models. To enable non-linear (with  respect to the original feature space) learning with efficient linear learning algorithms, explicit  embeddings that approximate popular kernels have recently been proposed. However, the kernels are usually  designed so that their dot product in the high dimensional space is efficient, while we want the embedding  itself to be efficient (and rich enough). We propose a simple and effective pairwise piecewise-linear  embedding to approximate models under a factorization-like assumption. The method is based on discretization  and interpolation of individual features values and feature pairs.  The discretization allows us to model  different regimes of the feature space separately, while the interpolation preseves the original continuous  values.  Pairs allows us to approximate cross-feature relationships. Using this embedding within an SVM  strictly generalizes linear SVM. Additionally, some cross-features relations such as feature similarity can  be modeled exactly, while other cross-feature relations are approximated.  We conducted an extensive  experimental study and show results for a large number of datasets. We compared our method to linear,  polynomial, $\chi^2$-like and RBF kernels and embeddings. Our method consistently achieves good performance  significantly outperforming all other methods, including the RBF kernel on the majority of the  datasets. This is in contrast to other proposed embeddings that were faster than kernel methods, but with  lower accuracy. Additionally, our method is as efficient as the second polynomial explicit feature map. The  code will be made available if the paper is accepted."
Calibration in Cost-sensitive Multiclass Classification: an Application to Reinforcement Learning,"In this paper we propose a computationally efficient version of classification based policy iteration. The key idea of these algorithms is to view the problem of coming up with the next policy in policy iteration as a classification problem, where a policy is viewed as a classifier.The main novelty is that we propose to replace the non-convex optimization problem of earlier algorithms with a convex one, where a new cost-sensitive surrogate loss is optimized in each iteration.The new loss is shown to be classification calibrated, which makes it a ``sound'' surrogate loss. As far as we know, this is the first calibration result in the context of multiclass classification. As a result, we are able to extend  theoretical guarantees that existed for the previous inefficient classification-based policy algorithms to our efficient method, thereby giving the first computationally efficient, theoretically sound version of classification-based policy iteration."
Adaptive Radial Filtering for Multi-Oriented Character Recognition,"The recognition of fully multi-oriented handwritten characters is a difficult problem. Contrary to univarite signals where the shift invariance property in the Fourier transform can be used, multivariate signals like images require special care for extracting rotation invariant features. The proposed method considers first input features obtained by the Polar transform. A convolutional neural network is then used for extracting features of higher level. This classifier includes in addition the Fast Fourier Transform for extracting shift invariant features at the neural level. The convolutional layers process the image at the pixel level while the Fourier transform and the upper layers of the neural networks process rotation invariant features. The average recognition rate for multi-oriented characters is 91.68\% for the Latin digits."
Bayesian Inference Reveals Synapse-Specific Short-Term Plasticity in Neocortical Microcircuits,"Short-term synaptic plasticity is highly diverse and varies according to brain area, cortical layer, and developmental stage. Since this form of plasticity shapes neural dynamics, its diversity suggests a specific and essential role in neural information processing. Therefore, a correct identification of short-term plasticity is an important step towards understanding and modeling neural systems. Although accurate phenomenological models have been developed, they are usually fitted to experimental data using least-mean square methods. We demonstrate that, for typical synaptic dynamics, such fitting gives unreliable results. Instead, we introduce a Bayesian approach based on a Markov Chain Monte Carlo method, which provides the full posterior distribution over the parameters of the model. We test the approach on simulated data over different regimes and show that common short-term plasticity protocols yield broad distributions over some of the parameters. Finally, we infer the model parameters using experimental data from three different neocortical excitatory connection types, revealing novel synapse-specific distributions and synaptic transfer functions, while the approach yields more robust clustering results. We conclude that ? because short-term plasticity presumably provides key computational features ? our approach to demarcate synapse-specific synaptic dynamics is an important improvement on the state of the art."
Kuhn meets Rosenblatt: Combinatorial Algorithms for Online Structured Prediction,"Online algorithms have been successful at a variety of prediction tasks.  In structured prediction settings, the model produced by an online learner is fed as input to some combinatorial algorithm for producing structured outputs.  This combinatorial algorithm is predominantly considered a black box, which severely limits the control available to the learner.  In this paper, we break open this black box.  For each example, it aims to change its model minimally subject to a margin-based optimality condition on the output.  We define a flexible linear framework that exploits the combinatorial properties of the desired structured output to achieve this in a convex optimization framework. We demonstrate the efficacy of this framework in two applications: dependency parsing via maximum spanning trees and word alignment via bipartite matching."
"Causality Analysis in Time Series: Foundations, Consistency and New Development","Granger causality is the primary technique of causality analysis for time series data. While it has been well studied in the literature for two time series, its performance for multivariate time series in the presence of hidden variables as well as its connections to true causality has always been debated. In this work, we reexamine the theoretical foundations of Granger causality and strive to provide insights into three fundamental questions on Granger causality for time series data. Specifically, we resort to the Structural Equation Modeling (SEM),  a widely accepted framework for true causality analysis, and statistical consistency analysis, which reveals consistency behavior of statistical methods to uncover Granger causality, in order to answer the following questions: (1) what are the advantages of Granger causality in avoiding spurious causation;  (2) what are the consistency properties of two popular approaches to uncover Granger causality, including significance test and L1-penalized regression; (3) what are the consistency properties of advanced algorithms for nonlinear dependencies, i.e.,  semi-parametric approach, to uncover Granger causality for high-dimensional time series. Experiment results on synthetic datasets and social media application data are shown to support our theoretical analysis."
Convergence Rate Analysis of MAP Coordinate Minimization Algorithms,"Finding maximum aposteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used.Solving these relaxations efficiently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However,these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence.Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima."
Dual-view Dirichlet Process Mixture Models for Cross-modal Data Analysis,"We propose Dual-view Dirichlet Process Mixture Models for analyzing cross-modal data. This model is a Bayesian nonparametric model incorporating a prior of infinite mixture distribution of data in any single modality, and it also captures the correspondences between mixture components from different modalities. We develop an efficient variational inference algorithm for learning the joint distribution of cross-modal data which can contribute to identifying latent structures. For prediction tasks, we provide fast approximated methods based on a latent subspace derived from this generative model and kernel regression. Comparisons of experimental results to other state of the art models on benchmark datasets demonstrate the superiority of our model in significantly improving performances on cross-modal information retrieval and image annotation."
A nonparametric Bayesian approach to learning directed acylic graph structures,"The learning of graph structure is an important problem in machine learning. To this effect, we present a new stochastic process defining a probability distribution on infinite directed acyclic graph structures. This distribution can be used as a nonparametric Bayesian prior on the structure of graphical models having an unbounded number of hidden random variables. The proposed stochastic process is an extension of the cascading Indian buffet process that removes the limitation of purely layered structures. We evaluate the performance of both approaches in discovering the structure of belief networks and compare the structure complexity of the posterior distribution, showing that our approach can extract graphs with fewer units without scarifying predictive precision."
Model Selection for Degree-corrected Block Models,"A central problem in analyzing networks is splitting them into modules or communities, clusters with a statistically homogeneous pattern of links to each other or to the rest of the network. One of the best tools for this is the stochastic block model, which in its basic form imposes a Poisson degree distribution on all nodes within a community or block. In contrast, degree-corrected block models allow for heterogeneity of degree within blocks. Since these two model classes often lead to very different partitions of nodes into communities, we need an automatic way of deciding which model is more appropriate to a given graph. We present a principled and scalable algorithm for this model selection problem, and apply it to both synthetic and real-world networks. Specifically, we use belief propagation to efficiently approximate the log-likelihood of each class of models, summed over all community partitions, in the form of the Bethe free energy. We then derive asymptotic results on the mean and variance of the log-likelihood ratio we would observe if the null hypothesis were true, i.e. if the network were generated according to the non-degree-corrected block model. Interestingly, we find that for sparse networks, significant corrections to the classic asymptotic likelihood-ratio theory (underlying 2 hypothesis testing or the AIC) must be taken into account."
Mixing-time Regularized Policy Gradient,"Policy gradient reinforcement learning (PGRL) methods have received substantial attention as a mean for seeking stochastic policies that maximize a cumulative reward. However, PRRL methods can often take a huge number of learning steps before it finds a reasonable stochastic policy. This learning speed depends on the mixing time of the Markov chains that are given by the policies that PGRL explores. In this paper, we give a new PGRL approach that regularizes the rule of updating the policy with the hitting time that bounds the mixing time.  In particular, hitting-time regressions based on temporal-difference learning are proposed. This will keep the Markov chain compact and can improve the learning efficiency. Numerical experiments show the proposed method outperforms the conventional PG methods."
Buy-in-Bulk Active Learning,"In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time.This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch.In this work, we study the label complexity of active learning algorithms thatrequest labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once.  In particluar, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increasethe total number of labels requested, it reduces the total cost requiredfor learning."
Forward Model Extraction from Neural Population Activity,"Internal forward models are believed to explain the nervous system's ability to compensate for sensory feedback delays and adapt to changes in effector dynamics.  Single-neuron and behavioral studies have provided evidence of forward models, but to our knowledge it has not yet been possible to extract a full forward model of the effector directly from neural activity.  Here, we develop a novel probabilistic framework for forward model extraction that integrates neural commands with sensory feedback.  Using this framework, we can i) extract the subject's forward model, which is represented as parameters in the probabilistic model, and ii) infer the subject's timestep-by-timestep internal estimates of the motor effector position, which are latent variables in the model.  We leverage brain-computer interface (BCI) infrastructure, in which all neural commands driving the effector and sensory feedback are fully observed. We applied this framework to neural population activity recorded in macaque motor cortex during BCI control of a computer cursor to acquire visual targets. We found that recorded neural commands were more consistent with aiming straight toward targets from the subject's internal estimates of cursor position, as inferred by our probabilistic framework, than from the cursor positions displayed during online control. The extracted forward models explain about 75% of the subject's aiming errors. We believe that the probabilistic framework developed provides a critical link between sensory feedback and motor commands, and will likely facilitate the study of feedback motor control and motor learning."
Online Learning of Hierarchical Balancing Strategies for Bipedal Humanoid Robots,"  Bipedal humanoid robots will fall under unforeseen perturbations without active stabilization.  Humans use dynamic full body behaviors in response to perturbations, and recent bipedal robot controllers for balancing are based upon human biomechanical responses.  These controllers assume simple physical models and require very accurate state information, making them less effective on physical robots in uncertain environments.  To address this issue, we propose a hierarchical control architecture that learns to switch between three low-level biomechanically-motivated strategies in response to perturbations.  The high level strategy is learned in an online fashion from state trajectory information gathered during experimental trials.  This learning approach is evaluated in physics-based simulations as well as on a small humanoid robot. Our results demonstrate how well this method stabilizes the robot during walking and whole body manipulation tasks."
Hierarchical spike coding of sound,"We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task."
Noisy Bayesian Active Learning,"We consider the problem of noisy Bayesian active learning, where we are given a finite set of functions $\mathcal{H}$, and a sample space $\mathcal{X}$. A function in $\mathcal{H}$ assigns a label to a sample in $\mathcal{X}$, and the result of a label query on a sample is corrupted by independent noise. The goal is to identify the function in $\mathcal{H}$ that generates the labels with high confidence using as few label queries as possible, by selecting the queries adaptively in a strategic manner. Previous work in Bayesian active learning considers Generalized Binary Search, and its variants for the noisy case, and analyzes the number of queries required by these sampling strategies. In this paper, we show that these schemes are, in general, suboptimal. Instead we propose and analyze an alternative strategy for sample collection. Our sampling strategy is motivated by a connection between Bayesian active learning and active hypothesis testing, and is based on querying the label of a sample which maximizes the Extrinsic Jensen--Shannon Divergence at each step. We provide upper and lower bounds on the performance of this sampling strategy, and show that these bounds are better than previous bounds."
Infinite Tensor Factorization Priors,"There is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. We consider a general framework for nonparametric Bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure.  The mixing measure is assigned a novel infinite tensor factorization (ITF) prior that allows flexible dependence in cluster allocation across data types.  The ITF prior is formulated as a tensor product of stick-breaking processes.  Focusing on a convenient special case corresponding to a Parafac factorization, we provide basic theory justifying the flexibility of the proposed prior and resulting asymptotic properties.  Focusing on ITF mixtures of product kernels, we develop a  new Gibbs sampling algorithm for routine implementation relying on slice sampling. The methods are compared with alternative joint mixture models based on Dirichlet processes and related approaches through simulations and real data applications."
Modeling Two Functionally Distinct Ventral Pathways Representing Static Form and Color/Texture,"Anatomical and physiological data suggests that the ventral visual pathway of the primate brain is subdivided into specialized processing modalities. We combine a model of color/texture processing with a separately developed model of shape/form processing to determine whether such pathways can be both functionally independent and complimentary. Our hypothesis was that the combination would yield better performance on an invariant object localization and classification task than either model alone. Functional independence is established if the optimal combination corresponds to the Boolean rules used for combining two statistically independent binary classifiers. To extract color/texture information, we learned a sparse dictionary of features from representative training data, pooled over the dictionary elements using a winner-take-all heuristic and then clustered the pooled data groups using a k-means algorithm. In tandem, we extracted shape information by pre-processing the image with a canny edge filter, then computed difference kernels based on co-occurrence statistics for edge combinations characteristic of the target category. We represented the two pathways as binary classifiers and combined them with optimal Boolean operators, as defined using a Neyman-Pearson theorem for Receiver Operating Characteristics (ROC) curves. Using ground-truth for a high-definition video-stream from a helicopter flying over Los Angeles, CA, we demonstrate that the two pathways are functionally independent and when combined perform substantially better than either pathway alone. Our results suggest that the separate processing modalities found in the primate ventral visual pathway represent functionally independent and complimentary approaches to viewpoint invariant object detection and localization."
Lifted Parameter Learning for Markov Logic,"Statistical relational learning (SRL) augments probabilistic models with relational representations and facilitates reasoning over sets of objects. When learning the probabilistic parameters for SRL models, however, one often resorts to reasoning over individual objects. We propose to harness the full power of relational representations in the learning phase, by using lifted inference. For this we compile a Markov logic network into a compact and efficient first-order data structure and use weighted first-order model counting (WFOMC) to calculate the likelihood of the data in a lifted manner. By exploiting the relational structure in the model, it is possible to dramatically improve the run time of the likelihood calculation and learn parameters more accurately. This allows us to calculate the exact likelihood of the data for models where previously only approximate inference was feasible. Results on real-world data sets shows that this approach learns more accurate models."
Local Support Vector Machines: Formulation and Analysis,"We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs."
No More Pesky Learning Rates,"The performance of stochastic gradient descent (SGD) dependscritically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations accross samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained throughsystematic search, and effectively removes the need for learning rate tuning. "
Consensus Ranking with Signed Permutations,Signed permutations (also known as the hyperoctahedral group) are used in modeling genome rearrangements. The algorithmic problems they raise are computationally demanding when not NP-hard. This paper presents an algorithm for learning consensus ranking between signed permutations under the inversion distance. This can be extended to estimate a natural class of exponential models over the group of signed permutations. We investigate experimentally the efficiency of our algorithm for modeling data generated by random reversals.
Explanation with Causal Logic Models,"Despite their success in transferring the powerful human faculty of causal reasoning to a mathematical and computational form, causal models have not been widely used in the context of core AI applications such as robotics.  In this paper, we define Causal Logic Models (CLMs), a new discrete-time, probabilistic, first-order representation which uses causality as a fundamental building block. Rather than merely converting causal rules to first-order logic as various methods in Statistical Relational Learning have done, we treat the causal rules as basic primitives which cannot be altered without changing the system. We present an algorithm using CLMs for one type of causal reasoning known as causal explanation, i.e., understanding the causal links between events spaced out in time. Using CLMs rather than traditional fixed causal models allows causal explanation to be performed in dynamic situations where variables of interest are not necessarily known a priori. We show empirically that CLMs produce intuitive and succinct explanations given an evidence set, more in line with human causal reasoning. We also discuss how CLMs other types of causal reasoning such as prediction and counterfactuals can look qualitatively different from their counterparts with other representations."
Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models,"Recent experiments have demonstrated that humans and animals typically reasonprobabilistically about their environment. This ability requires a neural codethat represents probability distributions and neural circuits that are capable ofimplementing the operations of probabilistic inference. The proposed probabilisticpopulation coding (PPC) framework provides a statistically efficient neuralrepresentation of probability distributions that is both broadly consistent withphysiological measurements and capable of implementing some of the basic operationsof probabilistic inference in a biologically plausible way. However, theseexperiments and the corresponding neural models have largely focused on simple(tractable) probabilistic computations such as cue combination, coordinate transformations,and decision making. As a result it remains unclear how to generalizethis framework to more complex probabilistic computations. Here we addressthis short coming by showing that a very general approximate inference algorithmknown as Variational Bayesian Expectation Maximization can be implementedwithin the linear PPC framework. We apply this approach to a generic problemfaced by any given layer of cortex, namely the identification of latent causes ofcomplex mixtures of spikes. We identify a formal equivalent between this spikepattern demixing problem and topic models used for document classification, inparticular Latent Dirichlet Allocation (LDA). We then construct a neural networkimplementation of variational inference and learning for LDA that utilizes a linearPPC. This network relies critically on two non-linear operations: divisive normalizationand super-linear facilitation, both of which are ubiquitously observed inneural circuits. We also demonstrate how online learning can be achieved using avariation of Hebb?s rule and describe an extesion of this work which allows us todeal with time varying and correlated latent causes."
Hierarchical Estimation of Locomotion Mode and Gait Cycle using Switching Unscented Kalman Filters,"As we walk, the state of our limbs varies cyclically, while other variables such as walking speed vary along continuous axes and all are nonlinearly related to one another and to potentially observed aspects of gait. Moreover, our locomotion may switch between modes such as walking and standing. Here we present an efficient solution to nonlinear estimation problems with both cyclical and contin- uous state variables with dynamics that undergo switches. This solution is based on a Hidden Markov Model (HMM) over Unscented Kalman Filters (UKF). The resulting algorithm captures the total variability of the gait parameters with a to- tal variance accounted for (R-squared) of 0.99, outperforming existing linear regression estimators (R-squared = 0.92)"
Cost-Sensitive Exploration in Bayesian Reinforcement Learning,"In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems."
Modeling Expertise of Crowd by Normalized Gamma Decomposition,"We develop a flexible framework for modeling the expertise of a crowd, called normalized gamma decomposition of a confusion matrix.The proposed framework enables us to model the ability of workers, the labeling tendency (confusion) of workers, and the items' difficulties to correctly annotate.Moreover, we can apply our framework to a heterogynous labeling problem where we have to analyze a task that includes different types of labels."
Learning with Partially Absorbing Random Walks,"We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$, and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\mathcal{S}$, while dropping sharply outside $\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning."
Efficient MAP Inference in Binary Pairwise MRFs,"Markov random fields (MRFs) have broad application in many fields including computer vision. In general, however, finding the most likely (MAP) configuration of variables is NP-hard.  If we restrict attention to the class of associative (submodular) models then the task is tractable but further speed improvement would be welcome. Simplifications are revealed by reducing the MAP inference problem to maximum weight stable set (MWSS) on a compiled nand Markov random field (NMRF). This yields two results for general binary pairwise MRFs: (1) A rapid pre-processing algorithm that identifies an exact MAP configuration of a subset of the variables; the subset is larger for sparse MRFs with low associativity and random settings;local sensitivity parameters are also returned for every variable; and (2) We derive necessary and sufficient conditions for when such an MRF maps to a perfect NMRF, which guarantees efficient MAP inference."
Instance Level Multiple Instance Learning Using Similarity Preserving Quasi Cliques,In this paper we introduce an instance-level approach to multiple instance learning. Our bottom-up approach learns a discriminative notion of similarity between instances in positive bags and use it to form a discriminative similarity graph. We then introduce the notion of similarity preserving quasi-cliques that aims at discovering large quasi-cliques with high scores of within-clique similarities. We argue that such large cliques provide clue to infer the underlying structure between positive instances. We use a ranking function that takes into account pairwise similarities coupled with prospectiveness of edges to score all positive instances. We show that these scores yield to positive instance discovery. Our experimental evaluations show that our method outperforms state-of-the-art MIL methods both at the bag-level and instance-level predictions in standard benchmarks and image and text datasets.
Robust Distance Metric Learning via Simultaneous $\ell_1$-Norm Minimization and Maximization,"Traditional distance metric learning with side information often formulates the learning objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Since covariance matrices are prone to outliers, it is desirable to develop a robust distance metric learning method. In this paper, motivated by existing studies that improve the robustness of machine learning models via the L1-norm, we propose a robust formulation of distance metric learning using the L1-norm distances. However, solving the formulated objective is very challenging because it simultaneously minimizes and maximizes (minmax) the non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is scarcely studied in literature. Extensive empirical evaluations on the proposed robust distance metric learning method are performed, in which our new method outperforms related state-of-the-art methods in a variety of experimental settings and demonstrate their effectiveness in the clustering tasks on both noiseless and noisy data.  "
Extending generalized delta rules for efficient Hessian calculations through backpropagation,"Recent extensions of first-order backpropagation (BP), also known asgeneralized delta rules, of Rumelhart et al. (1986) lead to the development of efficient Hessian calculations for second-orderoptimization (e.g., Levenberg-Marquardt methods).  Consider, for instance, the evaluation of  the so-called Gauss-Newton Hessian matrix J'*J of size n x n when optimizing a multi-layer neural network that has multipleZ outputs (Z > 1).  Fairbank & Alonso~(2012) described how to use first-order BPfor (Z+n) times per data pattern in forming Z rows of J and then J'*Jexplicitly column by column.  Their claim is thatthe proposed method works faster than the ``standard'' algebraic method bya factor of Z. Yet, their analysis totally ignores several key factors thatare already discussed individually in other computational techniques.Even under their assumption 1 << Z <= square root of n,the standard method can work faster in some situations.By combining the strengths of existing algorithms,we have derived an efficient algorithm that performs backward passes only forB times, followed by some algebraic manipulations, where B denotes the total number of hidden nodes.Since B is approximately equal to sqaure root of n, our improvement would be significant.We also show its further extensions and an efficient matrix-freealgorithm that combines BP with a forward mode of automatic differentiation."
Learning mixture models with the hierarchical expectation maximization algorithm,"Driven by the need for computationally efficient parameter estimation from large, web-scale data sets, the hierarchical EM (HEM) algorithm has been proposed and proven effective for a variety of modeling tasks and applications. In this paper, we investigate the benefits of HEM as a general-purpose algorithm for parameter estimation in mixture models, compared to regular EM. First, we re-derive the algorithm in more generality, for generic exponential family distributions, with and without unobserved variables. Second, we discuss and experimentally verify its benefits across a broad spectrum of model classes and applications. Besides scalability, HEM's implicit regularization and adaptation for multiple instance learning make it an appealing alternative to standard EM, for practitioners."
Sparse projections onto the simplex,"The past decade has seen the rise of $\ell_1$-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard $\ell_1$-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation."
Coordinated collision avoidance of multiple agents with continuous stochastic plant dynamics,"We describe an approach to multi-agent planning under continuous stochastic dynamics. The approach yields collision-free state trajectories with adjustably high certainty while aiming for lowsocial cost.  To this end we describe a collision-detection module based on a distribution-independent probabilistic bound and compare fixed priority and auction-based coordination protocols to resolve collisions.While our experiments were conducted with agents governed by linear stochastic dynamics with state-independent noise, our methods extend to more general settings of state-dependent noise and with non-linear dynamics."
Modelling the Lexicon in Unsupervised Part of Speech Induction,"Automatically inducing the syntactic part-of-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single  part-of-speech tag. This one-tag-per-type heuristic, which counters the tendency of Hidden Markov Model based taggers to over generate tags for a given word type, is clearly incompatible with basic syntactic theory. In this paper we extend the current state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model of the lexicon. In doing so we are able to incorporate a soft bias towards inducing few tags per type. We develop a novel particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with the state-of-the-art without making any unrealistic restrictions."
Efficient and direct estimation of a neural subunit model for sensory coding,"Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a `subunit' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (``convolutional??) copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the `subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency."
Spectral Learning of Latent-Variable HMMs ,We derive a spectral algorithm for learning the parameters of a latent-variable HMM. This method avoids the problem of local optima and provides a consistent estimate of the parameters. We demonstrate the method on a phoneme recognition task and show that it performs competitively with EM. 
Getting the First Page Right: Bayesian Active Retrieval under Uncertainty,"Triggered by the idea of an information retrieval system for objects with noisy and missing features, we investigate the general problem of actively learning a similarity function of complex objects when the inputs to this function are not known exactly. To reduce the uncertainty in the inputs, and in turn improve the similarity function, we are interested in acquiring more information about the input objects. As gathering clean and complete information is costly or even impossible, it is important to carefully select the information needed and to be able to deal with uncertainty in order to retrieve meaningful results fast and with low total cost. Hence, we propose a Bayesian active learning approach to efficiently learn the most similar objects to a given query object in the setting where only partial and noisy information about entities is available. In our information retrieval case this corresponds to the task of getting the first page (of retrieval results) right. We evaluate the proposed Bayesian decision theoretic framework to actively acquire information on several retrieval problems, including a real-world document retrieval task."
Large-Scale Sparse PCA through Low-rank Approximations,We introduce a novel scheme for sparse PCA that has provable approximation guarantees.We first introduce an algorithm that can exactly solve sparse PCA for matrices of constant rank in polynomial time. Given a full-rank frequency matrix we obtain a constant rank matrix approximation and subsequently execute the exact sparse PCA solver on this low-rank approximation. We surprisingly see that this low-rank approximation step introduces very small errors. We theoretically explain this behavior by showing that data sets with few high-degree words must have data matrices that are close to low-rank. Our formalism allows us to show that our sparse PCA algorithm is asymptotically tight. Experimentally we evaluate our algorithm in a very large Twitter data set. A feature elimination step allows us to perform sparse PCA in millions of Tweets in a few minutes. Our scheme typically captures $80-90\%$ of the data variance by using rank-3 or rank-4 approximations. 
Near Optimal Chernoff Bounds for Markov Decision Processes,"The expected return is a widely used objective in decision making under uncertainty.  Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize.  We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw connections to previously proposed objectives for risk-aware planing: minmax, exponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded.  Additionally, we present an efficient algorithm for optimizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale."
Imitation Learning by Coaching,"Imitation Learning has been shown to be successful in solving many challenging real-world problems.Some recent approaches give strong performance guarantees by training the policy iteratively.However, it is important to note that these guarantees depend on  how well the policy we found can imitate the oracle on the training data. When there is a substantial difference between the oracle's ability and the learner's policy space,we may fail to find a policy that has low error on the training set.In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner and gradually approaches the oracle.By a reduction of learning by demonstration to online learning, we prove that coaching can yield a lower regret bound than using the oracle.We apply our algorithm to a novel cost-sensitive dynamic feature selection problem,a hard decision problem that considers a user-specified accuracy-cost trade-off. Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods."
Stochastically Emerging Medoids: Application of Classical Problems in Probability Theory for Clustering Massive Data Sets,"K-medoid methods for clustering data have many desirable properties such as robustness and the ability to use non-numerical values, but their typically high computational complexity has made their application to large data sets difficult. In this paper, we present AGORAS, a novel stochastic algorithm for the k-medoids problem that is especially well-suited to clustering massive data sets. Our approach involves taking a sequence of uniform sample sets and a heuristic for determining the sample size and identifying cluster medoids from the sampled items. As a result, computing the final solution only involves solving k trivial sub-problems of centrality, which can be done much more efficiently on large data sets than searching a combinatorial space for the optimal value of an objective function. As a result, the complexity of AGORAS is effectively independent of the full data size, and it can scale to arbitrarily large data sets.  We evaluate AGORAS experimentally against PAM and CLARANS, the best-known existing algorithms for the k- medoids problem, across a variety of published and synthetic data sets.  We find that AGORAS outperforms PAM by up to four orders of magnitude for data sets with less than 10,000 points, and it outperforms CLARANS by two orders of magnitude on a data set of just 64,000 points.  Moreover, we find in some cases that AGORAS also outperforms these algorithms in terms of cluster quality. "
Modeling Scientific Impact with Citation Influence Regression,"When reviewing the scientific literature in a specific subject area, it would be usefulto have automatic tools that identify the most influential scientific articles aswell as how ideas propagate between articles. Bibliometric measures based on citationcounts, such as impact factors, provide some indication of the influence ofan article or the prestige of its publication venue. However, citations can occur fordifferent reasons and may not always indicate the transfer of ideas, so that citationcounts alone can be misleading. In this paper we develop latent variable probabilisticmodels for inferring influence in scientific corpora. The models operateon a collection of documents embedded in a citation graph with articles as nodesand citations as edges, where the latent topics of cited papers influence the priordistribution over topics in citing papers. We show how the proposed models canbe used to automatically determine the degree of influence of scientific articles,and their influence along the edges of the citation graph."
Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models,"Links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics, i.e., letting the variance of particular distributions in a graphical model go to zero. For instance, in the context of clustering, such an approach yields precise connections between the k-means and EM algorithms.  In thispaper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models.  Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models.  We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis."
A latent factor model for highly multi-relational data,"Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations."
Simultaneously Leveraging Output and Task Structures  for Multiple-Output Regression,"Multiple-output regression models require estimating multiple functions, one for each output. To improve parameter estimation in such models, methods based on structural regularization of the model parameters are usually needed. In this paper, we present a multiple-output regression model that leverages the covariance structure of the functions (i.e., how the multiple functions are related with each other) as well as the conditional covariance structure of the outputs. This is in contrast with existing methods that usually take into account only one of these structures. More importantly, unlike most of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data. Several previously proposed structural regularization based  multiple-output regression models turn out to be special cases of our model. Moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs). Experimental results on both synthetic and real datasets demonstrate the effectiveness of our method."
A Minimum Frame Error Criterion for Hidden Markov Model Training,"Abstract Hidden Markov models (HMM) have been widely studied and applied over decades. The standard supervised learning method for HMM is maximum likelihood estimation (MLE) which maximizes the joint probability of training data. However, the most natural way of training would be finding the parameters that directly minimize the error rate of a given training set. In this article, we propose a novel learning method that minimizes the number of incorrectly decoded labels frame-wise. To do this, we construct a smooth function that is arbitrarily close to the exact frame error rate and minimize it directly using a gradient-based optimization algorithm. The proposed approach is intuitive and simple. We applied our method to the task of chord recognition in music, and the results show that it performs better than Maximum Likelihood Estimation and Minimum Classification Error. "
Continuous Relaxations for Discrete Hamiltonian Monte Carlo,"Continuous relaxations play an important role in discreteoptimization, but have not seen much use in approximate probabilisticinference. Here we show that a general form of the GaussianIntegral Trick makes it possible to transform a wide class ofdiscrete variable undirected models into fully continuous systems. Thecontinuous representation allows the use of gradient-based HamiltonianMonte Carlo for inference,  results in new ways of estimatingnormalization constants (partition functions), and in general opens upa number of new avenues for inference in difficult discretesystems. We demonstrate some of these continuous relaxation inference algorithmson a number of illustrative problems."
Mechanism Design for Machine Learning Problems,"While machine learning competitions like the Netflix Prize have had relative success on their own, they pave the way to think about procedural aspects of developing predictors. We believe that applying optimal structures designed using game theoretical thinking can make the process of development of machine learning solution much more efficient. However, there are some special features thatare specific to learning scenarios. In this paper, we make the initial steps towards achieving this. In particular, we address the issue that in a prediction problem the outcome of a mechanism must depend on a quantity unknown to all parties (i.e., how well the proposed algorithms will perform). We also propose a specific auction where the developers can submit multiple proposed predictors."
Deep Learning of invariant features via tracked video sequences,"We use video sequences produced by tracking as training data to learn invariant features. These features are spatial instead of temporal, and well suited to extract from still images. With a temporal coherence objective, a multi-layer neural network encodes invariance that grow increasingly complex with layer hierarchy. Without fine-tuning with labels, we achieve competitive performance on five non-temporal image datasets and state-of-the-art classification accuracy 61% on STL-10 object recognition dataset."
Scaling Bayesian Optimization to High-Dimensions via Random Embedding,"Bayesian optimization is a powerful strategy for finding the extrema of objective functions. It is applicable in situations where one does not have a closed-form expression for the objective function, but where one can obtain noisy evaluations of this function at sampled values. It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex. Because of these properties, its popularity has increased in many domains, including robotics, planning, sensor placement, news recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension. Several NIPS workshops on this topic have identified the scaling of Bayesian optimization to high-dimensions as a core challenge. Despite this, little progress has been made in this direction. In this paper, we introduce random projection tools to scale Bayesian optimization to higher dimensions. Our proposed techniques enable us to treat continuous and categorical choices simultaneously. They perform well on several challenging domains, including a synthetic example of low intrinsic dimensionality but embedded in a million dimensions, automatic configuration of a practical linear programming solver and automatic configuration of a random forests classifier for the Kinect sensor."
Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence,"We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to an algorithm called, unified gap-based exploration (UGapE), with common structure and theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms. "
On Finite Alphabet Compressive Sensing,"This paper considers the problem of compressive sensing over a finite alphabet, where the finite alphabet may be inherent to the data or a result of quantization. There are multiple examples of finite alphabet static as well as time-series data with inherent sparse structure; and quantizing real values is an essential step while handling real data in practice. This paper shows that there are significant benefits to analyzing the problem while incorporating its finite alphabet nature, versus ignoring this and employing a conventional real-alphabet compressing sensing toolbox to the problem. Specifically, when the alphabet is finite, our techniques a. have a lower sample complexity than over reals when the sparsity is below a threshold, b. facilitate constructive designs of sensing matrices based on coding theoretic principles; c. enable one to solve the exact $\ell_0$-minimization problem directly in polynomial time rather than a approach of relaxation followed by sufficient conditions for when the relaxation matches the original problem; and finally, d. allow for smaller data storage (in bits) compared to its real counterpart."
Structure Learning for Weakly Dependent Observations,"We consider the problem of estimating the graph structure of a certain class of stochastic processes defined on an Ising model. This class contains as special cases the i.i.d. observation model, the geometric $\alpha$-mixing process, and the rapidly-mixing Glauber dynamics. We analyze thenode-based neighborhood estimation algorithm of \cite{RavWaiLaf09}, which reduces to $\ell_1$-regularized logistic regression. Our main result is to provide sufficient conditions on the triple $(\numobs, \pdim, \ddim)$ for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously."
On the Sample Complexity of Robust PCA,"We estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix.This estimator is used in a convex algorithm for robust subspace recovery (i.e., robust PCA).Our model assumes a sub-Gaussian underlying distribution and an i.i.d.~sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $N$ is of order $O(N^{-0.5+\eps})$ for arbitrarily small $\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation, i.e., $O(N^{-0.5})$. Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is $O(D^{2+\delta})$ for arbitrarily small $\delta>0$ (whereas the sample complexity of direct covariance estimation with Frobenius norm is $O(D^{2})$).These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm, which are close to those of PCA. To the best of our knowledge, this is the only work analyzing the sample complexity of any robust PCA algorithm."
View-invariance and mirror-symmetric tuning in a model of the macaque face-processing system,"Recent experimental results characterizing the face-processing network in macaque visual cortex pose a major puzzle. View-tuned units (found in patches ML/MF) are a natural step to a view-tolerant representation (found in patch AM), as predicted by several models. However, the observation that cells in patch AL are tuned to faces and their mirror reflections remains unexplained (cf. Freiwald and Tsao (2010)). We show that a model based on the hypothesis that the ventral stream implements a memory-based approach to transformation invariance predicts the main properties of ML/MF, AL and AM. In this view, a major computational goal of the ventral stream is to compute invariant ?signatures? that can be used to recognize novel objects under previously-seen transformations of arbitrary ``templates''.  These invariant signatures can be regarded as encodings of a novel object relative to a compressed memory of the transformation of familiar objects (PCA). "
Inferring ground truth from multi-annotator ordinal data: a probabilistic approach,"A popular approach for large scale data annotation tasks is crowdsourcing, wherein each data point is labeled by multiple noisy annotators. We consider the problem of inferring ground truth based on noisy ordinal labels from multiple annotators of varying and unknown expertise levels. We propose a new model for crowd sourced ordinal data that accounts for instance difficulty as well as annotator expertise, and derive a variational inference algorithm for parameter estima- tion. We analyze the ordinal extensions of several state-of-the-art annotator models for binary/categorical labels and evaluate the performance of all the models on a large real world dataset containing query-url relevance scores, collected through Amazon?s Mechanical Turk. Our results indicate that the proposed model performs better or as well as existing state-of-the-art methods and is more resistant to ?spammy? ratings than popular baselines such as mean, median, and majority vote which do not account for annotator expertise."
Active Inference for Brain-Computer Interfaces with Application to an SSVEP Speller,"We view a brain-computer interface as the means by which human users may communicate their intent to a computer. Based on this view, we cast the problem of brain-computer interface design as the problem of inferring the user?s intent by asking a sequence of queries to the human user. The approach is to use an active inference model to choose the next query to be asked based on the observed responses from the previous queries and expected response times for each query. We demonstrate this approach by developing a brain-computer interface for spelling English sentences based on steady-state visually evoked potentials (SSVEP). Results show that the interface allows subjects to spell more than 10 letters per minute, which provides an improvement of performance over previous SSVEP spellers."
Learning Stable Non-linear Features in Contractive Auto-encoders,"Unsupervised learning of feature hierarchies is often a good initialization for supervised training of deep architectures.  In existing deep learning methods, these feature hierarchies are built layer by layer in a greedy fashion using auto-encoders or restricted Boltzmann machines.  Both yield encoders, which compute linear projections followed by a smooth thresholding function.  We point out that these encoders fail to find stable features when the required computation is in the exclusive-or class.  To overcome this limitation, we propose a two-layer encoder which is not restricted in the type of features it can learn.  The proposed encoder can be regularized by an extension of previous work on contractive regularization.  We demonstrate the advantages of two-layer encoders qualitatively, as well as on commonly used benchmark datasets.  "
Out-of-Sample Extensions for Manifold Learning Using Sparse Kernel Ridge Regression,"Many manifold learning algorithms do not provide a mapping from the input space to the low-dimensional manifold, requiring an out-of-sample extension to project new points onto the low-dimensional manifold. We propose an out-of-sample extension approach based on a sparse approximation to kernel ridge regression to achieve significant computational savings. In particular, we present a convex program for approximating kernel ridge regression where given a set of points in $\mathbb{R}^d$ and corresponding points in $\mathbb{R}^p$, we find a mapping from $\mathbb{R}^d$ to $\mathbb{R}^p$ that depends only on a subset of the input data points acting as support vectors. Our construction uses group sparsity and guarantees an upper bound on the average squared Euclidean distance between the predicted points of the sparsified mapping and those of kernel ridge regression. We present two medical imaging applications that necessitate a fast projection to a low-dimensional space. The first is respiratory gating in ultrasound, where we assign the breathing state to each ultrasound frame during the acquisition in real-time. The second is the detection of the position of a patient in an MRI scanner while the bed the patient lies on is moving to a target location."
Trajectory-Based Short-Sighted Probabilistic Planning,"Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [ref] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs.  We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately $10^{70}$ states."
Conditional conjugate priors,"We extend the notion of conjugates in exponential families to  conditional distributions. This yields new sets of priors which are  computationally attractive. Moreover, we show that the commonly  used $\ell_1$ and $\ell_2$ priors are special cases of our  framework. We also point out the connections to the inference with Universum method. Experiments confirm the  efficiency of our approach."
Jointly Learning and Selecting Features via Conditional Point-wise Mixture RBMs,"Feature selection is an important technique for ?nding relevant features from high-dimensional data. However, the performance of feature selection methods is often limited by the raw feature representation. On the other hand, unsupervised feature learning has recently emerged as promising tools for extracting useful features from data. Although supervised information can be exploited in the process of supervised ?ne-tuning (preceded by unsupervised pre-training), the training becomes challenging when the unlabeled data contain signi?cant amounts of irrelevant information. To address these issues, we propose a new unsupervised feature learning algorithm, the conditional point-wise mixture restricted Boltzmann machine, which attempts to perform feature grouping while learning the features. Our model represents each input coordinate as a mixture model when conditioned on the hidden units, where each group of hidden units can generate the corresponding mixture component. Furthermore, we present an extension of our method that combines bottom-up feature learning and top-down feature selection in a coherent way, which can effectively handle irrelevant input patterns by focusing on relevant signals and thus learn more informative features. Our experiments show that our model is effective in learning separate groups of hidden units (e.g., that correspond to informative signals vs. irrelevant patterns) from complex, noisy data."
Tight Bounds on Redundancy and Distinguishability of Label-Invariant Distributions,"The minimax KL-divergence of any distribution from alldistributions in a given collection has several practicalimplications. In compression, it is the least additional number of bitsover the entropy needed in the worst case to encode theoutput of a distribution in the collection. In onlineestimation and learning, it is the lowestexpected log-loss regret when guessing a sequence of randomvalues. In hypothesis testing, it upper bounds the largestnumber of distinguishable distributions in the collection.Motivated by problems ranging from population estimation totext classification and speech recognition, severalmachine-learning and information-theory researchers haverecently considered label-invariant distributions and propertiesof \iid-drawn samples.Using techniques that reveal and exploit the structure of these distributions,we improve on a sequence of previous works and show that theminimax KL-divergence of the collection of label-invariantdistributions over length-$n$ \iid sequencesis between $0.3\cdot n^{1/3}$ and $n^{1/3}\log^2n$."
Phylogenetic inference based on alignment of etymological data,"We apply models developed for population genetics to induce phylogeniesbased on linguistic data, specifically, on a large corpus of geneticallyrelated, or {\em cognate}, words from languages within a languagefamily.  This is achieved via a novel and natural projection of thelinguistic data into genetic primitives.  First, we process the cognatesets to obtain a globally-optimal alignment of the corpus.  Thealignments then serve as input to the model for phylogeneticreconstruction, which produces family tree structures that stronglymatch the ``true'' (or expected) structures.  We place our methods inthe context of those reported in the literature and illustrate themusing data from Uralic language family.  A suite of etymologicalsoftware is released for public use.  "
Causal Inference on Spillover Effects by Assignment Strategies and Linear Models,"Estimating causal effects of treatment on a network is challenging because the potential outcome of one unit is affected by the treatment on others (spillovers). To estimate the spillover effects, we introduce a novel estimand and propose two estimation approaches.In a randomization-based approach, we characterize a bias-manipulability tradeoff.Randomizations that reveal more causal information tend to have more bias and vice versa.We propose a novel randomization, namely $\inrx$, which fixes the treatment of $x\%$ of common neighbors in order to control these two competing factors.In model-based approach, assuming additivity of spillover effects results in a linear model, allowing Bayesian inference on the causal estimands. The model not only accounts for network uncertainty but also gives insight on optimal assignment through Fisher information analysis.Empirical results demonstrate the strength of the model-based approach under linear-additive spillover effects,and the strength of randomization under non-linear effects. "
Toward improving the visual stimulus meaning for increasing the P300 detection,"The P300 speller is a well known Brain-Computer Interface paradigm that has been used for over two decades. A new P300 speller paradigm (XP300) is proposed. It includes several characteristics: (i) the items are not intensified by using rows and columns, (ii) the order of the visual stimuli is pseudo-random,(iii) a visual feedback is added on each item to increase the stimulus meaning, which is the main novelty. XP300 has been tested on ten healthy subjects on copy spelling mode, with only eight sensors. It has been compared with the classical P300 paradigm (CP300). With five repetitions, the average recognition rate across subjects is 85.25\% for XP300 and 77.25\% for CP300. Single-trial detection is significantly higher with XP300 by comparing the AUC (Area Under Curve) of the ROC (Receiver Operating Characteristic) curve. The mean AUC is 0.86 for XP300, 0.80 for CP300. More importantly, XP300 has also been judged as more convenient and user-friendly than CP300, hence being able to allow longer sessions."
Direct Optimization of Ranking Measures for Learning to Rank Models,"We present a novel learning algorithm that directly optimizes the ranking measures without resorting to any upper bounds or approximations. Our appraoch is essentially an iterative greedy coordinate descent method in optimization. For each iteration, we only update one parameter along one coordinate with all others fixed. Since the ranking measure is a stepwise functionof a single parameter, we exploit an exhaustive line search algorithmto locate the interval with the best ranking measure along each coordinate.We pick the coordinates that lead to the largest improvement of ranking measures. In order to determine  the optimal value of the parameter for the selected coordinates, we construct a probabilistic framework for the permutation, and maximize the likelihood of top-$m$ ranked documents. This iterative procedure is continued until convergence.We conduct experiments on five datasets selected from Microsoft LETOR datasets, our experimental results show that the proposed DirectRank algorithm outperforms severalwell-known state-of-the-art ranking algorithms."
Bayesian Learning in Bayesian Networks of Moderate Size,"We study the problem of learning Bayesian network structures from data.Koivisto and Sood (2004)presented a DP algorithm that can computethe exact posterior probabilities of modular features in Bayesian networks of moderate size.In this paper, we propose a new algorithm which is able to efficiently sample network structuresby using the results of the DP algorithm.The network samples can then be used to efficiently estimatethe posteriors of any features.We empirically show that our algorithm considerably outperforms previous state-of-the-art methods."
Convex Adversarial Collective Classification,"Many real-world domains, such as web spam, auctionfraud, and counter-terrorism, are both relational and adversarial.Existing work on adversarial machine learning assumes that theattributes of each instance can be manipulated independently.Collective classification violates this assumption, since objectlabels depend on the labels of related objects as well as their ownattributes.  In this paper, we present a novel method for robustlyperforming collective classification in the presence of a maliciousadversary that can modify up to a fixed number of binary-valuedattributes.  Our method is formulated as a convex quadratic programthat guarantees optimal weights against a worst-case adversary inpolynomial time.  In addition to increased robustness against activeadversaries, this kind of adversarial regularization can also lead toimproved generalization even when no adversary is present.  Inexperiments on real and simulated data, our method consistentlyoutperforms both non-adversarial and non-relational baselines."
The trace norm constrained matrix-variate Gaussian process for the prioritization of disease genes,"We propose the trace norm regularized matrix-variate Gaussian process model for low-rank matrix data.A variational constraint is enforced on the model inference; resulting in aposterior matrix-variate Gaussian process with a mean function of constrainedtrace norm. We show that the resulting inference is convex, and the meaninference may be interpreted as the matrix analogue of elastic netregularization; striking a balance between the Hilbert norm and the trace norm.The proposed approach is able to significantly improve the predictionquality when the matrix is partially observed , and all the observedentries have the same value. This is a known failure case for trace normconstrained matrix estimation with Dirac kernels.Our motivating application is the prioritization of candidate disease genes.This tasks seeks to identify new associations between human genes and humandiseases, using known associations as well as kernels induced by gene-geneinteraction networks and disease ontologies,"
Minimax vs. UCT: A Comparative Study Using Synthetic Games,"Upper Confidence bounds for Trees (UCT) and Minimax are two of themost prominent tree-search based adversarial reasoning strategies fora variety of challenging domains, such as Chess and Go. Theircomplementary strengths in different domains have been the motivationfor several works attempting to achieve a better understanding oftheir behaviors. In this paper, rather than using complex games as atestbed for deriving indirect insights into UCT and Minimax, wepropose the study of relatively simple synthetic trees that permitanalysis and afford a greater degree of experimental freedom. Using anovel tree model that does not suffer from the shortcomings ofpreviously studied models, we provide a relatively straightforwardcharacterization of the kinds of games where UCT is superior toMinimax, and vice versa --- to the best of our knowledge, this is thefirst time such an effort has been successful. In particular, we showthat UCT shines in games where heuristics are accurately modeled usingadditive Gaussian noise, that contrary to previous work, earlyterminal states by themselves do not necessarily hurt UCT, and that intrees with heuristic dispersion lag, UCT is outperformed byMinimax."
Laplacian and Distance Covariance Maps: Algorithms for Supervised and Unsupervised Manifold Learning,We propose algorithms for supervised and unsupervised manifold learning. In a supervised setting the dimensionality of the features is reduced while simultaneously preserving the neighborhood structure of the features and also maximizing a statistical measure of dependence known as distance covariance between the features and a response variable. This helps in solving the prediction problem with a low-dimensional set of features. In an unsupervised setting the manifold learning algorithm produces low-dimensional representations of a high-dimensional dataset while preserving the local geometric information. The algorithms are fomulated as majorization minimization and concave convex optimization problems and are iterative.
Interpreting prediction markets: a stochastic approach,"We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution.This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved.In particular, we show that the stationary point of the stochastic process of prices generated by the market is equal to the market's Walrasian equilibrium of classic market analysis.Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour."
MCMC algorithms for near deterministic systems,"Markov Chain Monte Carlo (MCMC) methods are used ubiquitously to generate samples from a probability distribution where exact sampling is not feasible. However, in the presence of near-deterministic components in a joint distribution, it is well-known that mixing is very slow and jumping from one mode to another can take very large amount of time.In this work, we explore one possible fix to this problem by visiting the deterministic problem corresponding to the near-deterministic components. If there exists an efficient algorithm to identify the solutions to this deterministic problem, then we show that it is often possible to design an MCMC algorithm where the proposal distribution is shaped by the efficient algorithm to the deterministic problem."
Auto-WEKA: Automated Selection and Hyper-Parameter Optimization of Classification Algorithms,"There exist many different machine learning algorithms; as most of these can be configured via hyper-parameters, there is a staggeringly large number of possible alternatives overall. There has been a considerable amount of previous work on choosing among learning algorithms and, separately, on optimizing hyper-parameters (mostly when these are continuous and very few in number) in a given use context. However, we are aware of no work that addresses both problems together. Here, we demonstrate the feasibility of using a fully automated approach for choosing both a learning algorithm and its hyper-parameters, leveraging recent innovations in Bayesian optimization. Specifically, we apply this approach to the entire space of classifiers implemented in WEKA, spanning 3 ensemble methods, 14 meta-methods, 30 base classifiers, and a large range of hyper-parameter settings for each of these. On each of 10 data sets from the UCI repository, we show classification performance better than that of complete cross-validation over the default hyper-parameter settings of our 47 classification algorithms. We believe that our approach, which we dubbed Auto-WEKA, will enable typical users of machine learning algorithms to make better choices and thus to obtain better performance in a fully automated fashion."
Risk-Aversion in Multi-armed Bandits,"In stochastic multi--armed bandits the objective is to solve the exploration--exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk--aversion where the objective is to compete against the arm with the best risk--return trade--off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, we investigate their theoretical guarantees, and we report preliminary empirical results."
Confusion-Based Online Learning and a Passive-Aggressive Scheme,"This paper provides the first ---to the best of our knowledge---analysis of online learning algorithms for multiclass problems whenthe {\em confusion} matrix is taken as a performance measure. The workbuilds upon recent and elegant results on noncommutativeconcentration inequalities, i.e. concentration inequalities that applyto matrices, and more precisely to matrix martingales. We do establish generalization bounds for online learningalgorithm and show how the theoretical study motivate the propositionof a new confusion-friendly learning procedure. This learningalgorithm, called \copa (for COnfusion Passive-Aggressive) is apassive-aggressive learning algorithm; it is shown that the updateequations for \copa can be computed analytically, thus allowing theuser from having to recours to any optimization package to implement it. "
Discounting Human Reward: Limitations of Episodicity,"Several studies have demonstrated that human-generated reward can be a powerful feedback signal for control learning algorithms. However, the algorithmic space for learning from human reward has hitherto not been explored systematically. Using model-based reinforcement learning from human reward in goal-based, episodic tasks, we investigate how anticipated future rewards should be discounted to create behavior that performs well on the task that the human trainer intends to teach. We identify a ``positive loops'' problem with low discounting (i.e., high discount factors) for episodic tasks that arises from an observed bias among humans towards giving positive reward. Empirical analysis verifies the existence of the positive loops problem and further indicates that high discounting (i.e., low discount factors) of human reward is necessary in goal-based, episodic tasks. Lastly, an alternate strategy for overcoming the positive loops problem --- converting the episodic task to a continuing one --- is shown to support a wide range of discounting and therefore provide greater algorithmic flexibility."
Unfolding Latent Tree Structures using 4th Order Tensors,"Existing approaches for discovering latent structures often require the number ofhidden states as an input, a quantity usually unknown in practice. In this paper, wepropose a quartet based approachwhich is agnostic to this number. Our key contributionis a novel rank characterization of the tensor associated with the marginaldistribution of a quartet; and this characterization allows us to design a nuclearnorm based test for resolving the quartet relations. We then use this quartet testas a subroutine in a divide-and-conquer algorithm for recovering the latent treestructure. We show that, under certain conditions, the algorithm is consistent andits error probability decays exponentially with increasing sample size. In experiments,we demonstrate that our approach compares favorably to alternatives fordiscovering latent structures. In real world datasets, our approach also discoversmeaningful groupings of variables."
Generalized Least Squares for Principled Complex Backups in Temporal Difference Learning,"We derive the form of an estimator that uses generalized least squares to obtaina principled form of the eligibility trace. We show that both the $\lambda$-return and $\gamma$-return can be thought of as assuming that the inverse covariance matrix of $n$-step returns has specific values on its diagonal, and is zero elsewhere.  The new weighting scheme has a single parameter that can easily be set from data,closely matches the empirical covariance weights, and performs very well across several settings of $\gamma$ and $\epsilon$. "
Focus of Attention for Linear Predictors,"We present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious. This trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets. We observe that some examples are easier to classify than others, a phenomenon which is characterized by the event when most of the features agree on the class of an example.By stopping the feature evaluation when encountering an easy-to-classify example, the predictor can achieve substantial gains in computation. Our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to include our attentive method we prove that the average number of features computed is $O(\sqrt{n \log \delta^{-0.5}})$ where $n$ is the original number of features, and $\delta$ is the error rate incurred due to early stopping. We demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim, Gisette, and synthetic datasets."
A Convex Extension of MKL to Local Mixtures,"In the context of metric learning, localized multiple kernel learning algorithms have been proposed, featuring a richer model than MKL and improved accuracy. The optimization problem is not solved exactly because of non-convexities or approximations. We present a class of convex problems based on a generalized hinge loss, which can be solved accurately and whose solution is a sparse nonparametric local mixture of kernels. Consistency results of SVMs are generalized, and combined with convexity provide theoretical guarenties. We study the optimization of two examples, and adapt the SMO solver with colsed-form line-search to one of them, achieving fast and scalable learning. Additionally, we extend MKL to our approach, thus combining global and local kernel weights, which further improves performance and scales better with the number of kernels. Promising experiments are conducted on 3 algorithms."
Using context and phonetic features for etymological alignment and reconstruction,"This paper presents methods for investigating etymological data. First, we introducealignment algorithms which explicitly utilize phonetic features and learnlong-range contextual rules that condition recurrent correspondences within a languagefamily. Second, we present an imputation procedure which allows us comparethe quality of alignment models, as well as the goodness of the data sets.We present evaluations to demonstrate that the new model yields improvements inperformance, compared to those previously reported in the literature. We releasethe suite of etymological software for public use."
SMO based Optimization for Quadratic Programming Feature Selection,"Domains such as vision, bioinformatics, web search and web rankings invariably produce datasets where number of features is very large. To carry out classification/regression, feature selection is commonly employed to deal with the curse of dimensionality.  Recently, Quadratic Programming Feature Selection (QPFS) has been shown to outperform many of the existing feature selection methods. A quadratic program solver is used to solve the problem. This requires time complexity cubic in the number of features. Further, the algorithm needs to store the entire feature similarity matrix in memeory.In this paper, we propose an SMO based framework for QPFS (SMO-QPFS). We develop the formulation for working set selection for SMO-QPFS using second order approximations.  Our proposed approach has computaional time quadratic in the number of features in the worst case.In practice, it is shown to take linear time to converge as demonstarted by our experiments.Further, we only need to store feature similarities for the current working set (size $2\times 2$), in contrast to QPFS.This memory saving can be critical for doing feature selection in the datasets with tens of thousands of features.The performance of the SMO-QPFS is evaluated using  eight publicly available benchmark microarray datasets. From our experimental study, it is found that the SMO-QPFS is many times faster than the QPFS approach while retaining the same level of performance."
Bayesian Inference from Non-Ignorable Network Sampling Designs,"Consider individuals interacting in a social network and a response that can be measured on each individual. We are interested in making inferences on a population quantity that is a function of both the response and the social interactions. In this paper, working within Rubin's inferential framework, we introduce a new notion of non-ignorable sampling design for the case of missing covariates. This notion is the key element for developing valid inferences in applications to epidemiology and healthcare in which hard-to-reach populations are sampled using link-tracing designs, including respondent-driven sampling, that carry information about the quantify of interest."
Module propagation: probabilistic frequent subgraph discovery,"A central task in graph data analysis is to discover subgraphs recurring in a single or multiple graphs. Although many graph mining algorithms have been proposed to identify identical subgraphs recurring in multiple clean graphs, it remains an open problem to discover frequent subgraphs that appear in a {\em single} or multiple {\em noisy} graphs. Solving this problem is critical because most real-world graph data is noisy and many subgraphs are repeated only in a single graph. In this paper, we propose a new approach, Module Propagation, as a principled and practical solution to this open problem. Instead of relying on deterministic search as previous graph mining algorithms do, we reformulate the problem in a probabilistic framework.By maximizing the probability of a new Markov random field model via a fast message passing algorithm, we decompose a single or multiple noisy graphs into recurring {densely connected} and possibly overlapped subgraphs efficiently.The procedure is done efficiently  based on a message passing algorithm that explores the sparsity of our new model.We not only demonstrate the advantage of \mp on synthetic data over alternative graph mining algorithms, but also successfully apply it to a novel application of graph mining --- the discovery of modules in chemically similar crystal structures. This application can pave the way for predicting unknown crystal structures, an important yet challenging task in computational materials science."
Cardinality Restricted Boltzmann Machines ,"The Restricted Boltzmann Machine (RBM) is a popular density model thatis also good for extracting features.A main source of tractability in RBM models is the model's assumptionthat given an input, hidden units activate independently from one another.Sparsity and competition in the hidden representation is believed tobe beneficial,and while an RBM with competition among its hidden units would acquire someof the attractive properties of sparse coding, such constraints are not added due to the widespread belief that the resulting model would become intractable.In this work, we show how a dynamic programming algorithm developed in 1981can be used to implement exact sparsity in the RBM's hidden units.We then expand on this and show how to pass derivatives through alayer of exact sparsity, which makes it possible to fine-tune adeep belief network (DBN) consisting of RBMs with sparse hiddenlayers.  We show that sparsity in the RBM's hidden layer improves theperformance of both the pre-trained representations and of thefine-tuned model."
Online Multi-modal Similarity Learning for Large-scale Applications,"In many real-word scenarios, e.g., multimedia applications, data often originates from multiple heterogeneous sources or are given by diverse types of representation, which is referred to as multi-modal data. The definition of similarity between any two items/objects on multi-modal data is a key challenge encountered by many real-world applications, including multimedia information retrieval. In this paper, we present a novel online learning framework for learning similarity on multi-modal data through the combination of multiple kernels. We propose fast online multi-modal similarity learning (OMSL) algorithms which are significantly more efficient and scalable than the state-of-the-art techniques. Extensive experiments were conducted on large-scale multi-modal image retrieval applications. "
Information Driven Exploration using Poisson Sampling over Ising Marginals,"We describe an information-gathering approach for exploring an unknown scene using a range sensor. It relies on an efficient approximation of the uncertainty in the map due to visibility (occlusions). The reduction in uncertainty due to a control action is represented on an Ising model using a Poisson covering. Our algorithm improves the performance of recent visibility-based planning approaches that come with guaranteed performance bounds on the expected path length to complete exploration, and extends them to allow exploration of an unbounded region, with an extension of the bounds to exploration rate rather than complete exploration."
Learning Hierarchical Compositional Models in the Presence of Clutter,"Our goal is to identify hierarchical compositional models from highlycluttered data. The data to learn from are assumed to be imperfect intwo respects. Firstly, large portion of the data is coming frombackground clutter. Secondly, data generated by a recursivecompositional model are subject to random replacements of correctdescendants by randomly chosen ones at every level of the hierarchy.In this paper, we show the limits and capabilities of an approachwhich is based on likelihood maximization. The algorithm makesexplicit probabilistic assignments of individual data to compositionalmodel and background clutter. It uses these assignments to effectivelyfocus on the data coming from the compositional model and iterativelyestimate their compositional structure."
