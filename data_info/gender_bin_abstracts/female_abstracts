title,abstract
Community Detection in the Labelled Stochastic Block Model,"We consider the problem of community detection from observed interactions between individuals, in the context where multiple types of interaction are possible. We use labelled stochastic block models to represent the observed data, where labels correspond to interaction types. Focusing on a two-community scenario, we formulate a conjecture that the detection task goes from infeasible to feasible as the average degree in the model crosses a particular threshold. To substantiate the conjecture, we prove that the given threshold correctly identifies a transition on the behaviour of belief propagation from insensitive to sensitive. We further prove that the same threshold corresponds to the transition in a related inference problem on a tree model from infeasible to feasible. Finally, numerical results using belief propagation for community detection give further support to the conjecture."
From Sparsity to Compositionality: Unsupervised Learning of Active Basis Models for Image Representation,"Sparsity and compositionality are two fundamental concepts in image representation and vision. In this paper, we explore the transition between these two concepts. Specifically, we adopt the Olshausen-Field model where images are represented by linear superpositions of small sets of wavelet elements selected from a large dictionary. We seek to learn recurring compositional patterns of the wavelet elements in the sparse representations. We represent each compositional pattern by an active basis model, which is a composition of a small number of Gabor wavelets automatically selected from a dictionary of such wavelets. The selected wavelets are allowed to perturb their locations and orientations so that the linear basis formed by the selected wavelets become active and the active basis forms a deformable template. For a given set of training images, our method learns a vocabulary of such active basis templates, so that each training image can be represented by a small number of templates that are translated, rotated, scaled and deformed versions of the learned templates in the vocabulary. "
Contextual-?-greedy for Context-aware recommender System,"Most existing approaches in Mobile Context-Aware Recommender Systems focus on recommending relevant items to users taking into account contextual information, such as time, location, or social aspects. However, none of them has considered the problem of user?s content dynamicity. We introduce in this paper an algorithm that tackles this dynamicity. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which user?s situation is most relevant to exploration or exploitation. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms."
A Constraint Boosting Approach for Matching Problems,"In matching problems, we have two goals: $1$) maximizing the compatibility function, and $2$) satisfying the matching constraints. Since matching constraints, such as one-to-one or many-to-one, fragment the feasible space, the matching problems usually have large numbers of local optima. Existing methods are vulnerable to these local optima and easily get stuck in poor local optima. In this paper, we propose a \textbf{constraint boosting algorithm}, where matching constraints are expressed as a penalty term in the objective function, and the weight of penalty term adaptively increases. When the weight of penalty term is small, the optimization process mainly depends on the compatibility function, and thus approaches regions with large compatible values; when the weight of penalty term is large, the penalty term dominates the optimization process and force it to reach a nearby point satisfying the matching constraints. Empirically, this optimization procedure can escape from poor local optima and finally reach a good optimum. Moreover, we devise dependent optimization processes which utilize the best known optimum to escape from worse optima and reach a better optimum. In this way, an optimal or close-to-optimal solution can be quickly obtained. The experiments on various matching problems clearly demonstrate the superiority of our proposed method."
Local and Global Manifold Preserving Embedding,"Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional Euclidean space. In this paper, we propose a novel linear subspace learning algorithm called Local and Global Manifold Preserving Embedding (LGMPE). LGMPE can explicitly preserve both the local and global manifold structures which respectively describe local linear reconstruction structure and global geodesic distance structure, and can balance the contributions of the two parts. Therefore, our algorithm has the merit of handling complex data space. Several experiments on synthetic and real face datasets demonstrate the effectiveness of the proposed algorithm."
Exponential weight algorithms for the Exploration and Exploitation of Finite Sequences,"The adversarial multi-armed bandit algorithms are efficient, easy to implement and useful to solve online optimization problems. This paper considers a new setting, which we call scratch-game, where the sequences of rewards are finite. Is it possible to adapt standard adversarial bandit algorithms to take advantage of the proposed setting? We propose two new algorithms for finite sequences of rewards. In order to tune the exploration factor ?, we provide a lower bound of expected gain for each algorithm. We have compared favorably these algorithms with Exp3 on synthetic problems, on an ad serving problem and on emailing campaigns."
Experimental Proposal on Simulating Artificial Neural Netwworks Using Local Area Networks.,"An experiment for the simulation of artificial neural networks using a Local Area Network (LAN) is proposed in order to see if fundamental questions can eventually be answered such as: what is thinking made of (visual initially, but it may be possible to generalize later)? Is it possible to simulate brains using the arrangement here proposed? The paper is intended to propose the experiments in order to gather feedback concerning its nature before it is actually implemented."
Coordinate Descent Optimization for L1 Norm Low Rank Tensor Factorization,"The L1 norm low-rank tensor factorization (LRTF) is recently attracting attention due to its intrinsic robustness to outliers and missing data. However, this problem is difficult to be resolved due to its non-convexity and non-smoothness. In this paper, we propose a coordinate descent (CoD) method to solve this problem. The main idea is to coordinate-wisely optimize each scalar parameter involved in the L1 LRTF model with all the others fixed. Each of these one-scalar subproblems is proved to have a closed-form (global) solution, and thus by recursively solving these small problems, an efficient algorithm can be readily constructed to tackle the original problem. The specific advantage of the proposed CoD strategy is that it provides a unified framework of solving L1 LRTF problems on both non-missing and missing data cases, and also can be easily extended to multiple other important tensor factorization tasks, such as nonnegative tensor factorization and sparse tensor factorization. Based on a series of TensorFace experiments, it is verified that our method performs significantly more robust than previous methods in the presence of outliers and/or missing data."
Shortest Path Gaussians For State Action Graphs: An Empirical Study,"We approximate action-value functions, defined on state graphs and state-action graphs derived from the Markov decision problem (MDP) to be solved, by a linear combination of shortest path Gaussian kernels. An empirical comparison on a testbed of 3 MDPs shows that this works better than using other basis functions derived from the state or state-action graph. Examples of such other basis functions are the smoothest eigenfunctions of the combinatorial and normalized Laplacian, eigenfunctions of random walk operator and diffusion wavelet bases."
Learning Separable Dictionaries,"Many techniques in neuroscience, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are better adapted to the considered class of signals. However, high dimensional signals and the numerical costs for applying the learned dictionary in reconstruction tasks pose enormous computational challenges.In this paper, we combine the advantages of fast implementation and of capturing the global structure of a signal. This is achieved by enforcing a separable structure on the dictionary throughout the learning process. Depending on the dimension of the signal, we propose two algorithms based on geometric optimization on the product of spheres. For signals of moderate dimension, we suggest a geometric conjugate gradient method, while for learning large scale dictionaries we use an adaption of stochastic gradient descent to the geometric setting. "
High Dimensional Semiparametric Scale-invariant Principal Component Analysis,"We propose a high dimensional semiparametric scale-invariant principal component analysis, named Copula Component Analysis (COCA). The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. The COCA accordingly estimates the leading eigenvector of the correlation matrix of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefficient estimator, Spearman?s rho, is exploited in estimation. We prove that, although the marginal distributions can be arbitrarily continuous, the COCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the simulated data are conducted under both ideal and noisy settings, which suggest that the COCA loses little even when the data are truely Gaussian. The COCA is also implemented on a large-scale genomic data to illustrate itsempirical usefulness."
CODA: High Dimensional Copula Discriminant Analysis,"We propose a high dimensional classification method, named Copula Discriminant Analysis(CODA), which generalizes the normal-based linear discriminant analysis to the larger nonparanormal as proposed by Han Liu (2009). To simultaneously achieve estimation efficiency and robustness, the nonparametric rank-based methods including the Spearman?s rho and Kendall?s tau are exploited in estimating the covariance matrix. In high dimensional settings, we prove that the sparsity pattern of the discriminant features can be consistently recovered with the parametric rate, and the expected misclassification error is consistent to the Bayes risk. Our theory is backed up by careful numerical experiments, which show that the extra flexibility gained by the CODA method incurs little efficiency loss even when the data are truly Gaussian. These results suggest that the CODA method can be a safe replacement of the normal-based high dimensional linear discriminant analysis."
Spectral Graph Cut from a Filtering Point of View,"We analyze spectral graph theory based image segmentation algorithms and show there is a natural connection with edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated application of bilateral filtering. Then, using this interpretation we present and implement a fast normalized cut algorithm. Experiments show that our implementation can solve the original optimization problem with a  10x-100x speedup. In addition to these practical advantages, our work shows a deep connection between two currently separate approaches to segmentation, which suggests further directions for improvements."
Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing,"Statistical features of neuronal spike trains are known to be non-Poisson. Here, we investigate the extent to which the non-Poissonian feature affects the efficiency of transmitting information on fluctuating firing rates. For this purpose, we introduce the Kullbuck-Leibler (KL) divergence as a measure of the efficiency of information encoding, and assume that spike trains are generated by time-rescaled renewal processes. We show that the KL divergence determines the lower bound of the degree of rate fluctuations below which the temporal variation of the firing rates is undetectable from sparse data. We also show that the KL divergence, as well as the lower bound, depends not only on the variability of spikes in terms of the coefficient of variation, but also significantly on the higher-order moments of interspike interval (ISI) distributions. We examine three specific models that are commonly used for describing the stochastic nature of spikes (the gamma, inverse Gaussian (IG) and lognormal ISI distributions), and find that the time-rescaled renewal process with the IG distribution achieves the largest KL divergence, followed by the lognormal and gamma distributions. "
What is foreground: From the view of photographers,"In this paper we focus on foreground extraction in digital images. We redefine foreground as an object close to the camera in real distance, and at the same time clear in the image. Based on this observation, we propose a depth of field measure which represents the distance of object from camera, and cooperate with saliency based approach. Specifically, for a given image, we firstly extract a Saliency Map (SM) and a Defocus Map (DM), with their values corresponding to saliency value and depth of field value. Then we segment images and extract segment feature based on these two extracted maps. A classifier is trained to separate foreground from background. On test images, segments are considered to be foreground if they have high enough confidence on the trained classification model. The proposed method is tested on MSRA Salient Object Detection image set and Flickr image set. Experimental result demonstrates that our method can obtain better result on foreground extraction over related methods."
Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a differentiable function given a finite number of evaluations to the function. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost as accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and we provide a finite-sample analysis."
HMM-based Temporal Pattern Modeling of Brain States in Smoke Rehabilitation using fMRI,"Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging method widely used in research on human physio-cognitive architecture. Substantial work has been performed using fMRI to map various cognitive functions to regions of the brain or to functional networks. Also, considerable effort has been made to interpret spatial activation patterns of these functional networks to better explain and understand the underlying cognitive states. Recent approaches to this problem involve data-driven analysis such as Independent Component Analysis (ICA) and Machine Learning (ML). However, these approaches do not fully account for the intrinsic temporal properties of fMRI data and do not always provide the explanatory power necessary to reveal underlying neural processes. To achieve a more thorough representation, we propose a novel technique based upon Hidden Markov Models (HMM). We apply it to classify cognitive states such as craving and resisting using fMRI data collected in a prior study. We address the challenges in modifying Independent Component (IC) based features to fit the proposed model, evaluate its classification accuracy along with its explanatory power, and compare it to other popular ML-based techniques such as Support Vector Machine (SVM) and Neural Network (NN). The results show that HMM-based models achieve an average classification accuracy of 83% which compares favorably to SVM. We also show that although NN achieves similar average accuracy across various numbers of classes, the HMM-based models show superior performance when distinguishing among a larger number of classes. Further, and most interestingly, we find that the optimal number of hidden states in HMM-based models agrees with the optimal number of functional networks for cognitive state classification previously found in the literature, which suggests that in this context HMM-based models possess the explanatory power often lacking in ML-based approaches."
Ordered Rules for Classification: A Discrete Optimization Approach to Associative Classification,"We aim to design classifiers that have the interpretability of association rules yet have predictive power on par with the top machine learning algorithms for classification. We propose a novel mixed integer optimization (MIO) approach called Ordered Rules for Classification (ORC) for this task. Our method has two parts. The first part mines a particular frontier of solutions in the space of rules, and we show that this frontier contains the best rules according to a variety of interestingness measures. The second part learns an optimal ranking for the rules to build a decision list classifier that is simple and insightful. We report empirical evidence using several different datasets to demonstrate the performance of this method."
Fast Resampling Weighted v-Statistics,"In this paper, a novel, computationally fast, and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed. To avoid any real resampling, we have linked this problem with finite group action and converted it into a problem of orbit enumeration. For further computational cost reduction, an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively. The computational complexity analysis shows reduction in the computational cost from n! or nn level to low-order polynomial level."
Multi-task Vector Field Learning,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach."
Regularization Cascade for Joint Learning,"We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. We propose a top-down iterative method which starts with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased, until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes where different groupings of tasks seem particularly useful for effective sharing, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods."
Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions,"We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to be met in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efficient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains."
Layered Dirichlet Process for Hierarchical Modeling and Multi-Level Segmentation of Sequential Data,"Hierarchical Bayesian Non-parametric models, such as the Hierarchical Dirichlet Process (HDP) and the HDP-HMM, have been proposed as infinite dimensional mixture models for grouped data problems. Many applications, such as multi-layer segmentation of news transcripts into broad categories and individual stories, require incorporation of prior knowledge at different layers of the model hierarchy. Such prior knowledge may include layer-specific exchangeability assumptions for the data, and group-specific prior distribution on atoms. Elegant incorporation of such knowledge requires a hierarchy of non-parametric processes where atoms at each layer correspond to one layer in the Bayesian parameter hierarchy. We propose the Layered Dirichlet Process (LDP), which consists of layered sets of DPs, where atoms at each layer map to Dirichlet Processes  for the next layer. This can also be interpreted as Layered Chinese Restaurant Process (LCRP), where table assignments for customers at one layer provide restaurant assignments at the next layer. We show how prior information at different layers can be naturally incorporated in our framework. For learning and inference, we propose a block-wise Gibbs sampling algorithm, that samples the layered table assignment of each data item as a block. We demonstrate using experiments that the proposed model outperforms the HDP-HMM and the sticky HDP-HMM in modeling and multi-layer segmentation of news transcripts. "
TCA: High Dimensional Principal Component Analysis for non-Gaussian Data,"We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate t and logistic and it is extended to the meta-elliptical by Fang (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s(log d/n)^{1/2} estimation consistency rate in the transelliptical distribution family, even if the distributions are very heavy-tailed, have infinite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is also implemented in both numerical simulations and large-scale stock data to illustrate its empirical performance. Both theories and experiments confirm that TCA can achieve model flexibility, estimation accuracy and robustness at almost no cost."
A Data-Dependent Risk Bound for Incremental Learning of Radial Basis Function Networks,"A data-dependent upper bound of an expected risk for radial basisfunction networks (RBFNs) is investigated. Because the risk bound isprovided in a quite practical form, it can be used for modelselection in nonlinear regression problems, especially when the RBFNincrementally recruits its basis functions. Asymptotic properties ofRBFNs are described for consistency of the incremental learningmethods. The risk bound is closely investigated for the leastsquares method and its properties are discussed."
Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity,"Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superfluous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach."
Maximize the Ratio of Split to Sum of Diameters with Applications to Image Segmentation,"The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster, and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster. In this paper, we study the following problem: maximize the ratio of the minimum split to the sum of cluster diameters. In general, the problem is NP-hard for k >= 3 (k is the number of clusters). Here, we present an exact bipartition algorithm with the worst-case runtime O(n^4logn), where n is the number of objects. We apply the proposed algorithm to image segmentation to verify the validity of the proposed clustering criterion. Since the proposed algorithm is with high computational complexity, it is impractical to directly apply it to an image. So, we first use the farthest-point clustering algorithm to obtain a given number of superpixels of the original images, and then apply the proposed algorithm to those superpixels. The experimental results on Weizmann image segmentation challenge database demonstrate that the proposed algorithm is promising."
Efficient Sample Reuse in Policy Gradients with Parameter-based Exploration,"The policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control.A common challenge in this scenario is how to stabilizepolicy gradient estimates for reliable policy updates.In this paper, we combine the following three ideas and givea highly stable and practical policy gradient method:(a) the policy gradients with parameter based exploration,which is a recently proposed policy search method with high stability,(b) an importance sampling technique, which allows us to reuse previously gathered data in a consistent way,and (c) an optimal baseline, which minimizes the variance of gradient estimates with their unbiasedness being maintained.For the proposed method, we give theoretical analysis of the variance of gradient estimates and show its usefulness through experiments."
A Topic Model for Continuous Word Embeddedings,"While words in documents are generally treated as discrete entities,they can be embedded in a Euclidean space which reflects an \textit{a priori} notion of similarity between them.In such a case, a text document can be viewed as a bag-of-embedded-words (BoEW):a set of real-valued vectors.We propose a topic model for the BoEW:the generation process of words is modeled with a continuous mixture modelwhere each mixture component can be identified with a different topic.Retrieval and clustering experiments with the proposed BoEW representationshow significant improvements with respect to topic models computed from the traditional bag-of-words."
Dense Scattering Layer Removal ,"We propose a new model, together with advanced optimization, to separate a thick scattering media layer from a single natural image. It is able to handle challenging underwater scenes and images taken in fog and sandstorm, both of which are with significantly reduced visibility. Our method addresses the critical issue -- this is, originally unnoticeable impurities will be greatly magnified after removing the scattering media layer -- with transmission-aware optimization. We introduce non-local structure-aware regularization to properly constrain transmission estimation without introducing the halo artifacts. A selective-neighbor criterion is presented to convert the unconventional constrained optimization problem to an unconstrained one where the latter can be efficiently solved."
Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button,"A brain-computer interface (BCI) allows users to ?communicate? with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue.This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm UCB-classif based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session.Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%."
Learning Useful Abstractions from the Web: Case Study on Patient Medications and Outcomes,"The successful application of machine learning to electronic medical records typically turns on the construction of an appropriate feature vector.  That often depends upon the ability to find an appropriate way to abstract the large number of variables found in such records.  In this paper, we explore the use of topic modeling to design feature vectors in an automated manner by harnessing expertise available on the Web. We test the proposed methods on the task of inferring useful abstractions from a list of thousands of medications. Using Latent Dirichlet Allocation we learn a topic model based on Web entries corresponding to each drug in the list. Using only knowledge from Wikipedia pages, we were able to learn a model that is similar to the curated drug classification scheme that serves as an industry standard. We further demonstrate the utility of these learned abstractions through the construction of a kernel based on the earth mover's distance and derived from the learned topic model. Applied to a corpus of 25,000 patient admissions, we use this kernel to predict three different adverse outcomes (death, an abnormally long stay, or admission through the emergency room) for the next hospital admission.  Somewhat surprisingly,  the classifiers built using the learned abstractions outperform classifiers learned from the curated drug classification scheme."
Patient Risk Stratification for Hospital-Associated C. Diff as a Time-Series Classification Task,"A patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state. We explore representing patient risk as a time series. In doing so, patient risk stratification becomes a time-series classification task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted. Thus, we begin by defining and extracting approximate \textit{risk processes}, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns. We apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \textit{Clostridium Difficile}. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05). "
A Stochastic Spiking Network Model of Sensorimotor Control,"Despite decades of research on sensorimotor control, there are no models thatprovide a link between the stochastic activity of neuron populations and humanbehavior when faced with uncertain, redundant, or novel environments. Here wepropose a new computational model that provides a direct link between neuronalactivity and behavior. Our model is based on a recent mathematical frameworkthat works with state probability densities rather than explicit state variables. Itcomprises a spiking neural network including sensory receptors, sensory cortex,control operators, and motoneurons. Sensory cortex computes internal state probabilityestimates by Bayesian filtering of measurements from sensory receptorsand efference information from the motor neurons. Motoneuron commands arecalculated by optimizing a cost/value function, using internal estimates and controloperators stored as synaptic weights. We simulated the model in Matlab andimplemented it on a Phantom robot arm. Simulations and robotic demonstrationspredicted a wide variety of behavior, such as reaching and tracking, reflexes intask-relevant directions, and reward/penalty trade-off responses. Our new computationalmodel can provide a neurophysiological explanation of specific humansensorimotor functions under uncertainty."
Multivariate Convex Regression with Adaptive Partitioning,"We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function.  Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is computationally feasible for more than a few hundred observations.  We introduce Convex Adaptive Partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is computationally efficient, in stark contrast to current methods. We show that CAP has a computational complexity of O(n log(n)^2) and also give consistency results. Empirically, CAP shows dramatic improvement over existing methods in terms of runtime and predictive error."
Human Activity Learning using Object Affordances from RGB-D Videos,"Human activities comprise several sub-activities performed in a sequence and involve interactions with various objects. This makes reasoning about the object affordances a central task for activity recognition. In this work, we consider the problem of jointly labeling the object affordances and human activities from RGB-D videos. We frame the problem as a Markov Random Field where the nodes represent  objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural SVM approach, where labeling over various alternate temporal segmentations are considered as latent variables. We tested our method on a dataset comprising 120 activity videos collected from four subjects, and obtained an end-to-end precision of 81.8% and recall of 80.0% for labeling the activities."
"Top-k Feature Selection via ?2,0-Norm Constraint","Real-world applications, such as bioinformatics, often need select top-$k$ features in the classification tasks. Previous sparse learning based feature selection methods impose the sparsity regularization to learn the features weights and rank them to select the top-$k$ features. Because the ranking weights were learned not for the exact top-$k$ features, such feature selection methods may not get the optimal results. We propose a novel and robust exact top-$k$ feature selection approach using an explicit $\ell_{2,0}$-norm constraint without any extra parameter. An efficient algorithm based on augmented Lagrangian method is derived to solve the $\ell_{2,0}$-norm constrained objective to find out the stable local solution. Extensive experiments on four biological datasets show that although our proposed model is not a convex problem, it outperforms the approximate convex counterparts and state-of-the-art feature selection methods in terms of classification accuracy on two popular classifiers. Because the regularization parameter of our method has explicit meaning (in bioinformatics applications $k$ is often fixed), \emph{i.e.} the number of selected feature, it avoids the burden of tuning the parameter and is a pragmatic feature selection method."
Affect Sensitive Music Recommendation using Brain Computer Interfaces,"This paper aims to explore the problem of building an affect sensitive music recommendation system. Specifically, the proposed system harnesses electroencephalograph (EEG) signals for the purpose of assessing the listener's affective state. The core idea lies on the hypothesis that cortical signals captured by off-the-shelf electrodes carry enough information about mental state of a listener and can be used to build preference models over musical taste for each individual user. We present a reinforcement learning algorithm that aims to build such models over a period of time and then use it effectively to provide recommendations such that enable the listener to achieve the target mental state. Our experiments on real users indicate that the recommendation policy learnt via the brain-computer interface provides better recommendations than commercial services such as Pandora, that do not incorporate affect."
Solving L1 Norm Matrix Factorization from the Elementary Unit by Coordinate Descent,"The L1 norm low-rank matrix factorization (LRMF) has been attracting much attention due to its intrinsic robustness to outliers and missing data. However, L1 norm LRMF is difficult to be resolved due to its non-convexity and non-smoothness. In this paper, we propose a novel solution, which is essentially a coordinate descent approach, to L1 norm LRMF. The main idea is to break the original difficult problem into its elementary sub-problems, each involving only one single scalar parameter, and recursively solve them. Each of these one-scalar sub-problems is convex and has a closed-form solution. The proposed method thus involves only simple computations and avoids any time-consuming inner loop numerical optimization. One important advantage of our method is that it provides a unified framework of solving L1 norm LRMF problems with or without missing data, and can be naturally extended to other matrix factorization tasks, such as nonnegative matrix factorization and sparse PCA. The extensive experimental results validate that our method outperforms state-of-the-arts in term of both computational time and accuracy, especially on large-scale applications such as face recognition and structure from motion."
3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model,"This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects  in 3D by enclosing them with tight  oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model[Felz.] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are  continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2D[Felz09] and 3D object detection[Hedau12]. "
Active MAP Inference in CRFs for Efficient Semantic Segmentation,"Most MAP inference algorithms for CRFs optimize an energy function knowing all the parameters of the potentials. In this note, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to \emph{on-the-fly} select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 and MSRC-21, show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains. "
Nonconvex Relaxation Approaches to Robust Matrix Recovery,"Motivated by the recent developments of nonconvex penalties in sparsity modeling, we propose a nonconvex optimization model for handing the low-rank matrix recovery problem.Different from the famous robust principal component analysis (RPCA), we suggest recovering low-rank and sparse matrices via a nonconvex loss function and a nonconvex penalty instead of the convex ones.The advantages of the nonconvex approach lie in its stronger robustness.To solve the model, we devise a majorization-minimization augmented Lagrange multiplier (MM-ALM) algorithm which finds the local optimal solutions of the proposed nonconvex model.We also provide two efficient strategies to speedup MM-ALM, which make the running time comparable with the state-of-the-art algorithms solving RPCA.Finally, the experimental results demonstrate the superiority of our nonconvex approach over RPCA in terms of matrix recovery accuracy."
Gossip-based distributed stochastic bandit algorithms,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called \emph{exploitation-exploration dilemma} in various bandit setups. But significantly less effort has been spent on adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines or peer-to-peer (P2P) environment, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms are available in each peer. In every iteration each peer can pull one arm independently, and then some limited communication is possible with a few random other peers.  The algorithm we chose to adapt is the \Algo{$\epsilon$-GREEDY} policy that was shown by Auer et. al.~\cite{AuCeFi02} to play a suboptimal arm with probability $\bigO(1/t)$ in iteration $t$, and thereby to achieve a regret of $\bigO(\log t)$. This is optimal up to some constant factors~\cite{LaRo85}. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability that a suboptimal arm is played at a peer in iteration $t = \Omega( \log N )$ is $\bigO(1/(Nt))$, where $N$ denotes the number of peers. This result, as mentioned above, means that we achieve the best speedup that is theoretically possible~\cite{LaRo85}. The restriction $t = \Omega(\log N)$ is unavoidable in adapting such algorithms to P2P networks.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
Online Learning via Optimizing the Variational Inequalities,"A wide variety of learning problems can be posed in the framework of convex optimization. Many efficient algorithms have been developed based on solving the induced optimization problems. However, there exists a gap between the theoretically unbeatable convergence rate and practically efficient learning speed. In this paper, we cast the regularized learning problem as a variational inequality (VI). Then, we solve the induced VI using the alternating direction method of multipliers (ADMM) in an online setting. For general convex problems, this new formulation enables our stochastic ADMM to achieve an $O(1/t)$ VI-convergence rate. The experiments demonstrate that the stochastic ADMM has almost the same performance as the state-of-the-art online algorithms but its $O(1/t)$ VI-convergence rate is capable of tightly characterizing the real learning speed."
Context-Aware Wide-Area Activity Recognition,"In this paper, rather than modeling activities in videos individually, we propose a structural representation that jointly models related activities with both motion and context information. This is motivated from the observations that activities related in space and time rarely occur independently and can serve as context for each other. The spatial layout of activities and their sequential patterns provide useful cues for the understanding of activities. Given initial classifying information generated by a base classifier, our model aims to improve recognition accuracy by jointly modeling related activities using these preliminary results and various context features. The proposed model automatically captures and weights motion and context patterns for each activity class, as well as groups of them, from sets of predefined attributes during the learning process. Then the learned model is used to generate globally optimum labels for activities in the testing videos. We show how to learn the model parameter via an unconstrained convex optimization problem and how to predict the correct labels for a testing instance consisting of multiple activities. We show promising results on the VIRAT Ground Dataset that demonstrates the benefit of joint modeling and recognizing contextual activities in a wide-area scene."
Linking Heterogeneous Input Spaces with Pivots for Multi-Task Learning,"Most existing works on multi-task learning (MTL) assume the same input space for different tasks. In this paper, we address a general setting where different tasks have heterogeneous input spaces. This setting has a lot of potential applications, yet it poses new algorithmic challenges - how can we link seemingly uncorrelatedtasks to mutually boost their learning performance?Our key observation is that in many real applications, there might exist some correspondence among the inputs of different tasks, which is referred to as pivots. Forsuch applications, we first propose a learning scheme for multiple tasks and analyze its generalization performance. Then we focus on the problems where only a limited number of the pivots are available, and propose a general frameworkto leverage the pivot information. The idea is to map the heterogeneous input spaces to a common space, and construct a single prediction model in this space for all the tasks. We further propose an effective optimization algorithm to find both the mappings and the prediction model. Experimental results demonstrate its effectiveness, especially with very limited number of pivots."
Multiresolution Gaussian Processes,"We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity."
Clustering Aggregation as Maximum-Weight Independent Set,"We formulate clustering aggregation as a special instance of Maximum-Weight Independent Set (MWIS) problem. For a given data-set, an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters. The vertices, which represent the distinct clusters, are weighted by an internal index measuring both cohesion and separation. The edges connect the vertices whose corresponding clusters overlap. Intuitively, an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together. We formalize this intuition as the MWIS problem on the attributed graph, i.e., finding the heaviest subset of mutually non-adjacent vertices.This MWIS problem exhibits a special structure. Since the clusters of each input clustering form a partition of the dataset, the vertices corresponding to each clustering form a maximal independent set (MIS) in the attributed graph. We propose a variant of simulated annealing method that takes advantage of this special structure. Our algorithm starts from each MIS, which is close to a distinct local optimum of the MWIS problem, and utilizes a local search heuristic to explore its neighborhood in order to find the MWIS. Extensive experiments on many challenging datasets show that: 1. our approach to clustering aggregation automatically decides the optimal number of clusters; 2. it does not require any parameter tuning for the underlying clustering algorithms; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance; 4. it is robust against moderate or even bad input clusterings."
Diagnosing learners' knowledge from their actions using inverse reinforcement learning,"The use of computerized environments in which students complete complex tasks is increasingly common in education. Data about students' actions in these environments has the potential to provide information about these students' knowledge, but can be difficult to interpret.In this paper, we focus on instances in which a student must take a series of actions to complete a goal, and develop a framework for automatically inferring the student's underlying beliefs based on these observed actions. This framework relies on modeling how student actions follow from beliefs about the effects of those actions. By framing the problem in terms of a Markov decision process, we specify a general model that can be applied to a wide range of situations. We first validate that this model can recover learners' beliefs in a lab experiment, and then use it to model data from an educational game. In the lab experiment, the model's inferences reflect participants' stated beliefs, and for the educational game, the model's inferences are consistent with conventional assessment measures."
Hierarchical Identification of Leaves,"There is massive diversity among deformable biological objects such as cells and plants, including a large number of genetic categories and a large variation in appearance within categories. We present a novel method for determining the species of botanical objects from scanned images. We focus on leaves but the strategy is generic, based on a hierarchical representation of latent variables called identification keys which embody domain knowledge about taxonomy and landmarks. Classification proceeds systematically from coarse-grained to fine-grained characterizations. First, keys are estimated, one at a time, starting with landmarks and proceeding to the genus, and finally the individual species is identified. Each step is conditional on previous estimates. Two other main ingredients are multiple key-based local coordinate systems and likelihood ratios of discriminant scores. We obtain the best performance to date on several databases of scanned simple leaves."
Adaptive Compressive Network Modeling,"Network data is ubiquitous nowadays, such as social networks, bio-networks, computer networks, and complex information network. However, the research of network data is still mostly heuristic and lacks rigorous theoretical underpinnings. Until very recently, a new line of work named compressive network modeling sheds light on this problem.  Specifically, [13] proposes a general framework to use compressed sensing techniques to analyze network data. This method explores a large latent dictionary which is able to recover the hidden structure within the network. However, one limitation of this framework is that the dictionary used to model the network must be pre-given, e.g. clique spaces. Such a pre-determined dictionary is not adaptive to unknown datasets. In this paper, instead of assuming the dictionary is prefixed, we propose a method to automatically learn a dictionary based on the observed networks, which better adapts to the empirical data. Our approach, named adaptive compressive network modeling, is amenable to theoretical analysis, computationally tractable and can be successfully applied to many areas. "
Cosegmentation with Subspace Constraints ? Identifying Shared Structure in Related Image Sets,"We develop new algorithms to analyze and exploit the joint subspace structure of a set of related images to facilitate the process of concurrent segmentation of a large set of images. Most existing approaches for this problem are either limited to extracting a single similar object across the given image set or do not scale wellto a large number of images containing multiple objects varying at different scales. One of the goals of this paper is to show that various desirable properties of such an algorithm (ability to handle multiple images with multiple objects showing arbitary scale variations) can be cast elegantly using simple constructs from linearalgebra: this signi?cantly extends the operating range of such methods. While intuitive, this formulation leads to a hard optimization problem where one must perform the image segmentation task together with appropriate constraints which enforce desired algebraic regularity (e.g., common subspace structure). We propose ef?cient iterative algorithms (with small computational requirements) whose key steps reduce to objective functions solvable by max-?ow and/or nearly closed form identities. We study the qualitative, theoretical, and empirical properties of the method,and present results on benchmark datasets."
L$^{\natural}$-CCCP : L$^{\natural}$-Concave Convex Procedure,"L$^{\natural}$-Convexity is a discrete counterpart of convexity in a continuous function. In this paper, we propose L$^{\natural}$-CCCP (L$^{\natural}$-ConCave Convex Procedure); an approximation algorithm for minimizing the difference of two L$^{\natural}$-convex functions, which can formulate any discrete function optimization. L$^{\natural}$-CCCP is basically a discrete analog of CCCP (ConCave Convex Procedure) for D.C.\@ programing problems. L$^{\natural}$-CCCP is performed as a terminating iterative procedure, each of whose iterations can be computed in the same order as submodular minimization. We~describe the implementation of L$^{\natural}$-CCCP and prove termination at a stationary point. Moreover, we describe an application of L$^{\natural}$-CCCP to multi-label energy minimization with empirical examples."
Efficient Sparse Group Feature Selection via Nonconvex Optimization,"Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2)statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance."
Link Prediction via Ranking with a Multiple Membership  Nonparametric Bayesian Model,"Link prediction in complex networks has found applications in a wide range of real-world domains involving relational data.  The goal is to predict some hidden relations between individuals based on the observed relations.  Existing models are unsatisfactory when more general multiple membership in latent groups can be found in the network data.  Taking the nonparametric Bayesian approach, we propose a multiple membership latent group model for link prediction.  Besides, we argue that existing performance evaluation methods for link prediction, which regard it as a binary classification problem, do not satisfy the nature of the problem.  As another contribution of this work, we propose a new evaluation method by regarding link prediction as ranking.  Based on this new evaluation method, we compare the proposed model with two related state-of-the-art models and find that the proposed model can learn more compact structure from the network data."
Max-Margin Transforms for Visual Domain Adaptation,"We present a new algorithm for training linear support vector machine classifiers across image domains. Our algorithm learns a linear transformation that maps points from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce a novel cost function for transformation learning based on the misclassification loss of the target points transformed into the source domain. One advantage of our method over previous SVM-based domain adaptation algorithms is that it performs multi-task adaptation, learning a shared component of the domain shift across all categories.  Experiments on both synthetic data and real image datasets demonstrate strong performance and computational advantages compared to previous approaches."
Stationary Component Analysis for Video Classification,"Low-dimensional modelling of videos is key to the success of efficient video classification algorithms.From a signal processing perspective, video data is a mixture of stationary and non-stationary components.While videos belonging to the same class share the same stationary components,they may differ in the non-stationary ones.Existing video classification methods based on dimensionality reductiontypically fail to explicitly separate stationary parts of the signal from non-stationary ones.As a consequence, the low-dimensionality representation of the videos contains information that is not class-specific,and thus irrelevant for classification.We propose an approach to video modelling that overcomes this issue by explicitly separatingstationary from non-stationary parts of the signal.To this end, we make use of Stationary Subspace Analysis,which factorises multivariate time series into stationary and non-stationary components.Video classification can be then performed based only on the relevant, stationary part of the video signal.We demonstrate the benefits of our approach over state-of-the-art techniques on the tasks of near-duplicate video detection,dynamic texture classification and scene recognition.Our study shows that modelling videos with their stationary components not only dramatically improves recognition accuracy,but also yields a significant advantage in computational cost of classification over existing methods."
A New Fast Stochastic Bayesian Learning Automata: a Machine Learning Perspective,"One of the drawbacks of Learning Automata is having a relatively slow rate of convergence, thus the main challenge of Learning Automata theory is designing faster learning algorithms. In this paper, we propose a new fast learning algorithm from a machine learning perspective. The key idea is that the estimator which estimates the probability of stochastic environment rewarding each action, is considered as reconstructing Bernoulli distribution from sequential data, and is formalized based on exponential conjugate family which enables our designed Learning Automata having relatively simple format and hence easy to be implemented. We emphasize that this approach is quite generous and applicable to existing estimator based Learning Automata. Meanwhile,the -optimality of the proposed Learn-ing Automata referred to as Generalized Bayesian Stochastic Estimator Learning Automata is also presented. Extensive experimental results on benchmark environments demonstrate our proposed learning scheme is faster than the LA state of the art."
Identifying Unique Features in Latent Generative Models:  Medicinally-Induced fMRI Networks in Buproprion Trials,"Machine learning and feature evaluation algorithms often assume a direct correspondence of features across observations, with the features themselves well-established and directly measurable. Oftentimes though the true number and form of the features are unknown, and observations are merely a permuted assortment of distorted variables. Four-dimensional neuroimaging data, such as fMRI, can be understood as a collection of brain networks, operating over time.  These networks vary both within and across subjects'  brain scans, and not all brain networks operate during a given scan. Because of this, it is non-trivial to identify generative, consistent features that are unique to certain treatment conditions.  We present a method for learning the true form, uniqueness, and function of latent generative features in situations where the observations are a permuted selection of distorted features, obtained from a higher-level generative set of possible parents. Two competing basis sets are used to model the observed features within the desired treatment condition, and the distribution of the model fits are used to compute a posterior probability for whether the observed feature originated from the set of generative features common across all treatment conditions, or whether that feature exists only within that treatment condition. The observed unique features are then used to create a new generative set that is specific to the treatment condition.  We illustrate these methods by identifying the treatment networks associated with the drug buproprion during a smoking-cessation study, obtaining possible brain networks that exist only post-treatment for subjects who took the medication, but for none of the other treatment groups."
Toward Optimal Uniform Stratification for Stratified Monte-Carlo Integration,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite budget n of noisy evaluations to the function. We tackle in this paper the problem of stratifying the domain in an efficient way. More precisely, it is interesting to refine the partition of the domain as much as possible, to have more flexibility on where to sample. On the other hand, having a (too) refined stratification is not optimal, since the more refined it is, the more difficult it is to estimate the variance of the noise and the variations of the function, in each stratum. We provide in this paper an algorithm, Adapt-MC-UCB, that is almost as efficient (up to a constant) as the algorithm MC-UCB (introduced in [Carpentier and Munos, 2011]) run on the best uniform partition defined in a hierarchical partitioning of the domain."
A Spectral Algorithm for Latent Dirichlet Allocation,"The problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that the observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem involving the estimation of  topic probability vectors (the distribution of words under each topic), when only the words are observed and the corresponding topics are hidden.We provide a simple and an efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which can be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order observed moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on $k \times k$ matrices, where $k$ is the number of latent factors (e.g., the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$)."
Controlling Transfer For Reinforcement Learning,"Recently, algorithms for transfer learning in reinforcement learning have been proposed that utilize a map between state-action pairs in the target and source tasks. This map is used to suggest possible values for states in the target task based on values of states in the source task. In this paper, we describe a generic transfer algorithm that, given any such map, determines online if the the map is correct and if not limits its use and minimizes {\em negative transfer}. We give bounds on the expected negative transfer and perform experiments to illustrate the usefulness of the algorithm."
Bayesian Conditional Tensor Factorizations for High-Dimensional Classification,"In many application areas, data are collected on a categorical response and highdimensional categorical features, with the goals being to build a parsimoniousmodel for classification while doing inferences on the important features. By using a carefully-structured Tucker factorization, we define a model that can characterizeany classification function, while facilitating variable selection and modeling of higher-order interactions. Following a Bayesian approach, we propose a Markov chain Monte Carlo algorithm for posterior computation accommodating uncertainty in the features to be included. Under near sparsity assumptions, the posterior distribution for the classification function is shown to achieve close tothe parametric rate of contraction even in ultra high-dimensional settings in which the number of candidate features increases exponentially with sample size. Themethods are illustrated through several applications."
Sample Bias Correction for Regression,"This paper presents a theoretical and empirical study of a discrepancy minimization (DM) sample bias correction algorithm in regression.  We give a theoretical analysis of sample bias correction for kernel ridge regression and prove new and more informative learning guarantees in terms of the \emph{discrepancy} of the empirical distributions. These results provide a strong theoretical support for the use and application of the DM algorithm. We have carried out an extensive empirical analysis of this algorithm both with artificial and real-world data sets and compared it with three other state-of-the-art sample bias correction algorithms applicable in regression. Till now, this algorithm had only been applied to the problem of domain adaptation and it was primarily evaluated for computational efficiency. Here we carry out a thorough comparative study of the algorithm used for the problem of sample bias correction. We report in detail the results of these empirical results which demonstrate the benefits of this algorithm."
Accuracy at the Top,"We introduce a new notion of classification accuracy based on the top $\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines. We define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems. We also present margin-based guarantees for this algorithm based on the $\tau$-quantile of the functions in the hypothesis set. Finally, we report the results of several experiments evaluating the performance of our algorithm. In a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top."
Multi-Class Classification with Maximum Margin Multiple Kernel,"We present a new algorithm for multi-class classification withmultiple kernels. Our algorithm is based on a natural notion of themulti-class margin of a kernel. We show that larger values ofthis quantity guarantee the existence of an accurate multi-classpredictor and also define a family of multiple kernel algorithms based onthe maximization of the multi-class margin of a kernel.  Wepresent an extensive theoretical analysis in support of our algorithm,including novel multi-class Rademacher complexity margin bounds.Finally, we also report the results of a series of experiments withseveral data sets, including comparisons where we improve upon theperformance of state-of-the-art algorithms both in binary andmulti-class classification with multiple kernels."
Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics,"Non-linear dynamical systems (DS) have been used extensively for building generative models of human behavior. Its applications range from modeling brain dynamics  to encoding motor commands. Many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space. Although these enable the robots to react against sudden perturbations without any re-planning, the motions are always directed towards a single target. In this work, we focus on combining several such DS with distinct attractors, resulting in a multi-stable DS. We show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object. While exploiting multiple attractors provides more flexibility in recovering from unseen perturbations, it also increases the complexity of the underlying learning problem. Here we present the Augmented-SVM (A-SVM) model which inherits region partitioning ability of the well known SVM classifier and is augmented with novel constraints derived from the individual DS. The new constraints modify the original SVM dual whose optimal solution then results in a new class of support vectors (SV). These new SV ensure that the resulting multi-stable DS incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction. We show, via implementations on a simulated 10 degrees of freedom mobile robotic platform, that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations. "
Predicting a Neural Spiking Probability Map,"This paper models signals and noise for extracellular neural recording. Although recorded data approximately follow Gaussian distribution, there are slight deviations that are critical for signal detection: a statistical examination of neural data in Hilbert space shows that noise forms an exponential term while signals form a polynomial term. These two terms can be used to estimate a spiking probability map which tells the probability of spike presence in any time window. The predictions of the two terms and the spiking probability map are quantitatively assessed with both animal and synthesized data. To demonstrate the usefulness of the work, a few application examples are presented with quantitative experimental results that have shown improved signal processing reliability. Last, we report an algorithm implementation in a 130nm CMOS process for on-the-fly processing multi-channel neural data."
Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs,"Graphical model selection refers to theproblem of estimating the unknown graph structure givenobservations at the nodes in the model. We consider achallenging instance of this problem when some of thenodes are latent or hidden.  We  characterize  conditionsfor tractable graph estimation and develop efficient methods with provable guarantees. We consider the class of Ising models Markov on  locally tree-likegraphs, which are in the regime of correlationdecay. We  propose an efficient method for graph estimation, andestablish its structural consistency when the number of samples$n$ scales as $n = \Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where $\theta_{\min}$ is theminimum edge potential, $\delta$ is the depth (i.e.,distance from a hidden node to the nearest  observed nodes),and $\eta$ is a parameter which depends on the minimum andmaximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides  flexibility to control  the number of latent variables and the cycle lengths in the output graph.  We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound  on sample requirements.  "
Learning Mixtures of Tree Graphical Models,"We consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with  provable guarantees.   Our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The   sample and computational requirements for our method scale as $\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs."
Multi-Label Classification with Relevance Ordering,"In many real multi-label tasks, it is often the case that the ordering of relevant labels for an example is important, while that of irrelevant labels is meaningless. Such a problem, however, could not be addressed by existing multi-label learning or label ranking approaches, because the former usually ignores the ordering among relevant labels while the latter often fails to explicitly distinguish relevance from irrelevance and involves more consideration on ordering irrelevant labels. Considering that there is no adequate criterion available, in this paper, we propose PRO Loss, a new criterion which concerns the ordering of only relevant labels and classification of all the labels. We then propose ProSVM that optimizes PRO Loss efficiently with the use of alternating direction method of multipliers. We further improve efficiency with an upper approximation that reduces the number of constraints from O(T^2) to O(T), where T is the number of labels. Experiments show that our proposals not only perform superior to state-of-the-art approaches on PRO Loss, but also achieve highly competitive performance on existing evaluation criteria."
A Mixed-Initiative Approach to Solving  Correspondence Problems,"Finding correspondences among objects in different images is a critical problem in computer vision. Correspondence procedures can fail when faced with deformations, occlusions, and differences in lighting and zoom levels across images. We present a methodology for augmenting correspondence matching algorithms with a means for triaging the focus of attention and effort in assisting the automated matching. The mix of human and automated initiatives is guided by a speci?c computation of the expected value of resolving correspondence uncertainties. We introduce this measure and show how it can be used to guide efforts by human taggers. We explore the value of the approach with experiments on benchmark data. "
ML Confidential: Machine Learning on Encrypted Data,"We demonstrate that by using a recently proposed somewhat homomorphic encryption (SHE) scheme it is possible to delegate the execution of a machine learning (ML) algorithm to a compute service while retaining confidentiality of the training and test data. Since the computational complexity of the SHE scheme depends primarily on the number of multiplications to be carried out on the encrypted data, we devise a new class of machine learning algorithms in which the algorithm's predictions viewed as functions of the input data can be expressed as polynomials of bounded degree. We propose confidential ML algorithms for binary classification based on polynomial approximations to least-squares solutions obtained by a small number of gradient descent steps. We present experimental validation of the confidential ML pipeline and discuss the trade-offs regarding computational complexity, prediction accuracy and cryptographic security."
GenDeR: A Generic Diversified Ranking Algorithm,"Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling,product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a wide range of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm."
Online Learning of Rotations Using Geometric Structures,"This paper provides a solution to online learning for rotations by employing exponentials of sparse antisymmetric matrices. The method performs similarly to Riemannian gradient based methods but is derived using simple matrix algebraic techniques. We first show that a general optimization problem with a rotation constraint can be transformed into an equivalent problem in the space of antisymmetric matrices. An efficient approach is then introduced to iteratively solve the problem using antisymmetric matrices with one or more nonzero columns and an equal number of nonzero rows. Specially, we show that it is sufficient to employ antisymmetric matrices with only one nonzero column and row. Fast implementation is also presented to simplify the computation of sparse antisymmetric matrix exponentials involved in the algorithm. Experimental results obtained by using a variety of loss functions indicate that the algorithm converges quickly and estimates unknown rotations accurately."
Correlations strike back (again): the case of associative memory retrieval,"It has long been recognised that statistical dependencies in neuronalactivity need to be taken into account when decoding stimuli encoded ina neural population. Less studied, though equally pernicious, is theneed to take account of dependencies between synaptic weights whendecoding stimuli previously encoded in an auto-associative memory. Weshow that activity-dependent learning generically produces suchcorrelations, and failing to take them into account in the dynamics ofmemory retrieval leads to catastrophically poor recall. We deriveoptimal network dynamics for recall in the face of synaptic correlationscaused by a range of synaptic plasticity rules. These dynamics involvewell-studied circuit motifs, such as forms of feedback inhibition andexperimentally observed dendritic nonlinearities. We therefore show howassuaging an old enemy leads to a novel functional account of keybiophysical features of the neural substrate."
Picking the Low-Hanging Fruit First with Non-Convex Learning to Rank,"  In many learning to rank applications the main goal is to boost  precision at least at low recall. This can usually be done by  focusing the learning on the ``low-hanging fruit'', that is the  documents that are easier to model. In principle, this can be  obtained automatically by optimising appropriate ranking losses such  as AP that reward precision. However, we show that existing convex  surrogates of these losses, as used in practice by many  learning-to-rank methods as a convenient optimisation tool, may fail  to do so to the point that simple binary SVM can be competitive with  more advanced approaches.  We then show how to modify the latter to  better capture easy sub-populations in the training, improving  precision as part of a non-convex optimisation step. We demonstrate  this algorithm on a retrieval task in computer vision, including the  stage-wise construction of a mixture of linear models."
Sampling with Deterministic Constraints,"Deterministic and near-deterministic relationships among subsets of variables in multivariate systems are known to causeserious problems for Monte Carlo algorithms. We examine the family of problems in whichthe relationship $Z = f(X_1,\ldots,X_k)$ holds and we wish to obtain exact samples from the conditional distribution $P(X_1,\ldots,X_k\mid Z= z)$. We begin with the case where $f$ is addition,showing that the problem is NP-hard even when the $X_i$s are independent and each has only two possible values.In more restricted cases---for example, i.i.d. Boolean or uniform continuous $X_i$s---efficient exact samplers have been obtained previously.For the case where each $X_i$ has a bounded range of integer values, we derive an $O(k)$ dynamic programming algorithm called {\em exact constrained sequential sampling} (ECSS).For the more general, continuous case, we propose a {\em dynamic scaling} algorithm (DYSC),a form of importance sampling. We evaluate these algorithms on several examplesand derive generalized forms that operate with any function $f$ that satisfies certain natural conditions."
fMRI Based Localization of EEG ,"This work introduces a novel EEG/fMRI integration approach that attempts to improve the spatial resolution of EEG using fMRI data, based on concurrent EEG/fMRI recordings. Advanced signal processing and machine learning, are used to improve EEG spatial resolution for specific regions based on simultaneous fMRI activity measurements of these areas.  We concentrate on demonstrating improved EEG localization in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model which is based on time/frequency representation of EEG data can predict the amygdala activity significantly better than traditional theta/alpha activity.  The important outcome is that with the proposed framework, the activity of sub-cortical regions can be analyzed with higher temporal resolution than obtained by fMRI. "
Group Regularization of Correlated Features in Conditional Random Fields,"Conditional random fields allow extracting completely arbitrary linguistic dependencies between labels and observations. Optimization and even storage of billions of features is a problem; that is why a number of model selection approaches have been proposed. Although modern feature selection methods are efficient, they usually do not take into consideration correlation between  dependencies.  In this contribution we consider application of group and hierarchical linguistic dependencies using composite norms. We propose a new optimization approach which exploits the matrix of the second derivatives that keeps important information about correlations between model dependencies.  We illustrate by experiments on standard natural language learning data sets that the proposed approach is efficient. "
Dictionary Learning from Ambiguously Labeled Data,"We propose a dictionary-based learning method for the problem of ambiguously labeled multiclass-classification, where each training sample has multiple labels and only one of the labels is correct. The dictionary learning problem is solved using an iterative alternating-algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. For each sample, a confidence/probability distribution is associated with the set of ambiguous labels. The dictionaries are updated using both soft and hard decision rules. Extensive evaluation on the Labeled Faces in the Wild, PIE, and Brodatz datasets demonstrates that the proposed method performs significantly better than state-of-the-art partially-labeled learning approaches."
Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation,"This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture.Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP.Extensive experiments on different data sets demonstrate that EC-PBP achieves a higher topic modeling accuracyand reduces more than $80\%$ communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm."
Improving Academic Homepage Crawling with Unlabeled Data,"We investigate the design of a focused crawler for obtaining academic homepages: Given a set ofseed URLs that comprise hubs for academic homepages, how easy is it to filter out unwanted webpagesfrom the content being crawled? Academic homepages or professional homepages ofresearchers are rich sources of metadata from the perspective of digital libraries. Content-gatheringprocesses in digital libraries therefore need to track these resources on a frequent-basis necessitatingtechniques for obtaining accurate lists of academic homepages.We present results indicating that publicly-available datasets for building content-based focusedcrawlers for academic homepagessuffer from a high-degree of false positives. We trace this problem to the disparityin the training and deployment environments of the crawler. We propose URL-based hints ascomplementary evidence to address this issue and propose the use of content features and URL n-gramsas independent views for identifying homepages. Then, we illustrate that these independentviews can be effectively used with unlabeled examples in a co-training framework to adapt thecontent-based classifier to the changed environment. Experiments on our validation setsshow that this process remarkably decreases the false-positive rate of our content-basedclassifier from 34% to 15%."
Kernel Clustering: Speedup via Efficient and Accurate Eigenfunction Approximation,"Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data and achieve state-of-the-art clustering performance. However, they suffer from two major drawbacks: (i) Their runtimecomplexity and memory requirements increase quadratically with the number of data points, rendering them unsuitable for large data sets, and (ii) the resulting partitions cannot be used to efficiently handle out-of-sample data points. We address these limitations by developing an efficient kernel clustering algorithm based on approximate eigenfunctions. The eigenfunctions are found using a small number of randomly sampled data points, thereby avoiding the computation of the full kernel matrix. The eigenfunctions are then used to compute a low-dimensional representation ofthe cluster centers, which can later be used to assign any out-of-sample point to a cluster. Unlike the existing approaches that compute the eigenfunctions using either thekernel similarity between all the points in the data set or only the similarity between the sampled points, we use the similarity between the sampled points and all the data points, leading to a better trade-off between efficiency and accuracy in eigenfunction approximation, as supported by ouranalysis. Our empirical studies demonstrate that, while the performance of the proposed kernel clustering technique is similar to the performance of state-of-the-art algorithms for large-scale data clustering, its runtime is significantly lower. This allows us to perform kernel clustering efficiently on data sets with millions of points. "
The flip-the-state transition operator for Restricted Boltzmann Machines,"Most learning algorithms for restricted Boltzmann machines (RMBs) rely onMarkov chain Monte Carlo (MCMC) methods using Gibbs sampling. The mostprominent examples are Contrastive Divergence learning (CD) and its variantsas well as Parallel Tempering (PT). The performance of these methods dependsseverely on the mixing properties of the Gibbs chain. We propose a Metropolis-type MCMC algorithm relying on a transition operator maximizing the probabilityof state changes. It is shown that the operator induces an irreducible and aperi-odic and, thus, a properly converging Markov chain. The transition operator canreplace Gibbs sampling in RBM learning algorithms without producing computa-tional overhead. It is shown empirically that this leads to faster mixing and in turnto more accurate learning."
Non-Linear Dimensionality Reduction by Isometric Patch Alignment,"We propose a novel dimensionality reduction method which has low computational cost. This method is inspired by two key observations: (i) the structure of reasonably large patches of high-dimensional data can be preserved as a whole, rather than divided into small neighborhoods; and (ii) attaching two neighboring patches will align them such that the overall rank does not increase. In the proposed approach, first the data is clustered, so that it is conceptually reduced to a set of overlapping low-rank clusters. Each cluster is embedded into a low-dimensional patch and then all of the patches are rearranged such that their border points are matched. We show that the rearrangement can be computed by solving a relatively small semi-definite program. The embedding computed by this optimization is provably low-rank. The proposed method is stable, fast, and scalable; experimental results demonstrate its capability for manifold learning, data visualization, and even complex tasks such as protein structure determination."
Developmental Stage Annotation of Drosophila Embryos using Transfer-Active Learning,"Drosophila melanogaster is the major model organism for explicating the function and interconnection of animal genes and for establishing a better understanding of human diseases. Today, images capturing gene expression have unprecedented spatial resolution, resulting in high quality maps (expression images) in model organisms. Based on morphological landmarks, the continuous process of Drosophila embryo genesis is traditionally divided into a series of consecutive stages (e.g., 1-16). The manual annotation of developmental stage of an image of an embryo is done by an expert. This manual annotation process is very expensive and time consuming. It is, therefore, tempting to design computational methods for the automated annotation of gene expression patterns. Images of geneexpression patterns vary between different image databases based on their imaging techniques and resolutions, leading to distribution difference across databases.Hence, it is a challenge to directly use an available annotated image database for building a classifier for a new database of images. In this paper, we propose a transfer and active learning based computational method to develop a classification system for annotating the gene expression patterns. Transfer learning or domain adaptation is performed on the labeled database and active sampling is performed on the new set of images.The proposed framework performs domain adaptation and active sampling simultaneously by  minimizing a common objective of reducing distribution difference between the domain adapted source, the queried samples and the rest of the unlabeled target domain data. Our empirical studies on synthetic and the Drosophila image databases demonstrate the potential of the proposed approach."
Analytic Tuning of the Elastic Net With an Application to Text Mining,"The elastic net is a proposed compromise between the stability of ridge regression and the model selection properties of the lasso. It is particularly useful for conservative model selection in the presence of data with highly correlated covariates, where the lasso is known to behave poorly. Tuning the regularization parameter in the elastic net via cross-validation, however, greatly reduces its model selection properties. We provide novel analytic tuning values for the elastic net estimator along with finite sample guarantees for parameter estimation. Our tuning method is applied to age prediction in a standard text corpus of internet blogs."
eXclusive Independent Component Analysis,"Independent Component Analysis (ICA) has been widely investigated for the both of the problems of blind source separation and efficient coding. In most of the relevant studies, the independent components of a signal are concerned. However, the exclusiveness of the components between two signals are rarely discussed. It is important to study the common basis set of two data sources as their complement basis sets. Thus, we introduce a method for finding the exclusive basis sets based on ICA, the eXclusive Independent Component Analysis (XICA). It aims to exclude the common features of two or more given signals from the the basis set in order to obtain more significant features of the given signals.In this paper, we investigated two image sets: nature images and urban images. The nature image set is composed by natural scene, and the urban image set contains artificial objects, such as buildings, cars, phones, etc. By using XICA, we will be able to see what is the essential difference between the nature and urban image sets in terms of their independent components."
Lazy Robust Derivative-Free Optimization for Hyperparameter Tuning,"Stochastic response surfaces pose a challenge to derivative-free optimization algorithmsthat rely on sampled gradients, empirical simplexes, or pointwise valuecomparisons. Achieving robustness can require averaging across many samples,which may be computationally expensive. This paper addresses both robustnessand efficiency in derivative optimization. It describes how the classic Nelder-Mead algorithm can be combined with statistical hypothesis testing to efficientlydeal with noisy evaluations that are commonplace when tuning machine learningalgorithm hyper-parameters. Employing lazy our lazy evaluation strategy leads toan efficient yet statistically sound algorithm, called Lazy Robust Nelder-Mead(LRNM). Empirical evaluation demonstrates the benefits of the approach overpopular alternatives."
Model checking For Improved Adaptive Behaviour,"Closed loop systems are traditionally analysed using simulation. We show how a formal approach, namely model checking, can be used to enhance and inform this analysis. Specifically, model checking can be used to verify properties for any execution of a system, not just a single experimental path. We describe how model checking has been used to investigate a system consisting of a robot navigating around an environment, avoiding obstacles using sequence learning. We illustrate the power of this approach by showing how a previous assumption about the system, gained though vigorous simulation, was demonstrated to be incorrect, using the formal approach."
Active Learning with Hinted Support Vector Machine,"The abundance of real-world data and limited labeling budget calls for active learning, which is an important learning paradigm for reducing human labeling efforts. Many recently developed active learning algorithms consider both uncertainty and representativeness when making querying decisions. However, considering representativeness with uncertainty concurrently usually requires tackling other sophisticated and challenging learning tasks, such as clustering. In this paper, we propose a new active learning framework, called hinted sampling, which takes both uncertainty and representativeness into account in a simpler way. We design a novel active learning algorithm within the hinted sampling frameworkwith an extended support vector machine. Experimental results validate that the novel active learning algorithm can result in a better and more stable performance than that achieved by state-of-the-art algorithms."
Robust Bijective Vector-Valued Function Learning by Jointly Learning Its Inverse,"We discuss about a quite challenging problem in this paper: given the train data with large proportion of the outliers, the goal is to robustly estimate a bijective vector-valued target function. The existing methods only learn the target function itself or learn its inverse respectively and thus can?t handle the outliers well. To address this problem, we propose a robust method for bijective vector-valued function learning. In this approach, the target function and its inverse are bounded together and then jointly optimized under the maximum likelihood estimation (MLE) framework. By associating each sample with a latent variable that indicates whether the sample is an inlier, Expectation Maximization algorithm is employed to solve the MLE problem. To show its usefulness, the proposed method is applied to solve a fundamental problem of computer vision tasks. In detailed, given a set of putative point correspondences between two images that large proportion of correspondences are mismatches, the objective is to estimate a mapping function which can identify correct matches as inliers and distinguish the mismatches as the outliers. The experimental results have demonstrated that our proposed method is very robust and outperforms the state-of-the-art methods."
Towards utilization of neural correlates of loss of control for enhanced human-machine interaction,"Perceived loss of control during human-machine interaction (HMI) is a well-known problem that reduces the usability of a technical device dramatically. Recently, it was proven that changes in cognitive user state can be detected by a passive Brain-Computer Interface (BCI) in a laboratory setup. Potentially, a passive BCI can significantly improve a given HMI in shared control systems. Applying it in real world applications implies its own  constraints,  e.g. reducing the number of  electrodes and limiting the time needed for calibrating the system for the sake of improved usability. Here, we investigate neural correlates of loss of control with an independent component analysis on a set of 32 channel EEG-datasets recorded from 12 subjects. Even though the number of channels is rather low for ICA, we could identify the level of mental workload on the channel level as well as in the source space to be correlated to loss of control. This outcome forms a first basis for selection of a neuropsychologically meaningful and low-dimensional feature space, which should lead to improved reliability and reduced calibration time for a passive BCI detecting loss of control in real world applications."
Semi-supervised Domain Adaptation on Manifolds,"In real-life problems the following semi-supervised domain adaptation scenario is often encountered: We have full access to some source data which is usually very large; The target data distribution is under certain unknown transformation of the source data distribution;Meanwhile only a small fraction of the target instances come with labels.The goal is to learn a prediction model by incorporating information from the source domain that is able to generalize well on the target test instances.We consider an explicit transformation function that maps examples from the source to the target domain, andwe argue that by proper preprocessing of the data from both source and target domains, the feasible transformation functions can be characterized by a set of rotation matrices.This naturally leads to an optimization formulation under the special orthogonal group constraints.We present an iterative coordinate descent solver that is able to jointly learn the transformation as well as the model parameters, while the geodesic update ensures the manifold constraints are always satisfied. Our framework is sufficiently general to work with a variety of loss functions and prediction problems. Empirical evaluations on synthetic and real-world experiments demonstrate the competitive performance of our method with respect to the state-of-the-art."
From Deformations to Parts: Motion-based Segmentation of 3D Objects,"We develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3D) poses.  We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.  To allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.  By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty.  Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods."
Operator-valued kernel-based autoregressive models with application to biological network inference,"Reverse-modeling of gene regulatory network from time-series of gene expression still remains a challenging problem in computational systems biology. Works concerning network inference from temporal data usually rely on sparse linear models or Granger causality tools. A very few address the issue in the nonlinear case. In this work, we propose a nonparametric approach to dynamical system modeling that makes no assumption about the nature of the underlying nonlinear system. We introduce a new family of vector autoregressive models based on operator-valued kernels to identify the dynamical system and retrieve the target network. As in the linear case, a key issue is to control the model's sparsity. We propose an alternate minimization procedure to learn both the kernel and the basis vectors. We show very good results both in  estimation on DREAM benchmarks as well as on the IRMA datasets."
Convex Approximation to Mixture Models Using Step Functions,"The {\em parameter estimation} to  mixture models has been shown as a local optimal solution for decades. In this paper, we propose   a  {\em functional estimation} to  mixture models using step functions. We show that the proposed functional inference  yields  a  convex formulation and  consequently the  mixture models  are feasible for a global optimum inference with an asymptotic consistency. Furthermore, the simple gradient ascent method to optimize the convex formulation provides a theoretical explanation and clarification for  the heuristics of clustering by affinity propagation \cite{fdcpmbd07}. The proposed method opens a series of possibilities to achieve global optimal solutions to important  problems such as clustering, graph partitioning, and  finding information cascades in networks."
A Geometric take on Metric Learning,"Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way.We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structuregives us a principled way to perform dimensionality reduction and regression according to the learned metrics.Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Combined these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data."
Novel Sparse Modeling by L2 + L0 Regularization,"We propose a novel sparse modeling method, the combination of L2 with L0 norms, to achieve feature selection while generating a well-predictive model at the same time.For many machine learning applications, feature selection is a crucial technique to construct the subset of features that is sufficient for prediction.We propose a novel sparse modeling framework, L0 elastic net.This framework encourages (1) reducing the redundancy of the predictive model and (2) searching for the most informative values and compact subset features.Furthermore, we propose the solver to search for the solution of L0 elastic net efficiently.As a theoretical analysis, we prove that the derived solution of L0 elastic net matches the minimal value of the L2-generalized objective function.Experimental results show that L0 elastic net is more suited to derive the compact predictive model than other previous regularization methods in a practical computational time."
Sample selection bias in unsupervised clustering,Sample selection bias has been studied in supervised settings when the training and test data are drawn from different distributions. We consider the effect of sample selection bias in an unsupervised setting when mixed-membership models are used for population stratification and topic modeling.We examined the effect of biased sampling on the accuracy of unsupervised clustering in terms of learning the low-dimensional representations of objects (documents or individual genotypes). We found that the accuracy of unsupervised clustering using a mixed-membership model is affected to a large extent by the sample used for analysis and how representative it is of the underlying populations. We also propose a correction for sample selection bias that is effective in real applications.
Robustness and Generalization for Metric Learning,"Metric learning has attracted a lot of interest over the last decade, but little work has been done about the generalization ability of such methods. In this paper, we address this issue by proposing  an adaptation of the notion of algorithmic robustness,  previously introduced by Xu and Mannor in  classic supervised learning, to derive generalization bounds for metric learning.  We also show that a weak notion of robustness is a necessary and  sufficient condition to generalize well, justifying that it  is fundamental for  metric learning. We provide some illustrative examples of our approach on a large class of existing algorithms."
Big-Five Personality Prediction from User Behaviors at Social Network Sites,"A central problem in psychology is the study of personality, which uniquely characterizes a human being through a set of psychological traits. Traditionally, personality assessment is performed by means of manually filling up a self-report inventory. However, its reliability could be influenced by subjective bias including e.g. social desirability bias, and subjective perception may also be unreliable, due to participants limited cognitive ability.In this paper, we propose a computational and objective approach to predict the so called Big-Five personality of an individual from his/her Social Network Site(SNS) behaviors. This is the first such attempt ever to objectively measure the Big-Five Personality from SNS. Inspired by the Multi-Task Learning (MTL) methods from machine learning community, we develop a dedicated learning method to address this problem. Empirical results on SNS behavior dataset demonstrate its superior performance comparing to the state-of-the-art MTL methods. Interestingly, the results suggest that features involving recent behaviors such as recent blog publishing frequency and frequency of making comments are more likely to be related to personality."
Tractable Objectives for Robust Policy Optimization,"Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance.  One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations.   In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty.  Instead we focus on identifying optimization objectives for which solutions can be efficiently approximated.  We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efficiently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP.  "
No voodoo here! Learning discrete graphical models via inverse covariance estimation,"We investigate the relationship between the support of the inverses of generalized covariance matrices and the structure of a discrete graphical model. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph. Our work extends results which were previously established only for multivariate Gaussian distributions, and partially answers an open question about the meaning of the inverse covariance matrix of a non-Gaussian distribution. We propose graph selection methods for a general discrete graphical model with bounded degree based on possibly corrupted observations, and verify our theoretical results via simulations. Along the way, we also establish new results for support recovery in the setting of sparse high-dimensional linear regression based on corrupted and missing observations."
Feature Selection at the Discrete Limit,"In this paper, we propose to use L2,p for feature selection with emphasis on small p. As p ? 0, feature selection becomes discrete feature selection problem. We provide two algorithms, proximal gradient algorithm and rank-one-update algorithm which is more efficient at large regularization \lambda. Experiments on real life datasets show that features selected at small p consistently out-perform features selected at p = 1, the standard L2,1 approach."
Domain-Unifying Embedding,"We propose Domain Unifying Embedding, together with its kernelized version, as a consolidated framework for domain adaptation and cross-domain recognition. Our approach embeds samples from one or more source domains and a target domain into a single latent shared domain, with the embedding represented by a linear or kernel transformation. In addition to allowing an arbitrary number of source domains, the approach allows these sources to be heterogeneous in the sense of having different dimensions. It also allows simultaneously exploiting a variety of types of semi-supervision, including target samples with explicit class labels when available, and instances for which corresponding, but unlabeled, samples are available in two or more domains."
Modeling 4D Human-Object Interactions for Object Detection,"In recent years, human-object context has been utilized for improving both object detection and action recognition, but most work has been primarily focused on modelling human-object relations in 2D static images and such contextual cues are often compromised due to their sensitivity to viewpoint changes. In this paper, we propose to build human-object interaction models in 3D space plus the time axis, using videos captured by Kineck cameras which provide rather accurate 3D human poses in time and the point clouds for the contextual objects.  Our contextual model are learned form such data and includes three components: i) co-occurrence of human action labels and object labels; ii) geometric compatibility between human body parts and object bounding boxes in 3D space; and iii) the sequence of actions/poses in time are grouped into events to provide temporal context. Such model provides much more reliable and accurate mutual context information for  object detection and action recognition through joint inference which resolve the ambiguities through a top-down and bottom-up computing process. In experiment, we show that our method achieve satisfactory results on an indoor dataset that we collect and will be released to the public. "
Inferring Hidden Fluents Using Action and Causality,"In object and event detection, the identification of fluents (object statuses) that are not directly observable has been overlooked.  Inferring the values of such hidden fluents is important to fully understanding which actions are available to agents.  These values can be filled in by applying causal reasoning to video.  This paper presents a reasoning framework for the changes in, or maintenance of, the values of fluents over time by extending the Causal And-Or Graph, a stochastic grammar model for causality that integrates with current grammar models for event and object detection. The model incorporates causal reasoning with spatio-temporal detection to generate multiple interpretations of what is transpiring in a scene and to rank those interpretations according to probability.  We show that such interpretations can be used to correct (mis)detections of fluents and events in video; results are comparable to humans' performance in reasoning values of hidden fluents."
Bayesian Adaptive Mean Shift (BAMS),"The Adaptive Mean Shift (AMS) algorithm is a popular and simple non-parametric clustering approach based on Kernel Density Estimation. In this paper AMS is reformulated in a Bayesian framework, which permits a natural generalization in several directions that are shown to improve performance. The Bayesian framework considers the AMS to be a method to obtain a  posterior mode. This allows the algorithm to be generalized with three components which are not considered in the conventional approach: node weights, a prior for a particular location and a posterior distribution for the bandwidth. Practical methods to build the three different components are considered. The Bayesian Adaptive Mean Shift (BAMS) algorithm is evaluated with synthetic datasets and several real datasets."
Geodesic Distance Function Learning: Theory and an Algorithm,"Learning a distance function is of great importance in machine learning and pattern recognition. Geodesic distance, which has been widely used, is one of the most important intrinsic distances on the manifold. In this paper, we study the geodesic distance function $d(p, x)$ for a fixed point $p$. We provide two theorems to exactly characterize such a distance function. Our theoretical analysis shows if a function $r_p(x)$ is a Euclidean distance function in a neighborhood of $p$ in exponential coordinates and the gradient field of $r_p(x)$ has unit norm almost everywhere, then $r_p(x)$ must be the unique geodesic distance function $d(p,x)$. Based on our theoretical analysis, a novel approach from vector field perspective is proposed to learn the geodesic distance function. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm."
Measuring Reproducibility of High-throughput Deep-sequencing Experiments based on Self-adaptive Mixture Copula,"Measurement of the statistical reproducibility between biological experiment replicates is vital first step of the entire series of bioinformatics analyses for meaningful biology finding in mega-data. To distinguish the real biological relevant signals from artificial signals, irreproducible discovery rate (IDR) employing Copula, which can separate dependence structure and marginal distribution from data, has been put forth. However, the main disadvantage of IDR is that it assumes the data subject to normal distribution, which does not match the real data's feature. To address the issue, we propose a Self-adaptive Mixture Copula (SaMiC) to measure the reproducibility of experiment replicates from high-throughput deep-sequencing data. Simple and easy to implement, the proposed SaMiC method can self-adaptively tune its coefficients so that the measurement of reproducibility is more effective for general distributions.  Experiments in simulated and real data indicate that compared with IDR, the SaMiC method can better estimate  reproducibility between replicate samples."
A Marginalized Particle Gaussian Process Regression,"We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods."
Structured Label Propagation in an Ensemble of Directed Acyclic Graphs,"This paper proposes a new approach to semi-supervised structured learning. Our structured learning formulation is based on energy minimization using graphic models that consist of overlapping local cliques. Local label patterns of cliques are propagated from training data to testing data, using an ensemble of directed acyclic graphs (DAGs). The key innovation in our approach is that the label propagation does not make the label smoothness assumption that  if two feature vectors are similar, then so should be their corresponding output labels. We argue that this assumption introduces a bias toward labels with dominant population because local cliques in a graphical model may and often exhibit similar weak local features but take different labels. In contrast, our label propagation makes a weaker label repetitiveness assumption that if one feature is similar to a few other features that may have different labels, then the label of this feature is one of the other features'; which label to use is determined by inferring the energy model. We present algorithms for structured label estimation marginalized over an ensemble of sampled DAGs. Our method compares favorably with the conventional approach that assumes label smoothness.  "
Building an Attribute based Semantic Hierarchy,"We propose a new framework to build attribute based hierarchies from visual datasets. Our desiderata is to construct a tree structure in which attributes which are used more frequently are associated with nodes which are closer to the root, whereas attributes which are used less frequently are associated with lower levels in the tree. Most of the existing works that are concerned with learning visual and semantic taxonomies are based on hierarchical topic models which entail the bag of features representation. Such approaches are therefore not suitable for dealing with an attribute based representation. An attribute based representation can facilitate information transfer from previously observed instances into new images, and therefore an attribute based hierarchy can capture richer semantics while limiting the use of costly annotation data. We develop a new generative model for hierarchical clustering of binary vectors, which we refer to as the attribute tree process (ATP), and which is based on a tree-structured stick breaking process. The ATP allows us to estimate the entire structure of the hierarchy and the model parameters in an unsupervised fashion. We evaluate the proposed framework using several widely available datasets, and demonstrate that the ATP is capable of constructing semantically meaningful hierarchical representations of the data. "
Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space,"This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications."
Minimum Uncertainty Gap for Robust Visual Tracking,"We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at that state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the bounds, our method finds the confident state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood score, especially when there are severe illumination changes, occlusions, and pose variations.A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that minimizes the gap between the bounds. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods."
Soft Bounding Box Representation for Visual Tracking ,"A new tracking algorithm that tracks highly non-rigid targets robustly is proposed using a new bounding box representation called a soft bounding box (SBB).In the soft bounding box representation, the target is described as a range of the bounding box, which is bounded by an inner bounding box and an outer bounding box.In the paper, the inner and outer bounding boxes are theoretically constructed based on the theory of evidence.With these bounding boxes, the proposed method can solve the inherent ambiguity in a single bounding box representation for highly non-rigid targets. In addition, the method does not deal with the ambiguous region directly, which includes the foreground and the background at the same time. Hence, it robustly tracks highly non-rigid targets. In the soft bounding box representation, the best state of the target is efficiently found using a new Constrained Markov Chain Monte Carlo sampling method,which uses the constraint in which the outer bounding box must include the inner bounding box. Experimental results show that our method can track non-rigid targets accurately and robustly, and outperform even state-of-the-art methods."
Learning-based Stereo Method using MMSE Estimation,"We model the stereo problem using a product of Gaussian mixture models(PGMM). This enables efficient sampling and optimization, which makes general parameter learning possible. The learning procedure, along with the strong modeling power of PGMM,  helps us to find prior model for stereo problem using training data. Another important contribution of this work is that the proposed method computes its solution via minimum-mean-squared-error(MMSE) estimation instead of maximum-a-posteriori(MAP) estimation. The benefits of this approach is two-fold: it utilizes the learned characteristics of our model better than MAP and the result is more robust to subtle errors in stereo model itself. Experimental results show that the performance of our method based on MMSE estimation is far better than that of MAP estimation with the same model, while achieving competitive quantitative evaluation score in comparison with other learning-based stereo methods."
Predicting Functional Cortical ROIs via Joint Modeling of Anatomical and Connectional Profiles,"Localization of functional cortical ROIs (regions of interests) in structural data such as DTI and MRI images has significant importance in basic and clinical neuroscience. However, this problem is challenging due to the lack of quantitative mapping between brain structure and function, which relies on both the availability of benchmark training data such as task-based fMRI and effective machine learning algorithms. This paper presents a novel joint modeling approach that learns predictive models of functional cortical ROIs from multimodal task-based fMRI, DTI and MRI datasets. In particular, the effective generalized multi-kernel learning algorithm was tailored to infer the intrinsic relationships between anatomical/connectional MRI/DTI features and fMRI-derived functional localizations. Then, the predictive models of functional cortical ROIs were evaluated by cross-validation studies, independent datasets and reproducibility studies, and experimental results are promising. We envision that these predictive models can be widely applied in the future in scenarios that have only DTI and/or MRI data, but without task-based fMRI data. "
Maximum Weight Subgraphs with Mutex Constraints,"In this paper, we propose a novel algorithm for computing maximum weight subgraphs(MWSs) that satisfy mutex constraints on a weighted graph. As opposedto commonly used linear equality constraints, the mutex constraints expressedin a quadratic equality form allow for a greater modeling flexibility, which canbe beneficial in many applications. Although the proposed algorithm solves arelaxed formulation of MWS problem, it obtains a discrete solution in all ourexperiments on real data, which in turn guarantees that the solution satisfies themutex constraints. We evaluated our algorithm on two hard combinatorial problems:matching of salient points under perspective and nonrigid distortion andsolving image jigsaw puzzles. It significantly outperforms known state-of-the-artalgorithms, including loopy believe propagation and Integer Projected Fixed PointMethod (IPFP), even if it is restricted to using only constraints equivalent to linearequality constraints."
Exact and Efficient Parallel Inference for Nonparametric Mixture Models,"Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to sample from the true posterior in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods."
Multi-Relational Learning via Hierarchical Nonparametric Bayesian Collective Matrix Factorization,"Relational learning addresses problems where the data come from multiple sources and are linked together throughcomplex relational networks. Two important goals are pattern discovery (e.g. by (co)-clustering) and predicting unknown values of a relation, given a set of entities and observed relations among entities. In the presence of multiple relations, combining information from different but related relations can lead to better insights and improved prediction. For this purpose we propose a nonparametric hierarchical Bayesian model that improves on existing collaborative factorization models and frames a large number of relational learning problems. The proposed model naturally incorporates (co)-clustering and prediction analysis in a single unified framework, and allows for the estimation of entire missing row or column vectors. We develop an efficient Gibbs algorithm and a hybrid Gibbs using Newton?s method to enable fast computation in high dimensions. We demonstrate the value of our framework on simulated experiments as well as two real world problems: discovering kinship systems and predicting the authors of certain articles based on article-word co-occurrence features."
Efficient coding connects prior and likelihood function in perceptual Bayesian inference,"  A common challenge for Bayesian approaches in modeling perceptual behavior is the fact that the two fundamental  components of a Bayesian model, the prior distribution and the likelihood function, are formally unconstrained. Here  we argue that a neural system that emulates Bayesian inference naturally imposes constraints by way of how it  represents sensory information in populations of neurons. More specifically, we propose an efficient encoding  principle that constrains both the likelihood and the prior based on low-level environmental statistics. The resulting  Bayesian estimates can show biases away from the peaks of a prior distribution, a behavior seemingly at odds  with the traditional view of Bayesian estimates yet one that has indeed been reported in human perception of visual orientation. We demonstrate that our framework correctly predicts these biases, and show  that the efficient encoding characteristics of the model neural population matches the reported orientation tuning  characteristics of neurons in primary visual cortex. Our results suggest that efficient coding can be a promising  hypothesis in constraining neural implementations of Bayesian inference."
Gene Context Analysis on Large-scale Genomic Data,"In this paper, we investigate one of the largest microarray datasets aggregated from the internet. We aim at detecting association between the variables (genes or gene pathways) and certain keywords of interest (tissue types or diseases, for example). We address the challenges of utilizing the text information and variable structure information in high dimensional feature selection and classification. To utilize the text information, we build keyword network borrowing the power of natural language processing and apply Nearest Shrunken Centroids (NSC) to context analysis. This procedure can fully utilize the text information and has the potential to scale up to the dimension of 1012 in minutes. To utilize the structure information, we develop a new discriminant analysis method called the group Nearest Shrunken Centroids (gNSC). By exploiting the text and variable structure information, our result verifies several biological associations and further leads to some new discoveries."
Incremental Beam Search,"Beam search is a widely applied heuristic search method. Given a beam-width, it explores that many nodes at each level until a goal node is found. However, the quality of the solution produced by beam search does not always monotonically improve with the increase in beam-width, which makes it difficult to choose an appropriate beam-width for effective use. We address this issue by proposing a new beam search algorithm called Incremental Beam search (IncB) which guarantees monotonicity. IncB is also an anytime algorithm. Experimental results on the sliding-tile puzzle problem and the traveling salesman problem show that IncB significantly outperforms iterative beam search as well as some of the state-of-the-art anytime heuristic search algorithms."
Co-Regularized Hashing for Multimodal Data,"Hashing-based methods provide a very promising approach to large-scale similarity search.  To obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically.  In this paper, we study hash function learning in the context of multimodal data.  We propose a novel multimodal hash function learning method, called Co-Regularized Hashing (CRH), based on a boosted co-regularization framework.  The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized.  We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets."
Symbolic Dynamic Programming for Continuous State and Observation POMDPs,"Partially-observable Markov decision processes (POMDPs) provide a powerfulmodel for real-world sequential decision-making problems. In recent years, point-based value iteration methods have proven to be extremely effective techniquesfor ?nding (approximately) optimal dynamic programming solutions to POMDPswhen an initial set of belief states is known. However, no point-based work hasprovided exact point-based backups for both continuous state and observationspaces, which we tackle in this paper. Our key insight is that while there maybe an in?nite number of possible observations, there are only a ?nite number ofobservation partitionings that are relevant for optimal decision-making when a?nite, ?xed set of reachable belief states is known. To this end, we make twoimportant contributions: (1) we show how previous exact symbolic dynamic pro-gramming solutions for continuous state MDPs can be generalized to continu-ous state POMDPs with discrete observations, and (2) we show how this solutioncan be further extended via recently developed symbolic methods to continuousstate and observations to derive the minimal relevant observation partitioning forpotentially correlated, multivariate observation spaces. We demonstrate proof-of-concept results on uni- and multi-variate state and observation steam plant control."
Bayesian Probabilistic Co-Subspace Addition,"For modeling data matrices, this paper introduces Probabilistic Co-Subspace Addition (PCSA) model by simultaneously capturing the dependent structures among both rows and columns. Briefly, PCSA assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two features, which distribute in the row-wise and column-wise latent subspaces. Consequently, it captures the dependencies among entries intricately, and is able to model the non-Gaussian and heteroscedastic density. Variational inference is proposed on PCSA for  approximate Bayesian learning, where the updating for posteriors is formulated into the problem of solving Sylvester equations. Furthermore, PCSA is extended to tackling and filling missing values, to adapting its sparseness, and to modelling tensor data. In comparison with several state-of-art approaches, experiments demonstrate the effectiveness and efficiency of Bayesian (sparse) PCSA on modeling matrix (tensor) data and filling missing values."
Multi-Armed Bandit Problem with Budget Constraint and Variable Costs,"In this paper, we study the multi-armed bandit problem with budget constraint and variable costs (MAB-BV). In this setting, pulling each arm is associated with an unknown and variable cost, and the objective of a learning algorithm is to pull a sequence of arms in order to maximize the expected total reward with the number of pulled arms complying with the budget constraint. This new setting describes many Internet applications (e.g., sponsored search and cloud computing) in a more accurate manner than previous settings that either assume the pulling of arms is costless or with a fixed cost. To tackle this new kind of multi-armed bandit problem, we extend the UCB algorithms by selecting arms according to the reward-cost ratio, and propose a new algorithm called UCB-BV. Our empirical results verify the effectiveness of this algorithm. Although the extension in UCB-BV seems natural and simple, and it is practically effective, the theoretical analysis on its regret bound turns out to be very difficult. We develop a set of new proof techniques and obtain a regret bound of $O(\ln B)$. Furthermore, we show that when applying the proposed algorithm to the setting with fixed costs (which is our special case), one can improve the corresponding regret bound obtained so far."
Hierarchical Optimistic Region Selection driven by Curiosity,"This paper aims to take a step forwards making the term ``intrinsic motivation'' from reinforcement learning theoretically well founded,focusing on curiosity-driven learning. To that end, we consider the setting where, a fixed partition P of a continuous space X being given,and a process \nu defined on X being unknown,we are asked to sequentially decide which cell of the partition to select as well as where to sample \nu in that cell,in order to minimize a loss function that is inspired from previous work on curiosity-driven learning.The loss on each cell consists of one term measuring a simple worst case quadratic sampling error,and a penalty term proportional to the range of the variance in that cell.The corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region,and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization canbe used in order to solve this problem. The resulting procedure,called Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C) is provided together with a finite-time regret analysis."
Optimally Learning Hashing Functions Using Column Generation,"Fast nearest neighbor search is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learningdata-dependent hashing functions using machine learning techniques have been developed. In this work, we propose a column generation based method for learninghashing functions on the basis of proximity comparison information. Given a set of examples of proximity comparisons among triples of data points the methodlearns hashing functions which preserve the relative distances between them as well as possible within the large-margin learning framework. The learning procedureis implemented using column generation and hence is named CGHash. At each iteration of column generation procedure the best hashing function is selected. Unlike other recent hashing methods, our method generalizes to newpoints naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposedmethod learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on publicly available datasets."
A Novel Adaptive Geometric Mapping For Multi-Class Classification,"Solving multi-class problems is one of the challenging problems in machine learning field. Although several powerful binary classifiers have been developed, it is still an ongoing research issue to effectively extend these binary classifiers for multi-class cases.  In this paper we propose a new method for multi-class classification which is based on an adaptive mapping of data points into linearly separable classes. Our strategy, which is inspired from geometry laws, can handle multi-class cases directly. It has the capability of not increasing the dimension of data while separating them in a stable dimension. This algorithm generates the suitable mapping matrix using a vector of linear coefficients. We evaluate the performance of our classifier with different state-of-art support vector machine based methods and KNN on five data sets named as IRIS, WDBC, Glass, Wine, and Liver disorders. On most of these datasets, our implementation outperforms the best reported results and achieves comparable accuracy on the rest which demonstrates its effectiveness."
Learning Hierarchical Spatial Tiling Representation for Scene Tagging,"In order to name and localize semantic tags/attributes on natural scene images, in this paper, we first propose a structure learning method to learn a novel representation for scene modeling, namely Hierarchical Space Tiling (HST). It is able to account for the structure variations of scene configurations using different parts/words in the learned tilling dictionary. Then, the association relationship between a part and a semantic tag/attribute is discovered by exploring their mutual information on scene images. Finally, given a naked image, we first parse it into a tree structure with the learned HST model, then assign tags to the terminal nodes/parts on the hierarchy. We evaluate the advantages of the proposed method from three aspects. (i) The proposed HST is compact and less ambiguous in constructing the compositions of scenes. (ii) The semantic tags are named and localized accurately on scene images. (iii) It has scalability potential to real applications by showing that the parsing + tagging is extremely fast. "
Constructing ?2-graph for Clustering,"Constructing a sparse similarity graph is an important step in graph-oriented clustering algorithms. In a similarity graph, the vertex denotes a data point and the connection weight between two data points represents the similarity. Some recent works use ?1-minimization based sparse coefficients to construct the graph for various applications, and impressive results are achieved. This paper proposes a method to construct the similarity graph, called ?2-graph, by using ?2-minimization based representation coefficients. This graph can produce a block-sparse similarity graph via enforcing locality onto the non-sparse representation. The representation is derived via solving an optimization problem to obtain an interesting closed-form solution. Experimental results using several facial databases demonstrate that the proposed method outperforms two state-of-the-art ?1-minimization based clustering algorithms, i.e., Sparse Subspace Clustering [1, 2] and Low Rank Representation [3], in accuracy, robustness and time saving."
An Indexing Method for Efficient Model-Based Search,"Large databases of patterns, such as faces, body poses, fingerprints, or gestures, are becoming increasingly widespread, thanks to advances in computer technology. In this paper we focus on the problem of efficient search in such databases, when using model-based search. In model-based search, the user submits as a query a classifier, that has been trained to recognize the type of patterns that the user wants to retrieve. While model-based search can lead to good retrieval accuracy, the efficiency of model-based search can be inadequate if we need to apply the query classifier to every single database pattern. We propose a method for improving the efficiency of model-based search. The proposed method assumes that classifiers have been trained using JointBoost, and operates by defining an embedding, which maps both classifiers and database patterns into a common vector space. Using this embedding, the problem of finding the database patterns maximizing the response of the query classifier is reduced to a nearest neighbor search problem in a vector space. This reduction allows the use of standard vector indexing method to speed up the search. In our experiments, we show that the proposed embedding, together with a simple PCA-based indexing scheme, significantly improve the efficiency of model-based search, as measured on a database of face images constructed from the public FRGC-2 dataset."
Feature-aware Label Space Dimension Reduction for Multi-label Classification,"Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets."
Joint Modeling of a Matrix with Associated Text via Latent Binary Features,"A new methodology is developed for joint analysis of a matrix and accompanyingdocuments, with the documents associated with the matrix rows/columns. Thedocuments are modeled with a focused topic model, inferring latent binary features(topics) for each document. A new matrix decomposition is developed, withlatent binary features associated with the rows/columns, and with imposition of alow-rank constraint. The matrix decomposition and topic model are coupled bysharing the latent binary feature vectors associated with each. The model is appliedto roll-call data, with the associated documents defined by the legislation.State-of-the-art results are manifested for prediction of votes on a new piece oflegislation, based only on the observed text legislation. The coupling of the textand legislation is also demonstrated to yield insight into the properties of the matrixdecomposition for roll-call data."
Partial Gaussian Graphical Model Estimation,"This paper studies the partial estimation of Gaussian graphical models from high-dimensional empirical observations. We derive a convex formulation for this problem using l1-regularized maximum-likelihood estimation, which can be solved via a block coordinate descent algorithm. Statistical estimation performance can be established for our method. The proposed approach has competitive empirical performance compared to existing methods, as demonstrated by various experiments on synthetic and real datasets."
Reinforcement Learning Behavior in Sponsored Search,"This paper is concerned with the modeling of advertiser behavior in sponsored search. Modeling advertiser behavior can help search engines better serve advertisers, improve auction mechanism, and forecast future revenue. Most of previous work assume that advertisers have perfect access to necessary information for making their decisions. However, as we know, there are many factors in the system which are hardly known to advertisers and make the above assumptions unreasonable. To tackle the challenge, we propose viewing sponsored search as a reinforcement learning (RL) system for each advertiser, and employing a RL behavior model to describe how each advertiser responds to the system. In the proposed model, advertisers estimate the utility for winning each rank position based on the signals provided by the search engine, target the most preferred position based on the estimation, and then adjust their bids accordingly to achieve this target. The proposed model does not assume perfect information access, but only assumes the observation of the signals provided by the search engine. Furthermore, it does not specify how advertisers actually utilize the observed information to make decision. Instead, one single parameter, the learning rate, is used to describe advertisers' ability in collecting and analyzing information, and learning how to appropriately behave. Our experiments show that our model outperforms previous models in the task of bid prediction and rank position prediction, demonstrating its advantage and flexibility in fitting real data. In addition to the short-term prediction capability, we also study the long-term outcome of the sponsored search system, if all the advertisers behave according to the proposed RL behavior model. Our theoretical analysis shows that under certain conditions, the dynamic system of sponsored search will converge to a locally envy-free equilibrium, which verifies the soundness of our model from another angle."
Towards An Order Preserving Approximation of LASSO Programs,"Sparse representation (SR) has recently drawn extensive attention in the signal processing, machine learning and machine vision communities. General SR theory emphasizes the importance of the \emph{sparsity} of a solution, as a natural regularization term for certain optimization problems. Exploiting sparsity has been shown to dramatically improve performance in specific real-world problems, e.g. \emph{face recognition}. Despite their proven success in enhancing classification performance, SR-based techniques are not readily applicable to large-scale problems because of the high computational burden incurred by solving sparse optimization programs, namely LASSO programs. As a result, there has been a recent push towards efficient techniques that adequately approximate the LASSO solution. In this paper, we propose such an approximation that serves the purpose of ordering a set of LASSO programs according to their optimal solutions. The need to order LASSO programs (e.g. to select the one with smallest objective) arises in important retrieval applications, from which we focus on two: online face recognition and visual object tracking. For such tasks, we propose a novel sampling-based algorithm that efficiently estimates the optimal solution for each LASSO program, while sufficiently preserving the relative order of these programs with high probability. To demonstrate the effectiveness and efficiency of the proposed method, we apply it to face recognition and object tracking on benchmark datasets. Our experiments show that it not only achieves state-of-the-art performance but also allows for significant computational speedup over baseline methods. %These results suggest that the proposed method can be a key enabler for such time-critical applications. "
Optimized dictionary based sparse representation for robust speaker recognition,"The mismatch between the training and the testing environments greatly degrades the performance of speaker recognition. Although many robust techniques have been proposed, speaker recognition in mismatch condition is still a challenge. To solve this problem, we propose an optimized dictionary based sparse representation for robust speaker recognition. To this end, we first train a speech dictionary and a noise dictionary, and concatenate them for sparse representation; then design an optimization algorithm to reduce the mutual coherence between the two learned dictionaries; after that, utilize mixture k-means to model speaker corresponding to sparse feature; and finally, present a distance divergence to measure the similarity. Compared with the standard Universal Background Model and Gaussian Mixture Models based speaker recognition, our preliminary experiments show that the proposed recognition framework consistently improve the robustness in mismatched condition."
Nonparametric Reduced Rank Regression,"We propose an approach to multivariate nonparametric regression thatgeneralizes reduced rank regression for linear models.  An additivemodel is estimated for each dimension of a $q$-dimensional response,with a shared $p$-dimensional predictor variable.  To control thecomplexity of the model, we employ a functional form of the Ky-Fanor nuclear norm, resulting in a set of function estimates that havelow rank.  Backfitting algorithms are derived and justified using anonparametric form of the nuclear norm subdifferential.  Oracleinequalities on excess risk are derived that exhibit the scalingbehavior of the procedure in the high dimensional setting.  Themethods are illustrated on gene expression data."
The Gaussian Nonlinear Poisson Process,"Recently, intracellular recordings of neurons in vivo have become increasingly available. There is a pressing need to develop models which can be used to characterize the statistical properties of the temporal dynamics of single cells. We propose a doubly stochastic continuous-time model of a single neuron which is supposed can capture both the membrane potential dynamics and the process of spiking. The model consists of a gaussian process which is transformed through a nonlinearity, providing the firing rate for an inhomogeneous Poisson process. We highlight the use of the moment-generating functional in order to derive the previously known $n$-point function and devise a method for fitting this model to in vivo data."
Optimizing a Multiple Linear Regression-based Approach for a Priori Decision Threshold Estimation in Biometric Recognition,"Biometric recognition is a complex classification problemwhere the goal is to classify a pattern (biometric sample) as belonging or not to a certain class (user). As in other pattern recognition problems, a correct estimation of the decision threshold is essential for optimizing the biometric system's performance. A successful new approach for this estimation (prediction) based on Multiple Linear Regression has been proposed by us in a previous work. Here, we go into this proposal in greater depth, optimizing the independent variables selection by meansof the use of their matrix correlation, and only the uncorrelated ones are incorporated to the model. A study of the threshold estimation accuracy with regard to the data set size used to train the linear model is alsoperformed. Other related works have focused on a single biometric and classifier. However, our proposal is applied to different biometrics (signature and speech) and with different classifiers (Artificial Neural Network and Dynamic Time Warping), showing a good accuracy in all the tested scenarios."
"Learning to Grasp using Vision, Haptics and Proprioception","The ability to grasp and manipulate objects is an integral part of a robot's physical interaction with the environment. Humans alike, robots are expected to grasp and manipulate objects in a goal-oriented manner. In other words, objects should be grasped so to afford subsequent actions: if I am to hammer a nail, the hammer should be grasped so to afford hammering. Most of the work on grasping, commonly addresses only the problem of finding a stable grasp without considering the task/action a robot is supposed to fulfill with an object.In this paper, we present work on modeling of goal-directed robot grasping tasks based on integration of multisensory data using probabilistic generative models.  Our probabilistic framework facilitates assessment of grasp success in a goal-oriented way, taking into account both geometric constraints imposed by the task and fulfilling grasp stability requirements.  The conditional relations between tasks and the sensory data (vision, haptics and proprioception) are modeled using graphical models. We integrate high-level task information introduced by a teacher in a supervised setting with low-level stability requirements acquired through a robot's self-exploration.  The generative modeling approach enables inference of appropriate grasping configurations, as well as prediction of grasp success. The framework provides insights into dependencies between variables and features relevant for object grasping in general."
Clustering Probability Densities,"We describe a clustering algorithm where each example is represented as a probability density and distortion is measured by Kullback-Leibler divergence. This setup is relevant to many applications where an example is better characterized by a probability distribution, to capture the underlying uncertainty of interest, than a finite-dimensional feature vector in Euclidean space. We first derive a k-means variant for clustering Gaussian densities which has a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalizes a single Gaussian and is typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. We report empirical results on a successfully deployed application: query clustering based on bid landscape for sponsored search auction optimization."
Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method,"We develop new stochastic optimization methods that are applicable to a wide range of {\it structured regularizations}.Basically our methods are a combination of stochastic optimization techniques and Alternating Direction Multiplier Method (ADMM).ADMM is a general framework for optimizing a composite function,and has a wide range of applications.We propose two types of online variants of ADMM corresponding to onine proximal gradient descent and regularized dual averaging respectively.The proposed algorithms are computationally efficient and easy to implement.It is shown that our methods yield $O(1/\sqrt{T})$ convergence of the expected risk.Numerical experiments show effectiveness of our methods in learning tasks with structured sparsitysuch as overlapped group lasso."
Simple Models for Shunting Inhibition,"The integration of excitatory and inhibitory inputs at a neuron follows a nonlinear process, which is generally termed shunting inhibition. The experimental data has revealed that the effect of shunting inhibitionon the somatic potential can be largely expressed as a simple arithmetic rule, in which the contribution of shunting inhibition is proportional to the product between the contributions of excitatory and inhibitory inputs when they are applied individually. In this study, we develop simple neuron and network models for shunting inhibition. Our simple neuron models reproduce the experimental results qualitatively. We show that shunting inhibition can provide a mechanism to retain persistent activity in a network, andcan be well approximated as divisive normalization in describing the stationary states of continuous attractor neural networks."
Global Multi-view Subspace Learning,"Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results, particularly for high dimensional data."
A lattice filter model of the visual pathway,"Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (LGN) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons. We propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals. The stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> LGN) and its filter weights can be learned using Hebbian rules in a stage-wise sequential manner. Moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in LGN neurons and fruit fly second-order visual neurons. Therefore, the lattice filter model is a useful abstraction that may help unravel visual system function."
An Iterative Soft-Hard Thresholding Method for Low-Rank Matrix Recovery and Completion,"Low-rank matrix recovery and completion problems can be solved via their convex relaxations, which minimize the nuclear norm instead of the rank function, and have to be solved iteratively and involve singular value decomposition (SVD) at each iteration. Therefore, those algorithms suffer from high computational cost of multiple SVDs. In this paper we propose an efficient iterative soft-hard thresholding (ISHT) method to approximate the original nuclear norm minimization (NNM) problem and mitigate the computational cost of performing SVDs. The proposed ISHT method can be used to address a wide range of low-rank matrix recovery and completion problems such as low-rank representation (LRR), robust principal component analysis (RPCA) and low-rank matrix completion (MC). The ISHT method relies on first order optimization with orthogonal constraint. Furthermore, we also prove that the proposed algorithm converges to local minima. Experimental results validate the efficiency, robustness and effectiveness of our ISHT method comparing with the state-of-the-art NNM algorithm."
Sparsest Combination under Linear Transformation,"We consider the following signal recovery problem: given a measurement matrix $\Phi\in \mathbb{R}^{n\times p}$ and a noisy observation vector $c\in \mathbb{R}^{n}$ constructed from $c = \Phi \theta^* + \epsilon$ where $\epsilon\in \mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal $\theta^*$ if $D\theta^*$ is sparse where $D\in\mathbb{R}^{m\times p}$? One natural method using convex optimization is to solve the following problem: $$\min_{\theta}~{1\over 2}\|\Phi\theta - c\|^2 + \lambda\|D\theta\|_1.$$ This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix $\Phi$ is a Gaussian random matrix:1) in the noiseless case, if the condition number of $D$ is bounded and the measurement number $n\geq \Omega(s\log(p))$ where $s$ is the sparsity number then the true solution can be recovered with high probability; and2) in the noisy case if the condition number of $D$ is bounded and the measurement increases faster than $s\log (p)$ (that is, $s\log(p)=o(n)$), the estimate error converges to zero with probability 1 when $p$ and $s$ go to infinity. Our results are consistent with those for the special case $D=\bold{I}_{p\times p}$ (equivalently LASSO) and improve the existing analysis for the same formulation. The condition number of $D$ plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if $m/p$ is larger than a certain constant. Numerical simulations are consistent with our theoretical results."
Semi-Supervised Learning with Probabilistic Smoothness on Graphs,"We study graph-based semi-supervised learning by exploiting the smoothness constraint with respect to the intrinsic structure that exists among labeled and unlabeled data. Unlike previous works that define the smoothness constraint by different cost functions, we formulate it under a probabilistic framework instead. Interestingly, our probabilistic smoothness constraint can be tied to and hence justifies a new cost function. Based on this probabilistic smoothness constraint, we further derive an algorithm PSmooth for semi-supervised learning, which can also be interpreted as a random walk on graphs. Finally, our experiments show that PSmooth consistently outperforms existing state-of-the-art algorithms on various public benchmark datasets."
Why MCA? Nonlinear Spike-and-slab Sparse Coding for Neurally Plausible Image Encoding,"Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level components, e.g. edges. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA). The major challenge is parameter optimization because a model with either (1) or (2) results in a strongly multimodal posterior. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posterior. We design an exact piecewise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric, bimodal, and sparse activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, due to the nonlinearity, the model predicts a large number of globular receptive fields (RFs), another significant difference from standard SC. The inferred prior and the high proportion of predicted globular fields make the model more consistent with neural data than previous SC models, suggesting closer tuning of simple cells to visual stimuli than has been predicted until now. "
Extrinsic Support Vector Machines for Riemannian Manifolds,"In computer vision and pattern recognition applications, the features often lie on Riemannian manifolds. In such cases, one needs good classification techniques that make use of the underlying manifold structure. Due to its superior generalization properties, in this paper, we focus on developing support vector machine (SVM) classifier for such features. For Riemannian manifolds, the popular approach for classification is to project the data onto the tangent space and learn classifiers in this space. However, the tangent space need not be optimal for classification. Furthermore, it does not even preserve the global structure of the manifold. Hence, we learn a feature space suitable for support vector classification, by minimizing the structural risk of SVM, and using the manifold structure as a regularizer. We formulate this problem of learning the optimal space as a kernel learning problem, which results in an instance of semidefinite programming, that can be solved using standard semidefinite programming solvers. We also discuss a computationally more efficient solution using the multiple kernel learning framework. Experimental evaluation on synthetic and real data sets clearly demonstrate the effectiveness of the proposed approach."
Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand,"In many large economic markets, goods are sold through sequential auctions. Such domains include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions. In this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown."
Cellular Neural Networks: A Scalable Architecture for Learning MIMO Systems,"Cellular neural networks (CNNs) are a class of sparsely connected dynamic recurrent networks (DRNs). Neural networks for implementing large complex interconnected systems consist of multiple inputs and multiple outputs. Many outputs lead to more number of parameters to be adapted. Each additional variable increases the dimensionality of the problem and hence learning becomes a challenge. By proper selection of a set of input elements that affect a particular output variable in a given application, a DRN can be modified into a CNN which significantly reduces the complexity of the neural network and allows use of simple training methods such as backpropagation for independent learning in each cell thus making it scalable. The paper demonstrates this concept of developing CNN using dimensionality reduction in a DRN for scalability and better performance. The concept has been empirically verified through applications."
Decision Tree Algorithm for Data Streams based on Alternate Formulation of Mutual Information,"Learning problems with large and streaming data sets are becoming prevalent now a days. Such large data sets require specialized algorithms that are able to learn in one pass through the data set. Standard Decision Tree algorithms are not suitable for data streams. This paper describes and evaluates IQ Tree, a novel decision tree construction algorithm that can handle data streams. We derived an alternate formulation of Mutual Information that facilitates variable levels of approximation of Information Gain. Based on this formulation, our algorithm goes through the data only once and uses pre-calculation and sampling to achieve fast decision tree construction that closely approximates standard decision tree algorithm. We demonstrate detail error analysis that shows the superiority of our algorithm. Detailed empirical evaluation also shows that IQ Tree is better than state of the art decision tree algorithms that can handle data streams."
Robust Joint Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix,"We propose a robust framework to jointly perform two critical tasks of high dimensional modeling in synergy: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among the responses while adjusting for their predictors. This framework is relevant to many applications. In computational biology, for instance, it enables the integration of genomic and transcriptomic datasets. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecification. This issue is exacerbated when dealing with high dimensional noisy data. We therefore adopt an alternative approach to minimizing a regularized distance criterion, which is motivated by minimum distance estimators used in nonparametric methods. The proposed method yields an efficient algorithm that alternates between weighted versions of lasso and graphical lasso, where the sample weights intuitively explain the robustness of our method. We demonstrate the value of our framework through extensive simulation and real eQTL data analysis."
Rule Extraction from Neural Networks Using an Ensemble of Stacked Decision Trees,"Neural Network is a powerful pattern recognition algorithm capable of learning complex non linear patterns. However, Neural Networks have a well-known drawback of being a ?Black Box? learner that is not comprehensible or transferable thus making it unsuitable for many high risk tasks that require a rational justification for making a decision. Rule Extraction methods can resolve this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. In this paper, we present an algorithm called HERETIC that uses a symbolic learning algorithm (Decision Tree) on each unit of the Neural Network. This stacked ensemble of Decision Trees is shown to be a good approach of approximating a Neural Network. We also present HERETIC+, an extension of the basic algorithm that exploits Neural Network connection weights to attain better accuracy. Experiments and theoretical analysis show HERETIC and HERETIC+ generates highly accurate rules that closely approximates the Neural Network."
Explicit Embedding Learning with Kernel and Boosting Frameworks for Image Categorization,"In this article, we propose a method to learn a kernel function following a two-stage schema: one for kernel learning, and one for image categorization. We adopt a Boosting framework to design and combine weak kernel functions targeting an ideal kernel. The weak kernel selection criterion adapted to kernel combination and the weight of this combination are computed thanks to an analytic solution. We show that our method actually builds mapping functions which turn the initial input space to a new feature space where categories are better classified. We propose to learn a single kernel/mapping for all categories."
"Halo, Hyperbole, and the Pragmatic Interpretation of Numbers","Numbers are interpreted flexibly in everyday language: imprecision, exaggeration, and hyperbole are everywhere. We propose a computational model of the pragmatic interpretation of numbers, building upon recent models of pragmatics as rational inference. We assume that speaker and listener perform a social inference regarding the intended meaning, precision, and affective subtext of a numerical utterance. This model predicts two pragmatic effects, pragmatic halo and hyperbole, and their interaction. We demonstrate that the model accurately predicts the qualitative effects of human interpretation of number words in five real-world domains."
Non-parametric Bayesian Clustering with Noisy Side Information,"In clustering tasks, the incorporation of side information can usually offer substantial benefits. In many practical applications the side information is extracted following empirical rules and is thus likely contaminated by errors. In this paper we propose a non-parametric Bayesian framework Two-View Clustering (TVClust) to incorporate noisy side information into clustering. We model the data instances and constraints as two independent sets of outcomes, or two views, generated from the latent cluster structure, and try to seek a consensus between the observed data and the noisy side information. Specifically, the data instances are modeled using the Mixture of Dirichlet Process and the side information is modeled as a random graph. For the estimation of model parameters and related posterior inference, we present an efficient Gibbs sampler. Experiments on six real datasets and one social media dataset demonstrate that our method achieves significant improvement over the other methods we compared to."
Restricting exchangeable nonparametric distributions,"Distributions over exchangeable matrices with infinitely many columns, such as the Indian buffet process, are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution."
Graph Denoising,"The paper is motivated by real-world applications for denoising graph data. We show that this problem amounts to solving matrix recovery for an adjacency matrix which is both sparse and low-rank under a random perturbation matrix which is also sparse. We formulate the problem as the minimization of a regularized convex objective with an $\ell_1$ loss. We present two methods: an exact method based on Douglas-Rachford splitting, and an approximate method using matrix factorization and rank-one updates which offers better scalability. Numerical experiments confirm the relevance of the approach compared to state-of-the-art methods such as robust PCA."
Minor Surfaces for Clustering and Manifold Learning,"We show that mode-based cluster boundaries exhibit themselves as minor surfaces of the data distribution. Following this observation, we propose a connectivity metric based on the minor surface search between samples. This extends the mode-seeking procedure into a connectivity graph representation, which could be used in many machine learning applications. The use of the graph construction is particularly demonstrated in clustering and manifold learning problems. The experiments are carried out on synthetic and real datasets using Gaussian mixture models and kernel density estimates. Instead of climbing the mode for each sample, we perform connected component analysis in the proposed graph and achieve the same performace as mean-shift algorithm. The minor surface search discards the connections that does not pass through the data cloud, which makes it possible to use in manifold learning problems. We employ a distance matrix masked by our connectivity graph as an input to the Isomap algorithm, and show that the dependence of the performance on the $knn$ and $\epsilon$-ball generalizations decreases with our approach."
A Soft-Label Model with Impact for Active Graph Search,"We consider the problem of active search on a graph where we seek nodes belonging to a certain positive class by iteratively selecting nodes to query for their class label. The problem has similarities with active learning on a graph except that the performance is measured by number of positives identified rather than classification accuracy. Good solutions must tradeoff exploration to better fit a model against exploitation to collect likely positives and thus the problem has similarities with bandit problems as well. However, bandit algorithms are hard to adapt to the  problem since we will never choose the same node more than once.Previous work showed that the optimal active search algorithm requires a look ahead evaluation of expected utility that is exponential in the number of node selections to be made and considered heuristics that do a truncated look ahead [1]. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs.  We test the algorithm empirically on citation and wikipedia graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps."
Supervising Unsupervised Learning: Alleviating label noise in behavioural EEG-BCI experiments,"Behavioural experiments in the neurosciences require the assessment of a given stimulus by a subject. We will study the audio signal quality judgements of subjects and their respective neural correlates as measured by an EEG-BCI.  At decision threshold the subject often guesses, thus, the psychophysical assessment of the stimulus is greatly hampered. So the labels are only partly correct and very often random, which is a problematic scenario when applying supervised learning. We contribute by devising a novel supervised-unsupervised learning scheme, that aims to diferentiate true labels from random ones. This iterated combination of unsupervised one-class outlier detection and semi-supervised one-class learning yields neuroscientifically plausible correlates to behaviour that are more pronounced and meaningful than results found by the commonly used vanilla supervised learning approach that ignores the problematic label noise. While we discuss the experimental evidence for our audio signal quality application, it should be noted that this novelsupervised-unsupervised learning proceedure is applicable also beyond the neurosciences for general psychophysical experiments or generally for high label noise."
A Stochastic Gradient Method with an Exponential Convergence ?Rate  with Finite Training Sets,"We propose a new stochastic gradient method for optimizing the sum of? a finite set of smooth functions, where the sum is strongly convex.? While standard stochastic gradient methods? converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence ?rate.  In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard? algorithms, both in terms of optimizing the training error and reducing the test error quickly."
Learning Networks of Heterogeneous Influence,"Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities. "
Expectation maximization for average reward decentralized POMDPs,"Planning for multiple agents under uncertainty is an important task where proposed solutions are often based on decentralized partially observable Markov decision processes (DEC-POMDPs). In current DEC-POMDP approaches, long-term effects of actions need to be de-emphasized by a discount factor. However, in real-life problems such as wireless networking, the agents (wireless devices) will be evaluated by their average performance over time, hence both short-term and long-term effects of actions are important and solutions based on discounting can perform poorly. We introduce a new DEC-POMDP method that optimizes average reward, based on a modified expectation-maximization approach. The method yields improved performance in benchmark problems compared to a state of the art discounted-reward DEC-POMDP approach."
Max-Product Particle Belief Propagation,"Belief Propagation (BP) is a popular message passing algorithm for inference in factored probabilistic graphical models. Sum-Product BP computes marginal distributions for node variables, while Max-Product BP outputs a solution that maximizes the joint distribution of all variables, and is used for MAP (maximum a posteriori) inference.BP is commonly applied to problems that assume a discrete (or discretized) state space. When the state space of a variables cannot be enumerated in practice, and the messages cannot be computed in closed form, methods based on sampling are considered. Several nonparametric Sum-Product approximations exist, however many inference problems of scientific interest require MAP inference for high-dimensional, continuous and multimodal distributed random variables, calling for nonparametric algorithms for Max-Product BP. In this paper we formulate a Max-Product version of the PBP algorithm, and analyze its behavior in performing inference for a model with continuous variables that do not easily admit a discrete representation. "
Bayesian Pedigree Analysis using Measure Factorization,"Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease.  With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites.  Some pedigrees number in the thousands of individuals.  Meanwhile, analysis methods have remained limited to pedigrees of <100 individuals which limits analyses to many small independent pedigrees.Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited.  This is because linkage anlysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order.  LODs are difficult to interpret and nontrivial to extend to consider interactions among sites.  These developments and difficulties call for the creation of modern methods of pedigree analysis.Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models.   We show that these disease models can be turned into accurate and efficient estimators.  The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models.  This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction."
Spatial Coarse-to-Fine Processing in a Neural Model with Cortical Feedback,"Methods utilizing coarse-to-fine processing, in which the general features of a stimulus are processed before more detailed structure, have found success in several applications, such as natural language processing, image processing, and computer vision. Such dynamics have also been observed in several neurological pathways, including the visual system. In this paper, we consider mechanisms of the spatial coarse-to-fine process in the central visual pathway. We present a model of the lateral geniculate  nucleus (LGN) and visual cortex (V1) incorporating both feedforward and feedback connections. We show that cortical feedback has a substantial effect on spatial dynamics in the LGN. Our results suggest that the LGN may use these recurrent connections to ?learn? the coarse-to-fine dynamic during development. We provide an ideal framework within which to explore this process through more computationally intensive simulations."
Hypergraph Complexity from Directed Line Graphs,"In this paper, we aim to characterize hypergraphs in terms of structural complexities. Measuring the complexity of a hypergraph in straightforward way tends to be elusive since hypergraph may exhibit varying relational orders. We thus transform a hypergraph into a line graph which not only accurately reflects the multiple relationships exhibited by the hypergraph but is also easy to be manipulated for complexity analysis. To locate dominant substructure within a line graph, we identify a centroid vertex by computing the minimum variance of its shortest path lengths. A family of centroid expansion subgraphs of the line graph is derived from the centroid vertex in an attempt to capture dominant structural characteristics of a hypergraph. We then compute the complexity traces of a hypergraph by measuring entropies on the centroid expansion subgraphs. The Shannon or von Neumann entropy measured on the condensed subgraph family enables an efficient characterisation of the complexity trace. We perform hypergraph clustering in the principal components space of the complexity trace vectors.Experiments on (hyper)graph datasets abstracted from bioinformatic and image data demonstrate effectiveness and efficiency of the hypergraphs complexity traces."
Accelerated Training for Matrix-norm Regularization: A Boosting Approach,"Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees $\epsilon$ accuracy within $O(1/\epsilon)$ iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle."
Risk-sensitive Reinforcement Learning for Applications in Human Decision Making,"This paper proposes a general framework for measuring risk in the context of Markov decision processes by introducing valuation maps, which can model both economically rational and irrational behaviors. Our framework covers most of existing literature in various fields as special cases. The induced risk-preferences are controlled by manipulating the forms of maps. To solve the derived infinite-stage discounted risk-sensitive optimization problems, we provide a dynamic programming algorithm for all maps within our framework and a generalized Q-learning algorithm for a sufficiently rich subfamily, called utility-based shortfall, by which the results in prospect theory can be well replicated. To test its applicability in real data, we apply the algorithm to analyze human behaviors in a sequential investment game. Our method outperforms standard models and the individual risk-preferences revealed by the model are consistent with behavioral data."
On the Difficulty of Learning Power Law Graphical Models,"A power-law graph is any graph $G=(V,E)$, whose degree distribution follows a power law \emph{i.e.} the number of vertices in the graph with degree $i, \;y_i$, is proportional to $i^{-\beta}$ : $y_i\propto i^{-\beta}$. In this paper, we provide information-theoretic lower bounds on the sample complexity of learning such power-law graphical models \emph{i.e.} graphical models whose Markov graph obeys the power law. In addition, we briefly revisit some existing state of the art estimators, and explicitly derive their sample complexity for power-law graphs."
Convex Subspace Representation Learning  for Multi-view Clustering,"Learning from multi-view data is important in many applications. In this paper, we propose a novel convex subspace representation learning method for unsupervised multi-view clustering. We first formulate the subspace learning in multiple views as one joint optimization problem with a common subspace representation matrix and a group sparsity inducing norm regularizer. By exploiting a dual matrix norm, we then show a convex min-max dual formulation with a sparsity inducing trace norm regularizer can be obtained. We develop a proximal bundle optimization algorithm to solve the min-max optimization problem for a global solution. Our empirical study shows the proposed approach can outperform the state of the art alternative methods across different multi-view learning scenarios."
Near-Tight Bounds for Cross-Validation via Loss Stability,Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds.  In this work we introduce a new and weak measure of stability (\emph{loss stability}) and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal.  Our work thus quantitatively improves the currentbest bounds on cross-validation.
Projection Retrieval for Classification,"In many applications classification systems often require in the loop human intervention. In such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution. To tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users."
Coarse-to-fine video segmentation using supervoxel trees,"Image and video segmentation is a task of immense importance in the field of computer vision. Existing algorithms like graph cuts solve this problem by considering the possibility of every adjoining pixel (superpixel) or voxel (supervoxel) getting different labels. However, real images tend to have spatial continuity and videos have additional temporal continuity. In this paper, we consider a hierarchical tree of supervoxels.We propose a coarse-to-fine video segmentation scheme whereby larger supervoxels belonging to the same label, need not be refined into finer supervoxels. For videos with significant spatio-temporal continuity, such a scheme can lead to significant computational savings. By using admissible heuristic estimates of the unary and binary potentials, we can show that this scheme leads to the exact segmentation that would have been obtained by considering the finest layer of supervoxels."
Relating Structural MRI and Behavioral Measure to Functional MRI Measures,"Structural magnetic resonance imaging (MRI) and behavioral measures in the elderly have been shown to be associated in someway or another with functional MRI measures in past studies. However, the goal of this study is to analyze the ability of structural MRI and behavioral measures to predict functional measures in relationship to one another. This study uses both linear regression and artificial neural networks to achieve this goal. The results show that linear regression performs better and the features that best relate to functional measures include age, mini-mental state examination scores, total regional volume, number of tracks connecting regions, and regional white matter hyperintensities volume."
An Integrated Method for Causality Structure and Hidden Causes Discovery,"When we focus on causality research of real-life application, we may need to discover causal structure accurately and detecting hidden causes automatically without a priori restriction of hidden variable (non)existence in the underlying causal network. Unfortunately, most existing causal discovery algorithms may confuse direction between cause and effect and measure the hidden causes dif?cultly. Motivated by these facts, we present an integrated method for causal discovery with automatic detection of probable hidden causes, where the causal structure is induced by conditional independency testing and ?cliques? of initial variables with relationships between ?cliques?. Clues of hidden cause are obtained when we identify redundant edges.  In this paper, we distinguish causality of be-to-be and not be-to-not. As a sample application, we present our integrated method (ICIC)for LUCAS released in NIPS 2008 and show the results ?nally."
One Permutation Hashing,"While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.In this paper, we develop a simple \textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing $k$ permutations to just one  would be much more \textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \& logistic regression also confirm the theoretical results."
A template model for fine-grained object recognition,"Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms. "
A new edge selection method for preserving the topology of persistent brain network,"The functional brain connectivity at the macro-scale studies how the localized areas of brain, i.e., the regions of interest (ROIs), work together during the specific mental functions.Their inter-regional connections estimated by the dissimilarity measure between observations in ROIs are usually too dense to visualize and interpret.So, we need to select the important edges in the network.Thresholding the edge weight matrix is most popular way to select the edges because it is assumed that only the strongly connected edges are important.However, there is no widely accepted rule for determining the threshold as well as the weakly connected edges also have the information which discriminates networks.In this paper, we propose a new edge selection method which preserves the topological structures of brain network based on the persistent homology.The persistent homology scans the topological structures by increasing the threshold and transforms the topological invariants to the algebraic form, known as Betti numbers.We seek the edges which affect the changes of the zeroth and first Betti numbers, i.e., connected components and holes.We applied the proposed method to the functional brain network based on FDG-PET data consisting of 24 attention deficit hyperactivity disorder (ADHD), 26 autism spectrum disorder (ASD) children and 11 pediatric control (PedCon) subjects.We showed that our edge selection method finds the minimum number of edges preserving the origianl geodesic distance of network."
Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression,"We present a new variational inference algorithm for Gaussianprocesses with non-conjugate likelihood functions.This includes binary and multi-class classification, as well asordinal regression.Our method constructs a convex lower bound, which can be optimizedby using an efficient fixed point update method.We then show empirically that our new approach is much faster than existing methods without any degradation in performance."
An online learning rule for a recurrent neural network with hidden neurons,Recent experimental finding suggest that the brain can spontaneously generate spiking patterns with the same statistics as stimulus-evoked activity patterns. Here we propose an online learning rule for a network of stochastic visible and hidden neurons that is able to adapt its spontaneous firing statistics to the stimulus-evoked firing statistics. We show furthermore that learning synaptic weights towards hidden neurons enables the network to bridge a silency gap in the activity pattern of visible neurons.
Entropy Estimations Using Correlated Symmetric Stable Random Projections,"Methods for efficiently estimating  the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of {\em Compressed Counting (CC)}~\cite{Proc:Li_Zhang_COLT11}  based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two \textbf{correlated} frequency moments estimated from  correlated samples of \textbf{symmetric} stable random variables. Interestingly, the  estimator for the moment we recommend for entropy estimation  barely has bounded variance itself, whereas the  geometric mean estimator (which has bounded higher-order moments) is not sufficient for entropy estimation. Our experiments confirm that this method is able to  well approximate the Shannon entropy using small storage (e.g., $100\sim 1000$ samples). A prior study~\cite{Proc:Zhao_IMC07}  approximated the Shannon entropy using symmetric stable random projections with (e.g.,) $10^5\sim1.6\times10^6$ \textbf{independent} samples."
Probabilistic Multi-label Classification with Sparse Feature Learning,"In this paper we propose a probabilistic multi-label classification model based on novel sparse feature learning. By employing an individual sparsity inducing $\ell_1$-norm and a group sparsity inducing $\ell_{2,1}$-norm, the proposed model has the capacity of capturing both label interdependencies and common predictive model structures.We formulate this sparse norm regularized learning problem as a non-smooth convex optimization problem, and develop a fast proximal gradient algorithm to solve it for an optimal solution. Our empirical study demonstrates the efficacy of theproposed method on a set of multi-label tasks given a limited number of labeled training instances."
Visual Object Classification is Consistent with Bayesian Generative vs Discriminative Representations,"The ability to learn and distinguish categories is essential for human behavior, and the underlying neural computations are actively investigated. Taking a normative view, we can relate categorisation to the distinction between generative and discriminative classification in machine learning. Generative approaches solve the categorization problem by building a probabilistic model of how each cate- gory was formed and infer then category labels. In contrast, the discriminative approach learns a direct mapping between input and label. Recent work shows how human classification is consistent with discriminative and generative classifi- cation depending on conditions. We hypothesize that humans employ generative mechanisms for classification, when not encouraged otherwise. To test this we exploit a counterintuitive prediction for generative classification, namely how the discrimination boundary between two classes shifts if one category?s distribution is revealed to be broader during learning. We tested N=20 subjects to distinguish two classes, A and B in two tasks (two artificial-script, armadillo-horse stick- drawings). The classes in each task were parameterized by two scalars; objects for each class are drawn from Gaussian parameter distributions, with equal variance and different means (class ?prototypes?). Next, subjects classify unlabelled examples drawn between the classes, so we can infer their discrimination boundary. This process is then repeated but includes training data for class A, which lie far away from B. Counter-intuitively, generative classification predicts a shift of the discrimination boundary closer to B. Conversely, discriminative classifiers will show either no shift of the boundary or a shift of the boundary away from class B. Our results show that categorization in both tasks is consistent with generative and not discriminative classifiers, as classification boundaries shifted towards B for both tasks in all subjects."
Forest Graph Estimation with Constraints,"We consider the problem of learning high dimensional forest graphical models.Unlike previous methods, we impose different structural constraints on the obtainedforest graphs: including (i) bounded tree constraint, (ii) bounded star constraint,and (iii) bounded path constraint. These constraints are well motivatedby empirical applications and theoretical considerations. We systematically studythese different constraints and illustrate their relationships. We develop hardnessresults and propose novel approximation algorithms with provable guarantees."
Mapping Overlapping Functional Networks via Multiple Relational Embedding,"Studying the modular composition of the functional cerebral architecture is a challenging line of research. Of particular interest are network structures that are active during specific cognitive tasks. In this paper we address the question of identifying partially overlapping networks that are active across different fMRI experiment conditions. We propose to use Multiple Relational Embedding (MRE) on functional brain imaging data acquired during different cognitive tasks. Multiple functional relationships are embedded into a single joint latent embedding, that encodes both joint, and individual network structure. Experiments demonstrate that this approach can identify shared-, and individual functional connectivity structure, and that it recovers functional networks with higher stability compared to the embedding of individual fMRI sequences."
Adaptive Sparsity in Gaussian Graphical Models,"An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffrey's hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation."
Hierarchical Model-based Control of Non-Linear Dynamical Systems using Reinforcement Learning,"Non-adaptive methods are currently state of the art in approximating solutions to non-linear optimal control problems.These carry a large computational  cost associated with iterative calculations and have to be solved individually for different start and end points.In addition they may not scale well for real-world problems and require considerable tuning to converge.As an alternative, we present a novel hierarchical approach to non-Linear Control using Reinforcement Learning to choose between locally linear controllers. These are dynamically learnt and repositioned in state space using Linear Dynamic Systems. We illustrate our approach with a solution to a benchmark problem.We show that our approach, Reinforcement Learning Optimal Control (RLOC) competes in terms of solution quality with a state-of-the-art control algorithm iLQR, and offers a robust, flexible framework to address large scale non-linear control problems with unknown dynamics."
Model-based Decision Strategy Recognition,"We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of decision agents on the basis of observation of their actions in solving sequential decision problems.  We model the problem faced by the decision agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of degrees of rationality with respect to optimal forward planning for the MDP.  To recognize the agents, we first use IRL to learn reward functions consistent with observed actions and then use these reward functions as the basis for clustering or classification models.   Experimental studies with GridWorld, a navigation problem,  and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for classifying automated decision rules (e.g., cutoff rule, successive first candidates), even in the presence of action noise and variations in the parameters of the rule.  We propose a new Bayesian IRL approach in which the likelihood function can be interpreted in terms of rationality models.  Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for recognition problems. "
Message passing with relaxed moment matching,"Bayesian learning is often hampered by large computational expense. As a powerful generalization of popular belief propagation,  expectation propagation (EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP can be sensitive to outliers and suffer from divergence for difficult cases.  To address this issue, we propose a new approximate inference approach, relaxed expectation propagation (REP). It relaxes the moment matching requirement of expectation propagation by adding a relaxation factor into the KL minimization. We penalize this relaxation with a $l_1$ penalty. With this penalty, when two distributions in the relaxed KL divergence are similar, we obtain the exact moment matching; in the presence of outliers, the relaxation factor will used to relax the moment matching constraint. Based on this penalized KL minimization, REP is robust to outliers and can greatly improve the posterior approximation quality over EP. To examine the effectiveness of REP,  we apply it to Gaussian process classification, a task known to be suitable to EP. Our classification results on synthetic and UCI benchmark datasets demonstrate significant improvement of REP over EP and Power EP---in terms of algorithmic stability, estimation accuracy and predictive performance."
Hierarchical Classification with strutured SVMs,"Multi-label classification from hierarchical structure rises from  many real world applications, for instance documents can  be labeled with multiple categories or topics. It is  difficult to learn most probable label configuration efficiently, which involves searching over large combinatory label space, specially label set size is large, namely over 10000.Also  incorporating hierarchical structure has shown notable improvement on prediction accuracyand reduction of learning complexity. We present efficient multi-labellearning method utilizing hierarchical label structure using  linear structuralSVM. Learning is done over entire label space, which infers most probable labels from all possible label configurations. We show an efficient but simpleto implement learning  method using stochastic primal  optimization,PEGASOS, and dynamic programming. The number of variables is independentof the number of instances in the training dataset, which suits large size problem. Experiments show improved results benefiting fromhierarchy."
A Hierarchical Motion Model for Short- and Long Range Motion,"Recent work [28] presented a unified model of motion perception inspired by theability of the human visual system to simultaneously perceive both short-rangeand long-range motion in dynamic scenes. Their model differed from conven-tional optical flow techniques because it performed inference compositionally bycombining local hypotheses for optical flow to build non-local hypotheses whichexploit non-local context and hence resolve local ambiguities. The model wasable to account for a range of psychophysical phenomena using random dot kine-matograms as stimuli. In this paper, we extend the model so that it can be appliedto natural image sequences. We test its performance on images from the KITTIoptical flow database [10], which include large ranges of motion. Our performanceis comparable to the performance of more standard algorithms on this database.To make the task even more challenging we introduce additional stimuli, mov-ing balls, into the KITTI dataset to give a richer motion field. We show that thehierarchical model still gives accurate results for these stimuli while more con-ventional algorithms degrade. The algorithm is naturally parallelizable and ourimplementation using a Graphics Processing Card (GPU) has a runtime of a fewminutes."
Learning Multi-Label Scene Classification using Asymmetric SIMPLS Classifier,"In this paper, we propose asymmetric SIMPLS classifier for automatic learning multi-label scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). We show that asymmetric SIMPLS is a sub-optimal solution of a joint learning framework in which we perform dimensionality reduction and multi-label classification simultaneously. Asymmetric SIMPLS and other five state-of-the-artlearning algorithms are evaluated and compared on a public dataset, which consists of a set of 2000 images with 5 clusters of basic scenes. Experimental results validate the effectiveness of our asymmetric SIMPLS compared toother methods. Furthermore, our work appears to generalize to other classification problems of the same nature."
